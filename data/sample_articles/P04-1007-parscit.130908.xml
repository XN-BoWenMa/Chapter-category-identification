<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant confidence="0.000009" no="0">
<title confidence="0.998821">
Discriminative Language Modeling with
Conditional Random Fields and the Perceptron Algorithm
</title>
<author confidence="0.925536">
Brian Roark Murat Saraclar Michael Collins Mark Johnson
</author>
<affiliation confidence="0.775679">
AT&amp;T Labs - Research MIT CSAIL Brown University
</affiliation>
<email confidence="0.971027">
{roark,murat}@research.att.com mcollins@csail.mit.edu Mark Johnson@Brown.edu
</email>
<sectionHeader confidence="0.993338" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999973066666667">This paper describes discriminative language modeling for a large vocabulary speech recognition task. We contrast two parameter estimation methods: the perceptron algorithm, and a method based on conditional random fields (CRFs). The models are encoded as deterministic weighted finite state automata, and are applied by intersecting the automata with word-lattices that are the output from a baseline recognizer. The perceptron algorithm has the benefit of automatically selecting a relatively small feature set in just a couple of passes over the training data. However, using the feature set output from the perceptron algorithm (initialized with their weights), CRF training provides an additional 0.5% reduction in word error rate, for a total 1.8% absolute reduction from the baseline of 39.2%.</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999956046153847">A crucial component of any speech recognizer is the language model (LM), which assigns scores or probabilities to candidate output strings in a speech recognizer. The language model is used in combination with an acoustic model, to give an overall score to candidate word sequences that ranks them in order of probability or plausibility. A dominant approach in speech recognition has been to use a “source-channel”, or “noisy-channel” model. In this approach, language modeling is effectively framed as density estimation: the language model’s task is to define a distribution over the source – i.e., the possible strings in the language. Markov (n-gram) models are often used for this task, whose parameters are optimized to maximize the likelihood of a large amount of training text. Recognition performance is a direct measure of the effectiveness of a language model; an indirect measure which is frequently proposed within these approaches is the perplexity of the LM (i.e., the log probability it assigns to some held-out data set). This paper explores alternative methods for language modeling, which complement the source-channel approach through discriminatively trained models. The language models we describe do not attempt to estimate a generative model P(w) over strings. Instead, they are trained on acoustic sequences with their transcriptions, in an attempt to directly optimize error-rate. Our work builds on previous work on language modeling using the perceptron algorithm, described in Roark et al. (2004). In particular, we explore conditional random field methods, as an alternative training method to the perceptron. We describe how these models can be trained over lattices that are the output from a baseline recognizer. We also give a number of experiments comparing the two approaches. The perceptron method gave a 1.3% absolute improvement in recognition error on the Switchboard domain; the CRF methods we describe give a further gain, the final absolute improvement being 1.8%. A central issue we focus on concerns feature selection. The number of distinct n-grams in our training data is close to 45 million, and we show that CRF training converges very slowly even when trained with a subset (of size 12 million) of these features. Because of this, we explore methods for picking a small subset of the available features.' The perceptron algorithm can be used as one method for feature selection, selecting around 1.5 million features in total. The CRF trained with this feature set, and initialized with parameters from perceptron training, converges much more quickly than other approaches, and also gives the optimal performance on the held-out set. We explore other approaches to feature selection, but find that the perceptron-based approach gives the best results in our experiments. While we focus on n-gram models, we stress that our methods are applicable to more general language modeling features – for example, syntactic features, as explored in, e.g., Khudanpur and Wu (2000).<FR>We intend
to explore methods with new features in the future.</FR> Ex-
perimental results with n-gram models on 1000-best lists
show a very small drop in accuracy compared to the use
of lattices. This is encouraging, in that it suggests that
models with more flexible features than n-gram models,
which therefore cannot be efficiently used with lattices,
may not be unduly harmed by their restriction to n-best
lists.
</bodyText>
<subsectionHeader confidence="0.679309">
1.1 Related Work
</subsectionHeader>
<bodyText confidence="0.995129027777778">Large vocabulary ASR has benefitted from discriminative estimation of Hidden Markov Model (HMM) parameters in the form of Maximum Mutual Information Estimation (MMIE) or Conditional Maximum Likelihood Estimation (CMLE). Woodland and Povey (2000) have shown the effectiveness of lattice-based MMIE/CMLE in challenging large scale ASR tasks such as Switchboard. In fact, state-of-the-art acoustic modeling, as seen, for example, at annual Switchboard evaluations, invariably includes some kind of discriminative training. Discriminative estimation of language models has also been proposed in recent years. Jelinek (1995) suggested an acoustic sensitive language model whose parameters 'Note also that in addition to concerns about training time, a language model with fewer features is likely to be considerably more efficient when decoding new utterances. are estimated by minimizing H(W |A), the expected uncertainty of the spoken text W, given the acoustic sequence A. Stolcke and Weintraub (1998) experimented with various discriminative approaches including MMIE with mixed results. This work was followed up with some success by Stolcke et al. (2000) where an “antiLM”, estimated from weighted N-best hypotheses of a baseline ASR system, was used with a negative weight in combination with the baseline LM. Chen et al. (2000) presented a method based on changing the trigram counts discriminatively, together with changing the lexicon to add new words. Kuo et al. (2002) used the generalized probabilistic descent algorithm to train relatively small language models which attempt to minimize string error rate on the DARPA Communicator task. Banerjee et al. (2003) used a language model modification algorithm in the context of a reading tutor that listens. Their algorithm first uses a classifier to predict what effect each parameter has on the error rate, and then modifies the parameters to reduce the error rate based on this prediction.</bodyText>
<sectionHeader confidence="0.985726333333333" genericHeader="method">
2 Linear Models, the Perceptron
Algorithm, and Conditional Random
Fields
</sectionHeader>
<bodyText confidence="0.999952">This section describes a general framework, global linear models, and two parameter estimation methods within the framework, the perceptron algorithm and a method based on conditional random fields. The linear models we describe are general enough to be applicable to a diverse range of NLP and speech tasks – this section gives a general description of the approach. In the next section of the paper we describe how global linear models can be applied to speech recognition. In particular, we focus on how the decoding and parameter estimation problems can be implemented over lattices using finite-state techniques.</bodyText>
<subsectionHeader confidence="0.986138">
2.1 Global linear models
</subsectionHeader>
<bodyText confidence="0.7519929">We follow the framework outlined in Collins (2002; 2004). The task is to learn a mapping from inputs x ∈ X to outputs y ∈ Y. We assume the following components: (1) Training examples (xi, yi) for i = 1... N. (2) A function GEN which enumerates a set of candidates GEN(x) for an input x. (3) A representation Φ mapping each (x, y) ∈ X × Y to a feature vector Φ(x, y) ∈ Rd. (4) A parameter vector α¯ ∈ Rd. The components GEN, Φ and α¯ define a mapping from an input x to an output F(x) through where Φ(x, y) · α¯ is the inner product Es αsΦs(x, y).</bodyText>
<equation confidence="0.9974195">
F(x) = argmax Φ(x, y) · α¯ (1)
y∈GEN(x)
</equation>
<bodyText confidence="0.9944274">The learning task is to set the parameter values α¯ using the training examples as evidence. The decoding algorithm is a method for searching for the y that maximizes Eq.1.</bodyText>
<subsectionHeader confidence="0.997748">
2.2 The Perceptron algorithm
</subsectionHeader>
<bodyText confidence="0.9999105">We now turn to methods for training the parameters α¯ of the model, given a set of training examples</bodyText>
<figure confidence="0.723728714285714">
Inputs: Training examples (xi, yi)
Initialization: Set α¯ = 0
Algorithm:
Fort = 1 ... T, i = 1 ... N
Calculate zi = argmaxz∈GEN(xi) Φ(xi, z) · α¯
If(zi =� yi) then α¯ = α¯ + Φ(xi, yi) − Φ(xi, zi)
Output: Parameters α¯
</figure>
<figureCaption confidence="0.996944">
Figure 1: A variant of the perceptron algorithm.
</figureCaption>
<bodyText confidence="0.997445038461539">(x1, y1) ... (xN, yN). This section describes the perceptron algorithm, which was previously applied to language modeling in Roark et al. (2004). The next section describes an alternative method, based on conditional random fields. The perceptron algorithm is shown in figure 1. At each training example (xi, yi), the current best-scoring hypothesis zi is found, and if it differs from the reference yi , then the cost of each feature2 is increased by the count of that feature in zi and decreased by the count of that feature in yi. The features in the model are updated, and the algorithm moves to the next utterance. After each pass over the training data, performance on a held-out data set is evaluated, and the parameterization with the best performance on the held out set is what is ultimately produced by the algorithm. Following Collins (2002), we used the averaged parameters from the training algorithm in decoding heldout and test examples in our experiments. Say ¯αti is the parameter vector after the i’th example is processed on the t’th pass through the data in the algorithm in figure 1. Then the averaged parameters ¯αAV G are defined as ¯αAV G = Ei,t ¯αti/NT. Freund and Schapire (1999) originally proposed the averaged parameter method; it was shown to give substantial improvements in accuracy for tagging tasks in Collins (2002).</bodyText>
<subsectionHeader confidence="0.998905">
2.3 Conditional Random Fields
</subsectionHeader>
<bodyText confidence="0.997162428571429">Conditional Random Fields have been applied to NLP tasks such as parsing (Ratnaparkhi et al., 1994; Johnson et al., 1999), and tagging or segmentation tasks (Lafferty et al., 2001; Sha and Pereira, 2003; McCallum and Li, 2003; Pinto et al., 2003). CRFs use the parameters α¯ to define a conditional distribution over the members of GEN(x) for a given input x:</bodyText>
<equation confidence="0.997305">
p¯α(y|x) = Z(x, ¯α) exp (Φ(x, y) · ¯α)
1
</equation>
<bodyText confidence="0.97751975">where Z(x, ¯α) = y∈GEN(x) exp (Φ(x, y) · ¯α) is a normalization constant that depends on x and ¯α. Given these definitions, the log-likelihood of the training data under parameters α¯ is</bodyText>
<equation confidence="0.9619645">
log p¯α(yi|xi)
[Φ(xi, yi) · α¯ − log Z(xi, ¯α)] (2)
</equation>
<footnote confidence="0.948331">
2Note that here lattice weights are interpreted as costs, which
changes the sign in the algorithm presented in figure 1.
</footnote>
<equation confidence="0.9762015">
N
LL(¯α) =
i=1
N
=
i=1
</equation>
<bodyText confidence="0.937169333333333">Following Johnson et al. (1999) and Lafferty et al. (2001), we use a zero-mean Gaussian prior on the parameters resulting in the regularized objective function:</bodyText>
<equation confidence="0.999064">
[Φ(xi, yi) ·α¯ − log Z(xi, ¯α)] − ||¯α||2
2σ2 (3)
</equation>
<bodyText confidence="0.996700466666667">The value u dictates the relative influence of the loglikelihood term vs. the prior, and is typically estimated using held-out data. The optimal parameters under this criterion are ¯α* = argmax¯α LLR(¯α). We use a limited memory variable metric method (Benson and Mor´e, 2002) to optimize LLR. There is a general implementation of this method in the Tao/PETSc software libraries (Balay et al., 2002; Benson et al., 2002). This technique has been shown to be very effective in a variety of NLP tasks (Malouf, 2002; Wallach, 2002). The main interface between the optimizer and the training data is a procedure which takes a parameter vector α¯ as input, and in turn returns LLR(¯α) as well as the gradient of LLR at ¯α. The derivative of the objective function with respect to a parameter αs at parameter</bodyText>
<equation confidence="0.996404">
values α¯ is
⎡ ⎤
αs
Φs(Xi, Yi) − X P¯α(Y|Xi)Φs(Xi, Y)v − 2 (4)
y∈GEN(xi)
</equation>
<bodyText confidence="0.999949357142857">Note that LLR(¯α) is a convex function, so that there is a globally optimal solution and the optimization method will find it. The use of the Gaussian prior term ||¯α||2/2u2 in the objective function has been found to be useful in several NLP settings. It effectively ensures that there is a large penalty for parameter values in the model becoming too large – as such, it tends to control over-training. The choice of LLR as an objective function can be justified as maximum a-posteriori (MAP) training within a Bayesian approach. An alternative justification comes through a connection to support vector machines and other large margin approaches. SVM-based approaches use an optimization criterion that is closely related to LLR – see Collins (2004) for more discussion.</bodyText>
<sectionHeader confidence="0.944763" genericHeader="method">
3 Linear models for speech recognition
</sectionHeader>
<bodyText confidence="0.999884666666667">We now describe how the formalism and algorithms in section 2 can be applied to language modeling for speech recognition.</bodyText>
<subsectionHeader confidence="0.99938">
3.1 The basic approach
</subsectionHeader>
<bodyText confidence="0.998587375">As described in the previous section, linear models require definitions of X, Y, xi, yi, GEN, 4) and a parameter estimation method. In the language modeling setting we take X to be the set of all possible acoustic inputs; Y is the set of all possible strings, E*, for some vocabulary E. Each xi is an utterance (a sequence of acoustic feature-vectors), and GEN(xi) is the set of possible transcriptions under a first pass recognizer. (GEN(xi) is a huge set, but will be represented compactly using a lattice – we will discuss this in detail shortly). We take yi to be the member of GEN(xi) with lowest error rate with respect to the reference transcription of xi. All that remains is to define the feature-vector representation, 4)(x, y). In the general case, each component 4)i(x, y) could be essentially any function of the acoustic input x and the candidate transcription y. The first feature we define is 4)0(x, y) as the log-probability of y given x under the lattice produced by the baseline recognizer. Thus this feature will include contributions from the acoustic model and the original language model. The remaining features are restricted to be functions over the transcription y alone and they track all n-grams up to some length (say n = 3), for example: 4)1(x, y) = Number of times “the the of” is seen in y At an abstract level, features of this form are introduced for all n-grams up to length 3 seen in some training data lattice, i.e., n-grams seen in any word sequence within the lattices. In practice, we consider methods that search for sparse parameter vectors ¯α, thus assigning many ngrams 0 weight. This will lead to more efficient algorithms that avoid dealing explicitly with the entire set of n-grams seen in training data.</bodyText>
<subsectionHeader confidence="0.999608">
3.2 Implementation using MFA
</subsectionHeader>
<bodyText confidence="0.999155478260869">We now give a brief sketch of how weighted finite-state automata (WFA) can be used to implement linear models for speech recognition. There are several papers describing the use of weighted automata and transducers for speech in detail, e.g., Mohri et al. (2002), but for clarity and completeness this section gives a brief description of the operations which we use. For our purpose, a WFA A = (E, Q, qs, F, E, ρ), where E is the vocabulary, Q is a (finite) set of states, qs E Q is a unique start state, F C Q is a set of final states, E is a (finite) set of transitions, and ρ : F → R is a function from final states to final weights. Each transition e E E is a tuple e = (l[e], p[e], n[e], w[e]), where l[e] E E is a label (in our case, words), p[e] E Q is the origin state of e, n[e] E Q is the destination state of e, and w[e] E R is the weight of the transition. A successful path 7r = e1 ... ej is a sequence of transitions, such that p[e1] = qs, n[ej] E F, and for 1 &lt; k &lt; j, n[ek_1] = p[ek]. Let IIA be the set of successful paths 7r in a WFA A. For any 7r = e1 ... ej, l[7r] = l[e1] ... l[ej]. The weights of the WFA in our case are always in the log semiring, which means that the weight of a path 7r = e1 ... ej E IIA is defined as:</bodyText>
<equation confidence="0.997785">
wA[7r] = � j w[ek] I + ρ(ej) (5)
k=1
</equation>
<bodyText confidence="0.999652583333333">By convention, we use negative log probabilities as weights, so lower weights are better. All WFA that we will discuss in this paper are deterministic, i.e. there are no a transitions, and for any two transitions e, e' E E, if p[e] = p[e'], then l[e] =� l[e']. Thus, for any string w = w1 ... wj, there is at most one successful path 7r E IIA, such that 7r = e1 ... ej and for 1 &lt; k &lt; j, l[ek] = wk, i.e. l[7r] = w. The set of strings w such that there exists a 7r E IIA with l[7r] = w define a regular language LA C E. We can now define some operations that will be used in this paper.</bodyText>
<equation confidence="0.97817775">
N
LLR(¯α) =
i=1
8LLR
=
XN
i=1
8αs
</equation>
<listItem confidence="0.99570147826087">• λA. For a set of transitions E and λ E ]IR, define λE = {(l[e], p[e], n[e], λw[e]) : e E E}. Then, for any WFA A = (E, Q, qs, F, E, ρ), define λA for λ E ]IR as follows: λA = (E, Q, qs, F, λE, λρ). • A o A'. The intersection of two deterministic WFAs A o A' in the log semiring is a deterministic WFA such that LAoA0 = LA n LA0. For any 7r E IIAoA0, wAoA0[7r] = wA[7r1] + wA0[7r2], where l[7r] = l[7r1] = l[7r2]. • BestPath(A). This operation takes a WFA A, and returns the best scoring path 7rˆ = argminπEnA wA[7r]. • MinErr(A, y). Given a WFA A, a string y, and an error-function E(y, w), this operation returns 7rˆ = argminπEnA E(y, l[7r]). This operation will generally be used with y as the reference transcription for a particular training example, and E(y, w) as some measure of the number of errors in w when compared to y. In this case, the MinErr operation returns the path 7r E IIA such l[7r] has the smallest number of errors when compared to y. • Norm(A). Given a WFA A, this operation yields a WFA A' such that LA = LA0 and for every 7r E IIA there is a 7r' E IIA0 such that l[7r] = l[7r'] and</listItem>
<equation confidence="0.838958">
Note that
E exp(−wNorm(A)[7r]) = 1 (7)
πENorm(A)
</equation>
<bodyText confidence="0.826933">In other words the weights define a probability distribution over the paths. • ExpCount(A, w). Given a WFA A and an n-gram w, we define the expected count of w in A as where C(w, l[7r]) is defined to be the number of times the n-gram w appears in a string l[7r].</bodyText>
<equation confidence="0.829669">
ExpCount(A, w) = E wNorm(A)[7r]C(w, l[7r])
πEnA
</equation>
<bodyText confidence="0.963521266666667">Given an acoustic input x, let Lx be a deterministic word-lattice produced by the baseline recognizer. The lattice Lx is an acyclic WFA, representing a weighted set of possible transcriptions of x under the baseline recognizer. The weights represent the combination of acoustic and language model scores in the original recognizer. The new, discriminative language model constructed during training consists of a deterministic WFA which we will denote D, together with a single parameter α0. The parameter α0 is the weight for the log probability feature Φ0 given by the baseline recognizer. The WFA D is constructed so that LD = E∗ and for all 7r E IID d</bodyText>
<equation confidence="0.918699">
wD[7r] = E Φj(x,l[7r])αj
j=1
</equation>
<bodyText confidence="0.714198">Recall that Φj(x, w) for j &gt; 0 is the count of the j’th ngram in w, and αj is the parameter associated with that n-gram.</bodyText>
<figureCaption confidence="0.999457">
Figure 2: Representation of a trigram model with failure transitions.
</figureCaption>
<bodyText confidence="0.992413">Then, by definition, α0L o D accepts the same set of strings as L, but and argmin Φ(x, l[7r]) · α¯ = BestPath(α0L o D).πEL Thus decoding under our new model involves first producing a lattice L from the baseline recognizer; second, scaling L with α0 and intersecting it with the discriminative language model D; third, finding the best scoring path in the new WFA.</bodyText>
<equation confidence="0.873833666666667">
d
wα0LoD[7r] = E Φj(x, l[7r])αj
j=0
</equation>
<bodyText confidence="0.970292407407407">We now turn to training a model, or more explicitly, deriving a discriminative language model (D, α0) from a set of training examples. Given a training set (xi, ri) for i = 1... N, where xi is an acoustic sequence, and ri is a reference transcription, we can construct lattices Li for i = 1... N using the baseline recognizer. We can also derive target transcriptions yi = MinErr(Li, ri). The training algorithm is then a mapping from (Li, yi) for i = 1... N to a pair (D, α0). Note that the construction of the language model requires two choices. The first concerns the choice of the set of n-gram features Φi for i = 1... d implemented by D. The second concerns the choice ofparameters αi for i = 0 ... d which assign weights to the n-gram features as well as the baseline feature Φ0. Before describing methods for training a discriminative language model using perceptron and CRF algorithms, we give a little more detail about the structure of D, focusing on how n-gram language models can be implemented with finite-state techniques.</bodyText>
<subsectionHeader confidence="0.999831">
3.3 Representation of n-gram language models
</subsectionHeader>
<bodyText confidence="0.999580470588235">An n-gram model can be efficiently represented in a deterministic WFA, through the use of failure transitions (Allauzen et al., 2003). Every string accepted by such an automaton has a single path through the automaton, and the weight of the string is the sum of the weights of the transitions in that path. In such a representation, every state in the automaton represents an n-gram history h, e.g. wi_2wi_1, and there are transitions leaving the state for every word wi such that the feature hwi has a weight. There is also a failure transition leaving the state, labeled with some reserved symbol φ, which can only be traversed if the next symbol in the input does not match any transition leaving the state. This failure transition points to the backoff state h', i.e. the n-gram history h minus its initial word. Figure 2 shows how a trigram model can be represented in such an automaton. See Allauzen et al. (2003) for more details.</bodyText>
<equation confidence="0.810184">
wA0[7r'] = wA [7r] + log( E
)exp(−wA[¯7r]) (6)
n
</equation>
<figure confidence="0.867375166666667">
�rE
A
wi w i-1 wi
w i-2w i-1
0
wi-1
wi
wi
0
E
0
wi
</figure>
<bodyText confidence="0.999739947368421">Note that in such a deterministic representation, the entire weight of all features associated with the word wi following history h must be assigned to the transition labeled with wi leaving the state h in the automaton. For example, if h = wi_2wi_1, then the trigram wi_2wi_1wi is a feature, as is the bigram wi_1wi and the unigram wi. In this case, the weight on the transition wi leaving state h must be the sum of the trigram, bigram and unigram feature weights. If only the trigram feature weight were assigned to the transition, neither the unigram nor the bigram feature contribution would be included in the path weight. In order to ensure that the correct weights are assigned to each string, every transition encoding an order k n-gram must carry the sum of the weights for all n-gram features of orders ≤ k. To ensure that every string in E* receives the correct weight, for any n-gram hw represented explicitly in the automaton, h'w must also be represented explicitly in the automaton, even if its weight is 0.</bodyText>
<subsectionHeader confidence="0.975706">
3.4 The perceptron algorithm
</subsectionHeader>
<bodyText confidence="0.9997235">The perceptron algorithm is incremental, meaning that the language model D is built one training example at a time, during several passes over the training set. Initially, we build D to accept all strings in E* with weight 0. For the perceptron experiments, we chose the parameter α0 to be a fixed constant, chosen by optimization on the held-out set. The loop in the algorithm in figure 1 is implemented as:</bodyText>
<listItem confidence="0.916043428571429">Fort= 1 ... T,i = 1 ... N: • Calculate zi = argmaxyEGEN(x) 4'(x, y) · α¯ = BestPath(α0Li o D) • If zi =� MinErr(Li, ri), then update the feature weights as in figure 1 (modulo the sign, because of the use of costs), and modify D so as to assign the correct weight to all strings.</listItem>
<bodyText confidence="0.999903375">In addition, averaged parameters need to be stored (see section 2.2). These parameters will replace the unaveraged parameters in D once training is completed. Note that the only n-gram features to be included in D at the end of the training process are those that occur in either a best scoring path zi or a minimum error path yi at some point during training. Thus the perceptron algorithm is in effect doing feature selection as a by-product of training. Given N training examples, and T passes over the training set, O(NT) n-grams will have non-zero weight after training. Experiments in Roark et al. (2004) suggest that the perceptron reaches optimal performance after a small number of training iterations, for example T = 1 or T = 2. Thus O(NT) can be very small compared to the full number of n-grams seen in all training lattices. In our experiments, the perceptron method chose around 1.4 million n-grams with non-zero weight. This compares to 43.65 million possible n-grams seen in the training data. This is a key contrast with conditional random fields, which optimize the parameters of a fixed feature set. Feature selection can be critical in our domain, as training and applying a discriminative language model over all n-grams seen in the training data (in either correct or incorrect transcriptions) may be computationally very demanding. One training scenario that we will consider will be using the output of the perceptron algorithm (the averaged parameters) to provide the feature set and the initial feature weights for use in the CRF algorithm. This leads to a model which is reasonably sparse, but has the benefit of CRF training, which as we will see gives gains in performance.</bodyText>
<subsectionHeader confidence="0.962443">
3.5 Conditional Random Fields
</subsectionHeader>
<bodyText confidence="0.99993125">The CRF methods that we use assume a fixed definition of the n-gram features 4'i for i = 1... d in the model. In the experimental section we will describe a number of ways of defining the feature set. The optimization methods we use begin at some initial setting for ¯α, and then search for the parameters ¯α* which maximize LLR(¯α) as defined in Eq. 3. The optimization method requires calculation of LLR(¯α) and the gradient of LLR(¯α) for a series of values for ¯α. The first step in calculating these quantities is to take the parameter values ¯α, and to construct an acceptor D which accepts all strings in E*, such that</bodyText>
<equation confidence="0.901161">
d
wD[π] = E 4'j(x, l[π])αj
j=1
</equation>
<bodyText confidence="0.9740713">For each training lattice Li, we then construct a new lattice L'i = Norm(α0Li o D). The lattice L'i represents (in the log domain) the distribution p¯α(y|xi) over strings y E GEN(xi). The value of log p¯α(yi|xi) for any i can be computed by simply taking the path weight of π such that l[π] = yi in the new lattice L'i. Hence computation of LLR(¯α) in Eq. 3 is straightforward. Calculating the n-gram feature gradients for the CRF optimization is also relatively simple, once L'i has been constructed. From the derivative in Eq. 4, for each i = must be computed.</bodyText>
<equation confidence="0.994748333333333">
1... N, j = 1... d the quantity
4'j(xi, yi) − � p¯α(y|xi)4'j(xi,y) (8)
yEGEN(x;)
</equation>
<bodyText confidence="0.997439171428571">The first term is simply the number of times the j’th n-gram feature is seen in yi. The second term is the expected number of times that the j’th n-gram is seen in the acceptor L'i. If the j’th n-gram is w1 ... w,,,, then this can be computed as ExpCount(L'i, w1 ... w,,,). The GRM library, which was presented in Allauzen et al.(2003), has a direct implementation of the function ExpCount, which simultaneously calculates the expected value of all n-grams of order less than or equal to a given n in a lattice L. The one non-ngram feature weight that is being estimated is the weight α0 given to the baseline ASR negative log probability. Calculation of the gradient of LLR with respect to this parameter again requires calculation of the term in Eq.8 for j = 0 and i = 1... N. Computation of EyEGEN(x;) p¯α(y|xi)4'0(xi, y) turns out to be not as straightforward as calculating n-gram expectations. To do so, we rely upon the fact that 4'0(xi, y), the negative log probability of the path, decomposes to the sum of negative log probabilities of each transition in the path. We index each transition in the lattice Li, and store its negative log probability under the baseline model. We can then calculate the required gradient from Li, by calculating the expected value in Li of each indexed transition in Li. We found that an approximation to the gradient of α0, however, performed nearly identically to this exact gradient, while requiring substantially less computation. Let wn1 be a string of n words, labeling a path in wordlattice Li. For brevity, let Pi(wn1 ) = p¯α(wn1 |xi) be the conditional probability under the current model, and let Qi(wn1 ) be the probability of wn1 in the normalized baseline ASR lattice Norm(Li). Let Li be the set of strings in the language defined by Li. Then we wish to compute Ei for i = 1... N, where where Si is the set of all trigrams seen in Li.</bodyText>
<equation confidence="0.9685633">
XEi = Pi(wn1 )log Qi(wn1 )
wn1 ELi
Pi(wn1 ) log Qi(wk|wk−1
1 ) (9)
The approximation is to make the following Markov
assumption:
Pi(wn1 ) log Qi(wk|wk−1
k−2)
X= ExpCount(Gz, xyz) log Qi(z|xy)(10)
xyzESi
</equation>
<bodyText confidence="0.999964125">The term log Qi(z|xy) can be calculated once before training for every lattice in the training set; the ExpCount term is calculated as before using the GRM library. We have found this approximation to be effective in practice, and it was used for the trials reported below. When the gradients and conditional likelihoods are collected from all of the utterances in the training set, the contributions from the regularizer are combined to give an overall gradient and objective function value. These values are provided to the parameter estimation routine, which then returns the parameters for use in the next iteration. The accumulation of gradients for the feature set is the most time consuming part of the approach, but this is parallelizable, so that the computation can be divided among many processors.</bodyText>
<sectionHeader confidence="0.988256" genericHeader="result">
4 Empirical Results
</sectionHeader>
<bodyText confidence="0.99934475">We present empirical results on the Rich Transcription 2002 evaluation test set (rt02), which we used as our development set, as well as on the Rich Transcription 2003 Spring evaluation CTS test set (rt03). The rt02 set consists of 6081 sentences (63804 words) and has three subsets: Switchboard 1, Switchboard 2, Switchboard Cellular. The rt03 set consists of 9050 sentences (76083 words) and has two subsets: Switchboard and Fisher. We used the same training set as that used in Roark et al. (2004). The training set consists of 276726 transcribed utterances (3047805 words), with an additional 20854 utterances (249774 words) as held out data. For each utterance, a weighted word-lattice was produced, representing alternative transcriptions, from the ASR system.</bodyText>
<subsubsectionHeader confidence="0.332062">
Iterations over training
</subsubsectionHeader>
<figureCaption confidence="0.988519">
Figure 3: Word error rate on the rt02 eval set versus training
iterations for CRF trials, contrasted with baseline recognizer
performance and perceptron performance. Points are at every
20 iterations. Each point (x,y) is the WER at the iteration with
the best objective function value in the interval (x-20,x].
</figureCaption>
<bodyText confidence="0.999978696969697">From each word-lattice, the oracle best path was extracted, which gives the best word-error rate from among all of the hypotheses in the lattice. The oracle word-error rate for the training set lattices was 12.2%. We also performed trials with 1000-best lists for the same training set, rather than lattices. The oracle score for the 1000-best lists was 16.7%. To produce the word-lattices, each training utterance was processed by the baseline ASR system. However, these same utterances are what the acoustic and language models are built from, which leads to better performance on the training utterances than can be expected when the ASR system processes unseen utterances. To somewhat control for this, the training set was partitioned into 28 sets, and baseline Katz backoff trigram models were built for each set by including only transcripts from the other 27 sets. Since language models are generally far more prone to overtrain than standard acoustic models, this goes a long way toward making the training conditions similar to testing conditions. There are three baselines against which we are comparing. The first is the ASR baseline, with no reweighting from a discriminatively trained n-gram model. The other two baselines are with perceptron-trained n-gram model re-weighting, and were reported in Roark et al.(2004). The first of these is for a pruned-lattice trained trigram model, which showed a reduction in word error rate (WER) of 1.3%, from 39.2% to 37.9% on rt02. The second is for a 1000-best list trained trigram model, which performed only marginally worse than the latticetrained perceptron, at 38.0% on rt02.</bodyText>
<subsectionHeader confidence="0.995824">
4.1 Perceptron feature set
</subsectionHeader>
<bodyText confidence="0.999972333333333">We use the perceptron-trained models as the starting point for our CRF algorithm: the feature set given to the CRF algorithm is the feature set selected by the perceptron algorithm; the feature weights are initialized to those of the averaged perceptron. Figure 3 shows the performance of our three baselines versus three trials of the CRF algorithm.</bodyText>
<figure confidence="0.987007904761905">
40
39.5
Baseline recognizer
Perceptron, Feat=PL, Lattice
Perceptron, Feat=PN, N=1000
CRF, σ = ∞, Feat=PL, Lattice
CRF, σ = 0.5, Feat=PL, Lattice
CRF, σ = 0.5, Feat=PN, N=1000
37.5
370 500 1000
38
39
38.5
X= X
wn1 ELi k=1...n
XEi ≈
wn1 ELi
X
k=1...n
Word error rate
Iterations over training
</figure>
<figureCaption confidence="0.9924326">
Figure 4: Word error rate on the rt02 eval set versus training
iterations for CRF trials, contrasted with baseline recognizer
performance and perceptron performance. Points are at every
20 iterations. Each point (x,y) is the WER at the iteration with
the best objective function value in the interval (x-20,x].
</figureCaption>
<bodyText confidence="0.999854678571429">In the first two trials, the training set consists of the pruned lattices, and the feature set is from the perceptron algorithm trained on pruned lattices. There were 1.4 million features in this feature set. The first trial set the regularizer constant u = ∞, so that the algorithm was optimizing raw conditional likelihood. The second trial is with the regularizer constant u = 0.5, which we found empirically to be a good parameterization on the held-out set. As can be seen from these results, regularization is critical. The third trial in this set uses the feature set from the perceptron algorithm trained on 1000-best lists, and uses CRF optimization on these on these same 1000-best lists. There were 0.9 million features in this feature set. For this trial, we also used u = 0.5. As with the perceptron baselines, the n-best trial performs nearly identically with the pruned lattices, here also resulting in 37.4% WER. This may be useful for techniques that would be more expensive to extend to lattices versus n-best lists (e.g.models with unbounded dependencies). These trials demonstrate that the CRF algorithm can do a better job of estimating feature weights than the perceptron algorithm for the same feature set. As mentioned in the earlier section, feature selection is a by-product of the perceptron algorithm, but the CRF algorithm is given a set of features. The next two trials looked at selecting feature sets other than those provided by the perceptron algorithm.</bodyText>
<subsectionHeader confidence="0.977528">
4.2 Other feature sets
</subsectionHeader>
<bodyText confidence="0.999530909090909">In order for the feature weights to be non-zero in this approach, they must be observed in the training set. The number of unigram, bigram and trigram features with non-zero observations in the training set lattices is 43.65 million, or roughly 30 times the size of the perceptron feature set. Many of these features occur only rarely with very low conditional probabilities, and hence cannot meaningfully impact system performance. We pruned this feature set to include all unigrams and bigrams, but only those trigrams with an expected count of greater than 0.01 in the training set. That is, to be included, a trigram must occur in a set of paths, the sum of the conditional probabilities of which must be greater than our threshold 0 = 0.01.</bodyText>
<table confidence="0.999883125">
Trial Iter rt02 rt03
ASR Baseline - 39.2 38.2
Perceptron, Lattice - 37.9 36.9
Perceptron, N-best - 38.0 37.2
CRF, Lattice, Percep Feats (1.4M) 769 37.4 36.5
CRF, N-best, Percep Feats (0.9M) 946 37.4 36.6
CRF, Lattice, 0 = 0.01 (12M) 2714 37.6 36.5
CRF, Lattice, 0 = 0.9 (1.5M) 1679 37.5 36.6
</table>
<tableCaption confidence="0.970705333333333">
Table 1: Word-error rate results at convergence iteration for
various trials, on both Switchboard 2002 test set (rt02), which
was used as the dev set, and Switchboard 2003 test set (rt03).
</tableCaption>
<bodyText confidence="0.999753387755102">This threshold resulted in a feature set of roughly 12 million features, nearly 10 times the size of the perceptron feature set. For better comparability with that feature set, we set our thresholds higher, so that trigrams were pruned if their expected count fell below 0 = 0.9, and bigrams were pruned if their expected count fell below 0 = 0.1. We were concerned that this may leave out some of the features on the oracle paths, so we added back in all bigram and trigram features that occurred on oracle paths, giving a feature set of 1.5 million features, roughly the same size as the perceptron feature set. Figure 4 shows the results for three CRF trials versus our ASR baseline and the perceptron algorithm baseline trained on lattices. First, the result using the perceptron feature set provides us with a WER of 37.4%, as previously shown. The WER at convergence for the big feature set (12 million features) is 37.6%; the WER at convergence for the smaller feature set (1.5 million features) is 37.5%. While both of these other feature sets converge to performance close to that using the perceptron features, the number of iterations over the training data that are required to reach that level of performance are many more than for the perceptron-initialized feature set. Table 1 shows the word-error rate at the convergence iteration for the various trials, on both rt02 and rt03. All of the CRF trials are significantly better than the perceptron performance, using the Matched Pair Sentence Segment test for WER included with SCTK (NIST, 2000). On rt02, the N-best and perceptron initialized CRF trials were were significantly better than the lattice perceptron at p &lt; 0.001; the other two CRF trials were significantly better than the lattice perceptron at p &lt; 0.01. On rt03, the N-best CRF trial was significantly better than the lattice perceptron at p &lt; 0.002; the other three CRF trials were significantly better than the lattice perceptron at p &lt; 0.001. Finally, we measured the time of a single iteration over the training data on a single machine for the perceptron algorithm, the CRF algorithm using the approximation to the gradient of α0, and the CRF algorithm using an exact gradient of α0. Table 2 shows these times in hours. Because of the frequent update of the weights in the model, the perceptron algorithm is more expensive than the CRF algorithm for a single iteration. Further, the CRF algorithm is parallelizable, so that most of the work of an iteration can be shared among multiple processors.</bodyText>
<figure confidence="0.9759995">
40
39.5
Baseline recognizer
Perceptron, Feat=PL, Lattice
CRF, σ = 0.5, Feat=PL, Lattice
CRF, σ = 0.5, Feat=E, θ=0.01
CRF, σ = 0.5, Feat=E, θ=0.9
38
37.5
370 500 1000 1500 2000 2500
39
38.5
</figure>
<table confidence="0.911328">
Word error rate
Features Percep CRF exact
approx
Lattice, Percep Feats (1.4M) 7.10 1.69 3.61
N-best, Percep Feats (0.9M) 3.40 0.96 1.40
Lattice, θ = 0.01 (12M) - 2.24 4.75
</table>
<tableCaption confidence="0.9969155">
Table 2: Time (in hours) for one iteration on a single Intel
Xeon 2.4Ghz processor with 4GB RAM.
</tableCaption>
<bodyText confidence="0.998949571428571">Our most common training setup for the CRF algorithm was parallelized between 20 processors, using the approximation to the gradient. In that setup, using the 1.4M feature set, one iteration of the perceptron algorithm took the same amount of real time as approximately 80 iterations of CRF.</bodyText>
<sectionHeader confidence="0.9993" genericHeader="conclusion">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999991">We have contrasted two approaches to discriminative language model estimation on a difficult large vocabulary task, showing that they can indeed scale effectively to handle this size of a problem. Both algorithms have their benefits. The perceptron algorithm selects a relatively small subset of the total feature set, and requires just a couple of passes over the training data. The CRF algorithm does a better job of parameter estimation for the same feature set, and is parallelizable, so that each pass over the training set can require just a fraction of the real time of the perceptron algorithm. The best scenario from among those that we investigated was a combination of both approaches, with the output of the perceptron algorithm taken as the starting point for CRF estimation. As a final point, note that the methods we describe do not replace an existing language model, but rather complement it. The existing language model has the benefit that it can be trained on a large amount of text that does not have speech transcriptions. It has the disadvantage of not being a discriminative model. The new language model is trained on the speech transcriptions, meaning that it has less training data, but that it has the advantage of discriminative training – and in particular, the advantage of being able to learn negative evidence in the form of negative weights on n-grams which are rarely or never seen in natural language text (e.g., “the of”), but are produced too frequently by the recognizer. The methods we describe combines the two language models, allowing them to complement each other.</bodyText>
<sectionHeader confidence="0.999477" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999701160919541">
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003. Generalized
algorithms for constructing language models. In Proceedings of the
41st Annual Meeting of the Association for Computational Linguis-
tics, pages 40–47.
Satish Balay, William D. Gropp, Lois Curfman McInnes, and Barry F.
Smith. 2002. Petsc users manual. Technical Report ANL-95/11-
Revision 2.1.2, Argonne National Laboratory.
Satanjeev Banerjee, Jack Mostow, Joseph Beck, and Wilson Tam.
2003. Improving language models by learning from speech recog-
nition errors in a reading tutor that listens. In Proceedings of the
Second International Conference on Applied Artificial Intelligence,
Fort Panhala, Kolhapur, India.
Steven J. Benson and Jorge J. Mor´e. 2002. A limited memory vari-
able metric method for bound constrained minimization. Preprint
ANL/ACSP909-0901, Argonne National Laboratory.
Steven J. Benson, Lois Curfman McInnes, Jorge J. Mor´e, and Jason
Sarich. 2002. Tao users manual. Technical Report ANL/MCS-TM-
242-Revision 1.4, Argonne National Laboratory.
Zheng Chen, Kai-Fu Lee, and Ming Jing Li. 2000. Discriminative
training on language model. In Proceedings of the Sixth Interna-
tional Conference on Spoken Language Processing (ICSLP), Bei-
jing, China.
Michael Collins. 2002. Discriminative training methods for hidden
markov models: Theory and experiments with perceptron algo-
rithms. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages 1–8.
Michael Collins. 2004. Parameter estimation for statistical parsing
models: Theory and practice of distribution-free methods. In Harry
Bunt, John Carroll, and Giorgio Satta, editors, New Developments
in Parsing Technology. Kluwer.
Yoav Freund and Robert Schapire. 1999. Large margin classification
using the perceptron algorithm. Machine Learning, 3(37):277–296.
Frederick Jelinek. 1995. Acoustic sensitive language modeling. Tech-
nical report, Center for Language and Speech Processing, Johns
Hopkins University, Baltimore, MD.
Mark Johnson, Stuart Geman, Steven Canon, Zhiyi Chi, and Stefan
Riezler. 1999. Estimators for stochastic “unification-based” gram-
mars. In Proceedings of the 37th Annual Meeting of the Association
for Computational Linguistics, pages 535–541.
Sanjeev Khudanpur and Jun Wu. 2000. Maximum entropy techniques
for exploiting syntactic, semantic and collocational dependencies in
language modeling. Computer Speech and Language, 14(4):355–
372.
Hong-Kwang Jeff Kuo, Eric Fosler-Lussier, Hui Jiang, and Chin-
Hui Lee. 2002. Discriminative training of language models for
speech recognition. In Proceedings of the International Conference
on Acoustics, Speech, and Signal Processing (ICASSP), Orlando,
Florida.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Con-
ditional random fields: Probabilistic models for segmenting and
labeling sequence data. In Proc. ICML, pages 282–289, Williams
College, Williamstown, MA, USA.
Robert Malouf. 2002. A comparison of algorithms for maximum en-
tropy parameter estimation. In Proc. CoNLL, pages 49–55.
Andrew McCallum and Wei Li. 2003. Early results for named entity
recognition with conditional random fields, feature induction and
web-enhanced lexicons. In Proc. CoNLL.
Mehryar Mohri, Fernando C. N. Pereira, and Michael Riley. 2002.
Weighted finite-state transducers in speech recognition. Computer
Speech and Language, 16(1):69–88.
NIST. 2000. Speech recognition scoring toolkit (sctk) version 1.2c.
Available at http://www.nist.gov/speech/tools.
David Pinto, Andrew McCallum, Xing Wei, and W. Bruce Croft. 2003.
Table extraction using conditional random fields. In Proc. ACM SI-
GIR.
Adwait Ratnaparkhi, Salim Roukos, and R. Todd Ward. 1994. A max-
imum entropy model for parsing. In Proceedings of the Interna-
tional Conference on Spoken Language Processing (ICSLP), pages
803–806.
Brian Roark, Murat Saraclar, and Michael Collins. 2004. Corrective
language modeling for large vocabulary ASR with the perceptron al-
gorithm. In Proceedings of the International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP), pages 749–752.
Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. HLT-NAACL, Edmonton, Canada.
A. Stolcke and M. Weintraub. 1998. Discriminitive language model-
ing. In Proceedings of the 9th Hub-5 Conversational Speech Recog-
nition Workshop.
A. Stolcke, H. Bratt, J. Butzberger, H. Franco, V. R. Rao Gadde,
M. Plauche, C. Richey, E. Shriberg, K. Sonmez, F. Weng, and
J. Zheng. 2000. The SRI March 2000 Hub-5 conversational speech
transcription system. In Proceedings of the NIST Speech Transcrip-
tion Workshop.
Hanna Wallach. 2002. Efficient training of conditional random fields.
Master’s thesis, University of Edinburgh.
P.C. Woodland and D. Povey. 2000. Large scale discriminative training
for speech recognition. In Proc. ISCAITRWASR2000, pages 7–16.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant confidence="0.660039" no="0">
<title confidence="0.998052">Discriminative Language Modeling with Conditional Random Fields and the Perceptron Algorithm</title>
<author confidence="0.999998">Brian Roark Murat Saraclar Michael Collins Mark Johnson</author>
<affiliation confidence="0.999623">AT&amp;T Labs - Research MIT CSAIL Brown University</affiliation>
<author confidence="0.936079">mcollinscsail mit edu Mark JohnsonBrown edu</author>
<abstract confidence="0.966757375">This paper describes discriminative language modeling for a large vocabulary speech recognition task. We contrast two parameter estimation methods: the perceptron algorithm, and a method based on conditional random fields (CRFs). The models are encoded as deterministic weighted finite state automata, and are applied by intersecting the automata with word-lattices that are the output from a baseline recognizer. The perceptron algorithm has the benefit of automatically selecting a relatively small feature set in just a couple of passes over the training data. However, using the feature set output from the perceptron algorithm (initialized with their weights), CRF training provides an additional 0.5% reduction in word error rate, for a total 1.8% absolute reduction from the baseline of 39.2%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Mehryar Mohri</author>
<author>Brian Roark</author>
</authors>
<title>Generalized algorithms for constructing language models.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>40--47</pages>
<contexts>
<context citStr="Allauzen et al., 2003" endWordPosition="3553" position="20234" startWordPosition="3550">gram features Φi for i = 1... d implemented by D. The second concerns the choice ofparameters αi for i = 0 ... d which assign weights to the n-gram features as well as the baseline feature Φ0. Before describing methods for training a discriminative language model using perceptron and CRF algorithms, we give a little more detail about the structure of D, focusing on how n-gram language models can be implemented with finite-state techniques. 3.3 Representation of n-gram language models An n-gram model can be efficiently represented in a deterministic WFA, through the use of failure transitions (Allauzen et al., 2003). Every string accepted by such an automaton has a single path through the automaton, and the weight of the string is the sum of the weights of the transitions in that path. In such a representation, every state in the automaton represents an n-gram history h, e.g. wi_2wi_1, and there are transitions leaving the state for every word wi such that the feature hwi has a weight. There is also a failure transition leaving the state, labeled with some reserved symbol φ, which can only be traversed if the next symbol in the input does not match any transition leaving the state. This failure transitio</context>
<context citStr="Allauzen et al. (2003)" endWordPosition="4642" position="26252" startWordPosition="4639">is straightforward. Calculating the n-gram feature gradients for the CRF optimization is also relatively simple, once L'i has been constructed. From the derivative in Eq. 4, for each i = 1... N, j = 1... d the quantity 4'j(xi, yi) − � p¯α(y|xi)4'j(xi,y) (8) yEGEN(x;) must be computed. The first term is simply the number of times the j’th n-gram feature is seen in yi. The second term is the expected number of times that the j’th n-gram is seen in the acceptor L'i. If the j’th n-gram is w1 ... w,,,, then this can be computed as ExpCount(L'i, w1 ... w,,,). The GRM library, which was presented in Allauzen et al. (2003), has a direct implementation of the function ExpCount, which simultaneously calculates the expected value of all n-grams of order less than or equal to a given n in a lattice L. The one non-ngram feature weight that is being estimated is the weight α0 given to the baseline ASR negative log probability. Calculation of the gradient of LLR with respect to this parameter again requires calculation of the term in Eq. 8 for j = 0 and i = 1... N. Computation of EyEGEN(x;) p¯α(y|xi)4'0(xi, y) turns out to be not as straightforward as calculating n-gram expectations. To do so, we rely upon the fact th</context>
</contexts>
<marker>Allauzen, Mohri, Roark, 2003</marker>
<rawString>Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003. Generalized algorithms for constructing language models. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 40–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satish Balay</author>
<author>William D Gropp</author>
<author>Lois Curfman McInnes</author>
<author>Barry F Smith</author>
</authors>
<title>Petsc users manual.</title>
<date>2002</date>
<tech>Technical Report ANL-95/11-Revision 2.1.2,</tech>
<institution>Argonne National Laboratory.</institution>
<contexts>
<context citStr="Balay et al., 2002" endWordPosition="1838" position="11148" startWordPosition="1835"> N = i=1 Following Johnson et al. (1999) and Lafferty et al. (2001), we use a zero-mean Gaussian prior on the parameters resulting in the regularized objective function: [Φ(xi, yi) ·α¯ − log Z(xi, ¯α)] − ||¯α||2 2σ2 (3) The value u dictates the relative influence of the loglikelihood term vs. the prior, and is typically estimated using held-out data. The optimal parameters under this criterion are ¯α* = argmax¯α LLR(¯α). We use a limited memory variable metric method (Benson and Mor´e, 2002) to optimize LLR. There is a general implementation of this method in the Tao/PETSc software libraries (Balay et al., 2002; Benson et al., 2002). This technique has been shown to be very effective in a variety of NLP tasks (Malouf, 2002; Wallach, 2002). The main interface between the optimizer and the training data is a procedure which takes a parameter vector α¯ as input, and in turn returns LLR(¯α) as well as the gradient of LLR at ¯α. The derivative of the objective function with respect to a parameter αs at parameter values α¯ is ⎡ ⎤ αs Φs(Xi, Yi) − X P¯α(Y|Xi)Φs(Xi, Y)v − 2 (4) y∈GEN(xi) Note that LLR(¯α) is a convex function, so that there is a globally optimal solution and the optimization method will find</context>
</contexts>
<marker>Balay, Gropp, McInnes, Smith, 2002</marker>
<rawString>Satish Balay, William D. Gropp, Lois Curfman McInnes, and Barry F. Smith. 2002. Petsc users manual. Technical Report ANL-95/11-Revision 2.1.2, Argonne National Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Jack Mostow</author>
<author>Joseph Beck</author>
<author>Wilson Tam</author>
</authors>
<title>Improving language models by learning from speech recognition errors in a reading tutor that listens.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second International Conference on Applied Artificial Intelligence,</booktitle>
<location>Fort Panhala, Kolhapur, India.</location>
<contexts>
<context citStr="Banerjee et al. (2003)" endWordPosition="964" position="6219" startWordPosition="961">ches including MMIE with mixed results. This work was followed up with some success by Stolcke et al. (2000) where an “antiLM”, estimated from weighted N-best hypotheses of a baseline ASR system, was used with a negative weight in combination with the baseline LM. Chen et al. (2000) presented a method based on changing the trigram counts discriminatively, together with changing the lexicon to add new words. Kuo et al. (2002) used the generalized probabilistic descent algorithm to train relatively small language models which attempt to minimize string error rate on the DARPA Communicator task. Banerjee et al. (2003) used a language model modification algorithm in the context of a reading tutor that listens. Their algorithm first uses a classifier to predict what effect each parameter has on the error rate, and then modifies the parameters to reduce the error rate based on this prediction. 2 Linear Models, the Perceptron Algorithm, and Conditional Random Fields This section describes a general framework, global linear models, and two parameter estimation methods within the framework, the perceptron algorithm and a method based on conditional random fields. The linear models we describe are general enough </context>
</contexts>
<marker>Banerjee, Mostow, Beck, Tam, 2003</marker>
<rawString>Satanjeev Banerjee, Jack Mostow, Joseph Beck, and Wilson Tam. 2003. Improving language models by learning from speech recognition errors in a reading tutor that listens. In Proceedings of the Second International Conference on Applied Artificial Intelligence, Fort Panhala, Kolhapur, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven J Benson</author>
<author>Jorge J Mor´e</author>
</authors>
<title>A limited memory variable metric method for bound constrained minimization.</title>
<date>2002</date>
<tech>Preprint ANL/ACSP909-0901,</tech>
<institution>Argonne National Laboratory.</institution>
<marker>Benson, Mor´e, 2002</marker>
<rawString>Steven J. Benson and Jorge J. Mor´e. 2002. A limited memory variable metric method for bound constrained minimization. Preprint ANL/ACSP909-0901, Argonne National Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven J Benson</author>
<author>Lois Curfman McInnes</author>
<author>Jorge J Mor´e</author>
<author>Jason Sarich</author>
</authors>
<title>Tao users manual.</title>
<date>2002</date>
<tech>Technical Report ANL/MCS-TM242-Revision 1.4,</tech>
<institution>Argonne National Laboratory.</institution>
<marker>Benson, McInnes, Mor´e, Sarich, 2002</marker>
<rawString>Steven J. Benson, Lois Curfman McInnes, Jorge J. Mor´e, and Jason Sarich. 2002. Tao users manual. Technical Report ANL/MCS-TM242-Revision 1.4, Argonne National Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng Chen</author>
<author>Kai-Fu Lee</author>
<author>Ming Jing Li</author>
</authors>
<title>Discriminative training on language model.</title>
<date>2000</date>
<booktitle>In Proceedings of the Sixth International Conference on Spoken Language Processing (ICSLP),</booktitle>
<location>Beijing, China.</location>
<contexts>
<context citStr="Chen et al. (2000)" endWordPosition="913" position="5880" startWordPosition="910">to concerns about training time, a language model with fewer features is likely to be considerably more efficient when decoding new utterances. are estimated by minimizing H(W |A), the expected uncertainty of the spoken text W, given the acoustic sequence A. Stolcke and Weintraub (1998) experimented with various discriminative approaches including MMIE with mixed results. This work was followed up with some success by Stolcke et al. (2000) where an “antiLM”, estimated from weighted N-best hypotheses of a baseline ASR system, was used with a negative weight in combination with the baseline LM. Chen et al. (2000) presented a method based on changing the trigram counts discriminatively, together with changing the lexicon to add new words. Kuo et al. (2002) used the generalized probabilistic descent algorithm to train relatively small language models which attempt to minimize string error rate on the DARPA Communicator task. Banerjee et al. (2003) used a language model modification algorithm in the context of a reading tutor that listens. Their algorithm first uses a classifier to predict what effect each parameter has on the error rate, and then modifies the parameters to reduce the error rate based on</context>
</contexts>
<marker>Chen, Lee, Li, 2000</marker>
<rawString>Zheng Chen, Kai-Fu Lee, and Ming Jing Li. 2000. Discriminative training on language model. In Proceedings of the Sixth International Conference on Spoken Language Processing (ICSLP), Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1--8</pages>
<contexts>
<context citStr="Collins (2002" endWordPosition="1132" position="7263" startWordPosition="1131">eter estimation methods within the framework, the perceptron algorithm and a method based on conditional random fields. The linear models we describe are general enough to be applicable to a diverse range of NLP and speech tasks – this section gives a general description of the approach. In the next section of the paper we describe how global linear models can be applied to speech recognition. In particular, we focus on how the decoding and parameter estimation problems can be implemented over lattices using finite-state techniques. 2.1 Global linear models We follow the framework outlined in Collins (2002; 2004). The task is to learn a mapping from inputs x ∈ X to outputs y ∈ Y. We assume the following components: (1) Training examples (xi, yi) for i = 1... N. (2) A function GEN which enumerates a set of candidates GEN(x) for an input x. (3) A representation Φ mapping each (x, y) ∈ X × Y to a feature vector Φ(x, y) ∈ Rd. (4) A parameter vector α¯ ∈ Rd. The components GEN, Φ and α¯ define a mapping from an input x to an output F(x) through F(x) = argmax Φ(x, y) · α¯ (1) y∈GEN(x) where Φ(x, y) · α¯ is the inner product Es αsΦs(x, y). The learning task is to set the parameter values α¯ using the </context>
<context citStr="Collins (2002)" endWordPosition="1504" position="9225" startWordPosition="1503">ceptron algorithm is shown in figure 1. At each training example (xi, yi), the current best-scoring hypothesis zi is found, and if it differs from the reference yi , then the cost of each feature2 is increased by the count of that feature in zi and decreased by the count of that feature in yi. The features in the model are updated, and the algorithm moves to the next utterance. After each pass over the training data, performance on a held-out data set is evaluated, and the parameterization with the best performance on the held out set is what is ultimately produced by the algorithm. Following Collins (2002), we used the averaged parameters from the training algorithm in decoding heldout and test examples in our experiments. Say ¯αti is the parameter vector after the i’th example is processed on the t’th pass through the data in the algorithm in figure 1. Then the averaged parameters ¯αAV G are defined as ¯αAV G = Ei,t ¯αti/NT. Freund and Schapire (1999) originally proposed the averaged parameter method; it was shown to give substantial improvements in accuracy for tagging tasks in Collins (2002). 2.3 Conditional Random Fields Conditional Random Fields have been applied to NLP tasks such as parsi</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Parameter estimation for statistical parsing models: Theory and practice of distribution-free methods.</title>
<date>2004</date>
<booktitle>New Developments in Parsing Technology.</booktitle>
<editor>In Harry Bunt, John Carroll, and Giorgio Satta, editors,</editor>
<publisher>Kluwer.</publisher>
<contexts>
<context citStr="Collins (2004)" endWordPosition="2052" position="12378" startWordPosition="2051">the Gaussian prior term ||¯α||2/2u2 in the objective function has been found to be useful in several NLP settings. It effectively ensures that there is a large penalty for parameter values in the model becoming too large – as such, it tends to control over-training. The choice of LLR as an objective function can be justified as maximum a-posteriori (MAP) training within a Bayesian approach. An alternative justification comes through a connection to support vector machines and other large margin approaches. SVM-based approaches use an optimization criterion that is closely related to LLR – see Collins (2004) for more discussion. 3 Linear models for speech recognition We now describe how the formalism and algorithms in section 2 can be applied to language modeling for speech recognition. 3.1 The basic approach As described in the previous section, linear models require definitions of X, Y, xi, yi, GEN, 4) and a parameter estimation method. In the language modeling setting we take X to be the set of all possible acoustic inputs; Y is the set of all possible strings, E*, for some vocabulary E. Each xi is an utterance (a sequence of acoustic feature-vectors), and GEN(xi) is the set of possible transc</context>
</contexts>
<marker>Collins, 2004</marker>
<rawString>Michael Collins. 2004. Parameter estimation for statistical parsing models: Theory and practice of distribution-free methods. In Harry Bunt, John Carroll, and Giorgio Satta, editors, New Developments in Parsing Technology. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>3</volume>
<issue>37</issue>
<contexts>
<context citStr="Freund and Schapire (1999)" endWordPosition="1568" position="9578" startWordPosition="1565"> algorithm moves to the next utterance. After each pass over the training data, performance on a held-out data set is evaluated, and the parameterization with the best performance on the held out set is what is ultimately produced by the algorithm. Following Collins (2002), we used the averaged parameters from the training algorithm in decoding heldout and test examples in our experiments. Say ¯αti is the parameter vector after the i’th example is processed on the t’th pass through the data in the algorithm in figure 1. Then the averaged parameters ¯αAV G are defined as ¯αAV G = Ei,t ¯αti/NT. Freund and Schapire (1999) originally proposed the averaged parameter method; it was shown to give substantial improvements in accuracy for tagging tasks in Collins (2002). 2.3 Conditional Random Fields Conditional Random Fields have been applied to NLP tasks such as parsing (Ratnaparkhi et al., 1994; Johnson et al., 1999), and tagging or segmentation tasks (Lafferty et al., 2001; Sha and Pereira, 2003; McCallum and Li, 2003; Pinto et al., 2003). CRFs use the parameters α¯ to define a conditional distribution over the members of GEN(x) for a given input x: p¯α(y|x) = Z(x, ¯α) exp (Φ(x, y) · ¯α) 1 where Z(x, ¯α) = y∈GEN</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Yoav Freund and Robert Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 3(37):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
</authors>
<title>Acoustic sensitive language modeling.</title>
<date>1995</date>
<tech>Technical report,</tech>
<institution>Center for Language and Speech Processing, Johns Hopkins University,</institution>
<location>Baltimore, MD.</location>
<contexts>
<context citStr="Jelinek (1995)" endWordPosition="798" position="5169" startWordPosition="797"> vocabulary ASR has benefitted from discriminative estimation of Hidden Markov Model (HMM) parameters in the form of Maximum Mutual Information Estimation (MMIE) or Conditional Maximum Likelihood Estimation (CMLE). Woodland and Povey (2000) have shown the effectiveness of lattice-based MMIE/CMLE in challenging large scale ASR tasks such as Switchboard. In fact, state-of-the-art acoustic modeling, as seen, for example, at annual Switchboard evaluations, invariably includes some kind of discriminative training. Discriminative estimation of language models has also been proposed in recent years. Jelinek (1995) suggested an acoustic sensitive language model whose parameters 'Note also that in addition to concerns about training time, a language model with fewer features is likely to be considerably more efficient when decoding new utterances. are estimated by minimizing H(W |A), the expected uncertainty of the spoken text W, given the acoustic sequence A. Stolcke and Weintraub (1998) experimented with various discriminative approaches including MMIE with mixed results. This work was followed up with some success by Stolcke et al. (2000) where an “antiLM”, estimated from weighted N-best hypotheses of</context>
</contexts>
<marker>Jelinek, 1995</marker>
<rawString>Frederick Jelinek. 1995. Acoustic sensitive language modeling. Technical report, Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stuart Geman</author>
<author>Steven Canon</author>
<author>Zhiyi Chi</author>
<author>Stefan Riezler</author>
</authors>
<title>Estimators for stochastic “unification-based” grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>535--541</pages>
<contexts>
<context citStr="Johnson et al., 1999" endWordPosition="1613" position="9876" startWordPosition="1610">ers from the training algorithm in decoding heldout and test examples in our experiments. Say ¯αti is the parameter vector after the i’th example is processed on the t’th pass through the data in the algorithm in figure 1. Then the averaged parameters ¯αAV G are defined as ¯αAV G = Ei,t ¯αti/NT. Freund and Schapire (1999) originally proposed the averaged parameter method; it was shown to give substantial improvements in accuracy for tagging tasks in Collins (2002). 2.3 Conditional Random Fields Conditional Random Fields have been applied to NLP tasks such as parsing (Ratnaparkhi et al., 1994; Johnson et al., 1999), and tagging or segmentation tasks (Lafferty et al., 2001; Sha and Pereira, 2003; McCallum and Li, 2003; Pinto et al., 2003). CRFs use the parameters α¯ to define a conditional distribution over the members of GEN(x) for a given input x: p¯α(y|x) = Z(x, ¯α) exp (Φ(x, y) · ¯α) 1 where Z(x, ¯α) = y∈GEN(x) exp (Φ(x, y) · ¯α) is a normalization constant that depends on x and ¯α. Given these definitions, the log-likelihood of the training data under parameters α¯ is log p¯α(yi|xi) [Φ(xi, yi) · α¯ − log Z(xi, ¯α)] (2) 2Note that here lattice weights are interpreted as costs, which changes the sign </context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>Mark Johnson, Stuart Geman, Steven Canon, Zhiyi Chi, and Stefan Riezler. 1999. Estimators for stochastic “unification-based” grammars. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 535–541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjeev Khudanpur</author>
<author>Jun Wu</author>
</authors>
<title>Maximum entropy techniques for exploiting syntactic, semantic and collocational dependencies in language modeling.</title>
<date>2000</date>
<journal>Computer Speech and Language,</journal>
<volume>14</volume>
<issue>4</issue>
<pages>372</pages>
<contexts>
<context citStr="Khudanpur and Wu (2000)" endWordPosition="641" position="4120" startWordPosition="638">e method for feature selection, selecting around 1.5 million features in total. The CRF trained with this feature set, and initialized with parameters from perceptron training, converges much more quickly than other approaches, and also gives the optimal performance on the held-out set. We explore other approaches to feature selection, but find that the perceptron-based approach gives the best results in our experiments. While we focus on n-gram models, we stress that our methods are applicable to more general language modeling features – for example, syntactic features, as explored in, e.g., Khudanpur and Wu (2000). We intend to explore methods with new features in the future. Experimental results with n-gram models on 1000-best lists show a very small drop in accuracy compared to the use of lattices. This is encouraging, in that it suggests that models with more flexible features than n-gram models, which therefore cannot be efficiently used with lattices, may not be unduly harmed by their restriction to n-best lists. 1.1 Related Work Large vocabulary ASR has benefitted from discriminative estimation of Hidden Markov Model (HMM) parameters in the form of Maximum Mutual Information Estimation (MMIE) or </context>
</contexts>
<marker>Khudanpur, Wu, 2000</marker>
<rawString>Sanjeev Khudanpur and Jun Wu. 2000. Maximum entropy techniques for exploiting syntactic, semantic and collocational dependencies in language modeling. Computer Speech and Language, 14(4):355– 372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong-Kwang Jeff Kuo</author>
<author>Eric Fosler-Lussier</author>
<author>Hui Jiang</author>
<author>ChinHui Lee</author>
</authors>
<title>Discriminative training of language models for speech recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP),</booktitle>
<location>Orlando, Florida.</location>
<contexts>
<context citStr="Kuo et al. (2002)" endWordPosition="936" position="6025" startWordPosition="933">e estimated by minimizing H(W |A), the expected uncertainty of the spoken text W, given the acoustic sequence A. Stolcke and Weintraub (1998) experimented with various discriminative approaches including MMIE with mixed results. This work was followed up with some success by Stolcke et al. (2000) where an “antiLM”, estimated from weighted N-best hypotheses of a baseline ASR system, was used with a negative weight in combination with the baseline LM. Chen et al. (2000) presented a method based on changing the trigram counts discriminatively, together with changing the lexicon to add new words. Kuo et al. (2002) used the generalized probabilistic descent algorithm to train relatively small language models which attempt to minimize string error rate on the DARPA Communicator task. Banerjee et al. (2003) used a language model modification algorithm in the context of a reading tutor that listens. Their algorithm first uses a classifier to predict what effect each parameter has on the error rate, and then modifies the parameters to reduce the error rate based on this prediction. 2 Linear Models, the Perceptron Algorithm, and Conditional Random Fields This section describes a general framework, global lin</context>
</contexts>
<marker>Kuo, Fosler-Lussier, Jiang, Lee, 2002</marker>
<rawString>Hong-Kwang Jeff Kuo, Eric Fosler-Lussier, Hui Jiang, and ChinHui Lee. 2002. Discriminative training of language models for speech recognition. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Orlando, Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. ICML,</booktitle>
<pages>282--289</pages>
<location>Williams College, Williamstown, MA, USA.</location>
<contexts>
<context citStr="Lafferty et al., 2001" endWordPosition="1622" position="9934" startWordPosition="1619">st examples in our experiments. Say ¯αti is the parameter vector after the i’th example is processed on the t’th pass through the data in the algorithm in figure 1. Then the averaged parameters ¯αAV G are defined as ¯αAV G = Ei,t ¯αti/NT. Freund and Schapire (1999) originally proposed the averaged parameter method; it was shown to give substantial improvements in accuracy for tagging tasks in Collins (2002). 2.3 Conditional Random Fields Conditional Random Fields have been applied to NLP tasks such as parsing (Ratnaparkhi et al., 1994; Johnson et al., 1999), and tagging or segmentation tasks (Lafferty et al., 2001; Sha and Pereira, 2003; McCallum and Li, 2003; Pinto et al., 2003). CRFs use the parameters α¯ to define a conditional distribution over the members of GEN(x) for a given input x: p¯α(y|x) = Z(x, ¯α) exp (Φ(x, y) · ¯α) 1 where Z(x, ¯α) = y∈GEN(x) exp (Φ(x, y) · ¯α) is a normalization constant that depends on x and ¯α. Given these definitions, the log-likelihood of the training data under parameters α¯ is log p¯α(yi|xi) [Φ(xi, yi) · α¯ − log Z(xi, ¯α)] (2) 2Note that here lattice weights are interpreted as costs, which changes the sign in the algorithm presented in figure 1. N LL(¯α) = i=1 N =</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. ICML, pages 282–289, Williams College, Williamstown, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In Proc. CoNLL,</booktitle>
<pages>49--55</pages>
<contexts>
<context citStr="Malouf, 2002" endWordPosition="1860" position="11262" startWordPosition="1859"> resulting in the regularized objective function: [Φ(xi, yi) ·α¯ − log Z(xi, ¯α)] − ||¯α||2 2σ2 (3) The value u dictates the relative influence of the loglikelihood term vs. the prior, and is typically estimated using held-out data. The optimal parameters under this criterion are ¯α* = argmax¯α LLR(¯α). We use a limited memory variable metric method (Benson and Mor´e, 2002) to optimize LLR. There is a general implementation of this method in the Tao/PETSc software libraries (Balay et al., 2002; Benson et al., 2002). This technique has been shown to be very effective in a variety of NLP tasks (Malouf, 2002; Wallach, 2002). The main interface between the optimizer and the training data is a procedure which takes a parameter vector α¯ as input, and in turn returns LLR(¯α) as well as the gradient of LLR at ¯α. The derivative of the objective function with respect to a parameter αs at parameter values α¯ is ⎡ ⎤ αs Φs(Xi, Yi) − X P¯α(Y|Xi)Φs(Xi, Y)v − 2 (4) y∈GEN(xi) Note that LLR(¯α) is a convex function, so that there is a globally optimal solution and the optimization method will find it. The use of the Gaussian prior term ||¯α||2/2u2 in the objective function has been found to be useful in sever</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>Robert Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In Proc. CoNLL, pages 49–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Wei Li</author>
</authors>
<title>Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons.</title>
<date>2003</date>
<booktitle>In Proc. CoNLL.</booktitle>
<contexts>
<context citStr="McCallum and Li, 2003" endWordPosition="1630" position="9980" startWordPosition="1627">e parameter vector after the i’th example is processed on the t’th pass through the data in the algorithm in figure 1. Then the averaged parameters ¯αAV G are defined as ¯αAV G = Ei,t ¯αti/NT. Freund and Schapire (1999) originally proposed the averaged parameter method; it was shown to give substantial improvements in accuracy for tagging tasks in Collins (2002). 2.3 Conditional Random Fields Conditional Random Fields have been applied to NLP tasks such as parsing (Ratnaparkhi et al., 1994; Johnson et al., 1999), and tagging or segmentation tasks (Lafferty et al., 2001; Sha and Pereira, 2003; McCallum and Li, 2003; Pinto et al., 2003). CRFs use the parameters α¯ to define a conditional distribution over the members of GEN(x) for a given input x: p¯α(y|x) = Z(x, ¯α) exp (Φ(x, y) · ¯α) 1 where Z(x, ¯α) = y∈GEN(x) exp (Φ(x, y) · ¯α) is a normalization constant that depends on x and ¯α. Given these definitions, the log-likelihood of the training data under parameters α¯ is log p¯α(yi|xi) [Φ(xi, yi) · α¯ − log Z(xi, ¯α)] (2) 2Note that here lattice weights are interpreted as costs, which changes the sign in the algorithm presented in figure 1. N LL(¯α) = i=1 N = i=1 Following Johnson et al. (1999) and Laffe</context>
</contexts>
<marker>McCallum, Li, 2003</marker>
<rawString>Andrew McCallum and Wei Li. 2003. Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons. In Proc. CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando C N Pereira</author>
<author>Michael Riley</author>
</authors>
<title>Weighted finite-state transducers in speech recognition.</title>
<date>2002</date>
<journal>Computer Speech and Language,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context citStr="Mohri et al. (2002)" endWordPosition="2445" position="14627" startWordPosition="2442"> training data lattice, i.e., n-grams seen in any word sequence within the lattices. In practice, we consider methods that search for sparse parameter vectors ¯α, thus assigning many ngrams 0 weight. This will lead to more efficient algorithms that avoid dealing explicitly with the entire set of n-grams seen in training data. 3.2 Implementation using MFA We now give a brief sketch of how weighted finite-state automata (WFA) can be used to implement linear models for speech recognition. There are several papers describing the use of weighted automata and transducers for speech in detail, e.g., Mohri et al. (2002), but for clarity and completeness this section gives a brief description of the operations which we use. For our purpose, a WFA A = (E, Q, qs, F, E, ρ), where E is the vocabulary, Q is a (finite) set of states, qs E Q is a unique start state, F C Q is a set of final states, E is a (finite) set of transitions, and ρ : F → R is a function from final states to final weights. Each transition e E E is a tuple e = (l[e], p[e], n[e], w[e]), where l[e] E E is a label (in our case, words), p[e] E Q is the origin state of e, n[e] E Q is the destination state of e, and w[e] E R is the weight of the tran</context>
</contexts>
<marker>Mohri, Pereira, Riley, 2002</marker>
<rawString>Mehryar Mohri, Fernando C. N. Pereira, and Michael Riley. 2002. Weighted finite-state transducers in speech recognition. Computer Speech and Language, 16(1):69–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>Speech recognition scoring toolkit (sctk) version 1.2c. Available at http://www.nist.gov/speech/tools.</title>
<date>2000</date>
<contexts>
<context citStr="NIST, 2000" endWordPosition="6439" position="36857" startWordPosition="6438">nce for the smaller feature set (1.5 million features) is 37.5%. While both of these other feature sets converge to performance close to that using the perceptron features, the number of iterations over the training data that are required to reach that level of performance are many more than for the perceptron-initialized feature set. Table 1 shows the word-error rate at the convergence iteration for the various trials, on both rt02 and rt03. All of the CRF trials are significantly better than the perceptron performance, using the Matched Pair Sentence Segment test for WER included with SCTK (NIST, 2000). On rt02, the N-best and perceptron initialized CRF trials were were significantly better than the lattice perceptron at p &lt; 0.001; the other two CRF trials were significantly better than the lattice perceptron at p &lt; 0.01. On rt03, the N-best CRF trial was significantly better than the lattice perceptron at p &lt; 0.002; the other three CRF trials were significantly better than the lattice perceptron at p &lt; 0.001. Finally, we measured the time of a single iteration over the training data on a single machine for the perceptron algorithm, the CRF algorithm using the approximation to the gradient </context>
</contexts>
<marker>NIST, 2000</marker>
<rawString>NIST. 2000. Speech recognition scoring toolkit (sctk) version 1.2c. Available at http://www.nist.gov/speech/tools.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Pinto</author>
<author>Andrew McCallum</author>
<author>Xing Wei</author>
<author>W Bruce Croft</author>
</authors>
<title>Table extraction using conditional random fields.</title>
<date>2003</date>
<booktitle>In Proc. ACM SIGIR.</booktitle>
<contexts>
<context citStr="Pinto et al., 2003" endWordPosition="1634" position="10001" startWordPosition="1631">r the i’th example is processed on the t’th pass through the data in the algorithm in figure 1. Then the averaged parameters ¯αAV G are defined as ¯αAV G = Ei,t ¯αti/NT. Freund and Schapire (1999) originally proposed the averaged parameter method; it was shown to give substantial improvements in accuracy for tagging tasks in Collins (2002). 2.3 Conditional Random Fields Conditional Random Fields have been applied to NLP tasks such as parsing (Ratnaparkhi et al., 1994; Johnson et al., 1999), and tagging or segmentation tasks (Lafferty et al., 2001; Sha and Pereira, 2003; McCallum and Li, 2003; Pinto et al., 2003). CRFs use the parameters α¯ to define a conditional distribution over the members of GEN(x) for a given input x: p¯α(y|x) = Z(x, ¯α) exp (Φ(x, y) · ¯α) 1 where Z(x, ¯α) = y∈GEN(x) exp (Φ(x, y) · ¯α) is a normalization constant that depends on x and ¯α. Given these definitions, the log-likelihood of the training data under parameters α¯ is log p¯α(yi|xi) [Φ(xi, yi) · α¯ − log Z(xi, ¯α)] (2) 2Note that here lattice weights are interpreted as costs, which changes the sign in the algorithm presented in figure 1. N LL(¯α) = i=1 N = i=1 Following Johnson et al. (1999) and Lafferty et al. (2001), we</context>
</contexts>
<marker>Pinto, McCallum, Wei, Croft, 2003</marker>
<rawString>David Pinto, Andrew McCallum, Xing Wei, and W. Bruce Croft. 2003. Table extraction using conditional random fields. In Proc. ACM SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
<author>Salim Roukos</author>
<author>R Todd Ward</author>
</authors>
<title>A maximum entropy model for parsing.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing (ICSLP),</booktitle>
<pages>803--806</pages>
<contexts>
<context citStr="Ratnaparkhi et al., 1994" endWordPosition="1609" position="9853" startWordPosition="1606"> used the averaged parameters from the training algorithm in decoding heldout and test examples in our experiments. Say ¯αti is the parameter vector after the i’th example is processed on the t’th pass through the data in the algorithm in figure 1. Then the averaged parameters ¯αAV G are defined as ¯αAV G = Ei,t ¯αti/NT. Freund and Schapire (1999) originally proposed the averaged parameter method; it was shown to give substantial improvements in accuracy for tagging tasks in Collins (2002). 2.3 Conditional Random Fields Conditional Random Fields have been applied to NLP tasks such as parsing (Ratnaparkhi et al., 1994; Johnson et al., 1999), and tagging or segmentation tasks (Lafferty et al., 2001; Sha and Pereira, 2003; McCallum and Li, 2003; Pinto et al., 2003). CRFs use the parameters α¯ to define a conditional distribution over the members of GEN(x) for a given input x: p¯α(y|x) = Z(x, ¯α) exp (Φ(x, y) · ¯α) 1 where Z(x, ¯α) = y∈GEN(x) exp (Φ(x, y) · ¯α) is a normalization constant that depends on x and ¯α. Given these definitions, the log-likelihood of the training data under parameters α¯ is log p¯α(yi|xi) [Φ(xi, yi) · α¯ − log Z(xi, ¯α)] (2) 2Note that here lattice weights are interpreted as costs, </context>
</contexts>
<marker>Ratnaparkhi, Roukos, Ward, 1994</marker>
<rawString>Adwait Ratnaparkhi, Salim Roukos, and R. Todd Ward. 1994. A maximum entropy model for parsing. In Proceedings of the International Conference on Spoken Language Processing (ICSLP), pages 803–806.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Murat Saraclar</author>
<author>Michael Collins</author>
</authors>
<title>Corrective language modeling for large vocabulary ASR with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP),</booktitle>
<pages>749--752</pages>
<contexts>
<context citStr="Roark et al. (2004)" endWordPosition="396" position="2625" startWordPosition="393">ently proposed within these approaches is the perplexity of the LM (i.e., the log probability it assigns to some held-out data set). This paper explores alternative methods for language modeling, which complement the source-channel approach through discriminatively trained models. The language models we describe do not attempt to estimate a generative model P(w) over strings. Instead, they are trained on acoustic sequences with their transcriptions, in an attempt to directly optimize error-rate. Our work builds on previous work on language modeling using the perceptron algorithm, described in Roark et al. (2004). In particular, we explore conditional random field methods, as an alternative training method to the perceptron. We describe how these models can be trained over lattices that are the output from a baseline recognizer. We also give a number of experiments comparing the two approaches. The perceptron method gave a 1.3% absolute improvement in recognition error on the Switchboard domain; the CRF methods we describe give a further gain, the final absolute improvement being 1.8%. A central issue we focus on concerns feature selection. The number of distinct n-grams in our training data is close </context>
<context citStr="Roark et al. (2004)" endWordPosition="1381" position="8516" startWordPosition="1378"> decoding algorithm is a method for searching for the y that maximizes Eq. 1. 2.2 The Perceptron algorithm We now turn to methods for training the parameters α¯ of the model, given a set of training examples Inputs: Training examples (xi, yi) Initialization: Set α¯ = 0 Algorithm: Fort = 1 ... T, i = 1 ... N Calculate zi = argmaxz∈GEN(xi) Φ(xi, z) · α¯ If(zi =� yi) then α¯ = α¯ + Φ(xi, yi) − Φ(xi, zi) Output: Parameters α¯ Figure 1: A variant of the perceptron algorithm. (x1, y1) ... (xN, yN). This section describes the perceptron algorithm, which was previously applied to language modeling in Roark et al. (2004). The next section describes an alternative method, based on conditional random fields. The perceptron algorithm is shown in figure 1. At each training example (xi, yi), the current best-scoring hypothesis zi is found, and if it differs from the reference yi , then the cost of each feature2 is increased by the count of that feature in zi and decreased by the count of that feature in yi. The features in the model are updated, and the algorithm moves to the next utterance. After each pass over the training data, performance on a held-out data set is evaluated, and the parameterization with the b</context>
<context citStr="Roark et al. (2004)" endWordPosition="4149" position="23491" startWordPosition="4146">strings. In addition, averaged parameters need to be stored (see section 2.2). These parameters will replace the unaveraged parameters in D once training is completed. Note that the only n-gram features to be included in D at the end of the training process are those that occur in either a best scoring path zi or a minimum error path yi at some point during training. Thus the perceptron algorithm is in effect doing feature selection as a by-product of training. Given N training examples, and T passes over the training set, O(NT) n-grams will have non-zero weight after training. Experiments in Roark et al. (2004) suggest that the perceptron reaches optimal performance after a small number of training iterations, for example T = 1 or T = 2. Thus O(NT) can be very small compared to the full number of n-grams seen in all training lattices. In our experiments, the perceptron method chose around 1.4 million n-grams with non-zero weight. This compares to 43.65 million possible n-grams seen in the training data. This is a key contrast with conditional random fields, which optimize the parameters of a fixed feature set. Feature selection can be critical in our domain, as training and applying a discriminative</context>
<context citStr="Roark et al. (2004)" endWordPosition="5182" position="29339" startWordPosition="5179">pproach, but this is parallelizable, so that the computation can be divided among many processors. 4 Empirical Results We present empirical results on the Rich Transcription 2002 evaluation test set (rt02), which we used as our development set, as well as on the Rich Transcription 2003 Spring evaluation CTS test set (rt03). The rt02 set consists of 6081 sentences (63804 words) and has three subsets: Switchboard 1, Switchboard 2, Switchboard Cellular. The rt03 set consists of 9050 sentences (76083 words) and has two subsets: Switchboard and Fisher. We used the same training set as that used in Roark et al. (2004). The training set consists of 276726 transcribed utterances (3047805 words), with an additional 20854 utterances (249774 words) as held out data. For Iterations over training Figure 3: Word error rate on the rt02 eval set versus training iterations for CRF trials, contrasted with baseline recognizer performance and perceptron performance. Points are at every 20 iterations. Each point (x,y) is the WER at the iteration with the best objective function value in the interval (x-20,x]. each utterance, a weighted word-lattice was produced, representing alternative transcriptions, from the ASR syste</context>
<context citStr="Roark et al. (2004)" endWordPosition="5483" position="31274" startWordPosition="5480">aining set was partitioned into 28 sets, and baseline Katz backoff trigram models were built for each set by including only transcripts from the other 27 sets. Since language models are generally far more prone to overtrain than standard acoustic models, this goes a long way toward making the training conditions similar to testing conditions. There are three baselines against which we are comparing. The first is the ASR baseline, with no reweighting from a discriminatively trained n-gram model. The other two baselines are with perceptron-trained n-gram model re-weighting, and were reported in Roark et al. (2004). The first of these is for a pruned-lattice trained trigram model, which showed a reduction in word error rate (WER) of 1.3%, from 39.2% to 37.9% on rt02. The second is for a 1000-best list trained trigram model, which performed only marginally worse than the latticetrained perceptron, at 38.0% on rt02. 4.1 Perceptron feature set We use the perceptron-trained models as the starting point for our CRF algorithm: the feature set given to the CRF algorithm is the feature set selected by the perceptron algorithm; the feature weights are initialized to those of the averaged perceptron. Figure 3 sho</context>
</contexts>
<marker>Roark, Saraclar, Collins, 2004</marker>
<rawString>Brian Roark, Murat Saraclar, and Michael Collins. 2004. Corrective language modeling for large vocabulary ASR with the perceptron algorithm. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pages 749–752.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proc. HLT-NAACL,</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context citStr="Sha and Pereira, 2003" endWordPosition="1626" position="9957" startWordPosition="1623">riments. Say ¯αti is the parameter vector after the i’th example is processed on the t’th pass through the data in the algorithm in figure 1. Then the averaged parameters ¯αAV G are defined as ¯αAV G = Ei,t ¯αti/NT. Freund and Schapire (1999) originally proposed the averaged parameter method; it was shown to give substantial improvements in accuracy for tagging tasks in Collins (2002). 2.3 Conditional Random Fields Conditional Random Fields have been applied to NLP tasks such as parsing (Ratnaparkhi et al., 1994; Johnson et al., 1999), and tagging or segmentation tasks (Lafferty et al., 2001; Sha and Pereira, 2003; McCallum and Li, 2003; Pinto et al., 2003). CRFs use the parameters α¯ to define a conditional distribution over the members of GEN(x) for a given input x: p¯α(y|x) = Z(x, ¯α) exp (Φ(x, y) · ¯α) 1 where Z(x, ¯α) = y∈GEN(x) exp (Φ(x, y) · ¯α) is a normalization constant that depends on x and ¯α. Given these definitions, the log-likelihood of the training data under parameters α¯ is log p¯α(yi|xi) [Φ(xi, yi) · α¯ − log Z(xi, ¯α)] (2) 2Note that here lattice weights are interpreted as costs, which changes the sign in the algorithm presented in figure 1. N LL(¯α) = i=1 N = i=1 Following Johnson </context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In Proc. HLT-NAACL, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>M Weintraub</author>
</authors>
<title>Discriminitive language modeling.</title>
<date>1998</date>
<booktitle>In Proceedings of the 9th Hub-5 Conversational Speech Recognition Workshop.</booktitle>
<contexts>
<context citStr="Stolcke and Weintraub (1998)" endWordPosition="860" position="5549" startWordPosition="857">ate-of-the-art acoustic modeling, as seen, for example, at annual Switchboard evaluations, invariably includes some kind of discriminative training. Discriminative estimation of language models has also been proposed in recent years. Jelinek (1995) suggested an acoustic sensitive language model whose parameters 'Note also that in addition to concerns about training time, a language model with fewer features is likely to be considerably more efficient when decoding new utterances. are estimated by minimizing H(W |A), the expected uncertainty of the spoken text W, given the acoustic sequence A. Stolcke and Weintraub (1998) experimented with various discriminative approaches including MMIE with mixed results. This work was followed up with some success by Stolcke et al. (2000) where an “antiLM”, estimated from weighted N-best hypotheses of a baseline ASR system, was used with a negative weight in combination with the baseline LM. Chen et al. (2000) presented a method based on changing the trigram counts discriminatively, together with changing the lexicon to add new words. Kuo et al. (2002) used the generalized probabilistic descent algorithm to train relatively small language models which attempt to minimize st</context>
</contexts>
<marker>Stolcke, Weintraub, 1998</marker>
<rawString>A. Stolcke and M. Weintraub. 1998. Discriminitive language modeling. In Proceedings of the 9th Hub-5 Conversational Speech Recognition Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>H Bratt</author>
<author>J Butzberger</author>
<author>H Franco</author>
<author>V R Rao Gadde</author>
<author>M Plauche</author>
<author>C Richey</author>
<author>E Shriberg</author>
<author>K Sonmez</author>
<author>F Weng</author>
<author>J Zheng</author>
</authors>
<title>Hub-5 conversational speech transcription system.</title>
<date>2000</date>
<booktitle>The SRI</booktitle>
<contexts>
<context citStr="Stolcke et al. (2000)" endWordPosition="883" position="5705" startWordPosition="880"> estimation of language models has also been proposed in recent years. Jelinek (1995) suggested an acoustic sensitive language model whose parameters 'Note also that in addition to concerns about training time, a language model with fewer features is likely to be considerably more efficient when decoding new utterances. are estimated by minimizing H(W |A), the expected uncertainty of the spoken text W, given the acoustic sequence A. Stolcke and Weintraub (1998) experimented with various discriminative approaches including MMIE with mixed results. This work was followed up with some success by Stolcke et al. (2000) where an “antiLM”, estimated from weighted N-best hypotheses of a baseline ASR system, was used with a negative weight in combination with the baseline LM. Chen et al. (2000) presented a method based on changing the trigram counts discriminatively, together with changing the lexicon to add new words. Kuo et al. (2002) used the generalized probabilistic descent algorithm to train relatively small language models which attempt to minimize string error rate on the DARPA Communicator task. Banerjee et al. (2003) used a language model modification algorithm in the context of a reading tutor that l</context>
</contexts>
<marker>Stolcke, Bratt, Butzberger, Franco, Gadde, Plauche, Richey, Shriberg, Sonmez, Weng, Zheng, 2000</marker>
<rawString>A. Stolcke, H. Bratt, J. Butzberger, H. Franco, V. R. Rao Gadde, M. Plauche, C. Richey, E. Shriberg, K. Sonmez, F. Weng, and J. Zheng. 2000. The SRI March 2000 Hub-5 conversational speech transcription system. In Proceedings of the NIST Speech Transcription Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna Wallach</author>
</authors>
<title>Efficient training of conditional random fields. Master’s thesis,</title>
<date>2002</date>
<institution>University of Edinburgh.</institution>
<contexts>
<context citStr="Wallach, 2002" endWordPosition="1862" position="11278" startWordPosition="1861">the regularized objective function: [Φ(xi, yi) ·α¯ − log Z(xi, ¯α)] − ||¯α||2 2σ2 (3) The value u dictates the relative influence of the loglikelihood term vs. the prior, and is typically estimated using held-out data. The optimal parameters under this criterion are ¯α* = argmax¯α LLR(¯α). We use a limited memory variable metric method (Benson and Mor´e, 2002) to optimize LLR. There is a general implementation of this method in the Tao/PETSc software libraries (Balay et al., 2002; Benson et al., 2002). This technique has been shown to be very effective in a variety of NLP tasks (Malouf, 2002; Wallach, 2002). The main interface between the optimizer and the training data is a procedure which takes a parameter vector α¯ as input, and in turn returns LLR(¯α) as well as the gradient of LLR at ¯α. The derivative of the objective function with respect to a parameter αs at parameter values α¯ is ⎡ ⎤ αs Φs(Xi, Yi) − X P¯α(Y|Xi)Φs(Xi, Y)v − 2 (4) y∈GEN(xi) Note that LLR(¯α) is a convex function, so that there is a globally optimal solution and the optimization method will find it. The use of the Gaussian prior term ||¯α||2/2u2 in the objective function has been found to be useful in several NLP settings.</context>
</contexts>
<marker>Wallach, 2002</marker>
<rawString>Hanna Wallach. 2002. Efficient training of conditional random fields. Master’s thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P C Woodland</author>
<author>D Povey</author>
</authors>
<title>Large scale discriminative training for speech recognition.</title>
<date>2000</date>
<booktitle>In Proc. ISCAITRWASR2000,</booktitle>
<pages>7--16</pages>
<contexts>
<context citStr="Woodland and Povey (2000)" endWordPosition="748" position="4795" startWordPosition="745">n the future. Experimental results with n-gram models on 1000-best lists show a very small drop in accuracy compared to the use of lattices. This is encouraging, in that it suggests that models with more flexible features than n-gram models, which therefore cannot be efficiently used with lattices, may not be unduly harmed by their restriction to n-best lists. 1.1 Related Work Large vocabulary ASR has benefitted from discriminative estimation of Hidden Markov Model (HMM) parameters in the form of Maximum Mutual Information Estimation (MMIE) or Conditional Maximum Likelihood Estimation (CMLE). Woodland and Povey (2000) have shown the effectiveness of lattice-based MMIE/CMLE in challenging large scale ASR tasks such as Switchboard. In fact, state-of-the-art acoustic modeling, as seen, for example, at annual Switchboard evaluations, invariably includes some kind of discriminative training. Discriminative estimation of language models has also been proposed in recent years. Jelinek (1995) suggested an acoustic sensitive language model whose parameters 'Note also that in addition to concerns about training time, a language model with fewer features is likely to be considerably more efficient when decoding new u</context>
</contexts>
<marker>Woodland, Povey, 2000</marker>
<rawString>P.C. Woodland and D. Povey. 2000. Large scale discriminative training for speech recognition. In Proc. ISCAITRWASR2000, pages 7–16.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>