<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant confidence="0.001270" no="0">
<title confidence="0.991547">
Computing weakest readings
</title>
<author confidence="0.99733">
Alexander Koller Stefan Thater
</author>
<affiliation confidence="0.998946">
Cluster of Excellence Dept. of Computational Linguistics
Saarland University Saarland University
</affiliation>
<email confidence="0.969392">
koller@mmci.uni-saarland.de stth@coli.uni-saarland.de
</email>
<sectionHeader confidence="0.997069" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999867222222222">We present an efficient algorithm for computing the weakest readings of semantically ambiguous sentences. A corpus-based evaluation with a large-scale grammar shows that our algorithm reduces over 80% of sentences to one or two readings, in negligible runtime, and thus makes it possible to work with semantic representations derived by deep large-scale grammars.</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999964753623189">Over the past few years, there has been considerable progress in the ability of manually created large-scale grammars, such as the English Resource Grammar (ERG, Copestake and Flickinger (2000)) or the ParGram grammars (Butt et al., 2002), to parse wide-coverage text and assign it deep semantic representations. While applications should benefit from these very precise semantic representations, their usefulness is limited by the presence of semantic ambiguity: On the Rondane Treebank (Oepen et al., 2002), the ERG computes an average of several million semantic representations for each sentence, even when the syntactic analysis is fixed. The problem of appropriately selecting one of them to work with would ideally be solved by statistical methods (Higgins and Sadock, 2003) or knowledge-based inferences. However, no such approach has been worked out in sufficient detail to support the disambiguation of treebank sentences. As an alternative, Bos (2008) proposes to compute the weakest reading of each sentence and then use it instead of the “true” reading of the sentence. This is based on the observation that the readings of a semantically ambiguous sentence are partially ordered with respect to logical entailment, and the weakest readings – the minimal (least informative) readings with respect to this order – only express “safe” information that is common to all other readings as well. However, when a sentence has millions of readings, finding the weakest reading is a hard problem. It is of course completely infeasible to compute all readings and compare all pairs for entailment; but even the best known algorithm in the literature (Gabsdil and Striegnitz, 1999) is only an optimization of this basic strategy, and would take months to compute the weakest readings for the sentences in the Rondane Treebank. In this paper, we propose a new, efficient approach to the problem of computing weakest readings. We follow an underspecification approach to managing ambiguity: Rather than deriving all semantic representations from the syntactic analysis, we work with a single, compact underspecified semantic representation, from which the semantic representations can then be extracted by need. We then approximate entailment with a rewrite system that rewrites readings into logically weaker readings; the weakest readings are exactly those readings that cannot be rewritten into some other reading any more (the relative normal forms). We present an algorithm that computes the relative normal forms, and evaluate it on the underspecified descriptions that the ERG derives on a 624-sentence subcorpus of the Rondane Treebank. While the mean number of scope readings in the subcorpus is in the millions, our system computes on average 4.5 weakest readings for each sentence, in less than twenty milliseconds; over 80% of all sentences are reduced to at most two weakest readings. In other words, we make it feasible for the first time to build an application that uses the individual (weakest) semantic representations computed by the ERG, both in terms of the remaining ambiguity and in terms of performance. Our technique is not limited to the ERG, but should be applicable to other underspecification-based grammars as well. Technically, we use underspecified descriptions that are regular tree grammars derived from dominance graphs (Althaus et al., 2003; Koller et al.,</bodyText>
<page confidence="0.978527">
30
</page>
<note confidence="0.9432325">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 30–39,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.99995805">2008). We compute the weakest readings by intersecting these grammars with other grammars representing the rewrite rules. This approach can be used much more generally than just for the computation of weakest readings; we illustrate this by showing how a more general version of the redundancy elimination algorithm by Koller et al. (2008) can be seen as a special case of our construction. Thus our system can serve as a general framework for removing unintended readings from an underspecified representation. The paper is structured as follows. Section 2 starts by reviewing related work. We recall dominance graphs, regular tree grammars, and the basic ideas of underspecification in Section 3, before we show how to compute weakest readings (Section 4) and logical equivalences (Section 5). In Section 6, we define a weakening rewrite system for the ERG and evaluate it on the Rondane Treebank. Section 7 concludes and points to future work.</bodyText>
<sectionHeader confidence="0.999892" genericHeader="related work">
2 Related work
</sectionHeader>
<bodyText confidence="0.999729758620689">The idea of deriving a single approximative semantic representation for ambiguous sentences goes back to Hobbs (1983); however, Hobbs only works his algorithm out for a restricted class of quantifiers, and his representations can be weaker than our weakest readings. Rules that weaken one reading into another were popular in the 1990s underspecification literature (Reyle, 1995; Monz and de Rijke, 2001; van Deemter, 1996) because they simplify logical reasoning with underspecified representations. From a linguistic perspective, Kempson and Cormack (1981) even go so far as to claim that the weakest reading should be taken as the “basic” reading of a sentence, and the other readings only seen as pragmatically licensed special cases. The work presented here is related to other approaches that reduce the set of readings of an underspecified semantic representation (USR). Koller and Niehren (2000) showed how to strengthen a dominance constraint using information about anaphoric accessibility; later, Koller et al. (2008) presented and evaluated an algorithm for redundancy elimination, which removes readings from an USR based on logical equivalence. Our system generalizes the latter approach and applies it to a new inference problem (weakest readings) which they could not solve. This paper builds closely upon Koller and Thater (2010), which lays the formal groundwork for the</bodyText>
<figureCaption confidence="0.840201333333333">
Figure 1: A dominance graph describing the five
readings of the sentence “it is not the case that
every representative of a company saw a sample.”
work presented here.</figureCaption>
<bodyText confidence="0.999039">Here we go beyond that paper by applying a concrete implementation of our RTG construction for weakest readings to a real-world grammar, evaluating the system on practical inputs, and combining weakest readings with redundancy elimination.</bodyText>
<sectionHeader confidence="0.997171" genericHeader="method">
3 Underspecification
</sectionHeader>
<bodyText confidence="0.9998717">This section briefly reviews two formalisms for specifying sets of trees: dominance graphs and regular tree grammars. Both of these formalisms can be used to model scope ambiguities compactly by regarding the semantic representations of a sentence as trees. Some example trees are shown in Fig. 2. These trees can be read as simplified formulas of predicate logic, or as formulas involving generalized quantifiers (Barwise and Cooper, 1981). Formally, we assume a ranked signature E of tree constructors {f,g,a,...}, each of which is equipped with an arity ar(f) &gt; 0. We take a (finite constructor) tree t as a finite tree in which each node is labelled with a symbol of E, and the number of children of the node is exactly the arity of this symbol. For instance, the signature of the trees in Fig. 1 is {bx|2,ly|2,compz|0,...}. Finite constructor trees can be seen as ground terms over E that respect the arities. We write T(E) for the finite constructor trees over E.</bodyText>
<subsectionHeader confidence="0.999012">
3.1 Dominance graphs
</subsectionHeader>
<bodyText confidence="0.999167">A (labelled) dominance graph D (Althaus et al., 2003) is a directed graph that consists of a collection of trees called fragments, plus dominance edges relating nodes in different fragments. We distinguish the roots WD of the fragments from their holes, which are the unlabelled leaves. We write LD : WD —* E for the labeling function of D. The basic idea behind using dominance graphs to model scope underspecification is to specify the “semantic material” common to all readings as fragments, plus dominance relations between these fragments.</bodyText>
<figure confidence="0.995958333333333">
3z
∀x
3y
Â
1
4 2 3
5 6 7
compz
repr-ofx,z
8
seex,y
sampley
</figure>
<page confidence="0.998372">
31
</page>
<figureCaption confidence="0.999387">
Figure 2: The five configurations of the dominance graph in Fig. 1.
</figureCaption>
<figure confidence="0.998878042253521">
repr-ofx,z
repr-ofx,z
seex,y
compz
repr-ofx,z seex,y
(a) (b) (c)
(d)
(e)
¬
¬
¬
[+]
[-]
[+]
[-]
[+]
3y
[-]
3y
3z
3z
[-] [-]
Vx
[-] [-]
[-] [-]
3y
compz
sampley
[-] [-]
[-]
Vx
sampley
[+]
[-]
Vx
3z
compz
[+]
[-]
sampley
[+]
seex,y
[-]
[+] [+]
[-]
[+]
[-]
[+]
[-]
[-] [-]
repr-ofx,z
[-]
sampley
3z
¬
Vx
[-]
3y
[-]
seex,y
[+] [+]
3z
[+] [-]
¬
Vx
[-] [-]
3y
compz
[+]
sampley seex,y
compz repr-ofx,z
</figure>
<bodyText confidence="0.996955142857143">An example dominance graph D is shown in Fig.1. It represents the five readings of the sentence “it is not the case that every representative of a company saw a sample.” Each reading is encoded as a (labeled) configuration of the dominance graph, which can be obtained by “plugging” the tree fragments into each other, in a way that respects the dominance edges: The source node of each dominance edge must dominate (be an ancestor of) the target node in each configuration. The trees in Fig.2 are the five labeled configurations of the example graph.</bodyText>
<subsectionHeader confidence="0.999174">
3.2 Regular tree grammars
</subsectionHeader>
<bodyText confidence="0.999786615384615">Regular tree grammars (RTGs) are a general grammar formalism for describing languages of trees (Comon et al., 2007). An RTG is a 4-tuple G = (S,N,E,P), where N and E are nonterminal and terminal alphabets, S ∈ N is the start symbol, and P is a finite set of production rules. Unlike in context-free string grammars (which look superficially the same), the terminal symbols are tree constructors from E. The production rules are of the form A → t, where A is a nonterminal and t is a tree from T(E ∪ N); nonterminals count as having arity zero, i.e. they must label leaves. A derivation starts with a tree containing a single node labeled with S. Then in each step of the derivation, some leaf u which is labelled with a nonterminal A is expanded with a rule A → t; this results in a new tree in which u has been replaced by t, and the derivation proceeds with this new tree. The language L(G) generated by the grammar is the set of all trees in T(E) that can be derived in this way. Fig. 3 shows an RTG as an example. This grammar uses sets of root names from D as nonterminal symbols, and generates exactly the five configurations of the graph in Fig. 1. The languages that can be accepted by regular tree grammars are called regular tree languages (RTLs), and regular tree grammars are equivalent to finite tree automata, which are defined essentially like the well-known finite string automata, except that they assign states to the nodes in a tree rather than the positions in a string.</bodyText>
<equation confidence="0.770652307692308">
{1,2,3,4,5,6,7,8} → ¬({2,3,4,5,6,7,8})
{2,3,4,5,6,7,8} → ∀x({4,5,6},{3,7,8})
{2,3,4,5,6,7,8} → ∃y({7},{2,4,5,6,8})
{2,3,4,5,6,7,8} → ∃,({5},{2,3,6,7,8})
{2,4,5,6,8} → ∀x({4,5,6},{8})
 |∃,({5},{2,6,8})
{2,3,6,7,8} → ∀x({6},{3,7,8})
 |∃y({7},{2,6,8})
{2,6,8} → ∀x({6},{8})
{3,7,8} → ∃y({7},{8})
{4,5,6} → ∃,({5},{6})
{5} → comp, {7} → sampley
{6} → repr-ofx,, {8} → seex,y
</equation>
<figureCaption confidence="0.971696">
Figure 3: A regular tree grammar that generates
the five trees in Fig. 2.
</figureCaption>
<bodyText confidence="0.999758333333333">Regular tree languages enjoy many of the closure properties of regular string languages. In particular, we will later exploit that RTLs are closed under intersection and complement.</bodyText>
<subsectionHeader confidence="0.9988">
3.3 Dominance graphs as RTGs
</subsectionHeader>
<bodyText confidence="0.998767153846154">An important class of dominance graphs are hypernormally connected (hnc) dominance graphs (Koller et al., 2003). The precise definition of hnc graphs is not important here, but note that virtually all underspecified descriptions that are produced by current grammars are hypernormally connected (Flickinger et al., 2005), and we will restrict ourselves to hnc graphs for the rest of the paper. Every hypernormally connected dominance graph D can be automatically translated into an equivalent RTG GD that generates exactly the same configurations (Koller et al., 2008); the RTG in Fig. 3 is an example. The nonterminals of GD are always hnc subgraphs of D. In the worst case, GD can be exponentially bigger than D, but in practice it turns out that the grammar size remains manageable: even the RTG for the most ambiguous sentence in the Rondane Treebank, which has about 4.5 × 1012 scope readings, has only about 75 000 rules and can be computed in a few seconds.</bodyText>
<page confidence="0.996056">
32
</page>
<sectionHeader confidence="0.973421" genericHeader="method">
4 Computing weakest readings
</sectionHeader>
<bodyText confidence="0.999984285714286">Now we are ready to talk about computing the weakest readings of a hypernormally connected dominance graph. We will first explain how we approximate logical weakening with rewrite systems. We will then discuss how weakest readings can be computed efficiently as the relative normal forms of these rewrite systems.</bodyText>
<subsectionHeader confidence="0.998851">
4.1 Weakening rewrite systems
</subsectionHeader>
<bodyText confidence="0.999764717948718">The different readings of a sentence with a scope ambiguity are not a random collection of formulas; they are partially ordered with respect to logical entailment, and are structurally related in a way that allows us to model this entailment relation with simpler technical means. To illustrate this, consider the five configurations in Fig. 2. The formula represented by (d) logically entails (c); we say that (c) is a weaker reading than (d) because it is satisfied by more models. Similar entailment relations hold between (d) and (e), (e) and (b), and so on (see also Fig. 5). We can define the weakest readings of the dominance graph as the minimal elements of the entailment order; in the example, these are (b) and (c). Weakest readings capture “safe” information in that whichever reading of the sentence the speaker had in mind, any model of this reading also satisfies at least one weakest reading; in the absence of convincing disambiguation methods, they can therefore serve as a practical approximation of the intended meaning of the sentence. A naive algorithm for computing weakest readings would explicitly compute the entailment order, by running a theorem prover on each pair of configurations, and then pick out the minimal elements. But this algorithm is quadratic in the number of configurations, and therefore impractically slow for real-life sentences. Here we develop a fast algorithm for this problem. The fundamental insight we exploit is that entailment among the configurations of a dominance graph can be approximated with rewriting rules (Baader and Nipkow, 1999). Consider the relation between (d) and (c). We can explain that (d) entails (c) by observing that (c) can be built from (d) by exchanging the positions of the adjacent quantifiers ∀x and ∃y; more precisely, by applying the following rewrite rule:</bodyText>
<equation confidence="0.934899">
[−] ∀x(Q,∃y(P,R)) → ∃y(P,∀x(Q,R)) (1)
</equation>
<bodyText confidence="0.9998412">The body of the rule specifies that an occurrence of ∀x which is the direct parent of an occurrence of ∃y may change positions with it; the subformulas P, Q, and R must be copied appropriately. The annotation [−] specifies that we must only apply the rule to subformulas in negative logical polarity: If the quantifiers in (d) were not in the scope of a negation, then applying the rule would actually make the formula stronger. We say that the rule (1) is logically sound because applying it to a subformula with the correct polarity of some configuration t always makes the result t0 logically weaker than t. We formalize these rewrite systems as follows. We assume a finite annotation alphabet Ann with a special starting annotation a0 ∈ Ann; in the example, we had Ann = {+,−} and a0 = +. We also assume an annotatorfunction ann : Ann×E×N → Ann. The function ann can be used to traverse a tree top-down and compute the annotation of each node from the annotation of its parent: Its first argument is the annotation and its second argument the node label of the parent, and the third argument is the position of the child among the parent’s children. In our example, the annotator ann models logical polarity by mapping, for instance, etc.</bodyText>
<equation confidence="0.9942245">
ann(+,∃z,1) = ann(+,∃z,2) = ann(+,∃y,2) = +,
ann(−,∃z,1) = ann(−,∃z,2) = ann(+,∀x,1) = −,
</equation>
<bodyText confidence="0.9998044375">We have labelled each node of the configurations in Fig.1 with the annotations that are computed in this way. Now we can define an annotated rewrite system R to be a finite set of pairs (a,r) where a is an annotation and r is an ordinary rewrite rule. The rule (1) above is an example of an annotated rewrite rule with a = −. A rewrite rule (a,r) can be applied at the node u of a tree t if ann assigns the annotation a to u and r is applicable at u as usual. The rule then rewrites t as described above. In other words, annotated rewrite systems are rewrite systems where rule applications are restricted to subtrees with specific annotations. We write t →R t0 if some rule of R can be applied at a node of t, and the result of rewriting is t0. The rewrite system R is called linear if every variable that occurs on the left-hand side of a rule occurs on its right-hand side exactly once.</bodyText>
<page confidence="0.996083">
33
</page>
<subsectionHeader confidence="0.975178">
4.2 Relative normal forms
</subsectionHeader>
<bodyText confidence="0.999935954545455">The rewrite steps of a sound weakening rewrite system are related to the entailment order: Because every rewrite step transforms a reading into a weaker reading, an actual weakest readings must be such that there is no other configuration into which it can be rewritten. The converse is not always true, i.e. there can be non-rewritable configurations that are not weakest readings, but we will see in Section 6 that this approximation is good enough for practical use. So one way to solve the problem of computing weakest readings is to find readings that cannot be rewritten further. One class of configurations that “cannot be rewritten” with a rewrite system R is the set of normal forms of R, i.e. those configurations to which no rule in R can be applied. In our example, (b) and (c) are indeed normal forms with respect to a rewrite system that consists only of the rule (1). However, this is not exactly what we need here. Consider a rewrite system that also contains the following annotated rewrite rule, which is also sound for logical entailment:</bodyText>
<equation confidence="0.882641">
[+] ¬(∃z(P,Q)) → ∃z(P,¬(Q)) , (2)
</equation>
<bodyText confidence="0.999951">This rule would allow us to rewrite the configuration (c) into the tree ∃z(compz,¬(∃y(sampley,∀x(repr−ofx,z,seex,y)))). But this is no longer a configuration of the graph. If we were to equate weakest readings with normal forms, we would erroneously classify (c) as not being a weakest reading. The correct concept for characterizing weakest readings in terms of rewriting is that of a relative normal form. We define a configuration t of a dominance graph D to be a R-relative normal form of (the configurations of) D iff there is no other configuration t0 of D such that t →R t0. These are the configurations that can’t be weakened further without obtaining a tree that is no longer a configuration of D. In other words, if R approximates entailment, then the R-relative normal forms approximate the weakest readings.</bodyText>
<subsectionHeader confidence="0.999397">
4.3 Computing relative normal forms
</subsectionHeader>
<bodyText confidence="0.999196525">We now show how the relative normal forms of a dominance graph can be computed efficiently. For lack of space, we only sketch the construction and omit all proofs. Details can be found in Koller and Thater (2010). The key idea of the construction is to represent the relation →R in terms of a context tree transducer M, and characterize the relative normal forms of a tree language L in terms of the pre-image of L under M. Like ordinary regular tree transducers (Comon et al., 2007), context tree transducers read an input tree, assigning states to the nodes, while emitting an output tree. But while ordinary transducers read the input tree symbol by symbol, a context tree transducer can read multiple symbols at once. In this way, they are equivalent to the extended left-hand side transducers of Graehl et al. (2008). We will now define context tree transducers. Let Σ be a ranked signature, and let Xm be a set of m variables. We write Con(m)(Σ) for the contexts with m holes, i.e. those trees in T(Σ∪Xm) in which each element of Xm occurs exactly once, and always as a leaf. If C ∈ Con(m)(Σ), then C[t1,...,tm] = C[t1/x1,...,tm/xm], where x1,...,xm are the variables from left to right. A (top-down) context tree transducer from Σ to Δ is a 5-tuple M = (Q,Σ,Δ,q0,δ). Σ and Δ are ranked signatures, Q is a finite set of states, and q0 ∈ Q is the start state. δ is a finite set of transition rules of the form q(C[x1,...,xn]) → D[q1(xi1),...,qm(xim)], where C ∈ Con(n)(Σ) and D ∈ Con(m)(Δ). If t ∈ T(Σ ∪ Δ ∪ Q), then we say that M derives t0 in one step from t, t →M t0, if t is of the form C0[q(C[t1,...,tn])] for some C0 ∈ Con(1)(Σ), t0 is of the form C0[D[q1(ti1),...,qm(tim)]], and there is a rule q(C[x1,...,xn]) → D[q1(xi1),...,qm(xim)] in δ. The derivation relation →∗M is the reflexive, transitive closure of →M. The translation relation τM of M is</bodyText>
<equation confidence="0.979848">
τM ={(t,t0)  |t ∈T(Σ)and t0 ∈T(Δ)and q0(t)→∗ t0}.
</equation>
<bodyText confidence="0.999186133333334">For each linear annotated rewrite system R, we can now build a context tree transducer MR such that t →R t0 iff (t,t0) ∈ τMR. The idea is that MR traverses t from the root to the leaves, keeping track of the current annotation in its state. MR can nondeterministically choose to either copy the current symbol to the output tree unchanged, or to apply a rewrite rule from R. The rules are built in such a way that in each run, exactly one rewrite rule must be applied. We achieve this as follows. MR takes as its states the set {¯q} ∪ {qa  |a ∈ Ann} and as its start state the state qa0. If MR reads a node u in state qa, this means that the annotator assigns annotation a to u and MR will rewrite a subtree at or below u.</bodyText>
<page confidence="0.997929">
34
</page>
<bodyText confidence="0.87612348">If MR reads u in state ¯q, this means that MR will copy the subtree below u unchanged because the rewriting has taken place elsewhere. Thus MR has three types of rewrite rules. First, for any f ∈ E, we have a rule ¯q(f (x1,...,xn)) → f(¯q(x1),..., ¯q(xn)). Second, for any f and 1 ≤ i ≤ n, we have a rule qa(f (x1,...,xn)) → f ( ¯q(x1),...,qann(a,f,i)(xi),..., ¯q(xn)), which nondeterministically chooses under which child the rewriting should take place, and assigns it the correct annotation. Finally, we have a rule qa(C[x1,...,xn]) → C0[¯q(xi1),..., ¯q(xin)] for every rewrite rule C[x1,...,xn] → C0[xi1,...,xin] with annotation a in R. Now let’s put the different parts together. We know that for each hnc dominance graph D, there is a regular tree grammar GD such that L(GD) is the set of configurations of D. Furthermore, the preimage i−1 M (L) = {t  |exists t0 ∈ L with (t,t0) ∈ iM} of a regular tree language L is also regular (Koller and Thater, 2010) if M is linear, and regular tree languages are closed under intersection and complement (Comon et al., 2007). So we can compute another RTG G0 such that</bodyText>
<equation confidence="0.9972335">
L(G0) = L(GD) ∩ i−1
MR (L(GD)).
</equation>
<bodyText confidence="0.9939306875">L(G0) consists of the members of L(GD) which cannot be rewritten by MR into members of L(GD); that is, L(G0) is exactly the set of R-relative normal forms of D. In general, the complement construction requires exponential time in the size of MR and GD. However, it can be shown that if the rules in R have at most depth two and GD is deterministic, then the entire above construction can be computed in time O(|GD |· |R|) (Koller and Thater, 2010). In other words, we have shown how to compute the weakest readings of a hypernormally connected dominance graph D, as approximated by a weakening rewrite system R, in time linear in the size of GD and linear in the size of R. This is a dramatic improvement over the best previous algorithm, which was quadratic in |conf(D)|.</bodyText>
<subsectionHeader confidence="0.999515">
4.4 An example
</subsectionHeader>
<bodyText confidence="0.9975875">Consider an annotated rewrite system that contains rule (1) plus the following rewrite rule:</bodyText>
<equation confidence="0.879691">
[−] ∃z(P,∀x(Q,R)) → ∀x(∃z(P,Q),R) (3)
</equation>
<bodyText confidence="0.999387">This rewrite system translates into a top-down context tree transducer MR with the following transition rules, omitting most rules of the first two</bodyText>
<equation confidence="0.994342">
{1,2,3,4,5,6,7,8}F → ¬({2,3,4,5,6,7,8}F)
{2,3,4,5,6,7,8}F → ∃y({7}{¯q},{2,4,5,6,8}F)
 |∃z({5}{¯q},{2,3,6,7,8}F)
{2,3,6,7,8}F → ∃y({7}{¯q},∀x({6}{¯q},{8}{¯q}))
{2,4,5,6,8}F → ∀x({4,5,6}{¯q},{8}{¯q})
{4,5,6}{¯q} → ∃z({5}{¯q},{6}{¯q})
{5}{¯q} → compz {6}{¯q} → repr-ofx,z
{7}{¯q} → sampley {8}{¯q} → seex,y
</equation>
<figureCaption confidence="0.962499">
Figure 4: RTG for the weakest readings of Fig. 1.
types for lack of space.
</figureCaption>
<equation confidence="0.999979">
q−(∀x(x1,∃y(x2,x3))) → ∃y(¯q(x2),∀x(¯q(x1), ¯q(x3)))
q−(∃y(x1,∀x(x2,x3))) → ∀x(∃y(¯q(x1), ¯q(x2)), ¯q(x3))
¯q(¬(x1)) → ¬(¯q(x1))
q+(¬(x1)) → ¬(q−(x1))
¯q(∀x(x1,x2)) → ∀x(¯q(x1), ¯q(x2))
q+(∀x(x1,x2)) → ∀x(¯q(x1),q+(x2))
q+(∀x(x1,x2)) → ∀x(q−(x1), ¯q(x2)) ...
</equation>
<bodyText confidence="0.9997568">The grammar G0 for the relative normal forms is shown in Fig. 4 (omitting rules that involve unproductive nonterminals). We obtain it by starting with the example grammar GD in Fig. 3; then computing a deterministic RTG GR for i−1 and then intersecting the complement of GR with GD.</bodyText>
<equation confidence="0.393826">
MR (L(GD));
</equation>
<bodyText confidence="0.999831625">The nonterminals of G0 are subgraphs of D, marked either with a set of states of MR or the symbol F, indicating that GR had no production rule for a given left-hand side. The start symbol of G0 is marked with F because G0 should only generate trees that GR cannot generate. As expected, G0 generates precisely two trees, namely (b) and (c).</bodyText>
<sectionHeader confidence="0.997882" genericHeader="method">
5 Redundancy elimination, revisited
</sectionHeader>
<bodyText confidence="0.996160285714286">The construction we just carried out – characterize the configurations we find interesting as the relative normal forms of an annotated rewrite system R, translate it into a transducer MR, and intersect conf(D) with the complement of the pre-image under MR – is more generally useful than just for the computation of weakest readings. We illustrate this on the problem of redundancy elimination (Vestre, 1991; Chaves, 2003; Koller et al., 2008) by showing how a variant of the algorithm of Koller et al. (2008) falls out of our technique as a special case. Redundancy elimination is the problem of computing, from a dominance graph D, another dominance graph D0 such that conf(D0) ⊆ conf(D) and every formula in conf(D) is logically equivalent to some formula in conf(D0).</bodyText>
<page confidence="0.990196">
35
</page>
<figure confidence="0.9987794">
(s)
(a) -∃y∃z∀x
(d) -∃z∀x∃y
(3)
(1)
(b) -∃y∀x∃z
(e) -∀x(∃z,∃y)
(1)
(3)
(c) -∃z∃y∀x
</figure>
<bodyText confidence="0.974624">We can approximate logical equivalence using a finite system of equations such as indicating that ∃y and ∃z can be permuted without changing the models of the formula.</bodyText>
<equation confidence="0.998995">
∃y(P,∃z(Q,R)) = ∃z(Q,∃y(P,R)), (4)
</equation>
<bodyText confidence="0.999897909090909">Following the approach of Section 4, we can solve the redundancy elimination problem by transforming the equation system into a rewrite system R such that t →R t0 implies that t and t0 are equivalent. To this end, we assume an arbitrary linear order &lt; on E, and orient all equations into rewrite rules that respect this order. If we assume ∃y &lt; ∃z, the example rule (4) translates into the annotated rewrite rules for all annotations a ∈ Ann; logical equivalence is not sensitive to the annotation.</bodyText>
<equation confidence="0.9849">
[a] ∃z(P,∃y(Q,R)) → ∃y(Q,∃z(P,R)) (5)
</equation>
<bodyText confidence="0.99978415625">Finally, we can compute the relative normal forms of conf(D) under this rewrite system as above. The result will be an RTG G0 describing a subset of conf(D). Every tree t in conf(D) that is not in L(G0) is equivalent to some tree t0 in L(G0), because if t could not be rewritten into such a t0, then t would be in relative normal form. That is, the algorithm solves the redundancy elimination problem. Furthermore, if the oriented rewrite system is confluent (Baader and Nipkow, 1999), no two trees in L(G0) will be equivalent to each other, i.e.we achieve complete reduction in the sense of Koller et al.(2008). This solution shares much with that of Koller et al.(2008), in that we perform redundancy elimination by intersecting tree grammars. However, the construction we present here is much more general: The algorithmic foundation for redundancy elimination is now exactly the same as that for weakest readings, we only have to use an equivalencepreserving rewrite system instead of a weakening one. This new formal clarity also simplifies the specification of certain equations, as we will see in Section 6. In addition, we can now combine the weakening rules (1), (3), and (5) into a single rewrite system, and then construct a tree grammar for the relative normal forms of the combined system. This algorithm performs redundancy elimination and computes weakest readings at the same time, and in our example retains only a single configuration, namely</bodyText>
<figureCaption confidence="0.962118333333333">
Figure 5: Structure of the configuration set of Fig. 1
in terms of rewriting.
(b); the configuration (c) is rejected because it can
be rewritten to (a) with (5). The graph in Fig. 5 il-
lustrates how the equivalence and weakening rules
conspire to exclude all other configurations.
</figureCaption>
<sectionHeader confidence="0.994124" genericHeader="evaluation and result">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999916944444445">In this section, we evaluate the effectiveness and efficiency of our weakest readings algorithm on a treebank. We compute RTGs for all sentences in the treebank and measure how many weakest readings remain after the intersection, and how much time this computation takes. Resources. For our experiment, we use the Rondane treebank (version of January 2006), a “Redwoods style” (Oepen et al., 2002) treebank containing underspecified representations (USRs) in the MRS formalism (Copestake et al., 2005) for sentences from the tourism domain. Our implementation of the relative normal forms algorithm is based on Utool (Koller and Thater, 2005), which (among other things) can translate a large class of MRS descriptions into hypernormally connected dominance graphs and further into RTGs as in Section 3. The implementation exploits certain properties of RTGs computed from dominance graphs to maximize efficiency. We will make this implementation publically available as part of the next Utool release. We use Utool to automatically translate the 999 MRS descriptions for which this is possible into RTGs. To simplify the specification of the rewrite systems, we restrict ourselves to the subcorpus in which all scope-taking operators (labels with arity &gt; 0) occur at least ten times. This subset contains 624 dominance graphs. We refer to this subset as “RON10.” Signature and annotations. For each dominance graph D that we obtain by converting an MRS description, we take GD as a grammar over the signature E = { fu  |u ∈ WD, f = LD(u)}. That is, we distinguish possible different occurrences of the same symbol in D by marking each occurrence with the name of the node.</bodyText>
<page confidence="0.995809">
36
</page>
<bodyText confidence="0.982869161290322">This makes GD a deterministic grammar. We then specify an annotator over E that assigns polarities for the weakening rewrite system. We distinguish three polarities: + for positive occurrences, − for negative occurrences (as in predicate logic), and ⊥ for contexts in which a weakening rule neither weakens or strengthens the entire formula. The starting annotation is +. Finally, we need to decide upon each scopetaking operator’s effects on these annotations. To this end, we build upon Barwise and Cooper’s (1981) classification of the monotonicity properties of determiners. A determiner is upward (downward) monotonic if making the denotation of the determiner’s argument bigger (smaller) makes the sentence logically weaker. For instance, every is downward monotonic in its first argument and upward monotonic in its second argument, i.e.every girl kissed a boy entails every blond girl kissed someone. Thus ann(everyu,a,1) = −a and ann(everyu,a,2) = a (where u is a node name as above). There are also determiners with nonmonotonic argument positions, which assign the annotation ⊥ to this argument. Negation reverses positive and negative polarity, and all other nonquantifiers simply pass on their annotation to the arguments.Weakest readings. We use the following weakening rewrite system for our experiment, where i ∈ {1,2}:</bodyText>
<listItem confidence="0.610023166666667">1. [+] (E/i,D/1), (D/2,D/1) 2. [+] (E/i,P/1), (D/2,P/1) 3. [+] (E/i,A/2), (D/1,A/2) 4. [+] (A/2,N/1) 5. [+] (N/1,E/i), (N/1,D/2) 6. [+] (E/i,M/1), (D/1,M/1)</listItem>
<bodyText confidence="0.999437022222222">Here the symbols E, D, etc. stand for classes of labels in E, and a rule schema [a] (C/i,C0/k) is to be read as shorthand for a set of rewrite rules which rearrange a tree where the i-th child of a symbol from C is a symbol from C0 into a tree where the symbol from C becomes the k-th child of the symbol from C0. For example, because we have allu ∈ A and notv ∈ N, Schema 4 licenses the following annotated rewrite rule: [+] allu(P,notv(Q)) → notv(allu(P,Q)). We write E and D for existential and definite determiners. P stands for proper names and pronouns, A stands for universal determiners like all and each, N for the negation not, and M for modal operators like can or would. M also includes intensional verbs like have to and want. Notice that while the reverse rules are applicable in negative polarities, no rules are applicable in polarity ⊥. Rule schema 1 states, for instance, that the specific (wide-scope) reading of the indefinite in the president of a company is logically stronger than the reading in which a company is within the restriction of the definite determiner. The schema is intuitively plausible, and it can also be proved to be logically sound if we make the standard assumption that the definite determiner the means “exactly one” (Montague, 1974). A similar argument applies to rule schema 2. Rule schema 3 encodes the classical entailment (1). Schema 4 is similar to the rule (2). Notice that it is not, strictly speaking, logically sound; however, because strong determiners like all or every carry a presupposition that their restrictions have a non-empty denotation (Lasersohn, 1993), the schema becomes sound for all instances that can be expressed in natural language. Similar arguments apply to rule schemas 5 and 6, which are potentially unsound for subtle reasons involving the logical interpretation of intensional expressions. However, these cases of unsoundness did not occur in our test corpus. Redundancy elimination. In addition, we assume the following equation system for redundancy elimination for i, j ∈ {1,2} and k ∈ N (again written in an analogous shorthand as above):</bodyText>
<listItem confidence="0.54567825">7. E/i = E/ j 8. D/1 = E/i, E/i = D/1 9. D/1 = D/1 10. E/k = P/2</listItem>
<bodyText confidence="0.999871727272727">These rule schemata state that permuting existential determiners with each other is an equivalence transformation, and so is permuting definite determiners with existential and definite determiners if one determiner is the second argument (in the scope) of a definite. Schema 10 states that proper names and pronouns, which the ERG analyzes as scope-bearing operators, can permute with any other label. We orient these equalities into rewrite rules by ordering symbols in P before symbols that are not applicable in 44 sentences.</bodyText>
<page confidence="0.997078">
37
</page>
<table confidence="0.998333">
All KRT08 RE RE+WR
8.5% 23.4% 34.9% 66.7%
20.5% 40.9% 57.9% 80.6%
3.2M 7603.1 119.0 4.5
25 4 2 1
8.1s 9.4s 8.7s 9.1s
</table>
<bodyText confidence="0.9890325">Nevertheless, where these rules do apply, they have a noticeable effect: Without them, the mean number of configurations in RON10 after RE+WR increases to 12.5.</bodyText>
<sectionHeader confidence="0.86619" genericHeader="conclusion">
7 Conclusion
</sectionHeader>
<figure confidence="0.9893024">
#conf = 1
#conf &lt; 2
avg(#conf)
med(#conf)
runtime
</figure>
<figureCaption confidence="0.983926">
Figure 6: Analysis of the numbers of configurations
in RON10.
</figureCaption>
<bodyText confidence="0.999457648648649">in P, and otherwise ordering a symbol fu before a symbol gv if u &lt; v by comparison of the (arbitrary) node names. Results. We used these rewrite systems to compute, for each USR in RON10, the number of all configurations, the number of configurations that remain after redundancy elimination, and the number of weakest readings (i.e., the relative normal forms of the combined equivalence and weakening rewrite systems). The results are summarized in Fig. 6. By computing weakest readings (WR), we reduce the ambiguity of over 80% of all sentences to one or two readings; this is a clear improvement even over the results of the redundancy elimination (RE). Computing weakest readings reduces the mean number of readings from several million to 4.5, and improves over the RE results by a factor of 30. Notice that the RE algorithm from Section 5 is itself an improvement over Koller et al.’s (2008) system (“KRT08” in the table), which could not process the rule schema 10. Finally, computing the weakest readings takes only a tiny amount of extra runtime compared to the RE elimination or even the computation of the RTGs (reported as the runtime for “All”).1 This remains true on the entire Rondane corpus (although the reduction factor is lower because we have no rules for the rare scope-bearers): RE+WR computation takes 32 seconds, compared to 30 seconds for RE. In other words, our algorithm brings the semantic ambiguity in the Rondane Treebank down to practically useful levels at a mean runtime investment of a few milliseconds per sentence. It is interesting to note how the different rule schemas contribute to this reduction. While the instances of Schemata 1 and 2 are applicable in 340 sentences, the other schemas 3–6 together are only</bodyText>
<footnote confidence="0.946767666666667">
1Runtimes were measured on an Intel Core 2 Duo CPU
at 2.8 GHz, under MacOS X 10.5.6 and Apple Java 1.5.0_16,
after allowing the JVM to just-in-time compile the bytecode.
</footnote>
<bodyText confidence="0.999943186046512">In this paper, we have shown how to compute the weakest readings of a dominance graph, characterized by an annotated rewrite system. Evaluating our algorithm on a subcorpus of the Rondane Treebank, we reduced the mean number of configurations of a sentence from several million to 4.5, in negligible runtime. Our algorithm can be applied to other problems in which an underspecified representation is to be disambiguated, as long as the remaining readings can be characterized as the relative normal forms of a linear annotated rewrite system. We illustrated this for the case of redundancy elimination. The algorithm presented here makes it possible, for the first time, to derive a single meaningful semantic representation from the syntactic analysis of a deep grammar on a large scale. In the future, it will be interesting to explore how these semantic representations can be used in applications. For instance, it seems straightforward to adapt MacCartney and Manning’s (2008) “natural logic”-based Textual Entailment system, because our annotator already computes the polarities needed for their monotonicity inferences. We could then perform such inferences on (cleaner) semantic representations, rather than strings (as they do). On the other hand, it may be possible to reduce the set of readings even further. We retain more readings than necessary in many treebank sentences because the combined weakening and equivalence rewrite system is not confluent, and therefore may not recognize a logical relation between two configurations. The rewrite system could be made more powerful by running the Knuth-Bendix completion algorithm (Knuth and Bendix, 1970). Exploring the practical tradeoff between the further reduction in the number of remaining configurations and the increase in complexity of the rewrite system and the RTG would be worthwhile. Acknowledgments. We are indebted to Joachim Niehren, who pointed out a crucial simplification in the algorithm to us. We also thank our reviewers for their constructive comments.</bodyText>
<page confidence="0.998789">
38
</page>
<sectionHeader confidence="0.99835" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999898261682243">
E. Althaus, D. Duchier, A. Koller, K. Mehlhorn,
J. Niehren, and S. Thiel. 2003. An efficient graph
algorithm for dominance constraints. Journal of Al-
gorithms, 48:194–219.
F. Baader and T. Nipkow. 1999. Term rewriting and all
that. Cambridge University Press.
J. Barwise and R. Cooper. 1981. Generalized quanti-
fiers and natural language. Linguistics and Philoso-
phy, 4:159–219.
J. Bos. 2008. Let’s not argue about semantics. In
Proceedings of the 6th international conference on
Language Resources and Evaluation (LREC 2008).
M. Butt, H. Dyvik, T. Holloway King, H. Masuichi,
and C. Rohrer. 2002. The parallel grammar
project. In Proceedings of COLING-2002 Workshop
on Grammar Engineering and Evaluation.
R. P. Chaves. 2003. Non-redundant scope disambigua-
tion in underspecified semantics. In Proceedings of
the 8th ESSLLI Student Session.
H. Comon, M. Dauchet, R. Gilleron, C. Löding,
F. Jacquemard, D. Lugiez, S. Tison, and M. Tom-
masi. 2007. Tree automata techniques and appli-
cations. Available on: http://www.grappa.
univ-lille3.fr/tata.
A. Copestake and D. Flickinger. 2000. An open-
source grammar development environment and
broad-coverage english grammar using HPSG. In
Proceedings of the 2nd International Conference on
Language Resources and Evaluation (LREC).
A. Copestake, D. Flickinger, C. Pollard, and I. Sag.
2005. Minimal recursion semantics: An introduc-
tion. Journal of Language and Computation.
D. Flickinger, A. Koller, and S. Thater. 2005. A new
well-formedness criterion for semantics debugging.
In Proceedings of the 12th International Conference
on HPSG, Lisbon.
M. Gabsdil and K. Striegnitz. 1999. Classifying scope
ambiguities. In Proceedings of the First Intl. Work-
shop on Inference in Computational Semantics.
J. Graehl, K. Knight, and J. May. 2008. Training tree
transducers. Computational Linguistics, 34(3):391–
427.
D. Higgins and J. Sadock. 2003. A machine learning
approach to modeling scope preferences. Computa-
tional Linguistics, 29(1).
J. Hobbs. 1983. An improper treatment of quantifi-
cation in ordinary English. In Proceedings of the
21st Annual Meeting of the Association for Compu-
tational Linguistics (ACL’83).
R. Kempson and A. Cormack. 1981. Ambiguity and
quantification. Linguistics and Philosophy, 4:259–
309.
D. Knuth and P. Bendix. 1970. Simple word problems
in universal algebras. In J. Leech, editor, Computa-
tional Problems in Abstract Algebra, pages 263–297.
Pergamon Press, Oxford.
A. Koller and J. Niehren. 2000. On underspecified
processing of dynamic semantics. In Proceedings of
the 18th International Conference on Computational
Linguistics (COLING-2000).
A. Koller and S. Thater. 2005. Efficient solving and ex-
ploration of scope ambiguities. In ACL-05 Demon-
stration Notes, Ann Arbor.
A. Koller and S. Thater. 2010. Computing relative nor-
mal forms in regular tree languages. In Proceedings
of the 21st International Conference on Rewriting
Techniques and Applications (RTA).
A. Koller, J. Niehren, and S. Thater. 2003. Bridg-
ing the gap between underspecification formalisms:
Hole semantics as dominance constraints. In Pro-
ceedings of the 10th EACL.
A. Koller, M. Regneri, and S. Thater. 2008. Regular
tree grammars as a formalism for scope underspeci-
fication. In Proceedings of ACL-08: HLT.
P. Lasersohn. 1993. Existence presuppositions and
background knowledge. Journal of Semantics,
10:113–122.
B. MacCartney and C. Manning. 2008. Modeling
semantic containment and exclusion in natural lan-
guage inference. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(COLING).
R. Montague. 1974. The proper treatment of quantifi-
cation in ordinary English. In R. Thomason, editor,
Formal Philosophy. Selected Papers of Richard Mon-
tague. Yale University Press, New Haven.
C. Monz and M. de Rijke. 2001. Deductions with
meaning. In Michael Moortgat, editor, Logical As-
pects of Computational Linguistics, Third Interna-
tional Conference (LACL’98), volume 2014 of LNAI.
Springer-Verlag, Berlin/Heidelberg.
S. Oepen, K. Toutanova, S. Shieber, C. Manning,
D. Flickinger, and T. Brants. 2002. The LinGO
Redwoods treebank: Motivation and preliminary
applications. In Proceedings of the 19th Inter-
national Conference on Computational Linguistics
(COLING).
Uwe Reyle. 1995. On reasoning with ambiguities. In
Proceedings of the 7th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL’95).
K. van Deemter. 1996. Towards a logic of ambiguous
expressions. In Semantic Ambiguity and Underspec-
ification. CSLI Publications, Stanford.
E. Vestre. 1991. An algorithm for generating non-
redundant quantifier scopings. In Proc. of EACL,
Berlin.
</reference>
<page confidence="0.999532">
39
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant confidence="0.591156" no="0">
<title confidence="0.999557">Computing weakest readings</title>
<author confidence="0.999971">Alexander Koller Stefan Thater</author>
<affiliation confidence="0.9988015">Cluster of Excellence Dept. of Computational Linguistics Saarland University Saarland University</affiliation>
<email confidence="0.613738">koller@mmci.uni-saarland.destth@coli.uni-saarland.de</email>
<abstract confidence="0.9963274">We present an efficient algorithm for computing the weakest readings of semantically ambiguous sentences. A corpus-based evaluation with a large-scale grammar shows that our algorithm reduces over 80% of sentences to one or two readings, in negligible runtime, and thus makes it possible to work with semantic representations derived by deep large-scale grammars.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Althaus</author>
<author>D Duchier</author>
<author>A Koller</author>
<author>K Mehlhorn</author>
<author>J Niehren</author>
<author>S Thiel</author>
</authors>
<title>An efficient graph algorithm for dominance constraints.</title>
<date>2003</date>
<journal>Journal of Algorithms,</journal>
<pages>48--194</pages>
<contexts>
<context citStr="Althaus et al., 2003" endWordPosition="614" position="3974" startWordPosition="611"> weakest readings for each sentence, in less than twenty milliseconds; over 80% of all sentences are reduced to at most two weakest readings. In other words, we make it feasible for the first time to build an application that uses the individual (weakest) semantic representations computed by the ERG, both in terms of the remaining ambiguity and in terms of performance. Our technique is not limited to the ERG, but should be applicable to other underspecification-based grammars as well. Technically, we use underspecified descriptions that are regular tree grammars derived from dominance graphs (Althaus et al., 2003; Koller et al., 30 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 30–39, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics 2008). We compute the weakest readings by intersecting these grammars with other grammars representing the rewrite rules. This approach can be used much more generally than just for the computation of weakest readings; we illustrate this by showing how a more general version of the redundancy elimination algorithm by Koller et al. (2008) can be seen as a special case of our construction. Thus</context>
<context citStr="Althaus et al., 2003" endWordPosition="1261" position="8003" startWordPosition="1258">and Cooper, 1981). Formally, we assume a ranked signature E of tree constructors {f,g,a,...}, each of which is equipped with an arity ar(f) &gt; 0. We take a (finite constructor) tree t as a finite tree in which each node is labelled with a symbol of E, and the number of children of the node is exactly the arity of this symbol. For instance, the signature of the trees in Fig. 1 is {bx|2,ly|2,compz|0,...}. Finite constructor trees can be seen as ground terms over E that respect the arities. We write T(E) for the finite constructor trees over E. 3.1 Dominance graphs A (labelled) dominance graph D (Althaus et al., 2003) is a directed graph that consists of a collection of trees called fragments, plus dominance edges relating nodes in different fragments. We distinguish the roots WD of the fragments from their holes, which are the unlabelled leaves. We write LD : WD —* E for the labeling function of D. The basic idea behind using dominance graphs to model scope underspecification is to specify 3z ∀x 3y Â 1 4 2 3 5 6 7 compz repr-ofx,z 8 seex,y sampley 31 Figure 2: The five configurations of the dominance graph in Fig. 1. repr-ofx,z repr-ofx,z seex,y compz repr-ofx,z seex,y (a) (b) (c) (d) (e) ¬ ¬ ¬ [+] [-] [+</context>
</contexts>
<marker>Althaus, Duchier, Koller, Mehlhorn, Niehren, Thiel, 2003</marker>
<rawString>E. Althaus, D. Duchier, A. Koller, K. Mehlhorn, J. Niehren, and S. Thiel. 2003. An efficient graph algorithm for dominance constraints. Journal of Algorithms, 48:194–219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Baader</author>
<author>T Nipkow</author>
</authors>
<title>Term rewriting and all that.</title>
<date>1999</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context citStr="Baader and Nipkow, 1999" endWordPosition="2412" position="14679" startWordPosition="2409">n therefore serve as a practical approximation of the intended meaning of the sentence. A naive algorithm for computing weakest readings would explicitly compute the entailment order, by running a theorem prover on each pair of configurations, and then pick out the minimal elements. But this algorithm is quadratic in the number of configurations, and therefore impractically slow for real-life sentences. Here we develop a fast algorithm for this problem. The fundamental insight we exploit is that entailment among the configurations of a dominance graph can be approximated with rewriting rules (Baader and Nipkow, 1999). Consider the relation between (d) and (c). We can explain that (d) entails (c) by observing that (c) can be built from (d) by exchanging the positions of the adjacent quantifiers ∀x and ∃y; more precisely, by applying the following rewrite rule: [−] ∀x(Q,∃y(P,R)) → ∃y(P,∀x(Q,R)) (1) The body of the rule specifies that an occurrence of ∀x which is the direct parent of an occurrence of ∃y may change positions with it; the subformulas P, Q, and R must be copied appropriately. The annotation [−] specifies that we must only apply the rule to subformulas in negative logical polarity: If the quanti</context>
<context citStr="Baader and Nipkow, 1999" endWordPosition="4660" position="27407" startWordPosition="4657">ed rewrite rules [a] ∃z(P,∃y(Q,R)) → ∃y(Q,∃z(P,R)) (5) for all annotations a ∈ Ann; logical equivalence is not sensitive to the annotation. Finally, we can compute the relative normal forms of conf(D) under this rewrite system as above. The result will be an RTG G0 describing a subset of conf(D). Every tree t in conf(D) that is not in L(G0) is equivalent to some tree t0 in L(G0), because if t could not be rewritten into such a t0, then t would be in relative normal form. That is, the algorithm solves the redundancy elimination problem. Furthermore, if the oriented rewrite system is confluent (Baader and Nipkow, 1999), no two trees in L(G0) will be equivalent to each other, i.e. we achieve complete reduction in the sense of Koller et al. (2008). This solution shares much with that of Koller et al. (2008), in that we perform redundancy elimination by intersecting tree grammars. However, the construction we present here is much more general: The algorithmic foundation for redundancy elimination is now exactly the same as that for weakest readings, we only have to use an equivalencepreserving rewrite system instead of a weakening one. This new formal clarity also simplifies the specification of certain equati</context>
</contexts>
<marker>Baader, Nipkow, 1999</marker>
<rawString>F. Baader and T. Nipkow. 1999. Term rewriting and all that. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Barwise</author>
<author>R Cooper</author>
</authors>
<title>Generalized quantifiers and natural language. Linguistics and Philosophy,</title>
<date>1981</date>
<pages>4--159</pages>
<contexts>
<context citStr="Barwise and Cooper, 1981" endWordPosition="1153" position="7399" startWordPosition="1150">G construction for weakest readings to a real-world grammar, evaluating the system on practical inputs, and combining weakest readings with redundancy elimination. 3 Underspecification This section briefly reviews two formalisms for specifying sets of trees: dominance graphs and regular tree grammars. Both of these formalisms can be used to model scope ambiguities compactly by regarding the semantic representations of a sentence as trees. Some example trees are shown in Fig. 2. These trees can be read as simplified formulas of predicate logic, or as formulas involving generalized quantifiers (Barwise and Cooper, 1981). Formally, we assume a ranked signature E of tree constructors {f,g,a,...}, each of which is equipped with an arity ar(f) &gt; 0. We take a (finite constructor) tree t as a finite tree in which each node is labelled with a symbol of E, and the number of children of the node is exactly the arity of this symbol. For instance, the signature of the trees in Fig. 1 is {bx|2,ly|2,compz|0,...}. Finite constructor trees can be seen as ground terms over E that respect the arities. We write T(E) for the finite constructor trees over E. 3.1 Dominance graphs A (labelled) dominance graph D (Althaus et al., 2</context>
</contexts>
<marker>Barwise, Cooper, 1981</marker>
<rawString>J. Barwise and R. Cooper. 1981. Generalized quantifiers and natural language. Linguistics and Philosophy, 4:159–219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bos</author>
</authors>
<title>Let’s not argue about semantics.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th international conference on Language Resources and Evaluation (LREC</booktitle>
<contexts>
<context citStr="Bos (2008)" endWordPosition="226" position="1559" startWordPosition="225">these very precise semantic representations, their usefulness is limited by the presence of semantic ambiguity: On the Rondane Treebank (Oepen et al., 2002), the ERG computes an average of several million semantic representations for each sentence, even when the syntactic analysis is fixed. The problem of appropriately selecting one of them to work with would ideally be solved by statistical methods (Higgins and Sadock, 2003) or knowledge-based inferences. However, no such approach has been worked out in sufficient detail to support the disambiguation of treebank sentences. As an alternative, Bos (2008) proposes to compute the weakest reading of each sentence and then use it instead of the “true” reading of the sentence. This is based on the observation that the readings of a semantically ambiguous sentence are partially ordered with respect to logical entailment, and the weakest readings – the minimal (least informative) readings with respect to this order – only express “safe” information that is common to all other readings as well. However, when a sentence has millions of readings, finding the weakest reading is a hard problem. It is of course completely infeasible to compute all reading</context>
</contexts>
<marker>Bos, 2008</marker>
<rawString>J. Bos. 2008. Let’s not argue about semantics. In Proceedings of the 6th international conference on Language Resources and Evaluation (LREC 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Butt</author>
<author>H Dyvik</author>
<author>T Holloway King</author>
<author>H Masuichi</author>
<author>C Rohrer</author>
</authors>
<title>The parallel grammar project.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING-2002 Workshop on Grammar Engineering and Evaluation.</booktitle>
<contexts>
<context citStr="Butt et al., 2002" endWordPosition="116" position="835" startWordPosition="113"> We present an efficient algorithm for computing the weakest readings of semantically ambiguous sentences. A corpus-based evaluation with a large-scale grammar shows that our algorithm reduces over 80% of sentences to one or two readings, in negligible runtime, and thus makes it possible to work with semantic representations derived by deep large-scale grammars. 1 Introduction Over the past few years, there has been considerable progress in the ability of manually created large-scale grammars, such as the English Resource Grammar (ERG, Copestake and Flickinger (2000)) or the ParGram grammars (Butt et al., 2002), to parse wide-coverage text and assign it deep semantic representations. While applications should benefit from these very precise semantic representations, their usefulness is limited by the presence of semantic ambiguity: On the Rondane Treebank (Oepen et al., 2002), the ERG computes an average of several million semantic representations for each sentence, even when the syntactic analysis is fixed. The problem of appropriately selecting one of them to work with would ideally be solved by statistical methods (Higgins and Sadock, 2003) or knowledge-based inferences. However, no such approach</context>
</contexts>
<marker>Butt, Dyvik, King, Masuichi, Rohrer, 2002</marker>
<rawString>M. Butt, H. Dyvik, T. Holloway King, H. Masuichi, and C. Rohrer. 2002. The parallel grammar project. In Proceedings of COLING-2002 Workshop on Grammar Engineering and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R P Chaves</author>
</authors>
<title>Non-redundant scope disambiguation in underspecified semantics.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th</booktitle>
<publisher>ESSLLI Student Session.</publisher>
<contexts>
<context citStr="Chaves, 2003" endWordPosition="4365" position="25746" startWordPosition="4364"> of G0 is marked with F because G0 should only generate trees that GR cannot generate. As expected, G0 generates precisely two trees, namely (b) and (c). 5 Redundancy elimination, revisited The construction we just carried out – characterize the configurations we find interesting as the relative normal forms of an annotated rewrite system R, translate it into a transducer MR, and intersect conf(D) with the complement of the pre-image under MR – is more generally useful than just for the computation of weakest readings. We illustrate this on the problem of redundancy elimination (Vestre, 1991; Chaves, 2003; Koller et al., 2008) by showing how a variant of the algorithm of Koller et al. (2008) falls out of our technique as a special case. Redundancy elimination is the problem of computing, from a dominance graph D, another dominance graph D0 such that conf(D0) ⊆ conf(D) and 35 (s) (a) -∃y∃z∀x (d) -∃z∀x∃y (3) (1) (b) -∃y∀x∃z (e) -∀x(∃z,∃y) (1) (3) (c) -∃z∃y∀x every formula in conf(D) is logically equivalent to some formula in conf(D0). We can approximate logical equivalence using a finite system of equations such as ∃y(P,∃z(Q,R)) = ∃z(Q,∃y(P,R)), (4) indicating that ∃y and ∃z can be permuted with</context>
</contexts>
<marker>Chaves, 2003</marker>
<rawString>R. P. Chaves. 2003. Non-redundant scope disambiguation in underspecified semantics. In Proceedings of the 8th ESSLLI Student Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Comon</author>
<author>M Dauchet</author>
<author>R Gilleron</author>
<author>C Löding</author>
<author>F Jacquemard</author>
<author>D Lugiez</author>
<author>S Tison</author>
<author>M Tommasi</author>
</authors>
<title>Tree automata techniques and applications. Available on:</title>
<date>2007</date>
<note>http://www.grappa. univ-lille3.fr/tata.</note>
<contexts>
<context citStr="Comon et al., 2007" endWordPosition="1580" position="9715" startWordPosition="1577">ings of the sentence “it is not the case that every representative of a company saw a sample.” Each reading is encoded as a (labeled) configuration of the dominance graph, which can be obtained by “plugging” the tree fragments into each other, in a way that respects the dominance edges: The source node of each dominance edge must dominate (be an ancestor of) the target node in each configuration. The trees in Fig. 2 are the five labeled configurations of the example graph. 3.2 Regular tree grammars Regular tree grammars (RTGs) are a general grammar formalism for describing languages of trees (Comon et al., 2007). An RTG is a 4-tuple G = (S,N,E,P), where N and E are nonterminal and terminal alphabets, S ∈ N is the start symbol, and P is a finite set of production rules. Unlike in context-free string grammars (which look superficially the same), the terminal symbols are tree constructors from E. The production rules are of the form A → t, where A is a nonterminal and t is a tree from T(E ∪ N); nonterminals count as having arity zero, i.e. they must label leaves. A derivation starts with a tree containing a single node labeled with S. Then in each step of the derivation, some leaf u which is labelled wi</context>
<context citStr="Comon et al., 2007" endWordPosition="3305" position="19649" startWordPosition="3302">s, if R approximates entailment, then the R-relative normal forms approximate the weakest readings. 4.3 Computing relative normal forms We now show how the relative normal forms of a dominance graph can be computed efficiently. For lack of space, we only sketch the construction and omit all proofs. Details can be found in Koller and Thater (2010). The key idea of the construction is to represent the relation →R in terms of a context tree transducer M, and characterize the relative normal forms of a tree language L in terms of the pre-image of L under M. Like ordinary regular tree transducers (Comon et al., 2007), context tree transducers read an input tree, assigning states to the nodes, while emitting an output tree. But while ordinary transducers read the input tree symbol by symbol, a context tree transducer can read multiple symbols at once. In this way, they are equivalent to the extended left-hand side transducers of Graehl et al. (2008). We will now define context tree transducers. Let Σ be a ranked signature, and let Xm be a set of m variables. We write Con(m)(Σ) for the contexts with m holes, i.e. those trees in T(Σ∪Xm) in which each element of Xm occurs exactly once, and always as a leaf. I</context>
<context citStr="Comon et al., 2007" endWordPosition="3900" position="22871" startWordPosition="3897">lace, and assigns it the correct annotation. Finally, we have a rule qa(C[x1,...,xn]) → C0[¯q(xi1),..., ¯q(xin)] for every rewrite rule C[x1,...,xn] → C0[xi1,...,xin] with annotation a in R. Now let’s put the different parts together. We know that for each hnc dominance graph D, there is a regular tree grammar GD such that L(GD) is the set of configurations of D. Furthermore, the preimage i−1 M (L) = {t |exists t0 ∈ L with (t,t0) ∈ iM} of a regular tree language L is also regular (Koller and Thater, 2010) if M is linear, and regular tree languages are closed under intersection and complement (Comon et al., 2007). So we can compute another RTG G0 such that L(G0) = L(GD) ∩ i−1 MR (L(GD)). L(G0) consists of the members of L(GD) which cannot be rewritten by MR into members of L(GD); that is, L(G0) is exactly the set of R-relative normal forms of D. In general, the complement construction requires exponential time in the size of MR and GD. However, it can be shown that if the rules in R have at most depth two and GD is deterministic, then the entire above construction can be computed in time O(|GD |· |R|) (Koller and Thater, 2010). In other words, we have shown how to compute the weakest readings of a hyp</context>
</contexts>
<marker>Comon, Dauchet, Gilleron, Löding, Jacquemard, Lugiez, Tison, Tommasi, 2007</marker>
<rawString>H. Comon, M. Dauchet, R. Gilleron, C. Löding, F. Jacquemard, D. Lugiez, S. Tison, and M. Tommasi. 2007. Tree automata techniques and applications. Available on: http://www.grappa. univ-lille3.fr/tata.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
<author>D Flickinger</author>
</authors>
<title>An opensource grammar development environment and broad-coverage english grammar using HPSG.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context citStr="Copestake and Flickinger (2000)" endWordPosition="108" position="790" startWordPosition="105">er@mmci.uni-saarland.de stth@coli.uni-saarland.de Abstract We present an efficient algorithm for computing the weakest readings of semantically ambiguous sentences. A corpus-based evaluation with a large-scale grammar shows that our algorithm reduces over 80% of sentences to one or two readings, in negligible runtime, and thus makes it possible to work with semantic representations derived by deep large-scale grammars. 1 Introduction Over the past few years, there has been considerable progress in the ability of manually created large-scale grammars, such as the English Resource Grammar (ERG, Copestake and Flickinger (2000)) or the ParGram grammars (Butt et al., 2002), to parse wide-coverage text and assign it deep semantic representations. While applications should benefit from these very precise semantic representations, their usefulness is limited by the presence of semantic ambiguity: On the Rondane Treebank (Oepen et al., 2002), the ERG computes an average of several million semantic representations for each sentence, even when the syntactic analysis is fixed. The problem of appropriately selecting one of them to work with would ideally be solved by statistical methods (Higgins and Sadock, 2003) or knowledg</context>
</contexts>
<marker>Copestake, Flickinger, 2000</marker>
<rawString>A. Copestake and D. Flickinger. 2000. An opensource grammar development environment and broad-coverage english grammar using HPSG. In Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
<author>D Flickinger</author>
<author>C Pollard</author>
<author>I Sag</author>
</authors>
<title>Minimal recursion semantics: An introduction.</title>
<date>2005</date>
<journal>Journal of Language and Computation.</journal>
<contexts>
<context citStr="Copestake et al., 2005" endWordPosition="4954" position="29181" startWordPosition="4951"> in Fig. 5 illustrates how the equivalence and weakening rules conspire to exclude all other configurations. 6 Evaluation In this section, we evaluate the effectiveness and efficiency of our weakest readings algorithm on a treebank. We compute RTGs for all sentences in the treebank and measure how many weakest readings remain after the intersection, and how much time this computation takes. Resources. For our experiment, we use the Rondane treebank (version of January 2006), a “Redwoods style” (Oepen et al., 2002) treebank containing underspecified representations (USRs) in the MRS formalism (Copestake et al., 2005) for sentences from the tourism domain. Our implementation of the relative normal forms algorithm is based on Utool (Koller and Thater, 2005), which (among other things) can translate a large class of MRS descriptions into hypernormally connected dominance graphs and further into RTGs as in Section 3. The implementation exploits certain properties of RTGs computed from dominance graphs to maximize efficiency. We will make this implementation publically available as part of the next Utool release. We use Utool to automatically translate the 999 MRS descriptions for which this is possible into R</context>
</contexts>
<marker>Copestake, Flickinger, Pollard, Sag, 2005</marker>
<rawString>A. Copestake, D. Flickinger, C. Pollard, and I. Sag. 2005. Minimal recursion semantics: An introduction. Journal of Language and Computation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Flickinger</author>
<author>A Koller</author>
<author>S Thater</author>
</authors>
<title>A new well-formedness criterion for semantics debugging.</title>
<date>2005</date>
<booktitle>In Proceedings of the 12th International Conference on HPSG,</booktitle>
<location>Lisbon.</location>
<contexts>
<context citStr="Flickinger et al., 2005" endWordPosition="1974" position="12065" startWordPosition="1971">except that they assign states to the nodes in a tree rather than the positions in a string. Regular tree languages enjoy many of the closure properties of regular string languages. In particular, we will later exploit that RTLs are closed under intersection and complement. 3.3 Dominance graphs as RTGs An important class of dominance graphs are hypernormally connected (hnc) dominance graphs (Koller et al., 2003). The precise definition of hnc graphs is not important here, but note that virtually all underspecified descriptions that are produced by current grammars are hypernormally connected (Flickinger et al., 2005), and we will restrict ourselves to hnc graphs for the rest of the paper. Every hypernormally connected dominance graph D can be automatically translated into an equivalent RTG GD that generates exactly the same configurations (Koller et al., 2008); the RTG in Fig. 3 is an example. The nonterminals of GD are 32 always hnc subgraphs of D. In the worst case, GD can be exponentially bigger than D, but in practice it turns out that the grammar size remains manageable: even the RTG for the most ambiguous sentence in the Rondane Treebank, which has about 4.5 × 1012 scope readings, has only about 75 </context>
</contexts>
<marker>Flickinger, Koller, Thater, 2005</marker>
<rawString>D. Flickinger, A. Koller, and S. Thater. 2005. A new well-formedness criterion for semantics debugging. In Proceedings of the 12th International Conference on HPSG, Lisbon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gabsdil</author>
<author>K Striegnitz</author>
</authors>
<title>Classifying scope ambiguities.</title>
<date>1999</date>
<booktitle>In Proceedings of the First Intl. Workshop on Inference in Computational Semantics.</booktitle>
<contexts>
<context citStr="Gabsdil and Striegnitz, 1999" endWordPosition="346" position="2281" startWordPosition="343">e” reading of the sentence. This is based on the observation that the readings of a semantically ambiguous sentence are partially ordered with respect to logical entailment, and the weakest readings – the minimal (least informative) readings with respect to this order – only express “safe” information that is common to all other readings as well. However, when a sentence has millions of readings, finding the weakest reading is a hard problem. It is of course completely infeasible to compute all readings and compare all pairs for entailment; but even the best known algorithm in the literature (Gabsdil and Striegnitz, 1999) is only an optimization of this basic strategy, and would take months to compute the weakest readings for the sentences in the Rondane Treebank. In this paper, we propose a new, efficient approach to the problem of computing weakest readings. We follow an underspecification approach to managing ambiguity: Rather than deriving all semantic representations from the syntactic analysis, we work with a single, compact underspecified semantic representation, from which the semantic representations can then be extracted by need. We then approximate entailment with a rewrite system that rewrites read</context>
</contexts>
<marker>Gabsdil, Striegnitz, 1999</marker>
<rawString>M. Gabsdil and K. Striegnitz. 1999. Classifying scope ambiguities. In Proceedings of the First Intl. Workshop on Inference in Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Graehl</author>
<author>K Knight</author>
<author>J May</author>
</authors>
<title>Training tree transducers.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>3</issue>
<pages>427</pages>
<contexts>
<context citStr="Graehl et al. (2008)" endWordPosition="3360" position="19987" startWordPosition="3357">ater (2010). The key idea of the construction is to represent the relation →R in terms of a context tree transducer M, and characterize the relative normal forms of a tree language L in terms of the pre-image of L under M. Like ordinary regular tree transducers (Comon et al., 2007), context tree transducers read an input tree, assigning states to the nodes, while emitting an output tree. But while ordinary transducers read the input tree symbol by symbol, a context tree transducer can read multiple symbols at once. In this way, they are equivalent to the extended left-hand side transducers of Graehl et al. (2008). We will now define context tree transducers. Let Σ be a ranked signature, and let Xm be a set of m variables. We write Con(m)(Σ) for the contexts with m holes, i.e. those trees in T(Σ∪Xm) in which each element of Xm occurs exactly once, and always as a leaf. If C ∈ Con(m)(Σ), then C[t1,...,tm] = C[t1/x1,...,tm/xm], where x1,...,xm are the variables from left to right. A (top-down) context tree transducer from Σ to Δ is a 5-tuple M = (Q,Σ,Δ,q0,δ). Σ and Δ are ranked signatures, Q is a finite set of states, and q0 ∈ Q is the start state. δ is a finite set of transition rules of the form q(C[x1</context>
</contexts>
<marker>Graehl, Knight, May, 2008</marker>
<rawString>J. Graehl, K. Knight, and J. May. 2008. Training tree transducers. Computational Linguistics, 34(3):391– 427.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Higgins</author>
<author>J Sadock</author>
</authors>
<title>A machine learning approach to modeling scope preferences.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context citStr="Higgins and Sadock, 2003" endWordPosition="200" position="1378" startWordPosition="197">(ERG, Copestake and Flickinger (2000)) or the ParGram grammars (Butt et al., 2002), to parse wide-coverage text and assign it deep semantic representations. While applications should benefit from these very precise semantic representations, their usefulness is limited by the presence of semantic ambiguity: On the Rondane Treebank (Oepen et al., 2002), the ERG computes an average of several million semantic representations for each sentence, even when the syntactic analysis is fixed. The problem of appropriately selecting one of them to work with would ideally be solved by statistical methods (Higgins and Sadock, 2003) or knowledge-based inferences. However, no such approach has been worked out in sufficient detail to support the disambiguation of treebank sentences. As an alternative, Bos (2008) proposes to compute the weakest reading of each sentence and then use it instead of the “true” reading of the sentence. This is based on the observation that the readings of a semantically ambiguous sentence are partially ordered with respect to logical entailment, and the weakest readings – the minimal (least informative) readings with respect to this order – only express “safe” information that is common to all o</context>
</contexts>
<marker>Higgins, Sadock, 2003</marker>
<rawString>D. Higgins and J. Sadock. 2003. A machine learning approach to modeling scope preferences. Computational Linguistics, 29(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hobbs</author>
</authors>
<title>An improper treatment of quantification in ordinary English.</title>
<date>1983</date>
<booktitle>In Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics (ACL’83).</booktitle>
<contexts>
<context citStr="Hobbs (1983)" endWordPosition="819" position="5258" startWordPosition="818">dings from an underspecified representation. The paper is structured as follows. Section 2 starts by reviewing related work. We recall dominance graphs, regular tree grammars, and the basic ideas of underspecification in Section 3, before we show how to compute weakest readings (Section 4) and logical equivalences (Section 5). In Section 6, we define a weakening rewrite system for the ERG and evaluate it on the Rondane Treebank. Section 7 concludes and points to future work. 2 Related work The idea of deriving a single approximative semantic representation for ambiguous sentences goes back to Hobbs (1983); however, Hobbs only works his algorithm out for a restricted class of quantifiers, and his representations can be weaker than our weakest readings. Rules that weaken one reading into another were popular in the 1990s underspecification literature (Reyle, 1995; Monz and de Rijke, 2001; van Deemter, 1996) because they simplify logical reasoning with underspecified representations. From a linguistic perspective, Kempson and Cormack (1981) even go so far as to claim that the weakest reading should be taken as the “basic” reading of a sentence, and the other readings only seen as pragmatically li</context>
</contexts>
<marker>Hobbs, 1983</marker>
<rawString>J. Hobbs. 1983. An improper treatment of quantification in ordinary English. In Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics (ACL’83).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kempson</author>
<author>A Cormack</author>
</authors>
<title>Ambiguity and quantification. Linguistics and Philosophy,</title>
<date>1981</date>
<pages>4--259</pages>
<contexts>
<context citStr="Kempson and Cormack (1981)" endWordPosition="884" position="5699" startWordPosition="881">bank. Section 7 concludes and points to future work. 2 Related work The idea of deriving a single approximative semantic representation for ambiguous sentences goes back to Hobbs (1983); however, Hobbs only works his algorithm out for a restricted class of quantifiers, and his representations can be weaker than our weakest readings. Rules that weaken one reading into another were popular in the 1990s underspecification literature (Reyle, 1995; Monz and de Rijke, 2001; van Deemter, 1996) because they simplify logical reasoning with underspecified representations. From a linguistic perspective, Kempson and Cormack (1981) even go so far as to claim that the weakest reading should be taken as the “basic” reading of a sentence, and the other readings only seen as pragmatically licensed special cases. The work presented here is related to other approaches that reduce the set of readings of an underspecified semantic representation (USR). Koller and Niehren (2000) showed how to strengthen a dominance constraint using information about anaphoric accessibility; later, Koller et al. (2008) presented and evaluated an algorithm for redundancy elimination, which removes readings from an USR based on logical equivalence.</context>
</contexts>
<marker>Kempson, Cormack, 1981</marker>
<rawString>R. Kempson and A. Cormack. 1981. Ambiguity and quantification. Linguistics and Philosophy, 4:259– 309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Knuth</author>
<author>P Bendix</author>
</authors>
<title>Simple word problems in universal algebras.</title>
<date>1970</date>
<booktitle>Computational Problems in Abstract Algebra,</booktitle>
<pages>263--297</pages>
<editor>In J. Leech, editor,</editor>
<publisher>Pergamon Press,</publisher>
<location>Oxford.</location>
<marker>Knuth, Bendix, 1970</marker>
<rawString>D. Knuth and P. Bendix. 1970. Simple word problems in universal algebras. In J. Leech, editor, Computational Problems in Abstract Algebra, pages 263–297. Pergamon Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Koller</author>
<author>J Niehren</author>
</authors>
<title>On underspecified processing of dynamic semantics.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics (COLING-2000).</booktitle>
<contexts>
<context citStr="Koller and Niehren (2000)" endWordPosition="943" position="6044" startWordPosition="940">weaken one reading into another were popular in the 1990s underspecification literature (Reyle, 1995; Monz and de Rijke, 2001; van Deemter, 1996) because they simplify logical reasoning with underspecified representations. From a linguistic perspective, Kempson and Cormack (1981) even go so far as to claim that the weakest reading should be taken as the “basic” reading of a sentence, and the other readings only seen as pragmatically licensed special cases. The work presented here is related to other approaches that reduce the set of readings of an underspecified semantic representation (USR). Koller and Niehren (2000) showed how to strengthen a dominance constraint using information about anaphoric accessibility; later, Koller et al. (2008) presented and evaluated an algorithm for redundancy elimination, which removes readings from an USR based on logical equivalence. Our system generalizes the latter approach and applies it to a new inference problem (weakest readings) which they could not solve. This paper builds closely upon Koller and Thater (2010), which lays the formal groundwork for the Figure 1: A dominance graph describing the five readings of the sentence “it is not the case that every representa</context>
</contexts>
<marker>Koller, Niehren, 2000</marker>
<rawString>A. Koller and J. Niehren. 2000. On underspecified processing of dynamic semantics. In Proceedings of the 18th International Conference on Computational Linguistics (COLING-2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Koller</author>
<author>S Thater</author>
</authors>
<title>Efficient solving and exploration of scope ambiguities.</title>
<date>2005</date>
<booktitle>In ACL-05 Demonstration Notes,</booktitle>
<location>Ann Arbor.</location>
<contexts>
<context citStr="Koller and Thater, 2005" endWordPosition="4976" position="29322" startWordPosition="4973">e evaluate the effectiveness and efficiency of our weakest readings algorithm on a treebank. We compute RTGs for all sentences in the treebank and measure how many weakest readings remain after the intersection, and how much time this computation takes. Resources. For our experiment, we use the Rondane treebank (version of January 2006), a “Redwoods style” (Oepen et al., 2002) treebank containing underspecified representations (USRs) in the MRS formalism (Copestake et al., 2005) for sentences from the tourism domain. Our implementation of the relative normal forms algorithm is based on Utool (Koller and Thater, 2005), which (among other things) can translate a large class of MRS descriptions into hypernormally connected dominance graphs and further into RTGs as in Section 3. The implementation exploits certain properties of RTGs computed from dominance graphs to maximize efficiency. We will make this implementation publically available as part of the next Utool release. We use Utool to automatically translate the 999 MRS descriptions for which this is possible into RTGs. To simplify the specification of the rewrite systems, we restrict ourselves to the subcorpus in which all scope-taking operators (labels</context>
</contexts>
<marker>Koller, Thater, 2005</marker>
<rawString>A. Koller and S. Thater. 2005. Efficient solving and exploration of scope ambiguities. In ACL-05 Demonstration Notes, Ann Arbor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Koller</author>
<author>S Thater</author>
</authors>
<title>Computing relative normal forms in regular tree languages.</title>
<date>2010</date>
<booktitle>In Proceedings of the 21st International Conference on Rewriting Techniques and Applications (RTA).</booktitle>
<contexts>
<context citStr="Koller and Thater (2010)" endWordPosition="1009" position="6487" startWordPosition="1006">d special cases. The work presented here is related to other approaches that reduce the set of readings of an underspecified semantic representation (USR). Koller and Niehren (2000) showed how to strengthen a dominance constraint using information about anaphoric accessibility; later, Koller et al. (2008) presented and evaluated an algorithm for redundancy elimination, which removes readings from an USR based on logical equivalence. Our system generalizes the latter approach and applies it to a new inference problem (weakest readings) which they could not solve. This paper builds closely upon Koller and Thater (2010), which lays the formal groundwork for the Figure 1: A dominance graph describing the five readings of the sentence “it is not the case that every representative of a company saw a sample.” work presented here. Here we go beyond that paper by applying a concrete implementation of our RTG construction for weakest readings to a real-world grammar, evaluating the system on practical inputs, and combining weakest readings with redundancy elimination. 3 Underspecification This section briefly reviews two formalisms for specifying sets of trees: dominance graphs and regular tree grammars. Both of th</context>
<context citStr="Koller and Thater (2010)" endWordPosition="3254" position="19378" startWordPosition="3251">inance graph D to be a R-relative normal form of (the configurations of) D iff there is no other configuration t0 of D such that t →R t0. These are the configurations that can’t be weakened further without obtaining a tree that is no longer a configuration of D. In other words, if R approximates entailment, then the R-relative normal forms approximate the weakest readings. 4.3 Computing relative normal forms We now show how the relative normal forms of a dominance graph can be computed efficiently. For lack of space, we only sketch the construction and omit all proofs. Details can be found in Koller and Thater (2010). The key idea of the construction is to represent the relation →R in terms of a context tree transducer M, and characterize the relative normal forms of a tree language L in terms of the pre-image of L under M. Like ordinary regular tree transducers (Comon et al., 2007), context tree transducers read an input tree, assigning states to the nodes, while emitting an output tree. But while ordinary transducers read the input tree symbol by symbol, a context tree transducer can read multiple symbols at once. In this way, they are equivalent to the extended left-hand side transducers of Graehl et a</context>
<context citStr="Koller and Thater, 2010" endWordPosition="3881" position="22762" startWordPosition="3878">...,qann(a,f,i)(xi),..., ¯q(xn)), which nondeterministically chooses under which child the rewriting should take place, and assigns it the correct annotation. Finally, we have a rule qa(C[x1,...,xn]) → C0[¯q(xi1),..., ¯q(xin)] for every rewrite rule C[x1,...,xn] → C0[xi1,...,xin] with annotation a in R. Now let’s put the different parts together. We know that for each hnc dominance graph D, there is a regular tree grammar GD such that L(GD) is the set of configurations of D. Furthermore, the preimage i−1 M (L) = {t |exists t0 ∈ L with (t,t0) ∈ iM} of a regular tree language L is also regular (Koller and Thater, 2010) if M is linear, and regular tree languages are closed under intersection and complement (Comon et al., 2007). So we can compute another RTG G0 such that L(G0) = L(GD) ∩ i−1 MR (L(GD)). L(G0) consists of the members of L(GD) which cannot be rewritten by MR into members of L(GD); that is, L(G0) is exactly the set of R-relative normal forms of D. In general, the complement construction requires exponential time in the size of MR and GD. However, it can be shown that if the rules in R have at most depth two and GD is deterministic, then the entire above construction can be computed in time O(|GD </context>
</contexts>
<marker>Koller, Thater, 2010</marker>
<rawString>A. Koller and S. Thater. 2010. Computing relative normal forms in regular tree languages. In Proceedings of the 21st International Conference on Rewriting Techniques and Applications (RTA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Koller</author>
<author>J Niehren</author>
<author>S Thater</author>
</authors>
<title>Bridging the gap between underspecification formalisms: Hole semantics as dominance constraints.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th EACL.</booktitle>
<contexts>
<context citStr="Koller et al., 2003" endWordPosition="1944" position="11856" startWordPosition="1941">ar tree grammar that generates the five trees in Fig. 2. (RTLs), and regular tree grammars are equivalent to finite tree automata, which are defined essentially like the well-known finite string automata, except that they assign states to the nodes in a tree rather than the positions in a string. Regular tree languages enjoy many of the closure properties of regular string languages. In particular, we will later exploit that RTLs are closed under intersection and complement. 3.3 Dominance graphs as RTGs An important class of dominance graphs are hypernormally connected (hnc) dominance graphs (Koller et al., 2003). The precise definition of hnc graphs is not important here, but note that virtually all underspecified descriptions that are produced by current grammars are hypernormally connected (Flickinger et al., 2005), and we will restrict ourselves to hnc graphs for the rest of the paper. Every hypernormally connected dominance graph D can be automatically translated into an equivalent RTG GD that generates exactly the same configurations (Koller et al., 2008); the RTG in Fig. 3 is an example. The nonterminals of GD are 32 always hnc subgraphs of D. In the worst case, GD can be exponentially bigger t</context>
</contexts>
<marker>Koller, Niehren, Thater, 2003</marker>
<rawString>A. Koller, J. Niehren, and S. Thater. 2003. Bridging the gap between underspecification formalisms: Hole semantics as dominance constraints. In Proceedings of the 10th EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Koller</author>
<author>M Regneri</author>
<author>S Thater</author>
</authors>
<title>Regular tree grammars as a formalism for scope underspecification.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT.</booktitle>
<contexts>
<context citStr="Koller et al. (2008)" endWordPosition="698" position="4518" startWordPosition="695">re regular tree grammars derived from dominance graphs (Althaus et al., 2003; Koller et al., 30 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 30–39, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics 2008). We compute the weakest readings by intersecting these grammars with other grammars representing the rewrite rules. This approach can be used much more generally than just for the computation of weakest readings; we illustrate this by showing how a more general version of the redundancy elimination algorithm by Koller et al. (2008) can be seen as a special case of our construction. Thus our system can serve as a general framework for removing unintended readings from an underspecified representation. The paper is structured as follows. Section 2 starts by reviewing related work. We recall dominance graphs, regular tree grammars, and the basic ideas of underspecification in Section 3, before we show how to compute weakest readings (Section 4) and logical equivalences (Section 5). In Section 6, we define a weakening rewrite system for the ERG and evaluate it on the Rondane Treebank. Section 7 concludes and points to futur</context>
<context citStr="Koller et al. (2008)" endWordPosition="960" position="6169" startWordPosition="957"> Deemter, 1996) because they simplify logical reasoning with underspecified representations. From a linguistic perspective, Kempson and Cormack (1981) even go so far as to claim that the weakest reading should be taken as the “basic” reading of a sentence, and the other readings only seen as pragmatically licensed special cases. The work presented here is related to other approaches that reduce the set of readings of an underspecified semantic representation (USR). Koller and Niehren (2000) showed how to strengthen a dominance constraint using information about anaphoric accessibility; later, Koller et al. (2008) presented and evaluated an algorithm for redundancy elimination, which removes readings from an USR based on logical equivalence. Our system generalizes the latter approach and applies it to a new inference problem (weakest readings) which they could not solve. This paper builds closely upon Koller and Thater (2010), which lays the formal groundwork for the Figure 1: A dominance graph describing the five readings of the sentence “it is not the case that every representative of a company saw a sample.” work presented here. Here we go beyond that paper by applying a concrete implementation of o</context>
<context citStr="Koller et al., 2008" endWordPosition="2014" position="12313" startWordPosition="2011">ection and complement. 3.3 Dominance graphs as RTGs An important class of dominance graphs are hypernormally connected (hnc) dominance graphs (Koller et al., 2003). The precise definition of hnc graphs is not important here, but note that virtually all underspecified descriptions that are produced by current grammars are hypernormally connected (Flickinger et al., 2005), and we will restrict ourselves to hnc graphs for the rest of the paper. Every hypernormally connected dominance graph D can be automatically translated into an equivalent RTG GD that generates exactly the same configurations (Koller et al., 2008); the RTG in Fig. 3 is an example. The nonterminals of GD are 32 always hnc subgraphs of D. In the worst case, GD can be exponentially bigger than D, but in practice it turns out that the grammar size remains manageable: even the RTG for the most ambiguous sentence in the Rondane Treebank, which has about 4.5 × 1012 scope readings, has only about 75 000 rules and can be computed in a few seconds. 4 Computing weakest readings Now we are ready to talk about computing the weakest readings of a hypernormally connected dominance graph. We will first explain how we approximate logical weakening with</context>
<context citStr="Koller et al., 2008" endWordPosition="4369" position="25768" startWordPosition="4366">ed with F because G0 should only generate trees that GR cannot generate. As expected, G0 generates precisely two trees, namely (b) and (c). 5 Redundancy elimination, revisited The construction we just carried out – characterize the configurations we find interesting as the relative normal forms of an annotated rewrite system R, translate it into a transducer MR, and intersect conf(D) with the complement of the pre-image under MR – is more generally useful than just for the computation of weakest readings. We illustrate this on the problem of redundancy elimination (Vestre, 1991; Chaves, 2003; Koller et al., 2008) by showing how a variant of the algorithm of Koller et al. (2008) falls out of our technique as a special case. Redundancy elimination is the problem of computing, from a dominance graph D, another dominance graph D0 such that conf(D0) ⊆ conf(D) and 35 (s) (a) -∃y∃z∀x (d) -∃z∀x∃y (3) (1) (b) -∃y∀x∃z (e) -∀x(∃z,∃y) (1) (3) (c) -∃z∃y∀x every formula in conf(D) is logically equivalent to some formula in conf(D0). We can approximate logical equivalence using a finite system of equations such as ∃y(P,∃z(Q,R)) = ∃z(Q,∃y(P,R)), (4) indicating that ∃y and ∃z can be permuted without changing the model</context>
<context citStr="Koller et al. (2008)" endWordPosition="4684" position="27536" startWordPosition="4681">ation. Finally, we can compute the relative normal forms of conf(D) under this rewrite system as above. The result will be an RTG G0 describing a subset of conf(D). Every tree t in conf(D) that is not in L(G0) is equivalent to some tree t0 in L(G0), because if t could not be rewritten into such a t0, then t would be in relative normal form. That is, the algorithm solves the redundancy elimination problem. Furthermore, if the oriented rewrite system is confluent (Baader and Nipkow, 1999), no two trees in L(G0) will be equivalent to each other, i.e. we achieve complete reduction in the sense of Koller et al. (2008). This solution shares much with that of Koller et al. (2008), in that we perform redundancy elimination by intersecting tree grammars. However, the construction we present here is much more general: The algorithmic foundation for redundancy elimination is now exactly the same as that for weakest readings, we only have to use an equivalencepreserving rewrite system instead of a weakening one. This new formal clarity also simplifies the specification of certain equations, as we will see in Section 6. In addition, we can now combine the weakening rules (1), (3), and (5) into a single rewrite sys</context>
</contexts>
<marker>Koller, Regneri, Thater, 2008</marker>
<rawString>A. Koller, M. Regneri, and S. Thater. 2008. Regular tree grammars as a formalism for scope underspecification. In Proceedings of ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Lasersohn</author>
</authors>
<title>Existence presuppositions and background knowledge.</title>
<date>1993</date>
<journal>Journal of Semantics,</journal>
<pages>10--113</pages>
<contexts>
<context citStr="Lasersohn, 1993" endWordPosition="5669" position="33470" startWordPosition="5668"> the reading in which a company is within the restriction of the definite determiner. The schema is intuitively plausible, and it can also be proved to be logically sound if we make the standard assumption that the definite determiner the means “exactly one” (Montague, 1974). A similar argument applies to rule schema 2. Rule schema 3 encodes the classical entailment (1). Schema 4 is similar to the rule (2). Notice that it is not, strictly speaking, logically sound; however, because strong determiners like all or every carry a presupposition that their restrictions have a non-empty denotation (Lasersohn, 1993), the schema becomes sound for all instances that can be expressed in natural language. Similar arguments apply to rule schemas 5 and 6, which are potentially unsound for subtle reasons involving the logical interpretation of intensional expressions. However, these cases of unsoundness did not occur in our test corpus. Redundancy elimination. In addition, we assume the following equation system for redundancy elimination for i, j ∈ {1,2} and k ∈ N (again written in an analogous shorthand as above): 7. E/i = E/ j 8. D/1 = E/i, E/i = D/1 9. D/1 = D/1 10. E/k = P/2 These rule schemata state that </context>
</contexts>
<marker>Lasersohn, 1993</marker>
<rawString>P. Lasersohn. 1993. Existence presuppositions and background knowledge. Journal of Semantics, 10:113–122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacCartney</author>
<author>C Manning</author>
</authors>
<title>Modeling semantic containment and exclusion in natural language inference.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (COLING).</booktitle>
<marker>MacCartney, Manning, 2008</marker>
<rawString>B. MacCartney and C. Manning. 2008. Modeling semantic containment and exclusion in natural language inference. In Proceedings of the 22nd International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Montague</author>
</authors>
<title>The proper treatment of quantification in ordinary English. In</title>
<date>1974</date>
<editor>R. Thomason, editor,</editor>
<publisher>Yale University Press,</publisher>
<location>New Haven.</location>
<contexts>
<context citStr="Montague, 1974" endWordPosition="5616" position="33129" startWordPosition="5615"> like can or would. M also includes intensional verbs like have to and want. Notice that while the reverse rules are applicable in negative polarities, no rules are applicable in polarity ⊥. Rule schema 1 states, for instance, that the specific (wide-scope) reading of the indefinite in the president of a company is logically stronger than the reading in which a company is within the restriction of the definite determiner. The schema is intuitively plausible, and it can also be proved to be logically sound if we make the standard assumption that the definite determiner the means “exactly one” (Montague, 1974). A similar argument applies to rule schema 2. Rule schema 3 encodes the classical entailment (1). Schema 4 is similar to the rule (2). Notice that it is not, strictly speaking, logically sound; however, because strong determiners like all or every carry a presupposition that their restrictions have a non-empty denotation (Lasersohn, 1993), the schema becomes sound for all instances that can be expressed in natural language. Similar arguments apply to rule schemas 5 and 6, which are potentially unsound for subtle reasons involving the logical interpretation of intensional expressions. However,</context>
</contexts>
<marker>Montague, 1974</marker>
<rawString>R. Montague. 1974. The proper treatment of quantification in ordinary English. In R. Thomason, editor, Formal Philosophy. Selected Papers of Richard Montague. Yale University Press, New Haven.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Monz</author>
<author>M de Rijke</author>
</authors>
<title>Deductions with meaning.</title>
<date>2001</date>
<booktitle>Logical Aspects of Computational Linguistics, Third International Conference (LACL’98), volume 2014 of LNAI.</booktitle>
<editor>In Michael Moortgat, editor,</editor>
<publisher>Springer-Verlag, Berlin/Heidelberg.</publisher>
<marker>Monz, de Rijke, 2001</marker>
<rawString>C. Monz and M. de Rijke. 2001. Deductions with meaning. In Michael Moortgat, editor, Logical Aspects of Computational Linguistics, Third International Conference (LACL’98), volume 2014 of LNAI. Springer-Verlag, Berlin/Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oepen</author>
<author>K Toutanova</author>
<author>S Shieber</author>
<author>C Manning</author>
<author>D Flickinger</author>
<author>T Brants</author>
</authors>
<title>The LinGO Redwoods treebank: Motivation and preliminary applications.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context citStr="Oepen et al., 2002" endWordPosition="157" position="1105" startWordPosition="154">s makes it possible to work with semantic representations derived by deep large-scale grammars. 1 Introduction Over the past few years, there has been considerable progress in the ability of manually created large-scale grammars, such as the English Resource Grammar (ERG, Copestake and Flickinger (2000)) or the ParGram grammars (Butt et al., 2002), to parse wide-coverage text and assign it deep semantic representations. While applications should benefit from these very precise semantic representations, their usefulness is limited by the presence of semantic ambiguity: On the Rondane Treebank (Oepen et al., 2002), the ERG computes an average of several million semantic representations for each sentence, even when the syntactic analysis is fixed. The problem of appropriately selecting one of them to work with would ideally be solved by statistical methods (Higgins and Sadock, 2003) or knowledge-based inferences. However, no such approach has been worked out in sufficient detail to support the disambiguation of treebank sentences. As an alternative, Bos (2008) proposes to compute the weakest reading of each sentence and then use it instead of the “true” reading of the sentence. This is based on the obse</context>
<context citStr="Oepen et al., 2002" endWordPosition="4940" position="29077" startWordPosition="4937">iting. (b); the configuration (c) is rejected because it can be rewritten to (a) with (5). The graph in Fig. 5 illustrates how the equivalence and weakening rules conspire to exclude all other configurations. 6 Evaluation In this section, we evaluate the effectiveness and efficiency of our weakest readings algorithm on a treebank. We compute RTGs for all sentences in the treebank and measure how many weakest readings remain after the intersection, and how much time this computation takes. Resources. For our experiment, we use the Rondane treebank (version of January 2006), a “Redwoods style” (Oepen et al., 2002) treebank containing underspecified representations (USRs) in the MRS formalism (Copestake et al., 2005) for sentences from the tourism domain. Our implementation of the relative normal forms algorithm is based on Utool (Koller and Thater, 2005), which (among other things) can translate a large class of MRS descriptions into hypernormally connected dominance graphs and further into RTGs as in Section 3. The implementation exploits certain properties of RTGs computed from dominance graphs to maximize efficiency. We will make this implementation publically available as part of the next Utool rel</context>
</contexts>
<marker>Oepen, Toutanova, Shieber, Manning, Flickinger, Brants, 2002</marker>
<rawString>S. Oepen, K. Toutanova, S. Shieber, C. Manning, D. Flickinger, and T. Brants. 2002. The LinGO Redwoods treebank: Motivation and preliminary applications. In Proceedings of the 19th International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uwe Reyle</author>
</authors>
<title>On reasoning with ambiguities.</title>
<date>1995</date>
<booktitle>In Proceedings of the 7th Conference of the European Chapter of the Association for Computational Linguistics (EACL’95).</booktitle>
<contexts>
<context citStr="Reyle, 1995" endWordPosition="859" position="5519" startWordPosition="858">weakest readings (Section 4) and logical equivalences (Section 5). In Section 6, we define a weakening rewrite system for the ERG and evaluate it on the Rondane Treebank. Section 7 concludes and points to future work. 2 Related work The idea of deriving a single approximative semantic representation for ambiguous sentences goes back to Hobbs (1983); however, Hobbs only works his algorithm out for a restricted class of quantifiers, and his representations can be weaker than our weakest readings. Rules that weaken one reading into another were popular in the 1990s underspecification literature (Reyle, 1995; Monz and de Rijke, 2001; van Deemter, 1996) because they simplify logical reasoning with underspecified representations. From a linguistic perspective, Kempson and Cormack (1981) even go so far as to claim that the weakest reading should be taken as the “basic” reading of a sentence, and the other readings only seen as pragmatically licensed special cases. The work presented here is related to other approaches that reduce the set of readings of an underspecified semantic representation (USR). Koller and Niehren (2000) showed how to strengthen a dominance constraint using information about an</context>
</contexts>
<marker>Reyle, 1995</marker>
<rawString>Uwe Reyle. 1995. On reasoning with ambiguities. In Proceedings of the 7th Conference of the European Chapter of the Association for Computational Linguistics (EACL’95).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K van Deemter</author>
</authors>
<title>Towards a logic of ambiguous expressions.</title>
<date>1996</date>
<booktitle>In Semantic Ambiguity and Underspecification.</booktitle>
<publisher>CSLI Publications, Stanford.</publisher>
<marker>van Deemter, 1996</marker>
<rawString>K. van Deemter. 1996. Towards a logic of ambiguous expressions. In Semantic Ambiguity and Underspecification. CSLI Publications, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Vestre</author>
</authors>
<title>An algorithm for generating nonredundant quantifier scopings.</title>
<date>1991</date>
<booktitle>In Proc. of EACL,</booktitle>
<location>Berlin.</location>
<contexts>
<context citStr="Vestre, 1991" endWordPosition="4363" position="25732" startWordPosition="4362">e start symbol of G0 is marked with F because G0 should only generate trees that GR cannot generate. As expected, G0 generates precisely two trees, namely (b) and (c). 5 Redundancy elimination, revisited The construction we just carried out – characterize the configurations we find interesting as the relative normal forms of an annotated rewrite system R, translate it into a transducer MR, and intersect conf(D) with the complement of the pre-image under MR – is more generally useful than just for the computation of weakest readings. We illustrate this on the problem of redundancy elimination (Vestre, 1991; Chaves, 2003; Koller et al., 2008) by showing how a variant of the algorithm of Koller et al. (2008) falls out of our technique as a special case. Redundancy elimination is the problem of computing, from a dominance graph D, another dominance graph D0 such that conf(D0) ⊆ conf(D) and 35 (s) (a) -∃y∃z∀x (d) -∃z∀x∃y (3) (1) (b) -∃y∀x∃z (e) -∀x(∃z,∃y) (1) (3) (c) -∃z∃y∀x every formula in conf(D) is logically equivalent to some formula in conf(D0). We can approximate logical equivalence using a finite system of equations such as ∃y(P,∃z(Q,R)) = ∃z(Q,∃y(P,R)), (4) indicating that ∃y and ∃z can be</context>
</contexts>
<marker>Vestre, 1991</marker>
<rawString>E. Vestre. 1991. An algorithm for generating nonredundant quantifier scopings. In Proc. of EACL, Berlin.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>