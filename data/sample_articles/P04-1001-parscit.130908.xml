<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant confidence="0.001529" no="0">
<title confidence="0.990413">
Optimization in Multimodal Interpretation
</title>
<author confidence="0.990581">
Joyce Y. Chai* Pengyu Hong+ Michelle X. Zhou‡ Zahar Prasov*
</author>
<affiliation confidence="0.99571">
*Computer Science and Engineering
Michigan State University
</affiliation>
<address confidence="0.975091">
East Lansing, MI 48824
</address>
<email confidence="0.983287">
{jchai@cse.msu.edu,
prasovz@cse.msu.edu}
</email>
<sectionHeader confidence="0.993044" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999856166666667">In a multimodal conversation, the way users communicate with a system depends on the available interaction channels and the situated context (e.g., conversation focus, visual feedback). These dependencies form a rich set of constraints from various perspectives such as temporal alignments between different modalities, coherence of conversation, and the domain semantics. There is strong evidence that competition and ranking of these constraints is important to achieve an optimal interpretation. Thus, we have developed an optimization approach for multimodal interpretation, particularly for interpreting multimodal references. A preliminary evaluation indicates the effectiveness of this approach, especially for complex user inputs that involve multiple referring expressions in a speech utterance and multiple gestures.</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.98873545">Multimodal systems provide a natural and effective way for users to interact with computers through multiple modalities such as speech, gesture, and gaze (Oviatt 1996). Since the first appearance of “Put-That-There” system (Bolt 1980), a variety of multimodal systems have emerged, from early systems that combine speech, pointing (Neal et al., 1991), and gaze (Koons et al, 1993), to systems that integrate speech with pen inputs (e.g., drawn graphics) (Cohen et al., 1996; Wahlster 1998; Wu et al., 1999), and systems that engage users in intelligent conversation (Cassell et al., 1999; Stent et al., 1999; Gustafson et al., 2000; Chai et al., 2002; Johnston et al., 2002). One important aspect of building multimodal systems is multimodal interpretation, which is a process that identifies the meanings of user inputs. ‡Intelligent Multimedia Interaction IBM T. J. Watson Research Ctr. Hawthorne, NY 10532 mzhou@us.ibm.com In a multimodal conversation, the way users communicate with a system depends on the available interaction channels and the situated context (e.g., conversation focus, visual feedback). These dependencies form a rich set of constraints from various aspects (e.g., semantic, temporal, and contextual). A correct interpretation can only be attained by simultaneously considering these constraints. In this process, two issues are important: first, a mechanism to combine information from various sources to form an overall interpretation given a set of constraints; and second, a mechanism that achieves the best interpretation among all the possible alternatives given a set of constraints. The first issue focuses on the fusion aspect, which has been well studied in earlier work, for example, through unificationbased approaches (Johnston 1998) or finite state approaches (Johnston and Bangalore, 2000). This paper focuses on the second issue of optimization. As in natural language interpretation, there is strong evidence that competition and ranking of constraints is important to achieve an optimal interpretation for multimodal language processing. We have developed a graph-based optimization approach for interpreting multimodal references. This approach achieves an optimal interpretation by simultaneously applying semantic, temporal, and contextual constraints. A preliminary evaluation indicates the effectiveness of this approach, particularly for complex user inputs that involve multiple referring expressions in a speech utterance and multiple gestures. In this paper, we first describe the necessities for optimization in multimodal interpretation, then present our graphbased optimization approach and discuss how our approach addresses key principles in Optimality Theory used for natural language interpretation (Prince and Smolensky 1993).</bodyText>
<note confidence="0.803798333333333">
+Department of Statistics
Harvard University
Cambridge, MA 02138
</note>
<email confidence="0.941563">
hong@stat.harvard.edu
</email>
<sectionHeader confidence="0.9581605" genericHeader="method">
2 Necessities for Optimization in
Multimodal Interpretation
</sectionHeader>
<bodyText confidence="0.999960681818182">In a multimodal conversation, the way a user interacts with a system is dependent not only on the available input channels (e.g., speech and gesture), but also upon his/her conversation goals, the state of the conversation, and the multimedia feedback from the system. In other words, there is a rich context that involves dependencies from many different aspects established during the interaction. Interpreting user inputs can only be situated in this rich context. For example, the temporal relations between speech and gesture are important criteria that determine how the information from these two modalities can be combined. The focus of attention from the prior conversation shapes how users refer to those objects, and thus, influences the interpretation of referring expressions. Therefore, we need to simultaneously consider the temporal relations between the referring expressions and the gestures, the semantic constraints specified by the referring expressions, and the contextual constraints from the prior conversation. It is important to have a mechanism that supports competition and ranking among these constraints to achieve an optimal interpretation, in particular, a mechanism to allow constraint violation and support soft constraints. We use temporal constraints as an example to illustrate this viewpoint1. The temporal constraints specify whether multiple modalities can be combined based on their temporal alignment. In earlier work, the temporal constraints are empirically determined based on user studies (Oviatt 1996). For example, in the unificationbased approach (Johnston 1998), one temporal constraint indicates that speech and gesture can be combined only when the speech either overlaps with gesture or follows the gesture within a certain time frame. This is a hard constraint that has to be satisfied in order for the unification to take place. If a given input does not satisfy these hard constraints, the unification fails. In our user studies, we found that, although the majority of user temporal alignment behavior may satisfy pre-defined temporal constraints, there are</bodyText>
<footnote confidence="0.98033125">
1 We implemented a system using real estate as an application
domain. The user can interact with a map using both speech
and gestures to retrieve information. All the user studies men-
tioned in this paper were conducted using this system.
</footnote>
<table confidence="0.997901">
Speech First Gesture First Total
Non-overlap 7% 45% 52%
Overlap 8% 40% 48%
Total 15% 85% 100%
</table>
<tableCaption confidence="0.9237265">
Table 1: Overall temporal relations between speech and
gesture
</tableCaption>
<figureCaption confidence="0.972078">
Figure 1: Temporal relations between speech and gesture
for individual users
some exceptions.</figureCaption>
<bodyText confidence="0.99995821875">Table 1 shows the percentage of different temporal relations collected from our user studies. The rows indicate whether there is an overlap between speech referring expressions and their accompanied gestures. The columns indicate whether the speech (more precisely, the referring expressions) or the gesture occurred first. Consistent with the previous findings (Oviatt et al, 1997), in most cases (85% of time), gestures occurred before the referring expressions were uttered. However, in 15% of the cases the speech referring expressions were uttered before the gesture occurred. Among those cases, 8% had an overlap between the referring expressions and the gesture and 7% had no overlap. Furthermore, as shown in (Oviatt et al., 2003), although multimodal behaviors such as sequential (i.e., non-overlap) or simultaneous (e.g., overlap) integration are quite consistent during the course of interaction, there are still some exceptions. Figure 1 shows the temporal alignments from seven individual users in our study. User 2 and User 6 maintained a consistent behavior in that User 2’s speech referring expressions always overlapped with gestures and User 6’s gesture always occurred ahead of the speech expressions. The other five users exhibited varied temporal alignment between speech and gesture during the interaction. It will be difficult for a system using pre-defined temporal constraints to anticipate and accommodate all these different behaviors. Therefore, it is desirable to have a mechanism that allows violation of these constraints and support soft or graded constraints.</bodyText>
<figure confidence="0.992154888888889">
Non-overlap Speech First Non-overlap Gesture First
Overlap Speech First Overlap Gesture First
Percentage 1
0.8
0.6
0.4
0.2
0 User
1 2 3 4 5 6 7
</figure>
<sectionHeader confidence="0.961437" genericHeader="evaluation and result">
3 A Graph-based Optimization Approach
</sectionHeader>
<bodyText confidence="0.9999855625">To address the necessities described above, we developed an optimization approach for interpreting multimodal references using graph matching. The graph representation captures both salient entities and their inter-relations. The graph matching is an optimization process that finds the best matching between two graphs based on constraints modeled as links or nodes in these graphs. This type of structure and process is especially useful for interpreting multimodal references. One graph can represent all the referring expressions and their inter-relations, and the other graph can represent all the potential referents. The question is how to match them together to achieve a maximum compatibility given a particular context.</bodyText>
<subsectionHeader confidence="0.9540005">
3.1 Overview
Graph-based Representation
</subsectionHeader>
<bodyText confidence="0.994137">Attribute Relation Graph (ARG) (Tsai and Fu, 1979) is used to represent information in our approach. An ARG consists of a set of nodes that are connected by a set of edges. Each node represents an entity, which in our case is either a referring expression to be resolved or a potential referent. Each node encodes the properties of the corresponding entity including:</bodyText>
<listItem confidence="0.885189">• Semantic information that indicates the semantic type, the number of potential referents, and the specific attributes related to the • Temporal information that indicates the time when the corresponding entity is introduced into the discourse (e.g., uttered or gestured).</listItem>
<figureCaption confidence="0.62142">
Speech: Compare this house, the green house
and the brown one
Figure 2: An example of a referring graph
corresponding entity (e.g., extracted from the referring expressions).</figureCaption>
<bodyText confidence="0.997192833333333">Each edge represents a set of relations between two entities. Currently we capture temporal relations and semantic type relations. A temporal relation indicates the temporal order between two related entities during an interaction, which may have one of the following values:</bodyText>
<listItem confidence="0.999384833333333">• Precede: Node A precedes Node B if the entity represented by Node A is introduced into the discourse before the entity represented by Node B. • Concurrent: Node A is concurrent with Node B if the entities represented by them are referred to or mentioned simultaneously. • Non-concurrent: Node A is non-concurrent with Node B if their corresponding objects/references cannot be referred/mentioned simultaneously. • Unknown: The temporal order between two entities is unknown. It may take the value of any of the above.</listItem>
<bodyText confidence="0.999786214285714">A semantic type relation indicates whether two related entities share the same semantic type. It currently takes the following discrete values: Same, Different, and Unknown. It could be beneficial in the future to consider a continuous function measuring the rate of compatibility instead. Specially, two graphs are generated. One graph, called the referring graph, captures referring expressions from speech utterances. For example, suppose a user says Compare this house, the green house, and the brown one. Figure 2 show a referring graph that represents three referring expressions from this speech input. Each node captures the semantic information such as the semantic type (i.e., Semantic Type), the attribute (Color), the number (Number) of the potential referents, as well as the temporal information about when this referring expression is uttered (BeginTime and EndTime). Each edge captures the semantic (e.g., SemanticTypeRelation) and temporal relations (e.g., TemporalRelation) between the referring expressions. In this case, since the green house is uttered before the brown one, there is a temporal Precede relationship between these two expressions. Furthermore, according to our heuristic that objects-to-be-compared should share the same semantic type, therefore, the SemanticTypeRelation between two nodes is set to Same.</bodyText>
<figure confidence="0.986360909090909">
SemanticType: House
Number.: 1
Attribute: Color = $Green
BeginTime: 32244242ms
EndTime: ...
SemanticTypeRelation: Same
TemporalRelation: Precede
Direction: Node 2 -&gt; Node 3
... ...
Node 2
the green
house
Node 1
this house
Node 3
the brown
one
Gesture: Point to one position and point to
another position
Chappaqua
First pointing Second pointing Conversation
Context
Semantic Type Relation: Diff
Temporal relation: Same
Direction:
Ossining
Object ID: MLS2365478
SemanticType: House
Attribute: Color = $Brown
BeginTime: 32244292 ms
SelectionProb: 0.65
... ...
Graph-matching Process
</figure>
<bodyText confidence="0.9980478">Given these graph representations, interpreting multimodal references becomes a graph-matching problem. The goal is to find the best match between a referring graph (Gs) and a referent graph (Gr). Suppose • A referring graph Gs = 〈{αm}, {ymn}), where {αm} are nodes and {ymn} are edges connecting nodes αm and αn.</bodyText>
<listItem confidence="0.973371666666667">Nodes in Gs are named referring nodes.• A referent graph Gr = 〈{ax}, {rxy}), where {ax} are nodes and {rxy} are edges connecting nodes ax and ay. Nodes in Gr are named referent nodes.</listItem>
<bodyText confidence="0.951168333333333">The following equation finds a match that achieves the maximum compatibility between Gr and Gs:</bodyText>
<figure confidence="0.993894823529412">
)
Q G G
( ,
r s
(1)
P(ax
,
)EdgeSim
)P( a
(rxy
y
,αm
αn
, )
ymn
= ExEm (ax,αm)NodeSim(ax,αm)+
LEyEmEn
</figure>
<figureCaption confidence="0.999322">
Figure 3: An example of referent graph
</figureCaption>
<bodyText confidence="0.997837586206897">Similarly, the second graph, called the referent graph, represents all potential referents from multiple sources (e.g., from the last conversation, gestured by the user, etc). Each node captures the semantic and temporal information about a potential referent (e.g., the time when the potential referent is selected by a gesture). Each edge captures the semantic and temporal relations between two potential referents. For instance, suppose the user points to one position and then points to another position. The corresponding referent graph is shown in Figure 3. The objects inside the first dashed rectangle correspond to the potential referents selected by the first pointing gesture and those inside the second dashed rectangle correspond to the second pointing gesture. Each node also contains a probability that indicates the likelihood of its corresponding object being selected by the gesture. Furthermore, the salient objects from the prior conversation are also included in the referent graph since they could also be the potential referents (e.g., the rightmost dashed rectangle in Figure 32). To create these graphs, we apply a grammarbased natural language parser to process speech inputs and a gesture recognition component to process gestures. The details are described in (Chai et al. 2004a). 2 Each node from the conversation context is linked to every node corresponding to the first pointing and the second pointing. In Equation (1), Q(Gr,Gs) measures the degree of the overall match between the referent graph and the referring graph. P(ax,αm) is the matching probability between a node ax in the referent graph and a node αm in the referring graph. The overall compatibility depends on the similarities between nodes (NodeSim) and the similarities between edges (EdgeSim). The function NodeSim(ax,αm) measures the similarity between a referent node ax and a referring node αm by combining semantic constraints and temporal constraints. The function EdgeSim(rxy,ymn) measures the similarity between rxy and ymn, which depends on the semantic and temporal constraints of the corresponding edges. These functions are described in detail in the next section. We use the graduated assignment algorithm (Gold and Rangarajan, 1996) to maximize Q(Gr,Gs) in Equation (1). The algorithm first initializes P(ax,αm) and then iteratively updates the values of P(ax,αm) until it converges. When the algorithm converges, P(ax,αm) gives the matching probabilities between the referent node ax and the referring node αm that maximizes the overall compatibility function. Given this probability matrix, the system is able to assign the most probable referent(s) to each referring expression.</bodyText>
<subsectionHeader confidence="0.998386">
3.2 Similarity Functions
</subsectionHeader>
<bodyText confidence="0.999973555555555">As shown in Equation (1), the overall compatibility between a referring graph and a referent graph depends on the node similarity function and the edge similarity function. Next we give a detailed account of how we defined these functions. Our focus here is not on the actual definitions of those functions (since they may vary for different applications), but rather a mechanism that leads to competition and ranking of constraints.</bodyText>
<subsectionHeader confidence="0.941667">
Node Similarity Function
</subsectionHeader>
<bodyText confidence="0.999438571428571">Given a referring expression (represented as αm in the referring graph) and a potential referent (represented as ax in the referent graph), the node similarity function is defined based on the semantic and temporal information captured in ax and αm through a set of individual compatibility functions:</bodyText>
<equation confidence="0.997605">
NodeSim(ax,αm) = Id(ax,αm) SemType(ax,αm)
rlk Attrk(ax,αm) Temp(ax,αm)
</equation>
<bodyText confidence="0.999265652173913">Currently, in our system, the specific return values for these functions are empirically determined through iterative regression tests. Id(ax,αm) captures the constraint of the compatibilities between identifiers specified in ax and αm. It indicates that the identifier of the potential referent, as expressed in a referring expression, should match the identifier of the true referent. This is particularly useful for resolving proper nouns. For example, if the referring expression is house number eight, then the correct referent should have the identifier number eight. We currently define this constraint as follows: Id(ax,αm) = 0 if the object identities of ax and αm are different. Id(ax,αm) = 100 if they are the same. Id(ax,αm) = 1 if at least one of the identities of ax and αm is unknown. The different return values enforce that a large reward is given to the case where the identifiers from the referring expressions match the identifiers from the potential referents. SemType(ax,αm) captures the constraint of semantic type compatibility between ax and αm. It indicates that the semantic type of a potential referent as expressed in the referring expression should match the semantic type of the correct referent. We define the following: SemType(ax,αm) = 0 if the semantic types of ax and αm are different. SemType(ax,αm) = 1 if they are the same. SemType(ax,αm) = 0.5 if at least one of the semantic types of ax and αm is unknown. Note that the return value given to the case where semantic types are the same (i.e., “1”) is much lower than that given to the case where identifiers are the same (i.e., “100”). This was designed to support constraint ranking. Our assumption is that the constraint on identifiers is more important than the constraint on semantic types. Because identifiers are usually unique, the corresponding constraint is a greater indicator of node matching if the identifier expressed from a referring expression matches the identifier of a potential referent. Attrk(ax,αm) captures the domain specific constraint concerning a particular semantic feature (indicated by the subscription k). This constraint indicates that the expected features of a potential referent as expressed in a referring expression should be compatible with features associated with the true referent. For example, in the referring expression the Victorian house, the style feature is Victorian. Therefore, an object can only be a possible referent if the style of that object is Victorian. Thus, we define the following: Ak(ax,αm) = 1 if both ax and αm share the kth feature with the same value. Ak(ax,αm) = 0 if both ax and αm have the feature k and the values of the feature k are not equal. Otherwise, when the kth feature is not present in either ax or αm, then Ak (ax,αm) = 0.1. Note that these feature constraints are dependent on the specific domain model for a particular application. Temp(ax,αm) captures the temporal constraint between a referring expression αm and a potential referent ax. As discussed in Section 2, a hard constraint concerning temporal relations between referring expressions and gestures will be incapable of handling the flexibility of user temporal alignment behavior. Thus the temporal constraint in our approach is a graded constraint, which is defined as follows:</bodyText>
<equation confidence="0.897967">
|Temp(ax,αm) = exp(− BeginTime(ax) − BeginTime(αm)
2000
</equation>
<bodyText confidence="0.9996918">This constraint indicates that the closer a referring expression and a potential referent in terms of their temporal alignment (regardless of the absolute precedence relationship), the more compatible they are.</bodyText>
<subsectionHeader confidence="0.694984">
Edge Similarity Function
</subsectionHeader>
<bodyText confidence="0.979384142857143">The edge similarity function measures the compatibility of relations held between referring expressions (i.e., an edge ymn in the referring graph) ) and relations between the potential referents (i.e., an edge rxy in the referent graph). It is defined by two individual compatibility functions as follows:</bodyText>
<equation confidence="0.557709">
EdgeSim(rxy, γmn) = SemType(rxy, γmn) Temp(rxy, γmn)
</equation>
<bodyText confidence="0.99986972972973">SemType(rxy, γmn) encodes the semantic type compatibility between an edge in the referring graph and an edge in the referent graph. It is defined in Table 2. This constraint indicates that the relation held between referring expressions should be compatible with the relation held between two correct referents. For example, consider the utterance How much is this green house and this blue house. This utterance indicates that the referent to the first expression this green house should share the same semantic type as the referent to the second expression this blue house. As shown in Table 2, if the semantic type relations of rxy and γmn are the same, SemType(rxy, γmn) returns 1. If they are different, SemType(rxy, γmn) returns zero. If either rxy or γmn is unknown, then it returns 0.5. Temp(rxy, γmn) captures the temporal compatibility between an edge in the referring graph and an edge in the referent graph. It is defined in Table 3. This constraint indicates that the temporal relationship between two referring expressions (in one utterance) should be compatible with the relations of their corresponding referents as they are introduced into the context (e.g., through gesture). The temporal relation between referring expressions (i.e., γmn) is either Precede or Concurrent. If the temporal relations of rxy and γmn are the same, then Temp(rxy, γmn) returns 1. Because potential references could come from prior conversation, even if rxy and γmn are not the same, the function does not return zero when γmn is Precede. Next, we discuss how these definitions and the process of graph matching address optimization, in particular, with respect to key principles of Optimality Theory for natural language interpretation.</bodyText>
<subsectionHeader confidence="0.999692">
3.3 Optimality Theory
</subsectionHeader>
<bodyText confidence="0.986528">Optimality Theory (OT) is a theory of language and grammar, developed by Alan Prince and Paul Smolensky (Prince and Smolensky, 1993). In Optimality Theory, a grammar consists of a set of well-formed constraints. These constraints are applied simultaneously to identify linguistic structures.</bodyText>
<table confidence="0.9978594">
SemType(rxy, γmn) rxy
Same Different Unknown
γmn Same 1 0 0.5
Different 0 1 0.5
Unknown 0.5 0.5 0.5
</table>
<tableCaption confidence="0.9185">
Table 2: Definition of SemType(rxy, γmn)
</tableCaption>
<table confidence="0.9994175">
Temp(rxy, γmn) rxy
Preceding Concurrent Non-concurrent Unknown
γmn Precede 1 0.5 0.7 0.5
Concurrent 0 1 0 0.5
</table>
<tableCaption confidence="0.999709">
Table 3: Definition of Temp(rxy, γmn)
</tableCaption>
<bodyText confidence="0.999507756756757">Optimality Theory does not restrict the content of the constraints (Eisner 1997). An innovation of Optimality Theory is the conception of these constraints as soft, which means violable and conflicting. The interpretation that arises for an utterance within a certain context maximizes the degree of constraint satisfaction and is consequently the best alternative (hence, optimal interpretation) among the set of possible interpretations. The key principles or components of Optimality Theory can be summarized as the following three components (Blutner 1998): 1) Given a set of input, Generator creates a set of possible outputs for each input.2) From the set of candidate output, Evaluator selects the optimal output for that input.3) There is a strict dominance in term of the ranking of constraints. Constraints are absolute and the ranking of the constraints is strict in the sense that outputs that have at least one violation of a higher ranked constraint outrank outputs that have arbitrarily many violations of lower ranked constraints. Although Optimality Theory is a grammar-based framework for natural language processing, its key principles can be applied to other representations. At a surface level, our approach addresses these main principles. First, in our approach, the matching matrix P(ax,αm) captures the probabilities of all the possible matches between a referring node αm and a referent node ax. The matching process updates these probabilities iteratively. This process corresponds to the Generator component in Optimality Theory. Second, in our approach, the satisfaction or violation of constraints is implemented via return values of compatibility functions.These</bodyText>
<table confidence="0.9992027">
G1: No G2: One G3: Multi- Total
Gesture Gesture Gestures Num
S1:No referring 1(1), 3(1), 0 4(2),
expression 0(0) 0(0) 0(0)
S2: One referring 6(4), 96(89), 8(7), 110(90),
expression 5(2) 58(21) 11(2) 74(25)
S3: Multiple referring 0(0), 3(1), 12(8), 15(9),
expressions 1(0) 7(1) 8(0) 16(1)
Total Num 7(5), 102(91), 20(15), 129(111)
6(2) 65(22) 19(2) 90(26)
</table>
<tableCaption confidence="0.998887">
Table 4: Evaluation Results. In each entry form “a(b), c(d)”,
</tableCaption>
<bodyText confidence="0.9301638">“a” indicates the number of inputs in which the referring expressions were correctly recognized by the speech recognizer; “b” indicates the number of inputs in which the referring expressions were correctly recognized and were correctly resolved; “c” indicates the number of inputs in which the referring expressions were not correctly recognized; “d” indicates the number of inputs in which the referring expressions also were not correctly recognized, but were correctly resolved. The sum of “a” and “c” gives the total number of inputs with a particular combination of speech and gesture. constraints can be violated during the matching process. For example, functions Id(ax,αm), SemType(ax,αm), and Attrk(ax,αm) return zero if the corresponding intended constraints are violated. In this case, the overall similarity function will return zero. However, because of the iterative updating nature of the matching algorithm, the system will still find the most optimal match as a result of the matching process even some constraints are violated. Furthermore, A function that never returns zero such as Temp(ax,αm) in the node similarity function implements a gradient constraint in Optimality Theory. Given these compatibility functions, the graph-matching algorithm provides an optimization process to find the best match between two graphs. This process corresponds to the Evaluator component of Optimality Theory. Third, in our approach, different compatibility functions return different values to address the Constraint Ranking component in Optimality Theory. For example, as discussed earlier, once ax and αm share the same identifier, Id(ax,αm) returns 100. If ax and αm share the same semantic type, SemType(ax,αm) returns 1. Here, we consider the compatibility between identifiers is more important than the compatibility between semantic types. However, currently we have not yet addressed the strict dominance aspect of Optimality Theory.</bodyText>
<subsectionHeader confidence="0.799388">
3.4 Evaluation
</subsectionHeader>
<bodyText confidence="0.999986114285714">We conducted several user studies to evaluate the performance of this approach. Users could interact with our system using both speech and deictic gestures. Each subject was asked to complete five tasks. For example, one task was to find the cheapest house in the most populated town. Data from eleven subjects was collected and analyzed. Table 4 shows the evaluation results of 219 inputs. These inputs were categorized in terms of the number of referring expressions in the speech input and the number of gestures in the gesture inputs. Out of the total 219 inputs, 137 inputs had their referents correctly interpreted. For the remaining 82 inputs in which the referents were not correctly identified, the problem did not come from the approach itself, but rather from other sources such as speech recognition and language understanding errors. These were two major error sources, which were accounted for 55% and 20% of total errors respectively (Chai et al. 2004b). In our studies, the majority of user references were simple in that they involved only one referring expression and one gesture as in earlier findings (Kehler 2000). It is trivial for our approach to handle these simple inputs since the size of the graph is usually very small and there is only one node in the referring graph. However, we did find 23% complex inputs (the row S3 and the column G3 in Table 4), which involved multiple referring expressions from speech utterances and/or multiple gestures. Our optimization approach is particularly effective to interpret these complex inputs by simultaneously considering semantic, temporal, and contextual constraints.</bodyText>
<sectionHeader confidence="0.999331" genericHeader="conclusion">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.99998415625">As in natural language interpretation addressed by Optimality Theory, the idea of optimizing constraints is beneficial and there is evidence in favor of competition and constraint ranking in multimodal language interpretation. We developed a graph-based approach to address optimization for multimodal interpretation; in particular, interpreting multimodal references. Our approach simultaneously applies temporal, semantic, and contextual constraints together and achieves the best interpretation among all alternatives. Although currently the referent graph corresponds to gesture input and conversation context, it can be easily extended to incorporate other modalities such as gaze inputs. We have only taken an initial step to investigate optimization for multimodal language processing. Although preliminary studies have shown the effectiveness of the optimization approach based on graph matching, this approach also has its limitations. The graph-matching problem is a NP complete problem and it can become intractable once the size of the graph is increased. However, we have not experienced the delay of system responses during real-time user studies. This is because most user inputs were relatively concise (they contained no more than four referring expressions). This brevity limited the size of the graphs and thus provided an opportunity for such an approach to be effective.<FR>Our future work will
address how to extend this approach to optimize
the overall interpretation of user multimodal inputs.</FR>
</bodyText>
<sectionHeader confidence="0.996039" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999936166666667">This work was partially supported by grant IIS0347548 from the National Science Foundation and grant IRGP-03-42111 from Michigan State University. The authors would like to thank John Hale and anonymous reviewers for their helpful comments and suggestions.</bodyText>
<sectionHeader confidence="0.999253" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999910209876543">
Bolt, R.A. 1980. Put that there: Voice and Gesture at the
Graphics Interface. Computer Graphics, 14(3): 262-270.
Blutner, R., 1998. Some Aspects of Optimality In Natural
Language Interpretation. Journal of Semantics, 17, 189-216.
Cassell, J., Bickmore, T., Billinghurst, M., Campbell, L.,
Chang, K., Vilhjalmsson, H. and Yan, H. 1999. Embodi-
ment in Conversational Interfaces: Rea. In Proceedings of
the CHI'99 Conference, 520-527.
Chai, J., Prasov, Z, and Hong, P. 2004b. Performance Evalua-
tion and Error Analysis for Multimodal Reference Resolu-
tion in a Conversational System. Proceedings of HLT-
NAACL 2004 (Companion Volumn).
Chai, J. Y., Hong, P., and Zhou, M. X. 2004a. A Probabilistic
Approach to Reference Resolution in Multimodal User In-
terfaces, Proceedings of 9th International Conference on
Intelligent User Interfaces (IUI): 70-77.
Chai, J., Pan, S., Zhou, M., and Houck, K. 2002. Context-
based Multimodal Interpretation in Conversational Systems.
Fourth International Conference on Multimodal Interfaces.
Cohen, P., Johnston, M., McGee, D., Oviatt, S., Pittman, J.,
Smith, I., Chen, L., and Clow, J. 1996. Quickset: Multimo-
dal Interaction for Distributed Applications. Proceedings of
ACM Multimedia.
Eisner, Jason. 1997. Efficient Generation in Primitive Opti-
mality Theory. Proceedings of ACL’97.
Gold, S. and Rangarajan, A. 1996. A Graduated Assignment
Algorithm for Graph-matching. IEEE Trans. Pattern
Analysis and Machine Intelligence, vol. 18, no. 4.
Gustafson, J., Bell, L., Beskow, J., Boye J., Carlson, R., Ed-
lund, J., Granstrom, B., House D., and Wiren, M. 2000.
AdApt – a Multimodal Conversational Dialogue System in
an Apartment Domain. Proceedings of 6th International
Conference on Spoken Language Processing (ICSLP).
Johnston, M, Cohen, P., McGee, D., Oviatt, S., Pittman, J. and
Smith, I. 1997. Unification-based Multimodal Integration,
Proceedings of ACL’97.
Johnston, M. 1998. Unification-based Multimodal Parsing,
Proceedings of COLING-ACL’98.
Johnston, M. and Bangalore, S. 2000. Finite-state Multimodal
Parsing and Understanding. Proceedings of COLING’00.
Johnston, M., Bangalore, S., Visireddy G., Stent, A., Ehlen,
P., Walker, M., Whittaker, S., and Maloor, P. 2002.
MATCH: An Architecture for Multimodal Dialog Systems,
Proceedings of ACL’02, Philadelphia, 376-383.
Kehler, A. 2000. Cognitive Status and Form of Reference in
Multimodal Human-Computer Interaction, Proceedings of
AAAI’01, 685-689.
Koons, D. B., Sparrell, C. J. and Thorisson, K. R. 1993. Inte-
grating Simultaneous Input from Speech, Gaze, and Hand
Gestures. In Intelligent Multimedia Interfaces, M. Maybury,
Ed. MIT Press: Menlo Park, CA.
Neal, J. G., and Shapiro, S. C. 1991. Intelligent Multimedia
Interface Technology. In Intelligent User Interfaces, J. Sul-
livan &amp; S. Tyler, Eds. ACM: New York.
Oviatt, S. L. 1996. Multimodal Interfaces for Dynamic Inter-
active Maps. In Proceedings of Conference on Human Fac-
tors in Computing Systems: CHI '96, 95-102.
Oviatt, S., DeAngeli, A., and Kuhn, K., 1997. Integration and
Synchronization of Input Modes during Multimodal Hu-
man-Computer Interaction, In Proceedings of Conference
on Human Factors in Computing Systems: CHI '97.
Oviatt, S., Coulston, R., Tomko, S., Xiao, B., Bunsford, R.
Wesson, M., and Carmichael, L. 2003. Toward a Theory of
Organized Multimodal Integration Patterns during Human-
Computer Interaction. In Proceedings of Fifth International
Conference on Multimodal Interfaces, 44-51.
Prince, A. and Smolensky, P. 1993. Optimality Theory. Con-
straint Interaction in Generative Grammar. ROA 537.
http://roa.rutgers.edu/view.php3?id=845.
Stent, A., J. Dowding, J. M. Gawron, E. O. Bratt, and R.
Moore. 1999. The Commandtalk Spoken Dialog System.
Proceedings of ACL’99, 183–190.
Tsai, W.H. and Fu, K.S. 1979. Error-correcting Isomorphism
of Attributed Relational Graphs for Pattern Analysis. IEEE
Transactions on Systems, Man and Cybernetics., vol. 9.
Wahlster, W., 1998. User and Discourse Models for Multimo-
dal Communication. Intelligent User Interfaces, M.
Maybury and W. Wahlster (eds.), 359-370.
Wu, L., Oviatt, S., and Cohen, P. 1999. Multimodal Integra-
tion – A Statistical View, IEEE Transactions on Multime-
dia, Vol. 1, No. 4, 334-341.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant confidence="0.861642" no="0">
<title confidence="0.999977">Optimization in Multimodal Interpretation</title>
<author confidence="0.999809">Y Pengyu Michelle X Zahar</author>
<affiliation confidence="0.992005">Science and Engineering Michigan State University</affiliation>
<address confidence="0.999779">East Lansing, MI 48824</address>
<email confidence="0.940148">{jchai@cse.msu.edu,prasovz@cse.msu.edu}</email>
<abstract confidence="0.999519">In a multimodal conversation, the way users communicate with a system depends on the available interaction channels and the situated context (e.g., conversation focus, visual feedback). These dependencies form a rich set of constraints from various perspectives such as temporal alignments between different modalities, coherence of conversation, and the domain semantics. There is strong evidence that competition and ranking of these constraints is important to achieve an optimal interpretation. Thus, we have developed an optimization approach for multimodal interpretation, particularly for interpreting multimodal references. A preliminary evaluation indicates the effectiveness of this approach, especially for complex user inputs that involve multiple referring expressions in a speech utterance and multiple gestures.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R A Bolt</author>
</authors>
<title>Put that there: Voice and Gesture at the Graphics Interface.</title>
<date>1980</date>
<journal>Computer Graphics,</journal>
<volume>14</volume>
<issue>3</issue>
<pages>262--270</pages>
<contexts>
<context citStr="Bolt 1980" endWordPosition="173" position="1311" startWordPosition="172">hieve an optimal interpretation. Thus, we have developed an optimization approach for multimodal interpretation, particularly for interpreting multimodal references. A preliminary evaluation indicates the effectiveness of this approach, especially for complex user inputs that involve multiple referring expressions in a speech utterance and multiple gestures. 1 Introduction Multimodal systems provide a natural and effective way for users to interact with computers through multiple modalities such as speech, gesture, and gaze (Oviatt 1996). Since the first appearance of “Put-That-There” system (Bolt 1980), a variety of multimodal systems have emerged, from early systems that combine speech, pointing (Neal et al., 1991), and gaze (Koons et al, 1993), to systems that integrate speech with pen inputs (e.g., drawn graphics) (Cohen et al., 1996; Wahlster 1998; Wu et al., 1999), and systems that engage users in intelligent conversation (Cassell et al., 1999; Stent et al., 1999; Gustafson et al., 2000; Chai et al., 2002; Johnston et al., 2002). One important aspect of building multimodal systems is multimodal interpretation, which is a process that identifies the meanings of user inputs. ‡Intelligent</context>
</contexts>
<marker>Bolt, 1980</marker>
<rawString>Bolt, R.A. 1980. Put that there: Voice and Gesture at the Graphics Interface. Computer Graphics, 14(3): 262-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Blutner</author>
</authors>
<title>Some Aspects of Optimality In Natural Language Interpretation.</title>
<date>1998</date>
<journal>Journal of Semantics,</journal>
<volume>17</volume>
<pages>189--216</pages>
<contexts>
<context citStr="Blutner 1998" endWordPosition="3599" position="23846" startWordPosition="3598">.5 Table 3: Definition of Temp(rxy, γmn) structures. Optimality Theory does not restrict the content of the constraints (Eisner 1997). An innovation of Optimality Theory is the conception of these constraints as soft, which means violable and conflicting. The interpretation that arises for an utterance within a certain context maximizes the degree of constraint satisfaction and is consequently the best alternative (hence, optimal interpretation) among the set of possible interpretations. The key principles or components of Optimality Theory can be summarized as the following three components (Blutner 1998): 1) Given a set of input, Generator creates a set of possible outputs for each input. 2) From the set of candidate output, Evaluator selects the optimal output for that input. 3) There is a strict dominance in term of the ranking of constraints. Constraints are absolute and the ranking of the constraints is strict in the sense that outputs that have at least one violation of a higher ranked constraint outrank outputs that have arbitrarily many violations of lower ranked constraints. Although Optimality Theory is a grammar-based framework for natural language processing, its key principles can</context>
</contexts>
<marker>Blutner, 1998</marker>
<rawString>Blutner, R., 1998. Some Aspects of Optimality In Natural Language Interpretation. Journal of Semantics, 17, 189-216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cassell</author>
<author>T Bickmore</author>
<author>M Billinghurst</author>
<author>L Campbell</author>
<author>K Chang</author>
<author>H Vilhjalmsson</author>
<author>H Yan</author>
</authors>
<date>1999</date>
<booktitle>Embodiment in Conversational Interfaces: Rea. In Proceedings of the CHI'99 Conference,</booktitle>
<pages>520--527</pages>
<contexts>
<context citStr="Cassell et al., 1999" endWordPosition="230" position="1664" startWordPosition="227">ultiple gestures. 1 Introduction Multimodal systems provide a natural and effective way for users to interact with computers through multiple modalities such as speech, gesture, and gaze (Oviatt 1996). Since the first appearance of “Put-That-There” system (Bolt 1980), a variety of multimodal systems have emerged, from early systems that combine speech, pointing (Neal et al., 1991), and gaze (Koons et al, 1993), to systems that integrate speech with pen inputs (e.g., drawn graphics) (Cohen et al., 1996; Wahlster 1998; Wu et al., 1999), and systems that engage users in intelligent conversation (Cassell et al., 1999; Stent et al., 1999; Gustafson et al., 2000; Chai et al., 2002; Johnston et al., 2002). One important aspect of building multimodal systems is multimodal interpretation, which is a process that identifies the meanings of user inputs. ‡Intelligent Multimedia Interaction IBM T. J. Watson Research Ctr. Hawthorne, NY 10532 mzhou@us.ibm.com In a multimodal conversation, the way users communicate with a system depends on the available interaction channels and the situated context (e.g., conversation focus, visual feedback). These dependencies form a rich set of constraints from various aspects (e.g</context>
</contexts>
<marker>Cassell, Bickmore, Billinghurst, Campbell, Chang, Vilhjalmsson, Yan, 1999</marker>
<rawString>Cassell, J., Bickmore, T., Billinghurst, M., Campbell, L., Chang, K., Vilhjalmsson, H. and Yan, H. 1999. Embodiment in Conversational Interfaces: Rea. In Proceedings of the CHI'99 Conference, 520-527.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chai</author>
<author>Z Prasov</author>
<author>P Hong</author>
</authors>
<title>Performance Evaluation and Error Analysis for Multimodal Reference Resolution in a Conversational System.</title>
<date>2004</date>
<booktitle>Proceedings of HLTNAACL</booktitle>
<location>(Companion Volumn).</location>
<contexts>
<context citStr="Chai et al. 2004" endWordPosition="2187" position="14739" startWordPosition="2184">d those inside the second dashed rectangle correspond to the second pointing gesture. Each node also contains a probability that indicates the likelihood of its corresponding object being selected by the gesture. Furthermore, the salient objects from the prior conversation are also included in the referent graph since they could also be the potential referents (e.g., the rightmost dashed rectangle in Figure 32). To create these graphs, we apply a grammarbased natural language parser to process speech inputs and a gesture recognition component to process gestures. The details are described in (Chai et al. 2004a). 2 Each node from the conversation context is linked to every node corresponding to the first pointing and the second pointing. In Equation (1), Q(Gr,Gs) measures the degree of the overall match between the referent graph and the referring graph. P(ax,αm) is the matching probability between a node ax in the referent graph and a node αm in the referring graph. The overall compatibility depends on the similarities between nodes (NodeSim) and the similarities between edges (EdgeSim). The function NodeSim(ax,αm) measures the similarity between a referent node ax and a referring node αm by combi</context>
<context citStr="Chai et al. 2004" endWordPosition="4287" position="28332" startWordPosition="4284">ws the evaluation results of 219 inputs. These inputs were categorized in terms of the number of referring expressions in the speech input and the number of gestures in the gesture inputs. Out of the total 219 inputs, 137 inputs had their referents correctly interpreted. For the remaining 82 inputs in which the referents were not correctly identified, the problem did not come from the approach itself, but rather from other sources such as speech recognition and language understanding errors. These were two major error sources, which were accounted for 55% and 20% of total errors respectively (Chai et al. 2004b). In our studies, the majority of user references were simple in that they involved only one referring expression and one gesture as in earlier findings (Kehler 2000). It is trivial for our approach to handle these simple inputs since the size of the graph is usually very small and there is only one node in the referring graph. However, we did find 23% complex inputs (the row S3 and the column G3 in Table 4), which involved multiple referring expressions from speech utterances and/or multiple gestures. Our optimization approach is particularly effective to interpret these complex inputs by s</context>
</contexts>
<marker>Chai, Prasov, Hong, 2004</marker>
<rawString>Chai, J., Prasov, Z, and Hong, P. 2004b. Performance Evaluation and Error Analysis for Multimodal Reference Resolution in a Conversational System. Proceedings of HLTNAACL 2004 (Companion Volumn).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Y Chai</author>
<author>P Hong</author>
<author>M X Zhou</author>
</authors>
<title>A Probabilistic Approach to Reference Resolution in Multimodal User Interfaces,</title>
<date>2004</date>
<booktitle>Proceedings of 9th International Conference on Intelligent User Interfaces (IUI):</booktitle>
<pages>70--77</pages>
<contexts>
<context citStr="Chai et al. 2004" endWordPosition="2187" position="14739" startWordPosition="2184">d those inside the second dashed rectangle correspond to the second pointing gesture. Each node also contains a probability that indicates the likelihood of its corresponding object being selected by the gesture. Furthermore, the salient objects from the prior conversation are also included in the referent graph since they could also be the potential referents (e.g., the rightmost dashed rectangle in Figure 32). To create these graphs, we apply a grammarbased natural language parser to process speech inputs and a gesture recognition component to process gestures. The details are described in (Chai et al. 2004a). 2 Each node from the conversation context is linked to every node corresponding to the first pointing and the second pointing. In Equation (1), Q(Gr,Gs) measures the degree of the overall match between the referent graph and the referring graph. P(ax,αm) is the matching probability between a node ax in the referent graph and a node αm in the referring graph. The overall compatibility depends on the similarities between nodes (NodeSim) and the similarities between edges (EdgeSim). The function NodeSim(ax,αm) measures the similarity between a referent node ax and a referring node αm by combi</context>
<context citStr="Chai et al. 2004" endWordPosition="4287" position="28332" startWordPosition="4284">ws the evaluation results of 219 inputs. These inputs were categorized in terms of the number of referring expressions in the speech input and the number of gestures in the gesture inputs. Out of the total 219 inputs, 137 inputs had their referents correctly interpreted. For the remaining 82 inputs in which the referents were not correctly identified, the problem did not come from the approach itself, but rather from other sources such as speech recognition and language understanding errors. These were two major error sources, which were accounted for 55% and 20% of total errors respectively (Chai et al. 2004b). In our studies, the majority of user references were simple in that they involved only one referring expression and one gesture as in earlier findings (Kehler 2000). It is trivial for our approach to handle these simple inputs since the size of the graph is usually very small and there is only one node in the referring graph. However, we did find 23% complex inputs (the row S3 and the column G3 in Table 4), which involved multiple referring expressions from speech utterances and/or multiple gestures. Our optimization approach is particularly effective to interpret these complex inputs by s</context>
</contexts>
<marker>Chai, Hong, Zhou, 2004</marker>
<rawString>Chai, J. Y., Hong, P., and Zhou, M. X. 2004a. A Probabilistic Approach to Reference Resolution in Multimodal User Interfaces, Proceedings of 9th International Conference on Intelligent User Interfaces (IUI): 70-77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chai</author>
<author>S Pan</author>
<author>M Zhou</author>
<author>K Houck</author>
</authors>
<title>Contextbased Multimodal Interpretation in Conversational Systems.</title>
<date>2002</date>
<booktitle>Fourth International Conference on Multimodal Interfaces.</booktitle>
<contexts>
<context citStr="Chai et al., 2002" endWordPosition="242" position="1727" startWordPosition="239">ral and effective way for users to interact with computers through multiple modalities such as speech, gesture, and gaze (Oviatt 1996). Since the first appearance of “Put-That-There” system (Bolt 1980), a variety of multimodal systems have emerged, from early systems that combine speech, pointing (Neal et al., 1991), and gaze (Koons et al, 1993), to systems that integrate speech with pen inputs (e.g., drawn graphics) (Cohen et al., 1996; Wahlster 1998; Wu et al., 1999), and systems that engage users in intelligent conversation (Cassell et al., 1999; Stent et al., 1999; Gustafson et al., 2000; Chai et al., 2002; Johnston et al., 2002). One important aspect of building multimodal systems is multimodal interpretation, which is a process that identifies the meanings of user inputs. ‡Intelligent Multimedia Interaction IBM T. J. Watson Research Ctr. Hawthorne, NY 10532 mzhou@us.ibm.com In a multimodal conversation, the way users communicate with a system depends on the available interaction channels and the situated context (e.g., conversation focus, visual feedback). These dependencies form a rich set of constraints from various aspects (e.g., semantic, temporal, and contextual). A correct interpretatio</context>
</contexts>
<marker>Chai, Pan, Zhou, Houck, 2002</marker>
<rawString>Chai, J., Pan, S., Zhou, M., and Houck, K. 2002. Contextbased Multimodal Interpretation in Conversational Systems. Fourth International Conference on Multimodal Interfaces.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cohen</author>
<author>M Johnston</author>
<author>D McGee</author>
<author>S Oviatt</author>
<author>J Pittman</author>
<author>I Smith</author>
<author>L Chen</author>
<author>J Clow</author>
</authors>
<title>Quickset: Multimodal Interaction for Distributed Applications.</title>
<date>1996</date>
<booktitle>Proceedings of ACM Multimedia.</booktitle>
<contexts>
<context citStr="Cohen et al., 1996" endWordPosition="212" position="1550" startWordPosition="209">oach, especially for complex user inputs that involve multiple referring expressions in a speech utterance and multiple gestures. 1 Introduction Multimodal systems provide a natural and effective way for users to interact with computers through multiple modalities such as speech, gesture, and gaze (Oviatt 1996). Since the first appearance of “Put-That-There” system (Bolt 1980), a variety of multimodal systems have emerged, from early systems that combine speech, pointing (Neal et al., 1991), and gaze (Koons et al, 1993), to systems that integrate speech with pen inputs (e.g., drawn graphics) (Cohen et al., 1996; Wahlster 1998; Wu et al., 1999), and systems that engage users in intelligent conversation (Cassell et al., 1999; Stent et al., 1999; Gustafson et al., 2000; Chai et al., 2002; Johnston et al., 2002). One important aspect of building multimodal systems is multimodal interpretation, which is a process that identifies the meanings of user inputs. ‡Intelligent Multimedia Interaction IBM T. J. Watson Research Ctr. Hawthorne, NY 10532 mzhou@us.ibm.com In a multimodal conversation, the way users communicate with a system depends on the available interaction channels and the situated context (e.g.,</context>
</contexts>
<marker>Cohen, Johnston, McGee, Oviatt, Pittman, Smith, Chen, Clow, 1996</marker>
<rawString>Cohen, P., Johnston, M., McGee, D., Oviatt, S., Pittman, J., Smith, I., Chen, L., and Clow, J. 1996. Quickset: Multimodal Interaction for Distributed Applications. Proceedings of ACM Multimedia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Efficient Generation in Primitive Optimality Theory.</title>
<date>1997</date>
<booktitle>Proceedings of ACL’97.</booktitle>
<contexts>
<context citStr="Eisner 1997" endWordPosition="3531" position="23366" startWordPosition="3530">d grammar, developed by Alan Prince and Paul Smolensky (Prince and Smolensky, 1993). In Optimality Theory, a grammar consists of a set of well-formed constraints. These constraints are applied simultaneously to identify linguistic SemType(rxy, γmn) rxy Same Different Unknown γmn Same 1 0 0.5 Different 0 1 0.5 Unknown 0.5 0.5 0.5 Table 2: Definition of SemType(rxy, γmn) Temp(rxy, γmn) rxy Preceding Concurrent Non-concurrent Unknown γmn Precede 1 0.5 0.7 0.5 Concurrent 0 1 0 0.5 Table 3: Definition of Temp(rxy, γmn) structures. Optimality Theory does not restrict the content of the constraints (Eisner 1997). An innovation of Optimality Theory is the conception of these constraints as soft, which means violable and conflicting. The interpretation that arises for an utterance within a certain context maximizes the degree of constraint satisfaction and is consequently the best alternative (hence, optimal interpretation) among the set of possible interpretations. The key principles or components of Optimality Theory can be summarized as the following three components (Blutner 1998): 1) Given a set of input, Generator creates a set of possible outputs for each input. 2) From the set of candidate outp</context>
</contexts>
<marker>Eisner, 1997</marker>
<rawString>Eisner, Jason. 1997. Efficient Generation in Primitive Optimality Theory. Proceedings of ACL’97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gold</author>
<author>A Rangarajan</author>
</authors>
<title>A Graduated Assignment Algorithm for Graph-matching.</title>
<date>1996</date>
<journal>IEEE Trans. Pattern Analysis and Machine Intelligence,</journal>
<volume>18</volume>
<contexts>
<context citStr="Gold and Rangarajan, 1996" endWordPosition="2330" position="15679" startWordPosition="2327">ferent graph and a node αm in the referring graph. The overall compatibility depends on the similarities between nodes (NodeSim) and the similarities between edges (EdgeSim). The function NodeSim(ax,αm) measures the similarity between a referent node ax and a referring node αm by combining semantic constraints and temporal constraints. The function EdgeSim(rxy,ymn) measures the similarity between rxy and ymn, which depends on the semantic and temporal constraints of the corresponding edges. These functions are described in detail in the next section. We use the graduated assignment algorithm (Gold and Rangarajan, 1996) to maximize Q(Gr,Gs) in Equation (1). The algorithm first initializes P(ax,αm) and then iteratively updates the values of P(ax,αm) until it converges. When the algorithm converges, P(ax,αm) gives the matching probabilities between the referent node ax and the referring node αm that maximizes the overall compatibility function. Given this probability matrix, the system is able to assign the most probable referent(s) to each referring expression. 3.2 Similarity Functions As shown in Equation (1), the overall compatibility between a referring graph and a referent graph depends on the node simila</context>
</contexts>
<marker>Gold, Rangarajan, 1996</marker>
<rawString>Gold, S. and Rangarajan, A. 1996. A Graduated Assignment Algorithm for Graph-matching. IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 18, no. 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gustafson</author>
<author>L Bell</author>
<author>J Beskow</author>
<author>J Boye</author>
<author>R Carlson</author>
<author>J Edlund</author>
<author>B Granstrom</author>
<author>D House</author>
<author>M Wiren</author>
</authors>
<date>2000</date>
<booktitle>AdApt – a Multimodal Conversational Dialogue System in an Apartment Domain. Proceedings of 6th International Conference on Spoken Language Processing (ICSLP).</booktitle>
<contexts>
<context citStr="Gustafson et al., 2000" endWordPosition="238" position="1708" startWordPosition="235">l systems provide a natural and effective way for users to interact with computers through multiple modalities such as speech, gesture, and gaze (Oviatt 1996). Since the first appearance of “Put-That-There” system (Bolt 1980), a variety of multimodal systems have emerged, from early systems that combine speech, pointing (Neal et al., 1991), and gaze (Koons et al, 1993), to systems that integrate speech with pen inputs (e.g., drawn graphics) (Cohen et al., 1996; Wahlster 1998; Wu et al., 1999), and systems that engage users in intelligent conversation (Cassell et al., 1999; Stent et al., 1999; Gustafson et al., 2000; Chai et al., 2002; Johnston et al., 2002). One important aspect of building multimodal systems is multimodal interpretation, which is a process that identifies the meanings of user inputs. ‡Intelligent Multimedia Interaction IBM T. J. Watson Research Ctr. Hawthorne, NY 10532 mzhou@us.ibm.com In a multimodal conversation, the way users communicate with a system depends on the available interaction channels and the situated context (e.g., conversation focus, visual feedback). These dependencies form a rich set of constraints from various aspects (e.g., semantic, temporal, and contextual). A co</context>
</contexts>
<marker>Gustafson, Bell, Beskow, Boye, Carlson, Edlund, Granstrom, House, Wiren, 2000</marker>
<rawString>Gustafson, J., Bell, L., Beskow, J., Boye J., Carlson, R., Edlund, J., Granstrom, B., House D., and Wiren, M. 2000. AdApt – a Multimodal Conversational Dialogue System in an Apartment Domain. Proceedings of 6th International Conference on Spoken Language Processing (ICSLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnston</author>
<author>P Cohen</author>
<author>D McGee</author>
<author>S Oviatt</author>
<author>J Pittman</author>
<author>I Smith</author>
</authors>
<title>Unification-based Multimodal Integration,</title>
<date>1997</date>
<booktitle>Proceedings of ACL’97.</booktitle>
<marker>Johnston, Cohen, McGee, Oviatt, Pittman, Smith, 1997</marker>
<rawString>Johnston, M, Cohen, P., McGee, D., Oviatt, S., Pittman, J. and Smith, I. 1997. Unification-based Multimodal Integration, Proceedings of ACL’97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnston</author>
</authors>
<title>Unification-based Multimodal Parsing,</title>
<date>1998</date>
<booktitle>Proceedings of COLING-ACL’98.</booktitle>
<contexts>
<context citStr="Johnston 1998" endWordPosition="403" position="2848" startWordPosition="402">ints from various aspects (e.g., semantic, temporal, and contextual). A correct interpretation can only be attained by simultaneously considering these constraints. In this process, two issues are important: first, a mechanism to combine information from various sources to form an overall interpretation given a set of constraints; and second, a mechanism that achieves the best interpretation among all the possible alternatives given a set of constraints. The first issue focuses on the fusion aspect, which has been well studied in earlier work, for example, through unificationbased approaches (Johnston 1998) or finite state approaches (Johnston and Bangalore, 2000). This paper focuses on the second issue of optimization. As in natural language interpretation, there is strong evidence that competition and ranking of constraints is important to achieve an optimal interpretation for multimodal language processing. We have developed a graph-based optimization approach for interpreting multimodal references. This approach achieves an optimal interpretation by simultaneously applying semantic, temporal, and contextual constraints. A preliminary evaluation indicates the effectiveness of this approach, p</context>
<context citStr="Johnston 1998" endWordPosition="787" position="5622" startWordPosition="786">nstraints from the prior conversation. It is important to have a mechanism that supports competition and ranking among these constraints to achieve an optimal interpretation, in particular, a mechanism to allow constraint violation and support soft constraints. We use temporal constraints as an example to illustrate this viewpoint1. The temporal constraints specify whether multiple modalities can be combined based on their temporal alignment. In earlier work, the temporal constraints are empirically determined based on user studies (Oviatt 1996). For example, in the unificationbased approach (Johnston 1998), one temporal constraint indicates that speech and gesture can be combined only when the speech either overlaps with gesture or follows the gesture within a certain time frame. This is a hard constraint that has to be satisfied in order for the unification to take place. If a given input does not satisfy these hard constraints, the unification fails. In our user studies, we found that, although the majority of user temporal alignment behavior may satisfy pre-defined temporal constraints, there are 1 We implemented a system using real estate as an application domain. The user can interact with</context>
</contexts>
<marker>Johnston, 1998</marker>
<rawString>Johnston, M. 1998. Unification-based Multimodal Parsing, Proceedings of COLING-ACL’98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnston</author>
<author>S Bangalore</author>
</authors>
<title>Finite-state Multimodal Parsing and Understanding.</title>
<date>2000</date>
<booktitle>Proceedings of COLING’00.</booktitle>
<contexts>
<context citStr="Johnston and Bangalore, 2000" endWordPosition="411" position="2906" startWordPosition="408">temporal, and contextual). A correct interpretation can only be attained by simultaneously considering these constraints. In this process, two issues are important: first, a mechanism to combine information from various sources to form an overall interpretation given a set of constraints; and second, a mechanism that achieves the best interpretation among all the possible alternatives given a set of constraints. The first issue focuses on the fusion aspect, which has been well studied in earlier work, for example, through unificationbased approaches (Johnston 1998) or finite state approaches (Johnston and Bangalore, 2000). This paper focuses on the second issue of optimization. As in natural language interpretation, there is strong evidence that competition and ranking of constraints is important to achieve an optimal interpretation for multimodal language processing. We have developed a graph-based optimization approach for interpreting multimodal references. This approach achieves an optimal interpretation by simultaneously applying semantic, temporal, and contextual constraints. A preliminary evaluation indicates the effectiveness of this approach, particularly for complex user inputs that involve multiple </context>
</contexts>
<marker>Johnston, Bangalore, 2000</marker>
<rawString>Johnston, M. and Bangalore, S. 2000. Finite-state Multimodal Parsing and Understanding. Proceedings of COLING’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnston</author>
<author>S Bangalore</author>
<author>G Visireddy</author>
<author>A Stent</author>
<author>P Ehlen</author>
<author>M Walker</author>
<author>S Whittaker</author>
<author>P Maloor</author>
</authors>
<title>MATCH: An Architecture for Multimodal Dialog Systems,</title>
<date>2002</date>
<booktitle>Proceedings of ACL’02,</booktitle>
<pages>376--383</pages>
<location>Philadelphia,</location>
<contexts>
<context citStr="Johnston et al., 2002" endWordPosition="246" position="1751" startWordPosition="243">ay for users to interact with computers through multiple modalities such as speech, gesture, and gaze (Oviatt 1996). Since the first appearance of “Put-That-There” system (Bolt 1980), a variety of multimodal systems have emerged, from early systems that combine speech, pointing (Neal et al., 1991), and gaze (Koons et al, 1993), to systems that integrate speech with pen inputs (e.g., drawn graphics) (Cohen et al., 1996; Wahlster 1998; Wu et al., 1999), and systems that engage users in intelligent conversation (Cassell et al., 1999; Stent et al., 1999; Gustafson et al., 2000; Chai et al., 2002; Johnston et al., 2002). One important aspect of building multimodal systems is multimodal interpretation, which is a process that identifies the meanings of user inputs. ‡Intelligent Multimedia Interaction IBM T. J. Watson Research Ctr. Hawthorne, NY 10532 mzhou@us.ibm.com In a multimodal conversation, the way users communicate with a system depends on the available interaction channels and the situated context (e.g., conversation focus, visual feedback). These dependencies form a rich set of constraints from various aspects (e.g., semantic, temporal, and contextual). A correct interpretation can only be attained b</context>
</contexts>
<marker>Johnston, Bangalore, Visireddy, Stent, Ehlen, Walker, Whittaker, Maloor, 2002</marker>
<rawString>Johnston, M., Bangalore, S., Visireddy G., Stent, A., Ehlen, P., Walker, M., Whittaker, S., and Maloor, P. 2002. MATCH: An Architecture for Multimodal Dialog Systems, Proceedings of ACL’02, Philadelphia, 376-383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kehler</author>
</authors>
<title>Cognitive Status and Form of Reference in Multimodal Human-Computer Interaction,</title>
<date>2000</date>
<booktitle>Proceedings of AAAI’01,</booktitle>
<pages>685--689</pages>
<contexts>
<context citStr="Kehler 2000" endWordPosition="4314" position="28500" startWordPosition="4313"> gesture inputs. Out of the total 219 inputs, 137 inputs had their referents correctly interpreted. For the remaining 82 inputs in which the referents were not correctly identified, the problem did not come from the approach itself, but rather from other sources such as speech recognition and language understanding errors. These were two major error sources, which were accounted for 55% and 20% of total errors respectively (Chai et al. 2004b). In our studies, the majority of user references were simple in that they involved only one referring expression and one gesture as in earlier findings (Kehler 2000). It is trivial for our approach to handle these simple inputs since the size of the graph is usually very small and there is only one node in the referring graph. However, we did find 23% complex inputs (the row S3 and the column G3 in Table 4), which involved multiple referring expressions from speech utterances and/or multiple gestures. Our optimization approach is particularly effective to interpret these complex inputs by simultaneously considering semantic, temporal, and contextual constraints. 4 Conclusion As in natural language interpretation addressed by Optimality Theory, the idea of</context>
</contexts>
<marker>Kehler, 2000</marker>
<rawString>Kehler, A. 2000. Cognitive Status and Form of Reference in Multimodal Human-Computer Interaction, Proceedings of AAAI’01, 685-689.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D B Koons</author>
<author>C J Sparrell</author>
<author>K R Thorisson</author>
</authors>
<title>Integrating Simultaneous Input from Speech, Gaze, and Hand Gestures.</title>
<date>1993</date>
<booktitle>In Intelligent Multimedia Interfaces,</booktitle>
<publisher>MIT Press: Menlo</publisher>
<location>Park, CA.</location>
<contexts>
<context citStr="Koons et al, 1993" endWordPosition="197" position="1457" startWordPosition="194">ting multimodal references. A preliminary evaluation indicates the effectiveness of this approach, especially for complex user inputs that involve multiple referring expressions in a speech utterance and multiple gestures. 1 Introduction Multimodal systems provide a natural and effective way for users to interact with computers through multiple modalities such as speech, gesture, and gaze (Oviatt 1996). Since the first appearance of “Put-That-There” system (Bolt 1980), a variety of multimodal systems have emerged, from early systems that combine speech, pointing (Neal et al., 1991), and gaze (Koons et al, 1993), to systems that integrate speech with pen inputs (e.g., drawn graphics) (Cohen et al., 1996; Wahlster 1998; Wu et al., 1999), and systems that engage users in intelligent conversation (Cassell et al., 1999; Stent et al., 1999; Gustafson et al., 2000; Chai et al., 2002; Johnston et al., 2002). One important aspect of building multimodal systems is multimodal interpretation, which is a process that identifies the meanings of user inputs. ‡Intelligent Multimedia Interaction IBM T. J. Watson Research Ctr. Hawthorne, NY 10532 mzhou@us.ibm.com In a multimodal conversation, the way users communicat</context>
</contexts>
<marker>Koons, Sparrell, Thorisson, 1993</marker>
<rawString>Koons, D. B., Sparrell, C. J. and Thorisson, K. R. 1993. Integrating Simultaneous Input from Speech, Gaze, and Hand Gestures. In Intelligent Multimedia Interfaces, M. Maybury, Ed. MIT Press: Menlo Park, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Neal</author>
<author>S C Shapiro</author>
</authors>
<title>Intelligent Multimedia Interface Technology.</title>
<date>1991</date>
<booktitle>In Intelligent User Interfaces,</booktitle>
<publisher>ACM:</publisher>
<location>New York.</location>
<marker>Neal, Shapiro, 1991</marker>
<rawString>Neal, J. G., and Shapiro, S. C. 1991. Intelligent Multimedia Interface Technology. In Intelligent User Interfaces, J. Sullivan &amp; S. Tyler, Eds. ACM: New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Oviatt</author>
</authors>
<title>Multimodal Interfaces for Dynamic Interactive Maps.</title>
<date>1996</date>
<booktitle>In Proceedings of Conference on Human Factors in Computing Systems: CHI '96,</booktitle>
<pages>95--102</pages>
<contexts>
<context citStr="Oviatt 1996" endWordPosition="164" position="1244" startWordPosition="163"> that competition and ranking of these constraints is important to achieve an optimal interpretation. Thus, we have developed an optimization approach for multimodal interpretation, particularly for interpreting multimodal references. A preliminary evaluation indicates the effectiveness of this approach, especially for complex user inputs that involve multiple referring expressions in a speech utterance and multiple gestures. 1 Introduction Multimodal systems provide a natural and effective way for users to interact with computers through multiple modalities such as speech, gesture, and gaze (Oviatt 1996). Since the first appearance of “Put-That-There” system (Bolt 1980), a variety of multimodal systems have emerged, from early systems that combine speech, pointing (Neal et al., 1991), and gaze (Koons et al, 1993), to systems that integrate speech with pen inputs (e.g., drawn graphics) (Cohen et al., 1996; Wahlster 1998; Wu et al., 1999), and systems that engage users in intelligent conversation (Cassell et al., 1999; Stent et al., 1999; Gustafson et al., 2000; Chai et al., 2002; Johnston et al., 2002). One important aspect of building multimodal systems is multimodal interpretation, which is </context>
<context citStr="Oviatt 1996" endWordPosition="778" position="5559" startWordPosition="777">specified by the referring expressions, and the contextual constraints from the prior conversation. It is important to have a mechanism that supports competition and ranking among these constraints to achieve an optimal interpretation, in particular, a mechanism to allow constraint violation and support soft constraints. We use temporal constraints as an example to illustrate this viewpoint1. The temporal constraints specify whether multiple modalities can be combined based on their temporal alignment. In earlier work, the temporal constraints are empirically determined based on user studies (Oviatt 1996). For example, in the unificationbased approach (Johnston 1998), one temporal constraint indicates that speech and gesture can be combined only when the speech either overlaps with gesture or follows the gesture within a certain time frame. This is a hard constraint that has to be satisfied in order for the unification to take place. If a given input does not satisfy these hard constraints, the unification fails. In our user studies, we found that, although the majority of user temporal alignment behavior may satisfy pre-defined temporal constraints, there are 1 We implemented a system using r</context>
</contexts>
<marker>Oviatt, 1996</marker>
<rawString>Oviatt, S. L. 1996. Multimodal Interfaces for Dynamic Interactive Maps. In Proceedings of Conference on Human Factors in Computing Systems: CHI '96, 95-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oviatt</author>
<author>A DeAngeli</author>
<author>K Kuhn</author>
</authors>
<title>Integration and Synchronization of Input Modes during Multimodal Human-Computer Interaction,</title>
<date>1997</date>
<booktitle>In Proceedings of Conference on Human Factors in Computing Systems: CHI '97.</booktitle>
<contexts>
<context citStr="Oviatt et al, 1997" endWordPosition="1003" position="6997" startWordPosition="1000">esture First Total Non-overlap 7% 45% 52% Overlap 8% 40% 48% Total 15% 85% 100% Table 1: Overall temporal relations between speech and gesture Figure 1: Temporal relations between speech and gesture for individual users some exceptions. Table 1 shows the percentage of different temporal relations collected from our user studies. The rows indicate whether there is an overlap between speech referring expressions and their accompanied gestures. The columns indicate whether the speech (more precisely, the referring expressions) or the gesture occurred first. Consistent with the previous findings (Oviatt et al, 1997), in most cases (85% of time), gestures occurred before the referring expressions were uttered. However, in 15% of the cases the speech referring expressions were uttered before the gesture occurred. Among those cases, 8% had an overlap between the referring expressions and the gesture and 7% had no overlap. Furthermore, as shown in (Oviatt et al., 2003), although multimodal behaviors such as sequential (i.e., non-overlap) or simultaneous (e.g., overlap) integration are quite consistent during the course of interaction, there are still some exceptions. Figure 1 shows the temporal alignments fr</context>
</contexts>
<marker>Oviatt, DeAngeli, Kuhn, 1997</marker>
<rawString>Oviatt, S., DeAngeli, A., and Kuhn, K., 1997. Integration and Synchronization of Input Modes during Multimodal Human-Computer Interaction, In Proceedings of Conference on Human Factors in Computing Systems: CHI '97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oviatt</author>
<author>R Coulston</author>
<author>S Tomko</author>
<author>B Xiao</author>
<author>R Wesson Bunsford</author>
<author>M</author>
<author>L Carmichael</author>
</authors>
<title>Toward a Theory of Organized Multimodal Integration Patterns during HumanComputer Interaction.</title>
<date>2003</date>
<booktitle>In Proceedings of Fifth International Conference on Multimodal Interfaces,</booktitle>
<pages>44--51</pages>
<contexts>
<context citStr="Oviatt et al., 2003" endWordPosition="1060" position="7353" startWordPosition="1057">r there is an overlap between speech referring expressions and their accompanied gestures. The columns indicate whether the speech (more precisely, the referring expressions) or the gesture occurred first. Consistent with the previous findings (Oviatt et al, 1997), in most cases (85% of time), gestures occurred before the referring expressions were uttered. However, in 15% of the cases the speech referring expressions were uttered before the gesture occurred. Among those cases, 8% had an overlap between the referring expressions and the gesture and 7% had no overlap. Furthermore, as shown in (Oviatt et al., 2003), although multimodal behaviors such as sequential (i.e., non-overlap) or simultaneous (e.g., overlap) integration are quite consistent during the course of interaction, there are still some exceptions. Figure 1 shows the temporal alignments from seven individual users in our study. User 2 and User 6 maintained a consistent behavior in that User 2’s speech referring expressions always overlapped with gestures and User 6’s gesture always occurred ahead of the speech expressions. The other five users exhibited varied temporal alignment between speech and gesture during the interaction. It will b</context>
</contexts>
<marker>Oviatt, Coulston, Tomko, Xiao, Bunsford, M, Carmichael, 2003</marker>
<rawString>Oviatt, S., Coulston, R., Tomko, S., Xiao, B., Bunsford, R. Wesson, M., and Carmichael, L. 2003. Toward a Theory of Organized Multimodal Integration Patterns during HumanComputer Interaction. In Proceedings of Fifth International Conference on Multimodal Interfaces, 44-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Prince</author>
<author>P Smolensky</author>
</authors>
<title>Optimality Theory. Constraint Interaction in Generative Grammar.</title>
<date>1993</date>
<journal>ROA</journal>
<volume>537</volume>
<pages>3--845</pages>
<contexts>
<context citStr="Prince and Smolensky 1993" endWordPosition="537" position="3862" startWordPosition="534">dal references. This approach achieves an optimal interpretation by simultaneously applying semantic, temporal, and contextual constraints. A preliminary evaluation indicates the effectiveness of this approach, particularly for complex user inputs that involve multiple referring expressions in a speech utterance and multiple gestures. In this paper, we first describe the necessities for optimization in multimodal interpretation, then present our graphbased optimization approach and discuss how our approach addresses key principles in Optimality Theory used for natural language interpretation (Prince and Smolensky 1993). +Department of Statistics Harvard University Cambridge, MA 02138 hong@stat.harvard.edu 2 Necessities for Optimization in Multimodal Interpretation In a multimodal conversation, the way a user interacts with a system is dependent not only on the available input channels (e.g., speech and gesture), but also upon his/her conversation goals, the state of the conversation, and the multimedia feedback from the system. In other words, there is a rich context that involves dependencies from many different aspects established during the interaction. Interpreting user inputs can only be situated in th</context>
<context citStr="Prince and Smolensky, 1993" endWordPosition="3449" position="22837" startWordPosition="3446">either Precede or Concurrent. If the temporal relations of rxy and γmn are the same, then Temp(rxy, γmn) returns 1. Because potential references could come from prior conversation, even if rxy and γmn are not the same, the function does not return zero when γmn is Precede. Next, we discuss how these definitions and the process of graph matching address optimization, in particular, with respect to key principles of Optimality Theory for natural language interpretation. 3.3 Optimality Theory Optimality Theory (OT) is a theory of language and grammar, developed by Alan Prince and Paul Smolensky (Prince and Smolensky, 1993). In Optimality Theory, a grammar consists of a set of well-formed constraints. These constraints are applied simultaneously to identify linguistic SemType(rxy, γmn) rxy Same Different Unknown γmn Same 1 0 0.5 Different 0 1 0.5 Unknown 0.5 0.5 0.5 Table 2: Definition of SemType(rxy, γmn) Temp(rxy, γmn) rxy Preceding Concurrent Non-concurrent Unknown γmn Precede 1 0.5 0.7 0.5 Concurrent 0 1 0 0.5 Table 3: Definition of Temp(rxy, γmn) structures. Optimality Theory does not restrict the content of the constraints (Eisner 1997). An innovation of Optimality Theory is the conception of these constra</context>
</contexts>
<marker>Prince, Smolensky, 1993</marker>
<rawString>Prince, A. and Smolensky, P. 1993. Optimality Theory. Constraint Interaction in Generative Grammar. ROA 537. http://roa.rutgers.edu/view.php3?id=845.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stent</author>
<author>J Dowding</author>
<author>J M Gawron</author>
<author>E O Bratt</author>
<author>R Moore</author>
</authors>
<title>The Commandtalk Spoken Dialog System.</title>
<date>1999</date>
<booktitle>Proceedings of ACL’99,</booktitle>
<pages>183--190</pages>
<contexts>
<context citStr="Stent et al., 1999" endWordPosition="234" position="1684" startWordPosition="231">troduction Multimodal systems provide a natural and effective way for users to interact with computers through multiple modalities such as speech, gesture, and gaze (Oviatt 1996). Since the first appearance of “Put-That-There” system (Bolt 1980), a variety of multimodal systems have emerged, from early systems that combine speech, pointing (Neal et al., 1991), and gaze (Koons et al, 1993), to systems that integrate speech with pen inputs (e.g., drawn graphics) (Cohen et al., 1996; Wahlster 1998; Wu et al., 1999), and systems that engage users in intelligent conversation (Cassell et al., 1999; Stent et al., 1999; Gustafson et al., 2000; Chai et al., 2002; Johnston et al., 2002). One important aspect of building multimodal systems is multimodal interpretation, which is a process that identifies the meanings of user inputs. ‡Intelligent Multimedia Interaction IBM T. J. Watson Research Ctr. Hawthorne, NY 10532 mzhou@us.ibm.com In a multimodal conversation, the way users communicate with a system depends on the available interaction channels and the situated context (e.g., conversation focus, visual feedback). These dependencies form a rich set of constraints from various aspects (e.g., semantic, tempora</context>
</contexts>
<marker>Stent, Dowding, Gawron, Bratt, Moore, 1999</marker>
<rawString>Stent, A., J. Dowding, J. M. Gawron, E. O. Bratt, and R. Moore. 1999. The Commandtalk Spoken Dialog System. Proceedings of ACL’99, 183–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W H Tsai</author>
<author>K S Fu</author>
</authors>
<title>Error-correcting Isomorphism of Attributed Relational Graphs for Pattern Analysis.</title>
<date>1979</date>
<journal>IEEE Transactions on Systems, Man and Cybernetics.,</journal>
<volume>9</volume>
<contexts>
<context citStr="Tsai and Fu, 1979" endWordPosition="1333" position="9210" startWordPosition="1330"> their inter-relations. The graph matching is an optimization process that finds the best matching between two graphs based on constraints modeled as links or nodes in these graphs. This type of structure and process is especially useful for interpreting multimodal references. One graph can represent all the referring expressions and their inter-relations, and the other graph can represent all the potential referents. The question is how to match them together to achieve a maximum compatibility given a particular context. 3.1 Overview Graph-based Representation Attribute Relation Graph (ARG) (Tsai and Fu, 1979) is used to represent information in our approach. An ARG consists of a set of nodes that are connected by a set of edges. Each node represents an entity, which in our case is either a referring expression to be resolved or a potential referent. Each node encodes the properties of the corresponding entity including: • Semantic information that indicates the semantic type, the number of potential referents, and the specific attributes related to the Speech: Compare this house, the green house and the brown one Figure 2: An example of a referring graph corresponding entity (e.g., extracted from </context>
</contexts>
<marker>Tsai, Fu, 1979</marker>
<rawString>Tsai, W.H. and Fu, K.S. 1979. Error-correcting Isomorphism of Attributed Relational Graphs for Pattern Analysis. IEEE Transactions on Systems, Man and Cybernetics., vol. 9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wahlster</author>
</authors>
<title>User and Discourse Models for Multimodal Communication. Intelligent</title>
<date>1998</date>
<pages>359--370</pages>
<editor>User Interfaces, M. Maybury and W. Wahlster (eds.),</editor>
<contexts>
<context citStr="Wahlster 1998" endWordPosition="214" position="1565" startWordPosition="213"> complex user inputs that involve multiple referring expressions in a speech utterance and multiple gestures. 1 Introduction Multimodal systems provide a natural and effective way for users to interact with computers through multiple modalities such as speech, gesture, and gaze (Oviatt 1996). Since the first appearance of “Put-That-There” system (Bolt 1980), a variety of multimodal systems have emerged, from early systems that combine speech, pointing (Neal et al., 1991), and gaze (Koons et al, 1993), to systems that integrate speech with pen inputs (e.g., drawn graphics) (Cohen et al., 1996; Wahlster 1998; Wu et al., 1999), and systems that engage users in intelligent conversation (Cassell et al., 1999; Stent et al., 1999; Gustafson et al., 2000; Chai et al., 2002; Johnston et al., 2002). One important aspect of building multimodal systems is multimodal interpretation, which is a process that identifies the meanings of user inputs. ‡Intelligent Multimedia Interaction IBM T. J. Watson Research Ctr. Hawthorne, NY 10532 mzhou@us.ibm.com In a multimodal conversation, the way users communicate with a system depends on the available interaction channels and the situated context (e.g., conversation f</context>
</contexts>
<marker>Wahlster, 1998</marker>
<rawString>Wahlster, W., 1998. User and Discourse Models for Multimodal Communication. Intelligent User Interfaces, M. Maybury and W. Wahlster (eds.), 359-370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Wu</author>
<author>S Oviatt</author>
<author>P Cohen</author>
</authors>
<date>1999</date>
<journal>Multimodal Integration – A Statistical View, IEEE Transactions on Multimedia,</journal>
<volume>1</volume>
<pages>334--341</pages>
<contexts>
<context citStr="Wu et al., 1999" endWordPosition="218" position="1583" startWordPosition="215">nputs that involve multiple referring expressions in a speech utterance and multiple gestures. 1 Introduction Multimodal systems provide a natural and effective way for users to interact with computers through multiple modalities such as speech, gesture, and gaze (Oviatt 1996). Since the first appearance of “Put-That-There” system (Bolt 1980), a variety of multimodal systems have emerged, from early systems that combine speech, pointing (Neal et al., 1991), and gaze (Koons et al, 1993), to systems that integrate speech with pen inputs (e.g., drawn graphics) (Cohen et al., 1996; Wahlster 1998; Wu et al., 1999), and systems that engage users in intelligent conversation (Cassell et al., 1999; Stent et al., 1999; Gustafson et al., 2000; Chai et al., 2002; Johnston et al., 2002). One important aspect of building multimodal systems is multimodal interpretation, which is a process that identifies the meanings of user inputs. ‡Intelligent Multimedia Interaction IBM T. J. Watson Research Ctr. Hawthorne, NY 10532 mzhou@us.ibm.com In a multimodal conversation, the way users communicate with a system depends on the available interaction channels and the situated context (e.g., conversation focus, visual feedb</context>
</contexts>
<marker>Wu, Oviatt, Cohen, 1999</marker>
<rawString>Wu, L., Oviatt, S., and Cohen, P. 1999. Multimodal Integration – A Statistical View, IEEE Transactions on Multimedia, Vol. 1, No. 4, 334-341.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>