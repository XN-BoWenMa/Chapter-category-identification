<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant confidence="0.000002" no="0">
<note confidence="0.4587112">
Statistical Machine Translation Features with Multitask Tensor Networks
Hendra Setiawan, Zhongqiang Huang, Jacob Devlin†∗, Thomas Lamar,
Rabih Zbib, Richard Schwartz and John Makhoul
Raytheon BBN Technologies, 10 Moulton St, Cambridge, MA 02138, USA
†Microsoft Research, One Microsoft Way, Redmond, WA 98052, USA
</note>
<email confidence="0.9874465">
{hsetiawa,zhuang,tlamar,rzbib,schwartz,makhoul}@bbn.com
jdevlin@microsoft.com
</email>
<sectionHeader confidence="0.994671" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999263">We present a three-pronged approach to improving Statistical Machine Translation (SMT), building on recent success in the application of neural networks to SMT. First, we propose new features based on neural networks to model various nonlocal translation phenomena. Second, we augment the architecture of the neural network with tensor layers that capture important higher-order interaction among the network units. Third, we apply multitask learning to estimate the neural network parameters jointly. Each of our proposed methods results in significant improvements that are complementary. The overall improvement is +2.7 and +1.8 BLEU points for Arabic-English and ChineseEnglish translation over a state-of-the-art system that already includes neural network features.</bodyText>
<sectionHeader confidence="0.998781" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.98001934375">Recent advances in applying Neural Networks to Statistical Machine Translation (SMT) have generally taken one of two approaches. They either develop neural network-based features that are used to score hypotheses generated from traditional translation grammars (Sundermeyer et al., 2014; Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012), or they implement the whole translation process as a single neural network (Bahdanau et al., 2014; Sutskever et al., 2014). The latter approach, sometimes referred to as Neural Machine Translation, attempts to overhaul SMT, while the former capitalizes on the strength of the current SMT paradigm and leverages the modeling power of neural networks to improve the scoring of hypotheses generated ∗* Research conducted when the author was at BBN. by phrase-based or hierarchical translation rules. This paper adopts the former approach, as n-best scores from state-of-the-art SMT systems often suggest that these systems can still be significantly improved with better features. We build on (Devlin et al., 2014) who proposed a simple yet powerful feedforward neural network model that estimates the translation probability conditioned on the target history and a large window of source word context. We take advantage of neural networks’ ability to handle sparsity, and to infer useful abstract representations automatically. At the same time, we address the challenge of learning the large set of neural network parameters. In particular,</bodyText>
<listItem confidence="0.988329222222222">• We develop new Neural Network Features to model non-local translation phenomena related to word reordering. Large fullylexicalized contexts are used to model these phenomena effectively, making the use of neural networks essential. All of the features are useful individually, and their combination results in significant improvements (Section 2). • We use a Tensor Neural Network Architecture (Yu et al., 2012) to automatically learn complex pairwise interactions between the network nodes. The introduction of the tensor hidden layer results in more powerful features with lower model perplexity and significantly improved MT performance for all of neural network features (Section 3). • We apply Multitask Learning (MTL) (Caruana, 1997) to jointly train related neural network features by sharing parameters.</listItem>
<bodyText confidence="0.88862175">This allows parameters learned for one feature to benefit the learning of the other features. This results in better trained models and achieves additional MT improvements (Section 4). We apply the resulting Multitask Tensor Networks to the new features and to existing ones, obtaining strong experimental results over the strongest previous results of (Devlin et al., 2014).</bodyText>
<page confidence="0.998552">
31
</page>
<note confidence="0.978664">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 31–41,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999206">We obtain improvements of +2.5 BLEU points for Arabic-English and +1.8 BLEU points for Chinese-English on the DARPA BOLT Web Forum condition. We also obtain improvements of +2.7 BLEU point for Arabic-English and +1.9 BLEU points for Chinese-English on the NIST Open12 test sets over the best previously published results in (Devlin et al., 2014). Both the tensor architecture and multitask learning are general techniques that are likely to benefit other neural network features.</bodyText>
<sectionHeader confidence="0.955244" genericHeader="method">
2 New Non-Local SMT Features
</sectionHeader>
<bodyText confidence="0.999885675">Existing SMT features typically focus on local information in the source sentence, in the target hypothesis, or both. For example, the n-gram language model (LM) predicts the next target word by using previously generated target words as context (local on target), while the lexical translation model (LTM) predicts the translation of a source word by taking into account surrounding source words as context (local on source). In this work, we focus on non-local translation phenomena that result from non-monotone reordering, where local context becomes non-local on the other side. We propose a new set of powerful MT features that are motivated by this simple idea. To facilitate the discussion, we categorize the features into hypothesis-enumerating features that estimates a probability for each generated target word (e.g., n-gram language model), and sourceenumerating features that estimates a probability for each source word (e.g., lexical translation). More concretely, we introduce a) Joint Model with Offset Source Context (JMO), a hypothesis enumerating feature that predicts the next target word the source context affiliated to the previous target words; and b) Translation Context Model (TCM), a source-enumerating feature that predicts the context of the translation of a source word rather than the translation itself. These two models extend pre-existing features: the Joint (language and translation) Model (JM) of (Devlin et al., 2014) and the LTM respectively respectively. We use a large lexicalized context for there features, making the choice of implementing them as neural networks essential. We also present neuralnetwork implementations of pre-existing sourceenumerating features: lexical translation, orientation and fertility models. We obtain additional gains from using tensor networks and multitask learning in the modeling and training of all the features.</bodyText>
<subsectionHeader confidence="0.99714">
2.1 Hypothesis-Enumerating Features
</subsectionHeader>
<bodyText confidence="0.999862533333334">As mentioned, hypothesis-enumerating features score each word in the hypothesis, typically by conditioning it on a context of n-1 previous target words as in the n-gram language model. One recent such model, the joint model of Devlin et al. (2014) achieves large improvements to the stateof-the-art SMT by using a large context window of 11 source words and 3 target words. The Joint Model with Offset Source Context (JMO) is an extension of the JM that uses the source words affiliated with the n-gram target history as context. The source contexts of JM and JMO overlap highly when the translation is monotone, but are complementary when the translation requires word reordering.</bodyText>
<subsubsectionHeader confidence="0.411234">
2.1.1 Joint Model with Offset Source Context
</subsubsectionHeader>
<bodyText confidence="0.99978608">Formally, JMO estimates the probability of the target hypothesis E conditioned on the source sentence F and a target-to-source affiliation A: where ei is the word being predicted; ei−n+1 i−1 is the string of n − 1 previously generated words; Cai−k to the source context of m source words around fai−k, the source word affiliated with ei−k. We refer to k as the offset parameter. We use the definition of word affiliation introduced in Devlin et al. (2014). When no source context is used, the model is equivalent to an n-gram language model, while an offset parameter of k = 0 reduces the model to the JM of Devlin et al. (2014). When k &gt; 0, the JMO captures non-local context in the prediction of the next target word. More specifically, ei−k and ei, which are local on the target side, are affiliated to fai−k and fai which may be distant from each other on the source side due to non-monotone translation, even for k = 1. The offset model captures reordering constraints by encouraging the predicted target word ei to fit well with the previous affiliated source word fai−k and its surrounding words. We implement a separate feature for each value of k, and later train them jointly via multitask learning.</bodyText>
<equation confidence="0.994966">
P(E|F, A) ≈ � |E |P(ei|ei−n+1
i=1 i−1 , Cai−k = fai−k+m
ai−k−m)
</equation>
<page confidence="0.976887">
32
</page>
<bodyText confidence="0.853142368421053">As our experiments in Section 5.2.1 confirm, the historyaffiliated source context results in stronger SMT improvement than just increasing the number of surrounding words in JM. Fig.1 illustrates the difference between JMO and JM. Assuming n = 3 and m = 1, then JM estimates P(e5|e4, e3,Ca5 = {f6, f7, f8}). On the other hand, for k = 1 , JMOk=1 estimates P(e5|e4, e3, Ca4 = {f8, f9, f10}). Figure 1: Example to illustrate features.f95 is the source segment, e73 is the corresponding translation and lines refer to the alignment. We show hypothesis-enumerating features that look at f7 and source-enumerating features that look at e5. We surround the source words affiliated with e5 and its n-gram history with a bracket, and surround the source words affiliated with the history of e5 with squares.</bodyText>
<subsectionHeader confidence="0.999532">
2.2 Source-Enumerating Features
</subsectionHeader>
<bodyText confidence="0.9995992">Source-Enumerating Features iterate over words in the source sentence, including unaligned words, and assign it a score depending on what aspect of translation they are modeling. A sourceenumerating feature can be formulated as follows:</bodyText>
<equation confidence="0.9990175">
P(Yj|Cj = fj+m
j−m)
</equation>
<bodyText confidence="0.998730222222222">where Caj is the source context (similar to the hypothesis-enumerating features above) and Yj is the label being predicted by the feature. We first describe pre-existing source-enumerating features: the lexical translation model, the orientation model and the fertility model, and then discuss a new feature: Translation Context Model (TCM), which is an extension of the lexical translation model.</bodyText>
<subsectionHeader confidence="0.508876">
2.2.1 Pre-existing Features
</subsectionHeader>
<bodyText confidence="0.999668928571429">Lexical Translation model (LTM) estimates the probability of translating a source word fj to a target word l(fj) = ebj given a source context Cj, bj ∈ B is the source-to-target word affiliation as defined in (Devlin et al., 2014). When fj is translated to more than one word, we arbitrarily keep the left-most one. The target word vocabulary V is extended with a NULL token to accommodate unaligned source words. Orientation model (ORI) describes the probability of orientation of the translation of phrases surrounding a source word fj relative to its own translation. We follow (Setiawan et al., 2013) in modeling the orientation of the left and right phrases of fj with maximal orientation span (the longest neighboring phrase consistent with alignment), which we denote by Lj and Rj respectively. Thus, o(fj) = hoLj(fj), oRj(fj)i, where oLj and oRj refer to the orientation of Lj and Rj respectively. For unaligned fj, we set o(fj) = oLj(Rj), the orientation of Rj with respect to Lj. Fertility model (FM) models the probability that a source word fj generates 0(fj) words in the hypothesis. Our implemented model only distinguishes between aligned and unaligned source words (i.e., 0(fj) ∈ {0,1}). The generalization of the model to account for multiple values of 0(fz) is straightforward.</bodyText>
<subsectionHeader confidence="0.629453">
2.2.2 Translation Context Model
</subsectionHeader>
<bodyText confidence="0.998368470588235">As with JMO in Section 2.1.1, we aim to capture translation phenomena that appear local on the target hypothesis but non-local on the source side. Here, we do so by extending the LTM feature to predict not only the translated word ebj, but also its surrounding context. Formally, we model P (l(fj)|Cj), where l(fj) = ebj−d, · · · , ebj, · · · ebj+d is the hypothesis word window around ebj. In practice, we decompose TCM further into 7+7d P(ebj+d'|Cj) and impled'=−d mented each as a separate neural network-based feature. Note that TCM is equivalent to the LTM when d = 0. Because of word reordering, a given hypothesis word in l(fj) might not be affiliated with fj or even to the words in Cj. TCM can model non-local information in this way.</bodyText>
<subsubsectionHeader confidence="0.817854">
2.2.3 Combined Model
</subsubsectionHeader>
<bodyText confidence="0.999807333333333">Since the feature label is undefined for unaligned source words, we make the model hierarchical, based on whether the source word is aligned or not, and thus arrive at the following formulation:</bodyText>
<figure confidence="0.547116083333333">
f7
f6
. . .
f5
f8
f9
� C7 = Ca5
... e3 e4 e5 e6 e7 ...
. . .
|F|
P(E|F, A) ≈ H
j=1
</figure>
<page confidence="0.804152">
33
</page>
<equation confidence="0.999243">
P(l(fj)) · P(ori(fj)) · P(φ(fj)) =
P(φp(fj) = 0) · P(oLj(Rj))
P(φp(fj) ≥ 1) · +dH P(ebj+d')
d'=−d
·P (oLj(fj), oRj(fj))
</equation>
<bodyText confidence="0.999987727272727">We dropped the common context (Cj) for readability. We reuse Fig. 1 to illustrate the sourceenumerating features. Assuming d = 1, the scores associated with f7 are P(φ(f7) ≥ 1|C7) for the FM; P(e4|C7)·P(e5|C7)·P(e6)|C7) for the TCM; and P(o(f7) = hoL7(f7) = RA, oR7(f7) = RAi) for the ORI(RA refers to Reverse Adjacent). L7 and R7 (i.e. f6 and f98 respectively), the longest neighboring phrase of f7, are translated in reverse order and adjacent to e5.</bodyText>
<sectionHeader confidence="0.99421" genericHeader="method">
3 Tensor Neural Networks
</sectionHeader>
<bodyText confidence="0.999993103448276">The second part of this work improves SMT by improving the neural network architecture. Neural Networks derive their strength from their ability to learn a high-level representation of the input automatically from data. This high-level representation is typically constructed layer by layer through a weighted sum linear operation and a non-linear activation function. With sufficient training data, neural networks often achieve state-of-the-art performance on many tasks. This stands in sharp contrast to other algorithms that require tedious manual feature engineering. For the features presented in this paper, the context words are fed to the network network with minimal engineering. We further strengthen the network’s ability to learn rich interactions between its units by introducing tensors in the hidden layers. The multiplicative property of the tensor bares a close resemblance to collocation of context words which are useful in many natural language processing tasks. In conventional feedforward neural networks, the output of hidden layer l is produced by multiplying the output vector from the previous layer with a weight matrix (Wl) and then applying the activation function σ to the product. Tensor Neural Networks generalize this formulation by using a tensor Ul of order 3 for the weights. The output of node k in layer l is computed as follows:</bodyText>
<equation confidence="0.9779695">
)
hl[k] = σ (hl−1 · Ul[k] · hT l−1
</equation>
<bodyText confidence="0.999151142857143">where Ul[k], the k-th slice of Ul, is a square matrix. In our implementation, we follow (Yu et al., 2012; Hutchinson et al., 2013) and use a low-rank approximation of Ul[k] = Ql[k] · Rl[k]T, where Ql[k], Rl[k] ∈ Rn×r. The output of node k becomes:</bodyText>
<equation confidence="0.9527995">
hl [k] = σ (hl−1 · Ql [k] · Rl [k]T · hi—
hi 1 )
</equation>
<bodyText confidence="0.9480475">In In our experiments, we choose r = 1, and also apply the non-linear activation function σ distributively. We arrive at the following three equations for computing the hidden layer outputs (0 &lt; l &lt; L):</bodyText>
<equation confidence="0.999307">
vl = σ (hl−1 · Ql)
v0l = σ (hl−1 · Rl)
hl = vl ⊗ v0l
</equation>
<bodyText confidence="0.999994052631579">where hl−1 is double-projected to vl and v0l, and the two projections are merged using the Hadamard element-wise product operator ⊗. This formulation allows us to use the same infrastructure of the conventional neural networks by projecting the previous layer to two different spaces of the same dimensions, then multiplying them element-wise. The only component that is different from conventional feedforward neural networks is the multiplicative function, which is trivially differentiable with respect to the learnable parameters. Figure 3(b) illustrates the tensor architecture for two hidden layers. The tensor network can learn collocation features more easily. For example, it can learn a collocation feature that is activated only if hl−1[i] collocates with hl−1[j] by setting Ul[k][i][j] to some positive number. This results in SMT improvements as we describe in Section 5.</bodyText>
<sectionHeader confidence="0.969011" genericHeader="method">
4 Multitask Learning
</sectionHeader>
<bodyText confidence="0.999989545454546">The third part of this paper addresses the challenge of effectively learning a large number of neural network parameters without overfitting. The challenge is even larger for tensor network since they practically doubles the number of parameters. In this section, we propose to apply Multitask Learning (MTL) to partially address this issue. We implement MTL as parameter sharing among the networks. This effectively reduces the number of parameters, and more importantly, it takes advantage of parameters learned for one feature to better learn the parameters of the other features.</bodyText>
<equation confidence="0.808726055555556">
{
34
Output Output Task 1 Task M
h2
h2
h12
hM
2
· · ·
W2 v2 ⊗ v/ v12 ⊗ v/1 · · · vM ⊗ v/M
2 2 2 2
Q1 R1
2 2
QM RM
2
h1 2
h1 Q2 h1 R2
W1 v1 ⊗ v/1 v1 ⊗ v/1
</equation>
<figure confidence="0.786263">
Input Q1 Input R1 Q1 Input R1
(a) (b) (c)
</figure>
<figureCaption confidence="0.9737265">
Figure 2: The network architecture for (a) a conventional feedforward neural network, (b) tensor hidden
layers, and (c) multitask learning with M features that share the embedding and first hidden layers
</figureCaption>
<equation confidence="0.520678">
(t = 1).
</equation>
<bodyText confidence="0.999759242424242">Another way of looking at this is that MTL facilitates regularization through learning the other tasks. MTL is suitable for SMT features as they model different but closely related aspects of the same translation process. MTL has long been used by the wider machine learning community (Caruana, 1997) and more recently for natural language processing (Collobert and Weston, 2008; Collobert et al., 2011). The application of MTL to machine translation, however, has been much less restricted, which is rather surprising since SMT features arise from the same translation task and are naturally related. We apply MTL for the features described in Section 2. We design all the features to also share the same neural network architecture (in this case, the tensor architecture described in Section 3) and the same input, thus resulting in two large neural networks: one for the hypothesis-enumerating features and another for the source-enumerating ones. This simplifies the implementation of MTL. Using this setup, it is possible to vary the number of shared hidden layers t from 0 (only sharing the embedding layer) to L − 1 (sharing all the layers except the output). Note that in principle MTL is applicable to other set of networks that have different architecture or even different input set. With MTL, the training procedure is the same as that of standard neural networks. We use the back propagation algorithm, and use as the loss function the product of likelihood of each feature1:</bodyText>
<footnote confidence="0.9247405">
1In this and in the other parts of the paper, we add the
normalization regularization term described in (Devlin et al.,
2014) to the loss function to avoid computing the normaliza-
tion constant at model query/decoding time.
</footnote>
<equation confidence="0.839379">
log (P (Yj(Xi)))
</equation>
<bodyText confidence="0.999855888888889">where Xi is the training sample and Yj is one of the M models trained. We use the sum of log likelihoods since we assume that the features are independent. Fig. 3(c) illustrates MTL between M models sharing the input embedding layer and the first hidden layer (t = 1) compared to the separatelytrained conventional feedforward neural network and tensor neural network.</bodyText>
<sectionHeader confidence="0.999542" genericHeader="evaluation and result">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99997275">We demonstrate the impact of our work with extensive MT experiments on Arabic-English and Chinese-English translation for the DARPA BOLT Web Forum and the NIST OpenMT12 conditions.</bodyText>
<subsectionHeader confidence="0.976502">
5.1 Baseline MT System
</subsectionHeader>
<bodyText confidence="0.99832675">We run our experiments using a state-of-the-art string-to-dependency hierarchical decoder (Shen et al., 2010). The baseline we use includes a set of powerful features as follow:</bodyText>
<listItem confidence="0.999250272727273">• Forward and backward rule probabilities • Contextual lexical smoothing (Devlin, 2009) • 5-gram Kneser-Ney LM • Dependency LM (Shen et al., 2010) • Length distribution (Shen et al., 2010) • Trait features (Devlin and Matsoukas, 2012) • Factored source syntax (Huang et al., 2013) • Discriminative sparse feature, totaling 50k features (Chiang et al., 2009) • Neural Network Joint Model (NNJM) and Neural Network Lexical Translation Model</listItem>
<equation confidence="0.89546">
�Loss =
M
E
i j
</equation>
<page confidence="0.94822">
35
</page>
<bodyText confidence="0.9935975625">(NNLTM) (Devlin et al., 2014) As shown, our baseline system already includes neural network-based features. NNJM, NNLTM and use two hidden layers with 500 units and use embedding of size 200 for each input. We use the MADA-ARZ tokenizer (Habash et al., 2013) for Arabic word tokenization. For Chinese tokenization, we use a simple longest-matchfirst lexicon-based approach. We align the training data using GIZA++ (Och and Ney, 2003). For tuning the weights of MT features including the new features, we use iterative k-best optimization with an ExpectedBLEU objective function (Rosti et al., 2010), and decode the test sets after 5 tuning iteration. We report the lower-cased BLEU and TER scores.</bodyText>
<subsectionHeader confidence="0.930874">
5.2 BOLT Discussion Forum
</subsectionHeader>
<bodyText confidence="0.999961625">The bulk of our experiments is on the BOLT Web Discussion Forum domain, which uses data collected by the LDC. The parallel training data consists of all of the high-quality NIST training corpora, plus an additional 3 million words of translated forum data. The tuning and test sets consist of roughly 5000 segments each, with 2 independent references for Arabic and 3 for Chinese.</bodyText>
<subsectionHeader confidence="0.929637">
5.2.1 Effects of New Features
</subsectionHeader>
<bodyText confidence="0.999958155172414">We first look at the effects of the proposed features compared to the baseline system. Table 1 summarizes the primary results of the Arabic-English and Chinese-English experiments for the BOLT condition. We show the experimental results related to hypothesis-enumerating features (HypEn) in rows S2-S5, those related to source-enumerating features (SrcEn) in rows S6-S9, and the combination of the two in row S10. For all the features, we set the source context length to m = 5 (11-word window). For JM and JMO, we set the target context length to n = 4. For the offset parameter k of JMO, we use values 1 to 3. For TCM, we model one word around the translation (d = 1). Larger values of d did not result in further gains. The baseline is comparable to the best results of (Devlin et al., 2014). In rows S3 to S5, we incrementally add a model with different offset source context, from k = 1 to k = 3. For AR-EN, adding JMOs with different offset source context consistently yields positive effects in BLEU score, while in ZH-EN, it yields positive effects in TER score. Utilizing all offset source contexts “+JMOk&lt;3” (row S5) yields around 0.9 BLEU point improvement in AR-EN and around 0.3 BLEU in ZH-EN compared to the baseline. The JMO consistently provides better improvement compared to a larger JM context (row S2), validating our hypothesis that using offset source context captures important non-local context. Rows S6 to S9 present the improvements that result from implementing pre-existing sourceenumerating SMT features as neural networks, and highlight the contribution of our translation context model (TCM). This set of experiments is orthogonal to the HypEn experiments (rows S2S5). Each pre-existing model has a modest positive cumulative effect on both BLEU and TER. We see this result as further confirming the current trend of casting existing SMT features as neural network since our baseline already contains such features. The next row present the results of adding the translation context model, with one word surrounding the translation (d = 1). As shown, TCM yields a positive effect of around 0.5 BLEU and TER improvements in AR-EN and around 0.2 BLEU and TER improvements in ZHEN. Separately, the set of source-enumerating features and the set of target-enumerating features produce around 1.1 to 1.2 points BLEU gain in AR-EN and 0.3 to 0.5 points BLEU gain in ZHEN. The combination of the two sets produces a complementary gain in addition to the gains of the individual models as Row (S10) shows. The combined gain improves to 1.5 BLEU points in AREN and 0.7 BLEU points in ZH-EN.</bodyText>
<table confidence="0.995598333333333">
System AR-EN ZH-EN
BL TER BL TER
S1: Baseline 43.2 45.0 30.2 58.3
S2: S1+JMLC, 43.5 45.0 30.2 58.5
S3: S1+JMOk=1 43.9 44.7 30.8 57.8
S4: S3+JMOk=2 43.9 44.7 30.7 57.8
S5: S4+JMOk=3 44.4 44.5 30.5 57.5
S6: S1+LTM 43.5 44.7 30.3 58.0
S7: S6+ORI 43.7 44.6 30.4 57.8
S8: S7+FERT 43.8 44.7 30.5 57.8
S9: S8+TCM 44.3 44.2 30.7 57.5
S10: S9+JMOk&lt;3 44.7 44.1 30.9 57.3
</table>
<tableCaption confidence="0.9756705">
Table 1: MT results of various model combination
in BLEU and in TER.
</tableCaption>
<page confidence="0.997209">
36
</page>
<subsectionHeader confidence="0.836339">
5.2.2 Effects of Tensor Network and
Multitask Learning
</subsectionHeader>
<bodyText confidence="0.998650583333333">We first analyze the impact of tensor architecture and MTL intrinsically by reporting the models’ average log-likelihood on the validation sets (a subset of the test set) in Table 2. As mentioned, we group the models to HypEn (JM and JMOk≤3) and SrcEn (LTM, ORI,FERT and TCM) as we perform MTL on these two groups. Likelihood of these two groups in the previous subsection are in column “NN” (for Neural Network), which serves as a baseline. The application of the tensor architecture improves their likelihood as shown in column “Tensor” for both languages and models.</bodyText>
<table confidence="0.995699">
Independent MTL
Feat. NN Tensor t = 0 t = 1
L = 2 L = 3
AR HypEn -8.85 -8.54 -8.35 -
SrcEn -8.47 -8.32 -8.10 -8.09
ZH HypEn -11.48 -11.06 -10.87 -
SrcEn -10.77 -10.66 -10.54 -10.49
</table>
<tableCaption confidence="0.973187">
Table 2: Sum of the average log-likelihood of the
models in HypEn and SrcEn.</tableCaption>
<bodyText confidence="0.995914307692308">t = 0 refers to MTL that shares only the embedding layer, while t = 1 shares the first hidden layer as well. L refers to the network’s depth.Higher value is better. The likelihoods of the MTL-related experiments are in columns with “MTL” header. We present two set of results. In the first set (column “MTL,t=0,L=2”), we run MTL for features from column “Tensor” by sharing the embedding layer only (t = 0). This allows us to isolate the impact of MTL in the presence of Tensors. Column “MTL,t=1,l=3” corresponds to the experiment that produces the best intrinsic result, where each model uses Tensors with three hidden layers (500x500x500, l = 3) and the models share the embedding and the first hidden layers (t = 1). MTL consistently gives further intrinsic gain compared to tensors. More sharing provides an extra gain for SrcEn as shown in the last column. Note that we only experiment with different l and t for SrcEn and not for HypEn because the models in HypEn have different input sets. In our experiments, further sharing and more hidden layers resulted in no further gain. In total, we see a consistent positive effect in intrinsic evaluation from the tensor networks and multitask learning. Moving on to MT evaluation, we summarize the experiments showing the impact of Tensors and MTL in Table 3. For MTL, we use L = 3, t = 2 since it gives the best intrinsic score. Employing tensors instead of regular neural networks gives a significant and consistent positive impact for all models and language pairs. For the system with the baseline features, we use the tensor architecture for both the joint model and the lexical translation model of Devlin et al. resulting in an improvement of around 0.7 BLEU points, and showing the wide applicability of the tensor architecture. On top of this improved baseline, we also observe an improvement of the same scale for other models (column “Tensor”), except for HypEn features in AR-EN experiment. Moving to MTL experiments, we see improvements, especially from SrcEn features. MTL gives around 0.5 BLEU point improvement for AR-EN and around 0.4 BLEU point for ZH-EN. When we employ both HypEn and SrcEn together, MTL gives around 0.4 BLEU point in AR-EN and 0.2 BLEU point in ZH-EN. In total, our work results in an improvement of 2.5 BLEU point for AR-EN and 1.8 for BLEU point in ZH-EN on top of the best results in (Devlin et al., 2014).</bodyText>
<subsectionHeader confidence="0.96287">
5.3 NIST OpenMT12
</subsectionHeader>
<bodyText confidence="0.999323166666667">Our NIST system is compatible with the OpenMT12 constrained track, which consists of 10M words of high-quality parallel training for Arabic, and 25M words for Chinese. The n-gram LM is trained on 5B words of data from the English GigaWord. For test, we use the “Arabic-ToEnglish Original Progress Test” (1378 segments) and “Chinese-to-English Original Progress Test + OpenMT12 Current Test” (2190 segments), which consists of a mix of newswire and web data. All test segments have 4 references. Our tuning set contains 5000 segments, and is a mix of the MT02-05 eval set as well as additional held-out parallel data from the training corpora. We report the experiments for the NIST condition in Table 4. In particular, we investigate the impact of deploying our new features (column “Feat”) and demonstrate the effects of the tensor architecture (column “Tensor”) and multitask learning (column “MTL”). As shown the results are inline with the BOLT condition where we observe additive improvements from adding our new features, applying tensor network and multitask learning. On Arabic-English, we see a gain of 2.7</bodyText>
<page confidence="0.997377">
37
</page>
<table confidence="0.999828333333333">
Feature set AR-EN ZH-EN
NN Tensor MTL NN Tensor MTL
R1: Baseline Features 43.2 43.9 - 30.2 30.8 -
R2: R1 + HypEn 44.4 44.4 44.5 30.5 31.5 31.3
R3: R1 + SrcEn 44.3 44.9 45.5 30.7 31.5 31.9
R4: R1 + HypEn + SrcEn 44.7 45.3 45.7 30.9 31.8 32.0
</table>
<tableCaption confidence="0.992337666666667">
Table 3: Experimental results to investigate the effects of the new features, DTN and MTL. The top
part shows the BOLT results, while the bottom part shows the NIST results. The best results for each
conditions and each language-pair are in bold. The baselines are in italics. .
</tableCaption>
<table confidence="0.9995656">
Base. Feat Tensor MTL
AR-EN 53.7 55.4 55.9 56.4
mixed-case 51.8 53.1 53.7 54.1
ZH-EN 36.6 37.8 38.2 38.5
mixed-case 34.4 35.5 35.9 36.1
</table>
<tableCaption confidence="0.964154333333333">
Table 4: Experimental results for the NIST condi-
tion. Mixed-case scores are also reported. Base-
lines are in italics.
</tableCaption>
<bodyText confidence="0.996679625">BLEU point and on Chinese-English, we see a 1.9 BLEU point gain. We also report the mixed-cased BLEU scores for comparison with previous best published results, i.e. Devlin et al. (2014) report 52.8 BLEU for Arabic-English and 34.7 BLEU for Chinese-English. Thus, our results are around 1.31.4 BLEU point better. Note that they use additional rescoring features but we do not.</bodyText>
<sectionHeader confidence="0.999863" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999972633333334">Our work is most closely related to Devlin et al. (2014). They use a simple feedforward neural network to model two important MT features: A joint language and translation model, and a lexical translation model. They show very large improvements on Arabic-English and ChineseEnglish web forum and newswire baselines. We improve on their work in 3 aspects. First, we model more features using neural networks, including two novel ones: a joint model with offset source context and a translation context model. Second, we enhance the neural network architecture by using tensor layers, which allows us to model richer interactions. Lastly, we improve the performance of the individual features by training them using multitask learning. In the remainder of this section, we describe previous work relating to the three aspect of our work, namely MT modeling, neural network architecture and model learning. The features we propose in this paper address the major aspects of SMT modeling that have informed much of the research since the original IBM models (Brown et al., 1993): lexical translation, reordering, word fertility, and language models. Of particular relevance to our work are approaches that incorporate context-sensitivity into the models (Carpuat and Wu, 2007), formulate reordering as orientation prediction task (Tillman, 2004) and that use neural network language models (Bengio et al., 2003; Schwenk, 2010; Schwenk, 2012), and incorporate source-side context into them (Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012). Approaches to incorporating source context into a neural network model differ mainly in how they represent the source sentence and in how long is the history they keep. In terms of representation of the source sentence, we follow (Devlin et al., 2014) in using a window around the affiliated source word. To name some other approaches, Auli et al. (2013) uses latent semantic analysis and source sentence embeddings learned from the recurrent neural network; Sundermeyer et al. (2014) take the representation from a bidirectional LSTM recurrent neural network; and Kalchbrenner and Blunsom (2013) employ a convolutional sentence model. For target context, recent work has tried to look beyond the classical n-gram history. (Auli et al., 2013; Sundermeyer et al., 2014) consider an unbounded history, at the expense of making their model only applicable for N-best rescoring. Another recent line of research (Bahdanau et al., 2014; Sutskever et al., 2014) departs more radically from conventional feature-based SMT and implements the MT system as a single neural network. These models use a representation of the whole input sentence. We use a feedforward neural network in this work. Besides feedforward and recurrent networks, other network architectures that have been applied to SMT include convolutional networks (Kalchbrenner et al., 2014) and recursive networks (Socher et al., 2011).</bodyText>
<page confidence="0.997968">
38
</page>
<bodyText confidence="0.999949552631579">The simplicity of feedforward networks works to our advantage. More specifically, due to the absence of a feedback loop, the feedforward architecture allows us to treat individual decisions independently, which makes parallelization of the training easy and the querying the network at decoding time straightforward. The use of tensors in the hidden layers strengthens the neural network model, allowing us to model more complex feature interactions like collocation, which has been long recognized as important information for many NLP tasks (e.g.word sense disambiguation (Lee and Ng, 2002)). The tensor formulation we use is similar to that of (Yu et al., 2012; Hutchinson et al., 2013). Tensor Neural Networks have a wide application in other field, but have only been recently applied in NLP (Socher et al., 2013; Pei et al., 2014). To our knowledge, our work is the first to use tensor networks in SMT. Our approach to multitask learning is related to work that is often labeled joint training or transfer learning. To name a few of these works, Finkel and Manning (2009) successfully train name entity recognizers and syntactic parsers jointly, and Singh et al.(2013) train models for coreference resolution, named entity recognition and relation extraction jointly. Both efforts are motivated by the minimization of cascading errors. Our work is most closely related to Collobert and Weston (2008; Collobert et al.(2011), who apply multitask learning to train neural networks for multiple NLP models: part-of-speech tagging, semantic role labeling, named-entity recognition and language model variations.</bodyText>
<sectionHeader confidence="0.99819" genericHeader="conclusion">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999968952380952">This paper argues that a relatively simple feedforward neural network can still provides significant improvement to Statistical Machine Translation (SMT). We support this argument by presenting a multi-pronged approach that addresses modeling, architectural and learning aspects of pre-existing neural network-based SMT features. More concretely, we paper present a new set of neural network-based SMT features to capture important translation phenomena, extend feedforward neural network with tensor layers, and apply multitask learning to integrate the SMT features more tightly. Empirically, all our proposals successfully produce an improvement over state-of-the-art machine translation system for Arabic-to-English and Chinese-to-English and for both BOLT web forum and NIST conditions. Building on the success of this paper, we plan to develop other neuralnetwork-based features, and to also relax the limiteation of current rule extraction heuristics by generating translations word-by-word.</bodyText>
<sectionHeader confidence="0.935164" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999953875">This work was supported by DARPA/I2O Contract No. HR0011-12-C-0014 under the BOLT Program. The views, opinions, and/or findings contained in this article are those of the author and should not be interpreted as representing the official views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the Department of Defense.</bodyText>
<sectionHeader confidence="0.996963" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9987862">
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint language and translation
modeling with recurrent neural networks. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1044–
1054, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. Technical Report
1409.0473, arXiv.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Comput. Linguist., 19(2):263–
311, June.
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 61–72, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Rich Caruana. 1997. Multitask learning. Machine
Learning, 28(1):41–75.
</reference>
<page confidence="0.995387">
39
</page>
<reference confidence="0.998933190909091">
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In HLT-NAACL, pages 218–226.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine Learning, ICML ’08, pages 160–167, New
York, NY, USA. ACM.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493–2537,
November.
Jacob Devlin and Spyros Matsoukas. 2012. Trait-
based hypothesis selection for machine translation.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL HLT ’12, pages 528–532, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1370–1380, Baltimore, Maryland, June. Association
for Computational Linguistics.
Jacob Devlin. 2009. Lexical features for statistical
machine translation. Master’s thesis, University of
Maryland.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Pro-
ceedings of Human Language Technologies: The
2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 326–334, Boulder, Colorado, June.
Association for Computational Linguistics.
Nizar Habash, Ryan Roth, Owen Rambow, Ramy Es-
kander, and Nadi Tomeh. 2013. Morphological
analysis and disambiguation for dialectal arabic. In
HLT-NAACL, pages 426–432.
Zhongqiang Huang, Jacob Devlin, and Rabih Zbib.
2013. Factored soft source syntactic constraints for
hierarchical machine translation. In EMNLP, pages
556–566.
Brian Hutchinson, Li Deng, and Dong Yu. 2013. Ten-
sor deep stacking networks. IEEE Trans. Pattern
Anal. Mach. Intell., 35(8):1944–1957, August.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1700–1709, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
655–665, Baltimore, Maryland, June. Association
for Computational Linguistics.
Hai-Son Le, Alexandre Allauzen, and Franc¸ois Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of the 2012 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, NAACL HLT ’12, pages 39–
48, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Yoong Keok Lee and Hwee Tou Ng. 2002. An em-
pirical evaluation of knowledge sources and learn-
ing algorithms for word sense disambiguation. In
Proceedings of the ACL-02 Conference on Empiri-
cal Methods in Natural Language Processing - Vol-
ume 10, EMNLP ’02, pages 41–48, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Wenzhe Pei, Tao Ge, and Baobao Chang. 2014. Max-
margin tensor neural network for chinese word seg-
mentation. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 293–303, Bal-
timore, Maryland, June. Association for Computa-
tional Linguistics.
Antti Rosti, Bing Zhang, Spyros Matsoukas, and
Rich Schwartz. 2010. BBN system descrip-
tion for WMT10 system combination task. In
WMT/MetricsMATR, pages 321–326.
Holger Schwenk. 2010. Continuous-space language
models for statistical machine translation. Prague
Bull. Math. Linguistics, 93:137–146.
Holger Schwenk. 2012. Continuous space translation
models for phrase-based statistical machine transla-
tion. In COLING (Posters), pages 1071–1080.
Hendra Setiawan, Bowen Zhou, Bing Xiang, and Li-
bin Shen. 2013. Two-neighbor orientation model
with cross-boundary global contexts. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 1264–1274, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-dependency statistical machine transla-
tion. Computational Linguistics, 36(4):649–671,
December.
</reference>
<page confidence="0.967034">
40
</page>
<reference confidence="0.999727234042553">
Sameer Singh, Sebastian Riedel, Brian Martin, Jiap-
ing Zheng, and Andrew McCallum. 2013. Joint
inference of entities, relations, and coreference. In
Proceedings of the 2013 Workshop on Automated
Knowledge Base Construction, AKBC ’13, pages 1–
6, New York, NY, USA. ACM.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation
using comparable corpora. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ’08, pages 857–866,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Richard Socher, Cliff C. Lin, Andrew Y. Ng, and
Christopher D. Manning. 2011. Parsing Natural
Scenes and Natural Language with Recursive Neural
Networks. In Proceedings of the 26th International
Conference on Machine Learning (ICML).
Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Ng. 2013. Reasoning with neural ten-
sor networks for knowledge base completion. In
C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahra-
mani, and K.Q. Weinberger, editors, Advances in
Neural Information Processing Systems 26, pages
926–934. Curran Associates, Inc.
Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker,
and Hermann Ney. 2014. Translation modeling
with bidirectional recurrent neural networks. In Pro-
ceedings of the 2014 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 14–25, Doha, Qatar, October. Association for
Computational Linguistics.
Ilya Sutskever, Oriol Vinyals, and Quoc V. V Le.
2014. Sequence to sequence learning with neural
networks. In Z. Ghahramani, M. Welling, C. Cortes,
N.D. Lawrence, and K.Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
27, pages 3104–3112. Curran Associates, Inc.
Christoph Tillman. 2004. A unigram orienta-
tion model for statistical machine translation. In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Short Papers, pages 101–
104, Boston, Massachusetts, USA, May 2 - May 7.
Association for Computational Linguistics.
Dong Yu, Li Deng, and Frank Seide. 2012. Large vo-
cabulary speech recognition using deep tensor neu-
ral networks. In INTERSPEECH. ISCA.
</reference>
<page confidence="0.999447">
41
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant confidence="0.804841" no="0">
<title confidence="0.999178">Statistical Machine Translation Features with Multitask Tensor Networks</title>
<author confidence="0.980373">Zhongqiang Huang Setiawan</author>
<author confidence="0.980373">Jacob Thomas</author>
<affiliation confidence="0.928316">Rabih Zbib, Richard Schwartz and John</affiliation>
<address confidence="0.969166">Raytheon BBN Technologies, 10 Moulton St, Cambridge, MA 02138, Research, One Microsoft Way, Redmond, WA 98052,</address>
<email confidence="0.999773">jdevlin@microsoft.com</email>
<abstract confidence="0.995980142857143">We present a three-pronged approach to improving Statistical Machine Translation (SMT), building on recent success in the application of neural networks to SMT. we propose new on neural networks to model various nonlocal translation phenomena. Second, we the the neural network with tensor layers that capture important higher-order interaction among the network units. Third, we apply multitask estimate the neural network parameters jointly. Each of our proposed methods results in significant improvements that are complementary. The overall improvement is +2.7 and +1.8 BLEU points for Arabic-English and Chinese- English translation over a state-of-the-art system that already includes neural network features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Michel Galley</author>
<author>Chris Quirk</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Joint language and translation modeling with recurrent neural networks.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1044--1054</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context citStr="Auli et al., 2013" endWordPosition="210" position="1513" startWordPosition="207">ntly. Each of our proposed methods results in significant improvements that are complementary. The overall improvement is +2.7 and +1.8 BLEU points for Arabic-English and ChineseEnglish translation over a state-of-the-art system that already includes neural network features. 1 Introduction Recent advances in applying Neural Networks to Statistical Machine Translation (SMT) have generally taken one of two approaches. They either develop neural network-based features that are used to score hypotheses generated from traditional translation grammars (Sundermeyer et al., 2014; Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012), or they implement the whole translation process as a single neural network (Bahdanau et al., 2014; Sutskever et al., 2014). The latter approach, sometimes referred to as Neural Machine Translation, attempts to overhaul SMT, while the former capitalizes on the strength of the current SMT paradigm and leverages the modeling power of neural networks to improve the scoring of hypotheses generated ∗* Research conducted when the author was at BBN. by phrase-based or hierarchical translation rules. This paper adopts the former approach, as n-best scores from state-o</context>
<context citStr="Auli et al., 2013" endWordPosition="5238" position="31173" startWordPosition="5235"> features we propose in this paper address the major aspects of SMT modeling that have informed much of the research since the original IBM models (Brown et al., 1993): lexical translation, reordering, word fertility, and language models. Of particular relevance to our work are approaches that incorporate context-sensitivity into the models (Carpuat and Wu, 2007), formulate reordering as orientation prediction task (Tillman, 2004) and that use neural network language models (Bengio et al., 2003; Schwenk, 2010; Schwenk, 2012), and incorporate source-side context into them (Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012). Approaches to incorporating source context into a neural network model differ mainly in how they represent the source sentence and in how long is the history they keep. In terms of representation of the source sentence, we follow (Devlin et al., 2014) in using a window around the affiliated source word. To name some other approaches, Auli et al. (2013) uses latent semantic analysis and source sentence embeddings learned from the recurrent neural network; Sundermeyer et al. (2014) take the representation from a bidirectional LSTM recurrent neural network; and </context>
</contexts>
<marker>Auli, Galley, Quirk, Zweig, 2013</marker>
<rawString>Michael Auli, Michel Galley, Chris Quirk, and Geoffrey Zweig. 2013. Joint language and translation modeling with recurrent neural networks. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1044– 1054, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dzmitry Bahdanau</author>
<author>Kyunghyun Cho</author>
<author>Yoshua Bengio</author>
</authors>
<title>Neural machine translation by jointly learning to align and translate.</title>
<date>2014</date>
<tech>Technical Report 1409.0473, arXiv.</tech>
<contexts>
<context citStr="Bahdanau et al., 2014" endWordPosition="233" position="1645" startWordPosition="230">nd +1.8 BLEU points for Arabic-English and ChineseEnglish translation over a state-of-the-art system that already includes neural network features. 1 Introduction Recent advances in applying Neural Networks to Statistical Machine Translation (SMT) have generally taken one of two approaches. They either develop neural network-based features that are used to score hypotheses generated from traditional translation grammars (Sundermeyer et al., 2014; Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012), or they implement the whole translation process as a single neural network (Bahdanau et al., 2014; Sutskever et al., 2014). The latter approach, sometimes referred to as Neural Machine Translation, attempts to overhaul SMT, while the former capitalizes on the strength of the current SMT paradigm and leverages the modeling power of neural networks to improve the scoring of hypotheses generated ∗* Research conducted when the author was at BBN. by phrase-based or hierarchical translation rules. This paper adopts the former approach, as n-best scores from state-of-the-art SMT systems often suggest that these systems can still be significantly improved with better features. We build on (Devlin</context>
<context citStr="Bahdanau et al., 2014" endWordPosition="5391" position="32137" startWordPosition="5388">name some other approaches, Auli et al. (2013) uses latent semantic analysis and source sentence embeddings learned from the recurrent neural network; Sundermeyer et al. (2014) take the representation from a bidirectional LSTM recurrent neural network; and Kalchbrenner and Blunsom (2013) employ a convolutional sentence model. For target context, recent work has tried to look beyond the classical n-gram history. (Auli et al., 2013; Sundermeyer et al., 2014) consider an unbounded history, at the expense of making their model only applicable for N-best rescoring. Another recent line of research (Bahdanau et al., 2014; Sutskever et al., 2014) departs more radically from conventional feature-based SMT and implements the MT system as a single neural network. These models use a representation of the whole input sentence. We use a feedforward neural network in this work. Besides feedforward and recurrent net38 works, other network architectures that have been applied to SMT include convolutional networks (Kalchbrenner et al., 2014) and recursive networks (Socher et al., 2011). The simplicity of feedforward networks works to our advantage. More specifically, due to the absence of a feedback loop, the feedforwar</context>
</contexts>
<marker>Bahdanau, Cho, Bengio, 2014</marker>
<rawString>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. Technical Report 1409.0473, arXiv.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context citStr="Bengio et al., 2003" endWordPosition="5219" position="31055" startWordPosition="5216">s work relating to the three aspect of our work, namely MT modeling, neural network architecture and model learning. The features we propose in this paper address the major aspects of SMT modeling that have informed much of the research since the original IBM models (Brown et al., 1993): lexical translation, reordering, word fertility, and language models. Of particular relevance to our work are approaches that incorporate context-sensitivity into the models (Carpuat and Wu, 2007), formulate reordering as orientation prediction task (Tillman, 2004) and that use neural network language models (Bengio et al., 2003; Schwenk, 2010; Schwenk, 2012), and incorporate source-side context into them (Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012). Approaches to incorporating source context into a neural network model differ mainly in how they represent the source sentence and in how long is the history they keep. In terms of representation of the source sentence, we follow (Devlin et al., 2014) in using a window around the affiliated source word. To name some other approaches, Auli et al. (2013) uses latent semantic analysis and source sentence embeddings learned from the recurrent neur</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Comput. Linguist.,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>311</pages>
<contexts>
<context citStr="Brown et al., 1993" endWordPosition="5171" position="30723" startWordPosition="5168">fset source context and a translation context model. Second, we enhance the neural network architecture by using tensor layers, which allows us to model richer interactions. Lastly, we improve the performance of the individual features by training them using multitask learning. In the remainder of this section, we describe previous work relating to the three aspect of our work, namely MT modeling, neural network architecture and model learning. The features we propose in this paper address the major aspects of SMT modeling that have informed much of the research since the original IBM models (Brown et al., 1993): lexical translation, reordering, word fertility, and language models. Of particular relevance to our work are approaches that incorporate context-sensitivity into the models (Carpuat and Wu, 2007), formulate reordering as orientation prediction task (Tillman, 2004) and that use neural network language models (Bengio et al., 2003; Schwenk, 2010; Schwenk, 2012), and incorporate source-side context into them (Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012). Approaches to incorporating source context into a neural network model differ mainly in how they represent the sour</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Comput. Linguist., 19(2):263– 311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Improving statistical machine translation using word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>61--72</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context citStr="Carpuat and Wu, 2007" endWordPosition="5198" position="30921" startWordPosition="5195"> performance of the individual features by training them using multitask learning. In the remainder of this section, we describe previous work relating to the three aspect of our work, namely MT modeling, neural network architecture and model learning. The features we propose in this paper address the major aspects of SMT modeling that have informed much of the research since the original IBM models (Brown et al., 1993): lexical translation, reordering, word fertility, and language models. Of particular relevance to our work are approaches that incorporate context-sensitivity into the models (Carpuat and Wu, 2007), formulate reordering as orientation prediction task (Tillman, 2004) and that use neural network language models (Bengio et al., 2003; Schwenk, 2010; Schwenk, 2012), and incorporate source-side context into them (Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012). Approaches to incorporating source context into a neural network model differ mainly in how they represent the source sentence and in how long is the history they keep. In terms of representation of the source sentence, we follow (Devlin et al., 2014) in using a window around the affiliated source word. To name </context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007. Improving statistical machine translation using word sense disambiguation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 61–72, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rich Caruana</author>
</authors>
<date>1997</date>
<booktitle>Multitask learning. Machine Learning,</booktitle>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context citStr="Caruana, 1997" endWordPosition="508" position="3429" startWordPosition="506">lized contexts are used to model these phenomena effectively, making the use of neural networks essential. All of the features are useful individually, and their combination results in significant improvements (Section 2). • We use a Tensor Neural Network Architecture (Yu et al., 2012) to automatically learn complex pairwise interactions between the network nodes. The introduction of the tensor hidden layer results in more powerful features with lower model perplexity and significantly improved MT performance for all of neural network features (Section 3). • We apply Multitask Learning (MTL) (Caruana, 1997) to jointly train related neural network features by sharing parameters. This allows parameters learned for one feature to benefit the learning of the other features. This results in better trained models and achieves additional MT improvements (Section 4). We apply the resulting Multitask Tensor Networks to the new features and to existing ones, 31 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 31–41, Beijing, China, July 26-31, 2015. c�2015 Association for Computational L</context>
<context citStr="Caruana, 1997" endWordPosition="2841" position="17226" startWordPosition="2840">⊗ v/1 v1 ⊗ v/1 Input Q1 Input R1 Q1 Input R1 (a) (b) (c) Figure 2: The network architecture for (a) a conventional feedforward neural network, (b) tensor hidden layers, and (c) multitask learning with M features that share the embedding and first hidden layers (t = 1). learn the parameters of the other features. Another way of looking at this is that MTL facilitates regularization through learning the other tasks. MTL is suitable for SMT features as they model different but closely related aspects of the same translation process. MTL has long been used by the wider machine learning community (Caruana, 1997) and more recently for natural language processing (Collobert and Weston, 2008; Collobert et al., 2011). The application of MTL to machine translation, however, has been much less restricted, which is rather surprising since SMT features arise from the same translation task and are naturally related. We apply MTL for the features described in Section 2. We design all the features to also share the same neural network architecture (in this case, the tensor architecture described in Section 3) and the same input, thus resulting in two large neural networks: one for the hypothesis-enumerating fea</context>
</contexts>
<marker>Caruana, 1997</marker>
<rawString>Rich Caruana. 1997. Multitask learning. Machine Learning, 28(1):41–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>218--226</pages>
<contexts>
<context citStr="Chiang et al., 2009" endWordPosition="3262" position="19777" startWordPosition="3259">or the DARPA BOLT Web Forum and the NIST OpenMT12 conditions. 5.1 Baseline MT System We run our experiments using a state-of-the-art string-to-dependency hierarchical decoder (Shen et al., 2010). The baseline we use includes a set of powerful features as follow: • Forward and backward rule probabilities • Contextual lexical smoothing (Devlin, 2009) • 5-gram Kneser-Ney LM • Dependency LM (Shen et al., 2010) • Length distribution (Shen et al., 2010) • Trait features (Devlin and Matsoukas, 2012) • Factored source syntax (Huang et al., 2013) • Discriminative sparse feature, totaling 50k features (Chiang et al., 2009) • Neural Network Joint Model (NNJM) and Neural Network Lexical Translation Model �Loss = M E i j 35 (NNLTM) (Devlin et al., 2014) As shown, our baseline system already includes neural network-based features. NNJM, NNLTM and use two hidden layers with 500 units and use embedding of size 200 for each input. We use the MADA-ARZ tokenizer (Habash et al., 2013) for Arabic word tokenization. For Chinese tokenization, we use a simple longest-matchfirst lexicon-based approach. We align the training data using GIZA++ (Och and Ney, 2003). For tuning the weights of MT features including the new features</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine translation. In HLT-NAACL, pages 218–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning, ICML ’08,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context citStr="Collobert and Weston, 2008" endWordPosition="2853" position="17304" startWordPosition="2850"> The network architecture for (a) a conventional feedforward neural network, (b) tensor hidden layers, and (c) multitask learning with M features that share the embedding and first hidden layers (t = 1). learn the parameters of the other features. Another way of looking at this is that MTL facilitates regularization through learning the other tasks. MTL is suitable for SMT features as they model different but closely related aspects of the same translation process. MTL has long been used by the wider machine learning community (Caruana, 1997) and more recently for natural language processing (Collobert and Weston, 2008; Collobert et al., 2011). The application of MTL to machine translation, however, has been much less restricted, which is rather surprising since SMT features arise from the same translation task and are naturally related. We apply MTL for the features described in Section 2. We design all the features to also share the same neural network architecture (in this case, the tensor architecture described in Section 3) and the same input, thus resulting in two large neural networks: one for the hypothesis-enumerating features and another for the source-enumerating ones. This simplifies the impleme</context>
<context citStr="Collobert and Weston (2008" endWordPosition="5691" position="34008" startWordPosition="5688">ly applied in NLP (Socher et al., 2013; Pei et al., 2014). To our knowledge, our work is the first to use tensor networks in SMT. Our approach to multitask learning is related to work that is often labeled joint training or transfer learning. To name a few of these works, Finkel and Manning (2009) successfully train name entity recognizers and syntactic parsers jointly, and Singh et al. (2013) train models for coreference resolution, named entity recognition and relation extraction jointly. Both efforts are motivated by the minimization of cascading errors. Our work is most closely related to Collobert and Weston (2008; Collobert et al. (2011), who apply multitask learning to train neural networks for multiple NLP models: part-of-speech tagging, semantic role labeling, named-entity recognition and language model variations. 7 Conclusion This paper argues that a relatively simple feedforward neural network can still provides significant improvement to Statistical Machine Translation (SMT). We support this argument by presenting a multi-pronged approach that addresses modeling, architectural and learning aspects of pre-existing neural network-based SMT features. More concretely, we paper present a new set of </context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th International Conference on Machine Learning, ICML ’08, pages 160–167, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>12--2493</pages>
<contexts>
<context citStr="Collobert et al., 2011" endWordPosition="2857" position="17329" startWordPosition="2854">r (a) a conventional feedforward neural network, (b) tensor hidden layers, and (c) multitask learning with M features that share the embedding and first hidden layers (t = 1). learn the parameters of the other features. Another way of looking at this is that MTL facilitates regularization through learning the other tasks. MTL is suitable for SMT features as they model different but closely related aspects of the same translation process. MTL has long been used by the wider machine learning community (Caruana, 1997) and more recently for natural language processing (Collobert and Weston, 2008; Collobert et al., 2011). The application of MTL to machine translation, however, has been much less restricted, which is rather surprising since SMT features arise from the same translation task and are naturally related. We apply MTL for the features described in Section 2. We design all the features to also share the same neural network architecture (in this case, the tensor architecture described in Section 3) and the same input, thus resulting in two large neural networks: one for the hypothesis-enumerating features and another for the source-enumerating ones. This simplifies the implementation of MTL. Using thi</context>
<context citStr="Collobert et al. (2011)" endWordPosition="5695" position="34033" startWordPosition="5692"> al., 2013; Pei et al., 2014). To our knowledge, our work is the first to use tensor networks in SMT. Our approach to multitask learning is related to work that is often labeled joint training or transfer learning. To name a few of these works, Finkel and Manning (2009) successfully train name entity recognizers and syntactic parsers jointly, and Singh et al. (2013) train models for coreference resolution, named entity recognition and relation extraction jointly. Both efforts are motivated by the minimization of cascading errors. Our work is most closely related to Collobert and Weston (2008; Collobert et al. (2011), who apply multitask learning to train neural networks for multiple NLP models: part-of-speech tagging, semantic role labeling, named-entity recognition and language model variations. 7 Conclusion This paper argues that a relatively simple feedforward neural network can still provides significant improvement to Statistical Machine Translation (SMT). We support this argument by presenting a multi-pronged approach that addresses modeling, architectural and learning aspects of pre-existing neural network-based SMT features. More concretely, we paper present a new set of neural network-based SMT </context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12:2493–2537, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Spyros Matsoukas</author>
</authors>
<title>Traitbased hypothesis selection for machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT ’12,</booktitle>
<pages>528--532</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context citStr="Devlin and Matsoukas, 2012" endWordPosition="3243" position="19654" startWordPosition="3240">xperiments We demonstrate the impact of our work with extensive MT experiments on Arabic-English and Chinese-English translation for the DARPA BOLT Web Forum and the NIST OpenMT12 conditions. 5.1 Baseline MT System We run our experiments using a state-of-the-art string-to-dependency hierarchical decoder (Shen et al., 2010). The baseline we use includes a set of powerful features as follow: • Forward and backward rule probabilities • Contextual lexical smoothing (Devlin, 2009) • 5-gram Kneser-Ney LM • Dependency LM (Shen et al., 2010) • Length distribution (Shen et al., 2010) • Trait features (Devlin and Matsoukas, 2012) • Factored source syntax (Huang et al., 2013) • Discriminative sparse feature, totaling 50k features (Chiang et al., 2009) • Neural Network Joint Model (NNJM) and Neural Network Lexical Translation Model �Loss = M E i j 35 (NNLTM) (Devlin et al., 2014) As shown, our baseline system already includes neural network-based features. NNJM, NNLTM and use two hidden layers with 500 units and use embedding of size 200 for each input. We use the MADA-ARZ tokenizer (Habash et al., 2013) for Arabic word tokenization. For Chinese tokenization, we use a simple longest-matchfirst lexicon-based approach. We</context>
</contexts>
<marker>Devlin, Matsoukas, 2012</marker>
<rawString>Jacob Devlin and Spyros Matsoukas. 2012. Traitbased hypothesis selection for machine translation. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT ’12, pages 528–532, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
<author>Zhongqiang Huang</author>
<author>Thomas Lamar</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Fast and robust neural network joint models for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1370--1380</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context citStr="Devlin et al., 2014" endWordPosition="206" position="1494" startWordPosition="203">etwork parameters jointly. Each of our proposed methods results in significant improvements that are complementary. The overall improvement is +2.7 and +1.8 BLEU points for Arabic-English and ChineseEnglish translation over a state-of-the-art system that already includes neural network features. 1 Introduction Recent advances in applying Neural Networks to Statistical Machine Translation (SMT) have generally taken one of two approaches. They either develop neural network-based features that are used to score hypotheses generated from traditional translation grammars (Sundermeyer et al., 2014; Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012), or they implement the whole translation process as a single neural network (Bahdanau et al., 2014; Sutskever et al., 2014). The latter approach, sometimes referred to as Neural Machine Translation, attempts to overhaul SMT, while the former capitalizes on the strength of the current SMT paradigm and leverages the modeling power of neural networks to improve the scoring of hypotheses generated ∗* Research conducted when the author was at BBN. by phrase-based or hierarchical translation rules. This paper adopts the former approach, as n-best </context>
<context citStr="Devlin et al., 2014" endWordPosition="612" position="4138" startWordPosition="609">meters learned for one feature to benefit the learning of the other features. This results in better trained models and achieves additional MT improvements (Section 4). We apply the resulting Multitask Tensor Networks to the new features and to existing ones, 31 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 31–41, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics obtaining strong experimental results over the strongest previous results of (Devlin et al., 2014). We obtain improvements of +2.5 BLEU points for Arabic-English and +1.8 BLEU points for Chinese-English on the DARPA BOLT Web Forum condition. We also obtain improvements of +2.7 BLEU point for Arabic-English and +1.9 BLEU points for Chinese-English on the NIST Open12 test sets over the best previously published results in (Devlin et al., 2014). Both the tensor architecture and multitask learning are general techniques that are likely to benefit other neural network features. 2 New Non-Local SMT Features Existing SMT features typically focus on local information in the source sentence, in the</context>
<context citStr="Devlin et al., 2014" endWordPosition="922" position="6106" startWordPosition="919">language model), and sourceenumerating features that estimates a probability for each source word (e.g., lexical translation). More concretely, we introduce a) Joint Model with Offset Source Context (JMO), a hypothesis enumerating feature that predicts the next target word the source context affiliated to the previous target words; and b) Translation Context Model (TCM), a source-enumerating feature that predicts the context of the translation of a source word rather than the translation itself. These two models extend pre-existing features: the Joint (language and translation) Model (JM) of (Devlin et al., 2014) and the LTM respectively respectively. We use a large lexicalized context for there features, making the choice of implementing them as neural networks essential. We also present neuralnetwork implementations of pre-existing sourceenumerating features: lexical translation, orientation and fertility models. We obtain additional gains from using tensor networks and multitask learning in the modeling and training of all the features. 2.1 Hypothesis-Enumerating Features As mentioned, hypothesis-enumerating features score each word in the hypothesis, typically by conditioning it on a context of n-</context>
<context citStr="Devlin et al. (2014)" endWordPosition="1192" position="7760" startWordPosition="1189">O overlap highly when the translation is monotone, but are complementary when the translation requires word reordering. 2.1.1 Joint Model with Offset Source Context Formally, JMO estimates the probability of the target hypothesis E conditioned on the source sentence F and a target-to-source affiliation A: where ei is the word being predicted; ei−n+1 i−1 is the string of n − 1 previously generated words; Cai−k to the source context of m source words around fai−k, the source word affiliated with ei−k. We refer to k as the offset parameter. We use the definition of word affiliation introduced in Devlin et al. (2014). When no source context is used, the model is equivalent to an n-gram language model, while an offset parameter of k = 0 reduces the model to the JM of Devlin et al. (2014). When k &gt; 0, the JMO captures non-local context in the prediction of the next target word. More specifically, ei−k and ei, which are local on the target side, are affiliated to fai−k and fai which may be distant from each other on the source side due to non-monotone translation, even for k = 1. The offset model captures reordering constraints by encouraging the predicted target word ei to fit well with the previous affilia</context>
<context citStr="Devlin et al., 2014" endWordPosition="1627" position="10328" startWordPosition="1624">milar to the hypothesis-enumerating features above) and Yj is the label being predicted by the feature. We first describe pre-existing source-enumerating features: the lexical translation model, the orientation model and the fertility model, and then discuss a new feature: Translation Context Model (TCM), which is an extension of the lexical translation model. 2.2.1 Pre-existing Features Lexical Translation model (LTM) estimates the probability of translating a source word fj to a target word l(fj) = ebj given a source context Cj, bj ∈ B is the source-to-target word affiliation as defined in (Devlin et al., 2014). When fj is translated to more than one word, we arbitrarily keep the left-most one. The target word vocabulary V is extended with a NULL token to accommodate unaligned source words. Orientation model (ORI) describes the probability of orientation of the translation of phrases surrounding a source word fj relative to its own translation. We follow (Setiawan et al., 2013) in modeling the orientation of the left and right phrases of fj with maximal orientation span (the longest neighboring phrase consistent with alignment), which we denote by Lj and Rj respectively. Thus, o(fj) = hoLj(fj), oRj(</context>
<context citStr="Devlin et al., 2014" endWordPosition="3063" position="18540" startWordPosition="3060">Using this setup, it is possible to vary the number of shared hidden layers t from 0 (only sharing the embedding layer) to L − 1 (sharing all the layers except the output). Note that in principle MTL is applicable to other set of networks that have different architecture or even different input set. With MTL, the training procedure is the same as that of standard neural networks. We use the back propagation algorithm, and use as the loss function the product of likelihood of each feature1: 1In this and in the other parts of the paper, we add the normalization regularization term described in (Devlin et al., 2014) to the loss function to avoid computing the normalization constant at model query/decoding time. log (P (Yj(Xi))) where Xi is the training sample and Yj is one of the M models trained. We use the sum of log likelihoods since we assume that the features are independent. Fig. 3(c) illustrates MTL between M models sharing the input embedding layer and the first hidden layer (t = 1) compared to the separatelytrained conventional feedforward neural network and tensor neural network. 5 Experiments We demonstrate the impact of our work with extensive MT experiments on Arabic-English and Chinese-Engl</context>
<context citStr="Devlin et al., 2014" endWordPosition="3286" position="19907" startWordPosition="3283">rt string-to-dependency hierarchical decoder (Shen et al., 2010). The baseline we use includes a set of powerful features as follow: • Forward and backward rule probabilities • Contextual lexical smoothing (Devlin, 2009) • 5-gram Kneser-Ney LM • Dependency LM (Shen et al., 2010) • Length distribution (Shen et al., 2010) • Trait features (Devlin and Matsoukas, 2012) • Factored source syntax (Huang et al., 2013) • Discriminative sparse feature, totaling 50k features (Chiang et al., 2009) • Neural Network Joint Model (NNJM) and Neural Network Lexical Translation Model �Loss = M E i j 35 (NNLTM) (Devlin et al., 2014) As shown, our baseline system already includes neural network-based features. NNJM, NNLTM and use two hidden layers with 500 units and use embedding of size 200 for each input. We use the MADA-ARZ tokenizer (Habash et al., 2013) for Arabic word tokenization. For Chinese tokenization, we use a simple longest-matchfirst lexicon-based approach. We align the training data using GIZA++ (Och and Ney, 2003). For tuning the weights of MT features including the new features, we use iterative k-best optimization with an ExpectedBLEU objective function (Rosti et al., 2010), and decode the test sets afte</context>
<context citStr="Devlin et al., 2014" endWordPosition="3618" position="21807" startWordPosition="3614">ments for the BOLT condition. We show the experimental results related to hypothesis-enumerating features (HypEn) in rows S2-S5, those related to source-enumerating features (SrcEn) in rows S6-S9, and the combination of the two in row S10. For all the features, we set the source context length to m = 5 (11-word window). For JM and JMO, we set the target context length to n = 4. For the offset parameter k of JMO, we use values 1 to 3. For TCM, we model one word around the translation (d = 1). Larger values of d did not result in further gains. The baseline is comparable to the best results of (Devlin et al., 2014). In rows S3 to S5, we incrementally add a model with different offset source context, from k = 1 to k = 3. For AR-EN, adding JMOs with different offset source context consistently yields positive effects in BLEU score, while in ZH-EN, it yields positive effects in TER score. Utilizing all offset source contexts “+JMOk&lt;3” (row S5) yields around 0.9 BLEU point improvement in AR-EN and around 0.3 BLEU in ZH-EN compared to the baseline. The JMO consistently provides better improvement compared to a larger JM context (row S2), validating our hypothesis that using offset source context captures imp</context>
<context citStr="Devlin et al., 2014" endWordPosition="4599" position="27342" startWordPosition="4596">hitecture. On top of this improved baseline, we also observe an improvement of the same scale for other models (column “Tensor”), except for HypEn features in AR-EN experiment. Moving to MTL experiments, we see improvements, especially from SrcEn features. MTL gives around 0.5 BLEU point improvement for AR-EN and around 0.4 BLEU point for ZH-EN. When we employ both HypEn and SrcEn together, MTL gives around 0.4 BLEU point in AR-EN and 0.2 BLEU point in ZH-EN. In total, our work results in an improvement of 2.5 BLEU point for AR-EN and 1.8 for BLEU point in ZH-EN on top of the best results in (Devlin et al., 2014). 5.3 NIST OpenMT12 Our NIST system is compatible with the OpenMT12 constrained track, which consists of 10M words of high-quality parallel training for Arabic, and 25M words for Chinese. The n-gram LM is trained on 5B words of data from the English GigaWord. For test, we use the “Arabic-ToEnglish Original Progress Test” (1378 segments) and “Chinese-to-English Original Progress Test + OpenMT12 Current Test” (2190 segments), which consists of a mix of newswire and web data. All test segments have 4 references. Our tuning set contains 5000 segments, and is a mix of the MT02-05 eval set as well a</context>
<context citStr="Devlin et al. (2014)" endWordPosition="4958" position="29440" startWordPosition="4955"> part shows the BOLT results, while the bottom part shows the NIST results. The best results for each conditions and each language-pair are in bold. The baselines are in italics. . Base. Feat Tensor MTL AR-EN 53.7 55.4 55.9 56.4 mixed-case 51.8 53.1 53.7 54.1 ZH-EN 36.6 37.8 38.2 38.5 mixed-case 34.4 35.5 35.9 36.1 Table 4: Experimental results for the NIST condition. Mixed-case scores are also reported. Baselines are in italics. BLEU point and on Chinese-English, we see a 1.9 BLEU point gain. We also report the mixed-cased BLEU scores for comparison with previous best published results, i.e. Devlin et al. (2014) report 52.8 BLEU for Arabic-English and 34.7 BLEU for Chinese-English. Thus, our results are around 1.3- 1.4 BLEU point better. Note that they use additional rescoring features but we do not. 6 Related Work Our work is most closely related to Devlin et al. (2014). They use a simple feedforward neural network to model two important MT features: A joint language and translation model, and a lexical translation model. They show very large improvements on Arabic-English and ChineseEnglish web forum and newswire baselines. We improve on their work in 3 aspects. First, we model more features using </context>
</contexts>
<marker>Devlin, Zbib, Huang, Lamar, Schwartz, Makhoul, 2014</marker>
<rawString>Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1370–1380, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
</authors>
<title>Lexical features for statistical machine translation. Master’s thesis,</title>
<date>2009</date>
<institution>University of Maryland.</institution>
<marker>Devlin, 2009</marker>
<rawString>Jacob Devlin. 2009. Lexical features for statistical machine translation. Master’s thesis, University of Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Joint parsing and named entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>326--334</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context citStr="Finkel and Manning (2009)" endWordPosition="5643" position="33680" startWordPosition="5640">e collocation, which has been long recognized as important information for many NLP tasks (e.g. word sense disambiguation (Lee and Ng, 2002)). The tensor formulation we use is similar to that of (Yu et al., 2012; Hutchinson et al., 2013). Tensor Neural Networks have a wide application in other field, but have only been recently applied in NLP (Socher et al., 2013; Pei et al., 2014). To our knowledge, our work is the first to use tensor networks in SMT. Our approach to multitask learning is related to work that is often labeled joint training or transfer learning. To name a few of these works, Finkel and Manning (2009) successfully train name entity recognizers and syntactic parsers jointly, and Singh et al. (2013) train models for coreference resolution, named entity recognition and relation extraction jointly. Both efforts are motivated by the minimization of cascading errors. Our work is most closely related to Collobert and Weston (2008; Collobert et al. (2011), who apply multitask learning to train neural networks for multiple NLP models: part-of-speech tagging, semantic role labeling, named-entity recognition and language model variations. 7 Conclusion This paper argues that a relatively simple feedfo</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>Jenny Rose Finkel and Christopher D. Manning. 2009. Joint parsing and named entity recognition. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 326–334, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Ryan Roth</author>
<author>Owen Rambow</author>
<author>Ramy Eskander</author>
<author>Nadi Tomeh</author>
</authors>
<title>Morphological analysis and disambiguation for dialectal arabic. In</title>
<date>2013</date>
<booktitle>HLT-NAACL,</booktitle>
<pages>426--432</pages>
<contexts>
<context citStr="Habash et al., 2013" endWordPosition="3324" position="20136" startWordPosition="3321"> Kneser-Ney LM • Dependency LM (Shen et al., 2010) • Length distribution (Shen et al., 2010) • Trait features (Devlin and Matsoukas, 2012) • Factored source syntax (Huang et al., 2013) • Discriminative sparse feature, totaling 50k features (Chiang et al., 2009) • Neural Network Joint Model (NNJM) and Neural Network Lexical Translation Model �Loss = M E i j 35 (NNLTM) (Devlin et al., 2014) As shown, our baseline system already includes neural network-based features. NNJM, NNLTM and use two hidden layers with 500 units and use embedding of size 200 for each input. We use the MADA-ARZ tokenizer (Habash et al., 2013) for Arabic word tokenization. For Chinese tokenization, we use a simple longest-matchfirst lexicon-based approach. We align the training data using GIZA++ (Och and Ney, 2003). For tuning the weights of MT features including the new features, we use iterative k-best optimization with an ExpectedBLEU objective function (Rosti et al., 2010), and decode the test sets after 5 tuning iteration. We report the lower-cased BLEU and TER scores. 5.2 BOLT Discussion Forum The bulk of our experiments is on the BOLT Web Discussion Forum domain, which uses data collected by the LDC. The parallel training da</context>
</contexts>
<marker>Habash, Roth, Rambow, Eskander, Tomeh, 2013</marker>
<rawString>Nizar Habash, Ryan Roth, Owen Rambow, Ramy Eskander, and Nadi Tomeh. 2013. Morphological analysis and disambiguation for dialectal arabic. In HLT-NAACL, pages 426–432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
</authors>
<title>Factored soft source syntactic constraints for hierarchical machine translation.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>556--566</pages>
<contexts>
<context citStr="Huang et al., 2013" endWordPosition="3251" position="19700" startWordPosition="3248">extensive MT experiments on Arabic-English and Chinese-English translation for the DARPA BOLT Web Forum and the NIST OpenMT12 conditions. 5.1 Baseline MT System We run our experiments using a state-of-the-art string-to-dependency hierarchical decoder (Shen et al., 2010). The baseline we use includes a set of powerful features as follow: • Forward and backward rule probabilities • Contextual lexical smoothing (Devlin, 2009) • 5-gram Kneser-Ney LM • Dependency LM (Shen et al., 2010) • Length distribution (Shen et al., 2010) • Trait features (Devlin and Matsoukas, 2012) • Factored source syntax (Huang et al., 2013) • Discriminative sparse feature, totaling 50k features (Chiang et al., 2009) • Neural Network Joint Model (NNJM) and Neural Network Lexical Translation Model �Loss = M E i j 35 (NNLTM) (Devlin et al., 2014) As shown, our baseline system already includes neural network-based features. NNJM, NNLTM and use two hidden layers with 500 units and use embedding of size 200 for each input. We use the MADA-ARZ tokenizer (Habash et al., 2013) for Arabic word tokenization. For Chinese tokenization, we use a simple longest-matchfirst lexicon-based approach. We align the training data using GIZA++ (Och and</context>
</contexts>
<marker>Huang, Devlin, Zbib, 2013</marker>
<rawString>Zhongqiang Huang, Jacob Devlin, and Rabih Zbib. 2013. Factored soft source syntactic constraints for hierarchical machine translation. In EMNLP, pages 556–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Hutchinson</author>
<author>Li Deng</author>
<author>Dong Yu</author>
</authors>
<title>Tensor deep stacking networks.</title>
<date>2013</date>
<journal>IEEE Trans. Pattern Anal. Mach. Intell.,</journal>
<volume>35</volume>
<issue>8</issue>
<contexts>
<context citStr="Hutchinson et al., 2013" endWordPosition="2366" position="14604" startWordPosition="2363">context words which are useful in many natural language processing tasks. In conventional feedforward neural networks, the output of hidden layer l is produced by multiplying the output vector from the previous layer with a weight matrix (Wl) and then applying the activation function σ to the product. Tensor Neural Networks generalize this formulation by using a tensor Ul of order 3 for the weights. The output of node k in layer l is computed as follows: ) hl[k] = σ (hl−1 · Ul[k] · hT l−1 where Ul[k], the k-th slice of Ul, is a square matrix. In our implementation, we follow (Yu et al., 2012; Hutchinson et al., 2013) and use a low-rank approximation of Ul[k] = Ql[k] · Rl[k]T, where Ql[k], Rl[k] ∈ Rn×r. The output of node k becomes: hl [k] = σ (hl−1 · Ql [k] · Rl [k]T · hi— hi 1 ) In In our experiments, we choose r = 1, and also apply the non-linear activation function σ distributively. We arrive at the following three equations for computing the hidden layer outputs (0 &lt; l &lt; L): vl = σ (hl−1 · Ql) v0l = σ (hl−1 · Rl) hl = vl ⊗ v0l where hl−1 is double-projected to vl and v0l, and the two projections are merged using the Hadamard element-wise product operator ⊗. This formulation allows us to use the same i</context>
<context citStr="Hutchinson et al., 2013" endWordPosition="5573" position="33292" startWordPosition="5570">e specifically, due to the absence of a feedback loop, the feedforward architecture allows us to treat individual decisions independently, which makes parallelization of the training easy and the querying the network at decoding time straightforward. The use of tensors in the hidden layers strengthens the neural network model, allowing us to model more complex feature interactions like collocation, which has been long recognized as important information for many NLP tasks (e.g. word sense disambiguation (Lee and Ng, 2002)). The tensor formulation we use is similar to that of (Yu et al., 2012; Hutchinson et al., 2013). Tensor Neural Networks have a wide application in other field, but have only been recently applied in NLP (Socher et al., 2013; Pei et al., 2014). To our knowledge, our work is the first to use tensor networks in SMT. Our approach to multitask learning is related to work that is often labeled joint training or transfer learning. To name a few of these works, Finkel and Manning (2009) successfully train name entity recognizers and syntactic parsers jointly, and Singh et al. (2013) train models for coreference resolution, named entity recognition and relation extraction jointly. Both efforts a</context>
</contexts>
<marker>Hutchinson, Deng, Yu, 2013</marker>
<rawString>Brian Hutchinson, Li Deng, and Dong Yu. 2013. Tensor deep stacking networks. IEEE Trans. Pattern Anal. Mach. Intell., 35(8):1944–1957, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent continuous translation models.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1700--1709</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context citStr="Kalchbrenner and Blunsom (2013)" endWordPosition="5339" position="31804" startWordPosition="5336">; Le et al., 2012; Schwenk, 2012). Approaches to incorporating source context into a neural network model differ mainly in how they represent the source sentence and in how long is the history they keep. In terms of representation of the source sentence, we follow (Devlin et al., 2014) in using a window around the affiliated source word. To name some other approaches, Auli et al. (2013) uses latent semantic analysis and source sentence embeddings learned from the recurrent neural network; Sundermeyer et al. (2014) take the representation from a bidirectional LSTM recurrent neural network; and Kalchbrenner and Blunsom (2013) employ a convolutional sentence model. For target context, recent work has tried to look beyond the classical n-gram history. (Auli et al., 2013; Sundermeyer et al., 2014) consider an unbounded history, at the expense of making their model only applicable for N-best rescoring. Another recent line of research (Bahdanau et al., 2014; Sutskever et al., 2014) departs more radically from conventional feature-based SMT and implements the MT system as a single neural network. These models use a representation of the whole input sentence. We use a feedforward neural network in this work. Besides feed</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700–1709, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>A convolutional neural network for modelling sentences.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>655--665</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context citStr="Kalchbrenner et al., 2014" endWordPosition="5456" position="32555" startWordPosition="5453">. (Auli et al., 2013; Sundermeyer et al., 2014) consider an unbounded history, at the expense of making their model only applicable for N-best rescoring. Another recent line of research (Bahdanau et al., 2014; Sutskever et al., 2014) departs more radically from conventional feature-based SMT and implements the MT system as a single neural network. These models use a representation of the whole input sentence. We use a feedforward neural network in this work. Besides feedforward and recurrent net38 works, other network architectures that have been applied to SMT include convolutional networks (Kalchbrenner et al., 2014) and recursive networks (Socher et al., 2011). The simplicity of feedforward networks works to our advantage. More specifically, due to the absence of a feedback loop, the feedforward architecture allows us to treat individual decisions independently, which makes parallelization of the training easy and the querying the network at decoding time straightforward. The use of tensors in the hidden layers strengthens the neural network model, allowing us to model more complex feature interactions like collocation, which has been long recognized as important information for many NLP tasks (e.g. word</context>
</contexts>
<marker>Kalchbrenner, Grefenstette, Blunsom, 2014</marker>
<rawString>Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 655–665, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai-Son Le</author>
<author>Alexandre Allauzen</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Continuous space translation models with neural networks.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT ’12,</booktitle>
<pages>39--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context citStr="Le et al., 2012" endWordPosition="214" position="1530" startWordPosition="211">roposed methods results in significant improvements that are complementary. The overall improvement is +2.7 and +1.8 BLEU points for Arabic-English and ChineseEnglish translation over a state-of-the-art system that already includes neural network features. 1 Introduction Recent advances in applying Neural Networks to Statistical Machine Translation (SMT) have generally taken one of two approaches. They either develop neural network-based features that are used to score hypotheses generated from traditional translation grammars (Sundermeyer et al., 2014; Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012), or they implement the whole translation process as a single neural network (Bahdanau et al., 2014; Sutskever et al., 2014). The latter approach, sometimes referred to as Neural Machine Translation, attempts to overhaul SMT, while the former capitalizes on the strength of the current SMT paradigm and leverages the modeling power of neural networks to improve the scoring of hypotheses generated ∗* Research conducted when the author was at BBN. by phrase-based or hierarchical translation rules. This paper adopts the former approach, as n-best scores from state-of-the-art SMT sys</context>
<context citStr="Le et al., 2012" endWordPosition="5242" position="31190" startWordPosition="5239">e in this paper address the major aspects of SMT modeling that have informed much of the research since the original IBM models (Brown et al., 1993): lexical translation, reordering, word fertility, and language models. Of particular relevance to our work are approaches that incorporate context-sensitivity into the models (Carpuat and Wu, 2007), formulate reordering as orientation prediction task (Tillman, 2004) and that use neural network language models (Bengio et al., 2003; Schwenk, 2010; Schwenk, 2012), and incorporate source-side context into them (Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012). Approaches to incorporating source context into a neural network model differ mainly in how they represent the source sentence and in how long is the history they keep. In terms of representation of the source sentence, we follow (Devlin et al., 2014) in using a window around the affiliated source word. To name some other approaches, Auli et al. (2013) uses latent semantic analysis and source sentence embeddings learned from the recurrent neural network; Sundermeyer et al. (2014) take the representation from a bidirectional LSTM recurrent neural network; and Kalchbrenner and </context>
</contexts>
<marker>Le, Allauzen, Yvon, 2012</marker>
<rawString>Hai-Son Le, Alexandre Allauzen, and Franc¸ois Yvon. 2012. Continuous space translation models with neural networks. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT ’12, pages 39– 48, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Keok Lee</author>
<author>Hwee Tou Ng</author>
</authors>
<title>An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing - Volume 10, EMNLP ’02,</booktitle>
<pages>41--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context citStr="Lee and Ng, 2002" endWordPosition="5555" position="33195" startWordPosition="5552"> (Socher et al., 2011). The simplicity of feedforward networks works to our advantage. More specifically, due to the absence of a feedback loop, the feedforward architecture allows us to treat individual decisions independently, which makes parallelization of the training easy and the querying the network at decoding time straightforward. The use of tensors in the hidden layers strengthens the neural network model, allowing us to model more complex feature interactions like collocation, which has been long recognized as important information for many NLP tasks (e.g. word sense disambiguation (Lee and Ng, 2002)). The tensor formulation we use is similar to that of (Yu et al., 2012; Hutchinson et al., 2013). Tensor Neural Networks have a wide application in other field, but have only been recently applied in NLP (Socher et al., 2013; Pei et al., 2014). To our knowledge, our work is the first to use tensor networks in SMT. Our approach to multitask learning is related to work that is often labeled joint training or transfer learning. To name a few of these works, Finkel and Manning (2009) successfully train name entity recognizers and syntactic parsers jointly, and Singh et al. (2013) train models for</context>
</contexts>
<marker>Lee, Ng, 2002</marker>
<rawString>Yoong Keok Lee and Hwee Tou Ng. 2002. An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation. In Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing - Volume 10, EMNLP ’02, pages 41–48, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context citStr="Och and Ney, 2003" endWordPosition="3351" position="20311" startWordPosition="3348">, 2013) • Discriminative sparse feature, totaling 50k features (Chiang et al., 2009) • Neural Network Joint Model (NNJM) and Neural Network Lexical Translation Model �Loss = M E i j 35 (NNLTM) (Devlin et al., 2014) As shown, our baseline system already includes neural network-based features. NNJM, NNLTM and use two hidden layers with 500 units and use embedding of size 200 for each input. We use the MADA-ARZ tokenizer (Habash et al., 2013) for Arabic word tokenization. For Chinese tokenization, we use a simple longest-matchfirst lexicon-based approach. We align the training data using GIZA++ (Och and Ney, 2003). For tuning the weights of MT features including the new features, we use iterative k-best optimization with an ExpectedBLEU objective function (Rosti et al., 2010), and decode the test sets after 5 tuning iteration. We report the lower-cased BLEU and TER scores. 5.2 BOLT Discussion Forum The bulk of our experiments is on the BOLT Web Discussion Forum domain, which uses data collected by the LDC. The parallel training data consists of all of the high-quality NIST training corpora, plus an additional 3 million words of translated forum data. The tuning and test sets consist of roughly 5000 seg</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenzhe Pei</author>
<author>Tao Ge</author>
<author>Baobao Chang</author>
</authors>
<title>Maxmargin tensor neural network for chinese word segmentation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>293--303</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context citStr="Pei et al., 2014" endWordPosition="5600" position="33439" startWordPosition="5597">allelization of the training easy and the querying the network at decoding time straightforward. The use of tensors in the hidden layers strengthens the neural network model, allowing us to model more complex feature interactions like collocation, which has been long recognized as important information for many NLP tasks (e.g. word sense disambiguation (Lee and Ng, 2002)). The tensor formulation we use is similar to that of (Yu et al., 2012; Hutchinson et al., 2013). Tensor Neural Networks have a wide application in other field, but have only been recently applied in NLP (Socher et al., 2013; Pei et al., 2014). To our knowledge, our work is the first to use tensor networks in SMT. Our approach to multitask learning is related to work that is often labeled joint training or transfer learning. To name a few of these works, Finkel and Manning (2009) successfully train name entity recognizers and syntactic parsers jointly, and Singh et al. (2013) train models for coreference resolution, named entity recognition and relation extraction jointly. Both efforts are motivated by the minimization of cascading errors. Our work is most closely related to Collobert and Weston (2008; Collobert et al. (2011), who </context>
</contexts>
<marker>Pei, Ge, Chang, 2014</marker>
<rawString>Wenzhe Pei, Tao Ge, and Baobao Chang. 2014. Maxmargin tensor neural network for chinese word segmentation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 293–303, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti Rosti</author>
<author>Bing Zhang</author>
<author>Spyros Matsoukas</author>
<author>Rich Schwartz</author>
</authors>
<title>BBN system description for WMT10 system combination task. In WMT/MetricsMATR,</title>
<date>2010</date>
<pages>321--326</pages>
<contexts>
<context citStr="Rosti et al., 2010" endWordPosition="3377" position="20476" startWordPosition="3374">l �Loss = M E i j 35 (NNLTM) (Devlin et al., 2014) As shown, our baseline system already includes neural network-based features. NNJM, NNLTM and use two hidden layers with 500 units and use embedding of size 200 for each input. We use the MADA-ARZ tokenizer (Habash et al., 2013) for Arabic word tokenization. For Chinese tokenization, we use a simple longest-matchfirst lexicon-based approach. We align the training data using GIZA++ (Och and Ney, 2003). For tuning the weights of MT features including the new features, we use iterative k-best optimization with an ExpectedBLEU objective function (Rosti et al., 2010), and decode the test sets after 5 tuning iteration. We report the lower-cased BLEU and TER scores. 5.2 BOLT Discussion Forum The bulk of our experiments is on the BOLT Web Discussion Forum domain, which uses data collected by the LDC. The parallel training data consists of all of the high-quality NIST training corpora, plus an additional 3 million words of translated forum data. The tuning and test sets consist of roughly 5000 segments each, with 2 independent references for Arabic and 3 for Chinese. 5.2.1 Effects of New Features We first look at the effects of the proposed features compared </context>
</contexts>
<marker>Rosti, Zhang, Matsoukas, Schwartz, 2010</marker>
<rawString>Antti Rosti, Bing Zhang, Spyros Matsoukas, and Rich Schwartz. 2010. BBN system description for WMT10 system combination task. In WMT/MetricsMATR, pages 321–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous-space language models for statistical machine translation.</title>
<date>2010</date>
<journal>Prague Bull. Math. Linguistics,</journal>
<pages>93--137</pages>
<contexts>
<context citStr="Schwenk, 2010" endWordPosition="5221" position="31070" startWordPosition="5220">e three aspect of our work, namely MT modeling, neural network architecture and model learning. The features we propose in this paper address the major aspects of SMT modeling that have informed much of the research since the original IBM models (Brown et al., 1993): lexical translation, reordering, word fertility, and language models. Of particular relevance to our work are approaches that incorporate context-sensitivity into the models (Carpuat and Wu, 2007), formulate reordering as orientation prediction task (Tillman, 2004) and that use neural network language models (Bengio et al., 2003; Schwenk, 2010; Schwenk, 2012), and incorporate source-side context into them (Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012). Approaches to incorporating source context into a neural network model differ mainly in how they represent the source sentence and in how long is the history they keep. In terms of representation of the source sentence, we follow (Devlin et al., 2014) in using a window around the affiliated source word. To name some other approaches, Auli et al. (2013) uses latent semantic analysis and source sentence embeddings learned from the recurrent neural network; Sun</context>
</contexts>
<marker>Schwenk, 2010</marker>
<rawString>Holger Schwenk. 2010. Continuous-space language models for statistical machine translation. Prague Bull. Math. Linguistics, 93:137–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous space translation models for phrase-based statistical machine translation.</title>
<date>2012</date>
<booktitle>In COLING (Posters),</booktitle>
<pages>1071--1080</pages>
<contexts>
<context citStr="Schwenk, 2012" endWordPosition="216" position="1546" startWordPosition="215">esults in significant improvements that are complementary. The overall improvement is +2.7 and +1.8 BLEU points for Arabic-English and ChineseEnglish translation over a state-of-the-art system that already includes neural network features. 1 Introduction Recent advances in applying Neural Networks to Statistical Machine Translation (SMT) have generally taken one of two approaches. They either develop neural network-based features that are used to score hypotheses generated from traditional translation grammars (Sundermeyer et al., 2014; Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012), or they implement the whole translation process as a single neural network (Bahdanau et al., 2014; Sutskever et al., 2014). The latter approach, sometimes referred to as Neural Machine Translation, attempts to overhaul SMT, while the former capitalizes on the strength of the current SMT paradigm and leverages the modeling power of neural networks to improve the scoring of hypotheses generated ∗* Research conducted when the author was at BBN. by phrase-based or hierarchical translation rules. This paper adopts the former approach, as n-best scores from state-of-the-art SMT systems often sugge</context>
<context citStr="Schwenk, 2012" endWordPosition="5223" position="31086" startWordPosition="5222">of our work, namely MT modeling, neural network architecture and model learning. The features we propose in this paper address the major aspects of SMT modeling that have informed much of the research since the original IBM models (Brown et al., 1993): lexical translation, reordering, word fertility, and language models. Of particular relevance to our work are approaches that incorporate context-sensitivity into the models (Carpuat and Wu, 2007), formulate reordering as orientation prediction task (Tillman, 2004) and that use neural network language models (Bengio et al., 2003; Schwenk, 2010; Schwenk, 2012), and incorporate source-side context into them (Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012). Approaches to incorporating source context into a neural network model differ mainly in how they represent the source sentence and in how long is the history they keep. In terms of representation of the source sentence, we follow (Devlin et al., 2014) in using a window around the affiliated source word. To name some other approaches, Auli et al. (2013) uses latent semantic analysis and source sentence embeddings learned from the recurrent neural network; Sundermeyer et al. </context>
</contexts>
<marker>Schwenk, 2012</marker>
<rawString>Holger Schwenk. 2012. Continuous space translation models for phrase-based statistical machine translation. In COLING (Posters), pages 1071–1080.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hendra Setiawan</author>
<author>Bowen Zhou</author>
<author>Bing Xiang</author>
<author>Libin Shen</author>
</authors>
<title>Two-neighbor orientation model with cross-boundary global contexts.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1264--1274</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context citStr="Setiawan et al., 2013" endWordPosition="1689" position="10702" startWordPosition="1686">-existing Features Lexical Translation model (LTM) estimates the probability of translating a source word fj to a target word l(fj) = ebj given a source context Cj, bj ∈ B is the source-to-target word affiliation as defined in (Devlin et al., 2014). When fj is translated to more than one word, we arbitrarily keep the left-most one. The target word vocabulary V is extended with a NULL token to accommodate unaligned source words. Orientation model (ORI) describes the probability of orientation of the translation of phrases surrounding a source word fj relative to its own translation. We follow (Setiawan et al., 2013) in modeling the orientation of the left and right phrases of fj with maximal orientation span (the longest neighboring phrase consistent with alignment), which we denote by Lj and Rj respectively. Thus, o(fj) = hoLj(fj), oRj(fj)i, where oLj and oRj refer to the orientation of Lj and Rj respectively. For unaligned fj, we set o(fj) = oLj(Rj), the orientation of Rj with respect to Lj. Fertility model (FM) models the probability that a source word fj generates 0(fj) words in the hypothesis. Our implemented model only distinguishes between aligned and unaligned source words (i.e., 0(fj) ∈ {0,1}). </context>
</contexts>
<marker>Setiawan, Zhou, Xiang, Shen, 2013</marker>
<rawString>Hendra Setiawan, Bowen Zhou, Bing Xiang, and Libin Shen. 2013. Two-neighbor orientation model with cross-boundary global contexts. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1264–1274, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>String-to-dependency statistical machine translation.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<contexts>
<context citStr="Shen et al., 2010" endWordPosition="3194" position="19351" startWordPosition="3191">the sum of log likelihoods since we assume that the features are independent. Fig. 3(c) illustrates MTL between M models sharing the input embedding layer and the first hidden layer (t = 1) compared to the separatelytrained conventional feedforward neural network and tensor neural network. 5 Experiments We demonstrate the impact of our work with extensive MT experiments on Arabic-English and Chinese-English translation for the DARPA BOLT Web Forum and the NIST OpenMT12 conditions. 5.1 Baseline MT System We run our experiments using a state-of-the-art string-to-dependency hierarchical decoder (Shen et al., 2010). The baseline we use includes a set of powerful features as follow: • Forward and backward rule probabilities • Contextual lexical smoothing (Devlin, 2009) • 5-gram Kneser-Ney LM • Dependency LM (Shen et al., 2010) • Length distribution (Shen et al., 2010) • Trait features (Devlin and Matsoukas, 2012) • Factored source syntax (Huang et al., 2013) • Discriminative sparse feature, totaling 50k features (Chiang et al., 2009) • Neural Network Joint Model (NNJM) and Neural Network Lexical Translation Model �Loss = M E i j 35 (NNLTM) (Devlin et al., 2014) As shown, our baseline system already inclu</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2010</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010. String-to-dependency statistical machine translation. Computational Linguistics, 36(4):649–671, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Singh</author>
<author>Sebastian Riedel</author>
<author>Brian Martin</author>
<author>Jiaping Zheng</author>
<author>Andrew McCallum</author>
</authors>
<title>Joint inference of entities, relations, and coreference.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Workshop on Automated Knowledge Base Construction, AKBC ’13,</booktitle>
<volume>1</volume>
<pages>pages</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context citStr="Singh et al. (2013)" endWordPosition="5658" position="33778" startWordPosition="5655">se disambiguation (Lee and Ng, 2002)). The tensor formulation we use is similar to that of (Yu et al., 2012; Hutchinson et al., 2013). Tensor Neural Networks have a wide application in other field, but have only been recently applied in NLP (Socher et al., 2013; Pei et al., 2014). To our knowledge, our work is the first to use tensor networks in SMT. Our approach to multitask learning is related to work that is often labeled joint training or transfer learning. To name a few of these works, Finkel and Manning (2009) successfully train name entity recognizers and syntactic parsers jointly, and Singh et al. (2013) train models for coreference resolution, named entity recognition and relation extraction jointly. Both efforts are motivated by the minimization of cascading errors. Our work is most closely related to Collobert and Weston (2008; Collobert et al. (2011), who apply multitask learning to train neural networks for multiple NLP models: part-of-speech tagging, semantic role labeling, named-entity recognition and language model variations. 7 Conclusion This paper argues that a relatively simple feedforward neural network can still provides significant improvement to Statistical Machine Translation</context>
</contexts>
<marker>Singh, Riedel, Martin, Zheng, McCallum, 2013</marker>
<rawString>Sameer Singh, Sebastian Riedel, Brian Martin, Jiaping Zheng, and Andrew McCallum. 2013. Joint inference of entities, relations, and coreference. In Proceedings of the 2013 Workshop on Automated Knowledge Base Construction, AKBC ’13, pages 1– 6, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Language and translation model adaptation using comparable corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>857--866</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Snover, Dorr, Schwartz, 2008</marker>
<rawString>Matthew Snover, Bonnie Dorr, and Richard Schwartz. 2008. Language and translation model adaptation using comparable corpora. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 857–866, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff C Lin</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing Natural Scenes and Natural Language with Recursive Neural Networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 26th International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context citStr="Socher et al., 2011" endWordPosition="5463" position="32600" startWordPosition="5460">sider an unbounded history, at the expense of making their model only applicable for N-best rescoring. Another recent line of research (Bahdanau et al., 2014; Sutskever et al., 2014) departs more radically from conventional feature-based SMT and implements the MT system as a single neural network. These models use a representation of the whole input sentence. We use a feedforward neural network in this work. Besides feedforward and recurrent net38 works, other network architectures that have been applied to SMT include convolutional networks (Kalchbrenner et al., 2014) and recursive networks (Socher et al., 2011). The simplicity of feedforward networks works to our advantage. More specifically, due to the absence of a feedback loop, the feedforward architecture allows us to treat individual decisions independently, which makes parallelization of the training easy and the querying the network at decoding time straightforward. The use of tensors in the hidden layers strengthens the neural network model, allowing us to model more complex feature interactions like collocation, which has been long recognized as important information for many NLP tasks (e.g. word sense disambiguation (Lee and Ng, 2002)). Th</context>
</contexts>
<marker>Socher, Lin, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christopher D. Manning. 2011. Parsing Natural Scenes and Natural Language with Recursive Neural Networks. In Proceedings of the 26th International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Danqi Chen</author>
<author>Christopher D Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Reasoning with neural tensor networks for knowledge base completion.</title>
<date>2013</date>
<booktitle>Advances in Neural Information Processing Systems 26,</booktitle>
<pages>926--934</pages>
<editor>In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors,</editor>
<publisher>Curran Associates, Inc.</publisher>
<contexts>
<context citStr="Socher et al., 2013" endWordPosition="5596" position="33420" startWordPosition="5593">ntly, which makes parallelization of the training easy and the querying the network at decoding time straightforward. The use of tensors in the hidden layers strengthens the neural network model, allowing us to model more complex feature interactions like collocation, which has been long recognized as important information for many NLP tasks (e.g. word sense disambiguation (Lee and Ng, 2002)). The tensor formulation we use is similar to that of (Yu et al., 2012; Hutchinson et al., 2013). Tensor Neural Networks have a wide application in other field, but have only been recently applied in NLP (Socher et al., 2013; Pei et al., 2014). To our knowledge, our work is the first to use tensor networks in SMT. Our approach to multitask learning is related to work that is often labeled joint training or transfer learning. To name a few of these works, Finkel and Manning (2009) successfully train name entity recognizers and syntactic parsers jointly, and Singh et al. (2013) train models for coreference resolution, named entity recognition and relation extraction jointly. Both efforts are motivated by the minimization of cascading errors. Our work is most closely related to Collobert and Weston (2008; Collobert </context>
</contexts>
<marker>Socher, Chen, Manning, Ng, 2013</marker>
<rawString>Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. 2013. Reasoning with neural tensor networks for knowledge base completion. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 926–934. Curran Associates, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Sundermeyer</author>
<author>Tamer Alkhouli</author>
<author>Joern Wuebker</author>
<author>Hermann Ney</author>
</authors>
<title>Translation modeling with bidirectional recurrent neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>14--25</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar,</location>
<contexts>
<context citStr="Sundermeyer et al., 2014" endWordPosition="202" position="1473" startWordPosition="199">g to estimate the neural network parameters jointly. Each of our proposed methods results in significant improvements that are complementary. The overall improvement is +2.7 and +1.8 BLEU points for Arabic-English and ChineseEnglish translation over a state-of-the-art system that already includes neural network features. 1 Introduction Recent advances in applying Neural Networks to Statistical Machine Translation (SMT) have generally taken one of two approaches. They either develop neural network-based features that are used to score hypotheses generated from traditional translation grammars (Sundermeyer et al., 2014; Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012), or they implement the whole translation process as a single neural network (Bahdanau et al., 2014; Sutskever et al., 2014). The latter approach, sometimes referred to as Neural Machine Translation, attempts to overhaul SMT, while the former capitalizes on the strength of the current SMT paradigm and leverages the modeling power of neural networks to improve the scoring of hypotheses generated ∗* Research conducted when the author was at BBN. by phrase-based or hierarchical translation rules. This paper adopts the former</context>
<context citStr="Sundermeyer et al. (2014)" endWordPosition="5324" position="31692" startWordPosition="5321">010; Schwenk, 2012), and incorporate source-side context into them (Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012). Approaches to incorporating source context into a neural network model differ mainly in how they represent the source sentence and in how long is the history they keep. In terms of representation of the source sentence, we follow (Devlin et al., 2014) in using a window around the affiliated source word. To name some other approaches, Auli et al. (2013) uses latent semantic analysis and source sentence embeddings learned from the recurrent neural network; Sundermeyer et al. (2014) take the representation from a bidirectional LSTM recurrent neural network; and Kalchbrenner and Blunsom (2013) employ a convolutional sentence model. For target context, recent work has tried to look beyond the classical n-gram history. (Auli et al., 2013; Sundermeyer et al., 2014) consider an unbounded history, at the expense of making their model only applicable for N-best rescoring. Another recent line of research (Bahdanau et al., 2014; Sutskever et al., 2014) departs more radically from conventional feature-based SMT and implements the MT system as a single neural network. These models </context>
</contexts>
<marker>Sundermeyer, Alkhouli, Wuebker, Ney, 2014</marker>
<rawString>Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker, and Hermann Ney. 2014. Translation modeling with bidirectional recurrent neural networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 14–25, Doha, Qatar, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc V V Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>Advances in Neural Information Processing Systems 27,</booktitle>
<pages>3104--3112</pages>
<editor>In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors,</editor>
<publisher>Curran Associates, Inc.</publisher>
<contexts>
<context citStr="Sutskever et al., 2014" endWordPosition="237" position="1670" startWordPosition="234"> Arabic-English and ChineseEnglish translation over a state-of-the-art system that already includes neural network features. 1 Introduction Recent advances in applying Neural Networks to Statistical Machine Translation (SMT) have generally taken one of two approaches. They either develop neural network-based features that are used to score hypotheses generated from traditional translation grammars (Sundermeyer et al., 2014; Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012), or they implement the whole translation process as a single neural network (Bahdanau et al., 2014; Sutskever et al., 2014). The latter approach, sometimes referred to as Neural Machine Translation, attempts to overhaul SMT, while the former capitalizes on the strength of the current SMT paradigm and leverages the modeling power of neural networks to improve the scoring of hypotheses generated ∗* Research conducted when the author was at BBN. by phrase-based or hierarchical translation rules. This paper adopts the former approach, as n-best scores from state-of-the-art SMT systems often suggest that these systems can still be significantly improved with better features. We build on (Devlin et al., 2014) who propos</context>
<context citStr="Sutskever et al., 2014" endWordPosition="5395" position="32162" startWordPosition="5392">hes, Auli et al. (2013) uses latent semantic analysis and source sentence embeddings learned from the recurrent neural network; Sundermeyer et al. (2014) take the representation from a bidirectional LSTM recurrent neural network; and Kalchbrenner and Blunsom (2013) employ a convolutional sentence model. For target context, recent work has tried to look beyond the classical n-gram history. (Auli et al., 2013; Sundermeyer et al., 2014) consider an unbounded history, at the expense of making their model only applicable for N-best rescoring. Another recent line of research (Bahdanau et al., 2014; Sutskever et al., 2014) departs more radically from conventional feature-based SMT and implements the MT system as a single neural network. These models use a representation of the whole input sentence. We use a feedforward neural network in this work. Besides feedforward and recurrent net38 works, other network architectures that have been applied to SMT include convolutional networks (Kalchbrenner et al., 2014) and recursive networks (Socher et al., 2011). The simplicity of feedforward networks works to our advantage. More specifically, due to the absence of a feedback loop, the feedforward architecture allows us </context>
</contexts>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc V. V Le. 2014. Sequence to sequence learning with neural networks. In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 3104–3112. Curran Associates, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillman</author>
</authors>
<title>A unigram orientation model for statistical machine translation.</title>
<date>2004</date>
<booktitle>HLT-NAACL 2004: Short Papers,</booktitle>
<volume>2</volume>
<pages>101--104</pages>
<editor>In Daniel Marcu Susan Dumais and Salim Roukos, editors,</editor>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context citStr="Tillman, 2004" endWordPosition="5208" position="30990" startWordPosition="5206">rning. In the remainder of this section, we describe previous work relating to the three aspect of our work, namely MT modeling, neural network architecture and model learning. The features we propose in this paper address the major aspects of SMT modeling that have informed much of the research since the original IBM models (Brown et al., 1993): lexical translation, reordering, word fertility, and language models. Of particular relevance to our work are approaches that incorporate context-sensitivity into the models (Carpuat and Wu, 2007), formulate reordering as orientation prediction task (Tillman, 2004) and that use neural network language models (Bengio et al., 2003; Schwenk, 2010; Schwenk, 2012), and incorporate source-side context into them (Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012). Approaches to incorporating source context into a neural network model differ mainly in how they represent the source sentence and in how long is the history they keep. In terms of representation of the source sentence, we follow (Devlin et al., 2014) in using a window around the affiliated source word. To name some other approaches, Auli et al. (2013) uses latent semantic analys</context>
</contexts>
<marker>Tillman, 2004</marker>
<rawString>Christoph Tillman. 2004. A unigram orientation model for statistical machine translation. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Short Papers, pages 101– 104, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong Yu</author>
<author>Li Deng</author>
<author>Frank Seide</author>
</authors>
<title>Large vocabulary speech recognition using deep tensor neural networks.</title>
<date>2012</date>
<booktitle>In INTERSPEECH. ISCA.</booktitle>
<contexts>
<context citStr="Yu et al., 2012" endWordPosition="456" position="3101" startWordPosition="453"> ability to handle sparsity, and to infer useful abstract representations automatically. At the same time, we address the challenge of learning the large set of neural network parameters. In particular, • We develop new Neural Network Features to model non-local translation phenomena related to word reordering. Large fullylexicalized contexts are used to model these phenomena effectively, making the use of neural networks essential. All of the features are useful individually, and their combination results in significant improvements (Section 2). • We use a Tensor Neural Network Architecture (Yu et al., 2012) to automatically learn complex pairwise interactions between the network nodes. The introduction of the tensor hidden layer results in more powerful features with lower model perplexity and significantly improved MT performance for all of neural network features (Section 3). • We apply Multitask Learning (MTL) (Caruana, 1997) to jointly train related neural network features by sharing parameters. This allows parameters learned for one feature to benefit the learning of the other features. This results in better trained models and achieves additional MT improvements (Section 4). We apply the r</context>
<context citStr="Yu et al., 2012" endWordPosition="2362" position="14578" startWordPosition="2359">o collocation of context words which are useful in many natural language processing tasks. In conventional feedforward neural networks, the output of hidden layer l is produced by multiplying the output vector from the previous layer with a weight matrix (Wl) and then applying the activation function σ to the product. Tensor Neural Networks generalize this formulation by using a tensor Ul of order 3 for the weights. The output of node k in layer l is computed as follows: ) hl[k] = σ (hl−1 · Ul[k] · hT l−1 where Ul[k], the k-th slice of Ul, is a square matrix. In our implementation, we follow (Yu et al., 2012; Hutchinson et al., 2013) and use a low-rank approximation of Ul[k] = Ql[k] · Rl[k]T, where Ql[k], Rl[k] ∈ Rn×r. The output of node k becomes: hl [k] = σ (hl−1 · Ql [k] · Rl [k]T · hi— hi 1 ) In In our experiments, we choose r = 1, and also apply the non-linear activation function σ distributively. We arrive at the following three equations for computing the hidden layer outputs (0 &lt; l &lt; L): vl = σ (hl−1 · Ql) v0l = σ (hl−1 · Rl) hl = vl ⊗ v0l where hl−1 is double-projected to vl and v0l, and the two projections are merged using the Hadamard element-wise product operator ⊗. This formulation a</context>
<context citStr="Yu et al., 2012" endWordPosition="5569" position="33266" startWordPosition="5566">ur advantage. More specifically, due to the absence of a feedback loop, the feedforward architecture allows us to treat individual decisions independently, which makes parallelization of the training easy and the querying the network at decoding time straightforward. The use of tensors in the hidden layers strengthens the neural network model, allowing us to model more complex feature interactions like collocation, which has been long recognized as important information for many NLP tasks (e.g. word sense disambiguation (Lee and Ng, 2002)). The tensor formulation we use is similar to that of (Yu et al., 2012; Hutchinson et al., 2013). Tensor Neural Networks have a wide application in other field, but have only been recently applied in NLP (Socher et al., 2013; Pei et al., 2014). To our knowledge, our work is the first to use tensor networks in SMT. Our approach to multitask learning is related to work that is often labeled joint training or transfer learning. To name a few of these works, Finkel and Manning (2009) successfully train name entity recognizers and syntactic parsers jointly, and Singh et al. (2013) train models for coreference resolution, named entity recognition and relation extracti</context>
</contexts>
<marker>Yu, Deng, Seide, 2012</marker>
<rawString>Dong Yu, Li Deng, and Frank Seide. 2012. Large vocabulary speech recognition using deep tensor neural networks. In INTERSPEECH. ISCA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>