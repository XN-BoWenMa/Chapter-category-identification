<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant confidence="0.002785" no="0">
<title confidence="0.998133">
Attention Shifting for Parsing Speech ∗
</title>
<author confidence="0.993482">
Keith Hall
</author>
<affiliation confidence="0.985598">
Department of Computer Science
Brown University
</affiliation>
<address confidence="0.866921">
Providence, RI 02912
</address>
<email confidence="0.998024">
kh@cs.brown.edu
</email>
<author confidence="0.992841">
Mark Johnson
</author>
<affiliation confidence="0.982576">
Department of Cognitive and Linguistic Science
Brown University
</affiliation>
<address confidence="0.858061">
Providence, RI 02912
</address>
<email confidence="0.970038">
Mark Johnson@Brown.edu
</email>
<sectionHeader confidence="0.996046" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999928846153846">We present a technique that improves the efficiency of word-lattice parsing as used in speech recognition language modeling. Our technique applies a probabilistic parser iteratively where on each iteration it focuses on a different subset of the wordlattice. The parser’s attention is shifted towards word-lattice subsets for which there are few or no syntactic analyses posited. This attention-shifting technique provides a six-times increase in speed (measured as the number of parser analyses evaluated) while performing equivalently when used as the first-stage of a multi-stage parsing-based language model.</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999652222222222">Success in language modeling has been dominated by the linear n-gram for the past few decades. A number of syntactic language models have proven to be competitive with the n-gram and better than the most popular n-gram, the trigram (Roark, 2001; Xu et al., 2002; Charniak, 2001; Hall and Johnson, 2003). Language modeling for speech could well be the first real problem for which syntactic techniques are useful.</bodyText>
<figure confidence="0.527805">
VP:ate
VB NP IN NP:plate IN NP:fork
John ate the pizza on a plate with a fork .
</figure>
<figureCaption confidence="0.9896215">
Figure 1: An incomplete parse tree with head-word an-
notations.
</figureCaption>
<bodyText confidence="0.995197729166667">One reason that we expect syntactic models to perform well is that they are capable of modeling long-distance dependencies that simple n-gram ∗ This research was supported in part by NSF grants 9870676 and 0085940. models cannot. For example, the model presented by Chelba and Jelinek (Chelba and Jelinek, 1998; Xu et al., 2002) uses syntactic structure to identify lexical items in the left-context which are then modeled as an n-gram process. The model presented by Charniak (Charniak, 2001) identifies both syntactic structural and lexical dependencies that aid in language modeling. While there are n-gram models that attempt to extend the left-context window through the use of caching and skip models (Goodman, 2001), we believe that linguistically motivated models, such as these lexical-syntactic models, are more robust. Figure 1 presents a simple example to illustrate the nature of long-distance dependencies. Using a syntactic model such as the the Structured Language Model (Chelba and Jelinek, 1998), we predict the word fork given the context {ate, with} where a trigram model uses the context {with, a}. Consider the problem of disambiguating between ... plate with a fork and ... plate with effort. The syntactic model captures the semantic relationship between the words ate and fork. The syntactic structure allows us to find lexical contexts for which there is some semantic relationship (e.g., predicateargument). Unfortunately, syntactic language modeling techniques have proven to be extremely expensive in terms of computational effort. Many employ the use of string parsers; in order to utilize such techniques for language modeling one must preselect a set of strings from the word-lattice and parse each of them separately, an inherently inefficient procedure. Of the techniques that can process word-lattices directly, it takes significant computation to achieve the same levels of accuracy as the n–best reranking method. This computational cost is the result of increasing the search space evaluated with the syntactic model (parser); the larger space resulting from combining the search for syntactic structure with the search for paths in the word-lattice. In this paper we propose a variation of a probabilistic word-lattice parsing technique that increases efficiency while incurring no loss of language modeling performance (measured as Word Error Rate – WER).</bodyText>
<figure confidence="0.60629">
PP:with
PP:on
</figure>
<figureCaption confidence="0.911238">
Figure 2: A partial word-lattice from the NIST HUB-1 dataset.
</figureCaption>
<figure confidence="0.99747134375">
outline/0
of/115.4
to/0
13
it/51.59
9
7
outline/2.573
strategy/0
a/71.30
the/115.3 strategy/0 11
&lt;/s&gt;/0
12/0
outlines/7.140
yesterday/0
0 1
and/4.004
in/14.73 14
3 tuesday/0 to/0 5
2
tuesday/0
4
two/8.769 6
to/0.000
outlaw/83.57
outlines/10.71
8
outlined/8.027
outline/0
in/0
10
outlined/12.58
</figure>
<bodyText confidence="0.997700086956522">In (Hall and Johnson, 2003) we presented a modular lattice parsing process that operates in two stages. The first stage is a PCFG word-lattice parser that generates a set of candidate parses over strings in a word-lattice, while the second stage rescores these candidate edges using a lexicalized syntactic language model (Charniak, 2001). Under this paradigm, the first stage is not only responsible for selecting candidate parses, but also for selecting paths in the word-lattice. Due to computational and memory requirements of the lexicalized model, the second stage parser is capable of rescoring only a small subset of all parser analyses. For this reason, the PCFG prunes the set of parser analyses, thereby indirectly pruning paths in the word lattice. We propose adding a meta-process to the firststage that effectively shifts the selection of wordlattice paths to the second stage (where lexical information is available). We achieve this by ensuring that for each path in the word-lattice the first-stage parser posits at least one parse.</bodyText>
<sectionHeader confidence="0.499911" genericHeader="method">
2 Parsing speech word-lattices
</sectionHeader>
<equation confidence="0.998884">
P(A,W) = P(A|W)P(W) (1)
</equation>
<bodyText confidence="0.9998751">The noisy channel model for speech is presented in Equation 1, where A represents the acoustic data extracted from a speech signal, and W represents a word string. The acoustic model P(A|W) assigns probability mass to the acoustic data given a word string and the language model P(W) defines a distribution over word strings. Typically the acoustic model is broken into a series of distributions conditioned on individual words (though these are based on false independence assumptions).</bodyText>
<equation confidence="0.983509">
n
P(A|w1 ... wi ... wn) = P(A|wi) (2)
i=1
</equation>
<bodyText confidence="0.999090769230769">The result of the acoustic modeling process is a set of string hypotheses; each word of each hypothesis is assigned a probability by the acoustic model. Word-lattices are a compact representation of output of the acoustic recognizer; an example is presented in Figure 2. The word-lattice is a weighted directed acyclic graph where a path in the graph corresponds to a string predicted by the acoustic recognizer. The (sum) product of the (log) weights on the graph (the acoustic probabilities) is the probability of the acoustic data given the string. Typically we want to know the most likely string given the acoustic data.</bodyText>
<equation confidence="0.626301666666667">
arg max P(W |A) (3)
= arg max P (A, W )
= arg max P(A|W)P(W)
</equation>
<bodyText confidence="0.999943222222222">In Equation 3 we use Bayes’ rule to find the optimal string given P(A|W), the acoustic model, and P(W), the language model. Although the language model can be used to rescore1 the word-lattice, it is typically used to select a single hypothesis. We focus our attention in this paper to syntactic language modeling techniques that perform complete parsing, meaning that parse trees are built upon the strings in the word-lattice.</bodyText>
<subsectionHeader confidence="0.990582">
2.1 n–best list reranking
</subsectionHeader>
<bodyText confidence="0.964084642857143">Much effort has been put forth in developing efficient probabilistic models for parsing strings (Caraballo and Charniak, 1998; Goldwater et al., 1998; Blaheta and Charniak, 1999; Charniak, 2000; Charniak, 2001); an obvious solution to parsing wordlattices is to use n–best list reranking. The n–best list reranking procedure, depicted in Figure 3, utilizes an external language model that selects a set of strings from the word-lattice. These strings are analyzed by the parser which computes a language model probability. This probability is combined 1To rescore a word-lattice, each arch is assigned a new score (probability) defined by a new model (in combination with the acoustic model).</bodyText>
<figure confidence="0.959337424242424">
duh/1.385
1 6
the/0
3
2
man/0 is/0
mans/1.385
man's/1.385
man/0
4
7 10
is/0
5
early/0
early/0
surly/0
surly/0.692
surely/0
8
early/0
9
n-best
list
extractor
w1, ..., wi, ..., wn1
w1, ..., wi, ..., wn2
w1, ..., wi, ..., wn3
w1, ..., wi, ..., wn4
w1, ..., wi, ..., wnm
...
Language
Model
o1, ..., oi, ..., on
</figure>
<figureCaption confidence="0.999447">
Figure 3: n–best list reranking
with the acoustic model probability to reranked the strings according to the joint probability P(A, W).</figureCaption>
<bodyText confidence="0.999972666666667">There are two significant disadvantages to this approach. First, we are limited by the performance of the language model used to select the n–best lists. Usually, the trigram model is used to select n paths through the lattice generating at most n unique strings. The maximum performance that can be achieved is limited by the performance of this extractor model. Second, of the strings that are analyzed by the parser, many will share common substrings. Much of the work performed by the parser is duplicated for these substrings. This second point is the primary motivation behind parsing word-lattices (Hall and Johnson, 2003).</bodyText>
<subsectionHeader confidence="0.983618">
2.2 Multi-stage parsing
</subsectionHeader>
<figureCaption confidence="0.984663">
Figure 4: Coarse-to-fine lattice parsing.
</figureCaption>
<bodyText confidence="0.825311">In Figure 4 we present the general overview of a multi-stage parsing technique (Goodman, 1997; Charniak, 2000; Charniak, 2001). This process</bodyText>
<listItem confidence="0.99988775">1. Parse word-lattice with PCFG parser 2. Overparse, generating additional candidates 3. Compute inside-outside probabilities 4. Prune candidates with probability threshold</listItem>
<tableCaption confidence="0.998637">
Table 1: First stage word-lattice parser
is know as coarse-to-fine modeling, where coarse models are more efficient but less accurate than fine models, which are robust but computationally expensive.</tableCaption>
<bodyText confidence="0.999894466666667">In this particular parsing model a PCFG best-first parser (Bobrow, 1990; Caraballo and Charniak, 1998) is used to search the unconstrained space of parses Π over a string. This first stage performs overparsing which effectively allows it to generate a set of high probability candidate parses π'. These parses are then rescored using a lexicalized syntactic model (Charniak, 2001). Although the coarse-to-fine model may include any number of intermediary stages, in this paper we consider this two-stage model. There is no guarantee that parses favored by the second stage will be generated by the first stage. In other words, because the first stage model prunes the space of parses from which the second stage rescores, the first stage model may remove solutions that the second stage would have assigned a high probability. In (Hall and Johnson, 2003), we extended the multi-stage parsing model to work on word-lattices. The first-stage parser, Table 1, is responsible for positing a set of candidate parses over the wordlattice. Were we to run the parser to completion it would generate all parses for all strings described by the word-lattice. As with string parsing, we stop the first stage parser early, generating a subset of all parses. Only the strings covered by complete parses are passed on to the second stage parser.</bodyText>
<figure confidence="0.729357">
Π
PCFG Parser
π' ⊂ Π
Lexicalized
Parser
</figure>
<bodyText confidence="0.998756761904762">This indirectly prunes the word-lattice of all word-arcs that were not covered by complete parses in the first stage. We use a first stage PCFG parser that performs a best-first search over the space of parses, which means that it depends on a heuristic “figure-ofmerit” (FOM) (Caraballo and Charniak, 1998). A good FOM attempts to model the true probability of a chart edge2 P(Nij,k). Generally, this probability is impossible to compute during the parsing process as it requires knowing both the inside and outside probabilities (Charniak, 1993; Manning and Sch¨utze, 1999). The FOM we describe is an approximation to the edge probability and is computed using an estimate of the inside probability times an approximation to the outside probability 3. The inside probability β(Nij,k) can be computed incrementally during bottom-up parsing. The normalized acoustic probabilities from the acoustic recognizer are included in this calculation.</bodyText>
<equation confidence="0.987296666666667">
ˆα(Nij,k) (4)
�= fwd(Tqi,j)p(Ni|T q)p(Tr|Ni)bkwd(Trk,l)
i,l,q,r
</equation>
<bodyText confidence="0.999734133333333">The outside probability is approximated with a bitag model and the standard tag/category boundary model (Caraballo and Charniak, 1998; Hall and Johnson, 2003). Equation 4 presents the approximation to the outside probability. Part-of-speech tags Tq and Tr are the candidate tags to the left and right of the constituent Nij,k. The fwd() and bkwd() functions are the HMM forward and backward probabilities calculated over a lattice containing the part-of-speech tag, the word, and the acoustic scores from the word-lattice to the left and right of the constituent, respectively. p(Ni|Tq) and p(Tr|Ni) are the boundary statistics which are estimated from training data (details of this model can be found in (Hall and Johnson, 2003)).</bodyText>
<equation confidence="0.997094">
FOM(Nij,k) = ˆα(Nij,k)β(Nij,k)ηC(j, k) (5)
</equation>
<bodyText confidence="0.9997702">The best-first search employed by the first stage parser uses the FOM defined in Equation 5, where η is a normalization factor based on path length C(j, k). The normalization factor prevents small constituents from consistently being assigned a higher probability than larger constituents (Goldwater et al., 1998).</bodyText>
<footnote confidence="0.9954866">
2A chart edge Nzj,k indicates a grammar category Nz can
be constructed from nodes j to k.
3An alternative to the inside and outside probabilities are
the Viterbi inside and outside probabilities (Goldwater et al.,
1998; Hall and Johnson, 2003).
</footnote>
<bodyText confidence="0.99967175">Although this heuristic works well for directing the parser towards likely parses over a string, it is not an ideal model for pruning the word-lattice. First, the outside approximation of this FOM is based on a linear part-of-speech tag model (the bitag). Such a simple syntactic model is unlikely to provide realistic information when choosing a word-lattice path to consider. Second, the model is prone to favoring subsets of the word-lattice causing it to posit additional parse trees for the favored sublattice rather than exploring the remainder of the word-lattice. This second point is the primary motivation for the attention shifting technique presented in the next section.</bodyText>
<sectionHeader confidence="0.997514" genericHeader="method">
3 Attention shifting4
</sectionHeader>
<bodyText confidence="0.996772333333333">We explore a modification to the multi-stage parsing algorithm that ensures the first stage parser posits at least one parse for each path in the word-lattice. The idea behind this is to intermittently shift the attention of the parser to unexplored parts of the word lattice.</bodyText>
<figure confidence="0.9979971">
PCFG
Word-lattice
Parser
Clear Agenda/
Add Edges for
Unused Words
yes
Continue
Multi-stage
Parsing
</figure>
<figureCaption confidence="0.8279915">
Figure 5: Attention shifting parser.
Figure 5 depicts the attention shifting first stage
parsing procedure. A used edge is a parse edge that
has non-zero outside probability. By definition of
</figureCaption>
<footnote confidence="0.829208666666667">
4The notion of attention shifting is motivated by the work on
parser FOM compensation presented in (Blaheta and Charniak,
1999).
</footnote>
<figure confidence="0.8891365">
Identify
Used Edges
Is Agenda no
Empty?
</figure>
<bodyText confidence="0.998630444444444">the outside probability, used edges are constituents that are part of a complete parse; a parse is complete if there is a root category label (e.g., S for sentence) that spans the entire word-lattice. In order to identify used edges, we compute the outside probabilities for each parse edge (efficiently computing the outside probability of an edge requires that the inside probabilities have already been computed). In the third step of this algorithm we clear the agenda, removing all partial analyses evaluated by the parser. This forces the parser to abandon analyses of parts of the word-lattice for which complete parses exist. Following this, the agenda is populated with edges corresponding to the unused words, priming the parser to consider these words. To ensure the parser builds upon at least one of these unused edges, we further modify the parsing algorithm:</bodyText>
<listItem confidence="0.9938395">• Only unused edges are added to the agenda. • When building parses from the bottom up, a parse is considered complete if it connects to a used edge.</listItem>
<bodyText confidence="0.999984166666667">These modifications ensure that the parser focuses on edges built upon the unused words. The second modification ensures the parser is able to determine when it has connected an unused word with a previously completed parse. The application of these constraints directs the attention of the parser towards new edges that contribute to parse analyses covering unused words. We are guaranteed that each iteration of the attention shifting algorithm adds a parse for at least one unused word, meaning that it will take at most |A |iterations to cover the entire lattice, where A is the set of word-lattice arcs. This guarantee is trivially provided through the constraints just described. The attention-shifting parser continues until there are no unused words remaining and each parsing iteration runs until it has found a complete parse using at least one of the unused words. As with multi-stage parsing, an adjustable parameter determines how much overparsing to perform on the initial parse. In the attention shifting algorithm an additional parameter specifies the amount of overparsing for each iteration after the first. The new parameter allows for independent control of the attention shifting iterations. After the attention shifting parser populates a parse chart with parses covering all paths in the lattice, the multi-stage parsing algorithm performs additional pruning based on the probability of the parse edges (the product of the inside and outside probabilities). This is necessary in order to constrain the size of the hypothesis set passed on to the second stage parsing model. The Charniak lexicalized syntactic language model effectively splits the number of parse states (an edges in a PCFG parser) by the number of unique contexts in which the state is found. These contexts include syntactic structure such as parent and grandparent category labels as well as lexical items such as the head of the parent or the head of a sibling constituent (Charniak, 2001). State splitting on this level causes the memory requirement of the lexicalized parser to grow rapidly. Ideally, we would pass all edges on to the second stage, but due to memory limitations, pruning is necessary. It is likely that edges recently discovered by the attention shifting procedure are pruned. However, the true PCFG probability model is used to prune these edges rather than the approximation used in the FOM. We believe that by considering parses which have a relatively high probability according to the combined PCFG and acoustic models that we will include most of the analyses for which the lexicalized parser assigns a high probability.</bodyText>
<sectionHeader confidence="0.99924" genericHeader="evaluation and result">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999720166666667">The purpose of attention shifting is to reduce the amount of work exerted by the first stage PCFG parser while maintaining the same quality of language modeling (in the multi-stage system). We have performed a set of experiments on the NIST ’93 HUB–1 word-lattices. The HUB–1 is a collection of 213 word-lattices resulting from an acoustic recognizer’s analysis of speech utterances. Professional readers reading Wall Street Journal articles generated the utterances. The first stage parser is a best-first PCFG parser trained on sections 2 through 22, and 24 of the Penn WSJ treebank (Marcus et al., 1993). Prior to training, the treebank is transformed into speech-like text, removing punctuation and expanding numerals, etc.5 Overparsing is performed using an edge pop6 multiplicative factor. The parser records the number of edge pops required to reach the first complete parse. The parser continues to parse a until multiple of the number of edge pops required for the first parse are popped off the agenda. The second stage parser used is a modified version of the Charniak language modeling parser described in (Charniak, 2001). We trained this parser on the BLLIP99 corpus (Charniak et al., 1999); a corpus of 30million words automatically parsed using the Charniak parser (Charniak, 2000).</bodyText>
<footnote confidence="0.99063">
5Brian Roark of AT&amp;T provided a tool to perform the
speech normalization.
6An edge pop is the process of the parser removing an edge
from the agenda and placing it in the parse chart.
</footnote>
<bodyText confidence="0.999924894736842">In order to compare the work done by the n–best reranking technique to the word-lattice parser, we generated a set of n–best lattices.50–best lists were extracted using the Chelba A* decoder7. A 50– best lattice is a sublattice of the acoustic lattice that generates only the strings found in the 50–best list. Additionally, we provide the results for parsing the full acoustic lattices (although these work measurements should not be compared to those of n–best reranking). We report the amount of work, shown as the cumulative # edge pops, the oracle WER for the word-lattices after first stage pruning, and the WER of the complete multi-stage parser. In all of the word-lattice parsing experiments, we pruned the set of posited hypothesis so that no more than 30,000 local-trees are generated8. We chose this threshold due to the memory requirements of the second stage parser. Performing pruning at the end of the first stage prevents the attention shifting parser from reaching the minimum oracle WER (most notable in the full acoustic word-lattice experiments). While the attention-shifting algorithm ensures all word-lattice arcs are included in complete parses, forward-backward pruning, as used here, will eliminate some of these parses, indirectly eliminating some of the word-lattice arcs. To illustrate the need for pruning, we computed the number of states used by the Charniak lexicalized syntactic language model for 30,000 local trees. An average of 215 lexicalized states were generated for each of the 30,000 local trees. This means that the lexicalized language model, on average, computes probabilities for over 6.5 million states when provided with 30,000 local trees.</bodyText>
<table confidence="0.99936325">
Model # edge pops O-WER WER
n–best (Charniak) 2.5 million 7.75 11.8
100x LatParse 3.4 million 8.18 12.0
10x AttShift 564,895 7.78 11.9
</table>
<tableCaption confidence="0.996677">
Table 2: Results for n–best lists and n–best lattices.
</tableCaption>
<bodyText confidence="0.995884166666667">Table 2 shows the results for n–best list reranking and word-lattice parsing of n–best lattices. We recreated the results of the Charniak language model parser used for reranking in order to measure the amount of work required. We ran the first stage parser with 4-times overparsing for each string in the n–best list.</bodyText>
<footnote confidence="0.9981495">
7The n–best lists were provided by Brian Roark (Roark,
2001)
8A local-tree is an explicit expansion of an edge and its chil-
dren. An example local tree is NP3,8 → DT3,4 NN4,8.
</footnote>
<bodyText confidence="0.999895">The LatParse result represents running the word-lattice parser on the n–best lattices performing 100–times overparsing in the first stage. The AttShift model is the attention shifting parser described in this paper. We used 10–times overparsing for both the initial parse and each of the attention shifting iterations. When run on the n–best lattice, this model achieves a comparable WER, while reducing the amount of parser work sixfold (as compared to the regular word-lattice parser).</bodyText>
<table confidence="0.9992685">
Model # edge pops O-WER WER
acoustic lats N/A 3.26 N/A
100x LatParse 3.4 million 5.45 13.1
10x AttShift 1.6 million 4.17 13.1
</table>
<tableCaption confidence="0.999202">
Table 3: Results for acoustic lattices.
</tableCaption>
<bodyText confidence="0.999975565217391">In Table 3 we present the results of the wordlattice parser and the attention shifting parser when run on full acoustic lattices. While the oracle WER is reduced, we are considering almost half as many edges as the standard word-lattice parser. The increased size of the acoustic lattices suggests that it may not be computationally efficient to consider the entire lattice and that an additional pruning phase is necessary. The most significant constraint of this multi-stage lattice parsing technique is that the second stage process has a large memory requirement. While the attention shifting technique does allow the parser to propose constituents for every path in the lattice, we prune some of these constituents prior to performing analysis by the second stage parser. Currently, pruning is accomplished using the PCFG model. One solution is to incorporate an intermediate pruning stage (e.g., lexicalized PCFG) between the PCFG parser and the full lexicalized model. Doing so will relax the requirement for aggressive PCFG pruning and allows for a lexicalized model to influence the selection of word-lattice paths.</bodyText>
<sectionHeader confidence="0.999301" genericHeader="conclusion">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99989584">We presented a parsing technique that shifts the attention of a word-lattice parser in order to ensure syntactic analyses for all lattice paths. Attention shifting can be thought of as a meta-process around the first stage of a multi-stage word-lattice parser. We show that this technique reduces the amount of work exerted by the first stage PCFG parser while maintaining comparable language modeling performance. Attention shifting is a simple technique that attempts to make word-lattice parsing more efficient. As suggested by the results for the acoustic lattice experiments, this technique alone is not sufficient.<FR>Solutions to improve these results include modify-
ing the first-stage grammar by annotating the cat-
egory labels with local syntactic features as sug-
gested in (Johnson, 1998) and (Klein and Manning,
2003) as well as incorporating some level of lexical-
ization. Improving the quality of the parses selected
by the first stage should reduce the need for gen-
erating such a large number of candidates prior to
pruning, improving efficiency as well as overall ac-
curacy. We believe that attention shifting, or some
variety of this technique, will be an integral part of
efficient solutions for word-lattice parsing.</FR>
</bodyText>
<sectionHeader confidence="0.999407" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993418095890411">
Don Blaheta and Eugene Charniak. 1999. Au-
tomatic compensation for parser figure-of-merit
flaws. In Proceedings of the 37th annual meeting
of the Association for Computational Linguistics,
pages 513–518.
Robert J. Bobrow. 1990. Statistical agenda pars-
ing. In DARPA Speech and Language Workshop,
pages 222–224.
Sharon Caraballo and Eugene Charniak. 1998.
New figures of merit for best-first probabilis-
tic chart parsing. Computational Linguistics,
24(2):275–298, June.
Eugene Charniak, Don Blaheta, Niyu Ge, Keith
Hall, John Hale, and Mark Johnson. 1999.
BLLIP 1987–89 wsj corpus release 1. LDC cor-
pus LDC2000T43.
Eugene Charniak. 1993. Statistical Language
Learning. MIT Press.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 2000 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics., ACL,
New Brunswick, NJ.
Eugene Charniak. 2001. Immediate-head parsing
for language models. In Proceedings of the 39th
Annual Meeting of the Association for Computa-
tional Linguistics.
Ciprian Chelba and Frederick Jelinek. 1998. A
study on richer syntactic dependencies for struc-
tured language modeling. In Proceedings of the
36th Annual Meeting of the Association for Com-
putational Linguistics and 17th International
Conference on Computational Linguistics, pages
225–231.
Sharon Goldwater, Eugene Charniak, and Mark
Johnson. 1998. Best-first edge-based chart pars-
ing. In 6th Annual Workshop for Very Large Cor-
pora, pages 127–133.
Joshua Goodman. 1997. Global thresholding and
multiple-pass parsing. In Proceedings of the Sec-
ond Conference on Empirical Methods in Natural
Language Processing, pages 11–25.
Joshua Goodman. 2001. A bit of progress in lan-
guage modeling, extendend version. In Microsoft
Research Technical Report MSR-TR-2001-72.
Keith Hall and Mark Johnson. 2003. Language
modeling using efficient best-first bottom-up
parsing. In Proceedings of IEEE Automated
Speech Recognition and Understanding Work-
shop.
Mark Johnson. 1998. PCFG models of linguistic
tree representations. Computational Linguistics,
24:617–636.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of
the 41st Meeting of the Association for Computa-
tional Linguistics (ACL-03).
Christopher D. Manning and Hinrich Sch¨utze.
1999. Foundations of statistical natural lan-
guage processing. MIT Press.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19:313–330.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguis-
tics, 27(3):249–276.
Peng Xu, Ciprian Chelba, and Frederick Jelinek.
2002. A study on richer syntactic dependencies
for structured language modeling. In Proceed-
ings of the 40th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 191–
198.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant confidence="0.907293" no="0">
<title confidence="0.999697">Shifting for Parsing Speech</title>
<author confidence="0.9584">Keith Hall</author>
<affiliation confidence="0.999949">Department of Computer Science Brown University</affiliation>
<address confidence="0.999108">Providence, RI 02912</address>
<email confidence="0.999802">kh@cs.brown.edu</email>
<author confidence="0.999997">Mark Johnson</author>
<affiliation confidence="0.9999735">Department of Cognitive and Linguistic Science Brown University</affiliation>
<address confidence="0.999357">Providence, RI 02912</address>
<author confidence="0.979748">Mark JohnsonBrown edu</author>
<abstract confidence="0.997712571428571">We present a technique that improves the efficiency of word-lattice parsing as used in speech recognition language modeling. Our technique applies a probabilistic parser iteratively where on each iteration it focuses on a different subset of the wordlattice. The parser’s attention is shifted towards word-lattice subsets for which there are few or no syntactic analyses posited. This attention-shifting technique provides a six-times increase in speed (measured as the number of parser analyses evaluated) while performing equivalently when used as the first-stage of a multi-stage parsing-based language model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Don Blaheta</author>
<author>Eugene Charniak</author>
</authors>
<title>Automatic compensation for parser figure-of-merit flaws.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>513--518</pages>
<contexts>
<context citStr="Blaheta and Charniak, 1999" endWordPosition="1148" position="7170" startWordPosition="1145">ation 3 we use Bayes’ rule to find the optimal string given P(A|W), the acoustic model, and P(W), the language model. Although the language model can be used to rescore1 the word-lattice, it is typically used to select a single hypothesis. We focus our attention in this paper to syntactic language modeling techniques that perform complete parsing, meaning that parse trees are built upon the strings in the word-lattice. 2.1 n–best list reranking Much effort has been put forth in developing efficient probabilistic models for parsing strings (Caraballo and Charniak, 1998; Goldwater et al., 1998; Blaheta and Charniak, 1999; Charniak, 2000; Charniak, 2001); an obvious solution to parsing wordlattices is to use n–best list reranking. The n–best list reranking procedure, depicted in Figure 3, utilizes an external language model that selects a set of strings from the word-lattice. These strings are analyzed by the parser which computes a language model probability. This probability is combined 1To rescore a word-lattice, each arch is assigned a new score (probability) defined by a new model (in combination with the acoustic model). duh/1.385 1 6 the/0 3 2 man/0 is/0 mans/1.385 man's/1.385 man/0 4 7 10 is/0 5 early/</context>
<context citStr="Blaheta and Charniak, 1999" endWordPosition="2317" position="14456" startWordPosition="2314">ensures the first stage parser posits at least one parse for each path in the word-lattice. The idea behind this is to intermittently shift the attention of the parser to unexplored parts of the word lattice. PCFG Word-lattice Parser Clear Agenda/ Add Edges for Unused Words yes Continue Multi-stage Parsing Figure 5: Attention shifting parser. Figure 5 depicts the attention shifting first stage parsing procedure. A used edge is a parse edge that has non-zero outside probability. By definition of 4The notion of attention shifting is motivated by the work on parser FOM compensation presented in (Blaheta and Charniak, 1999). Identify Used Edges Is Agenda no Empty? the outside probability, used edges are constituents that are part of a complete parse; a parse is complete if there is a root category label (e.g., S for sentence) that spans the entire word-lattice. In order to identify used edges, we compute the outside probabilities for each parse edge (efficiently computing the outside probability of an edge requires that the inside probabilities have already been computed). In the third step of this algorithm we clear the agenda, removing all partial analyses evaluated by the parser. This forces the parser to aba</context>
</contexts>
<marker>Blaheta, Charniak, 1999</marker>
<rawString>Don Blaheta and Eugene Charniak. 1999. Automatic compensation for parser figure-of-merit flaws. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics, pages 513–518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert J Bobrow</author>
</authors>
<title>Statistical agenda parsing.</title>
<date>1990</date>
<booktitle>In DARPA Speech and Language Workshop,</booktitle>
<pages>222--224</pages>
<contexts>
<context citStr="Bobrow, 1990" endWordPosition="1508" position="9411" startWordPosition="1507">e-to-fine lattice parsing. In Figure 4 we present the general overview of a multi-stage parsing technique (Goodman, 1997; Charniak, 2000; Charniak, 2001). This process 1. Parse word-lattice with PCFG parser 2. Overparse, generating additional candidates 3. Compute inside-outside probabilities 4. Prune candidates with probability threshold Table 1: First stage word-lattice parser is know as coarse-to-fine modeling, where coarse models are more efficient but less accurate than fine models, which are robust but computationally expensive. In this particular parsing model a PCFG best-first parser (Bobrow, 1990; Caraballo and Charniak, 1998) is used to search the unconstrained space of parses Π over a string. This first stage performs overparsing which effectively allows it to generate a set of high probability candidate parses π'. These parses are then rescored using a lexicalized syntactic model (Charniak, 2001). Although the coarse-to-fine model may include any number of intermediary stages, in this paper we consider this two-stage model. There is no guarantee that parses favored by the second stage will be generated by the first stage. In other words, because the first stage model prunes the spa</context>
</contexts>
<marker>Bobrow, 1990</marker>
<rawString>Robert J. Bobrow. 1990. Statistical agenda parsing. In DARPA Speech and Language Workshop, pages 222–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Caraballo</author>
<author>Eugene Charniak</author>
</authors>
<title>New figures of merit for best-first probabilistic chart parsing.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context citStr="Caraballo and Charniak, 1998" endWordPosition="1140" position="7118" startWordPosition="1136">A) (3) = arg max P (A, W ) = arg max P(A|W)P(W) In Equation 3 we use Bayes’ rule to find the optimal string given P(A|W), the acoustic model, and P(W), the language model. Although the language model can be used to rescore1 the word-lattice, it is typically used to select a single hypothesis. We focus our attention in this paper to syntactic language modeling techniques that perform complete parsing, meaning that parse trees are built upon the strings in the word-lattice. 2.1 n–best list reranking Much effort has been put forth in developing efficient probabilistic models for parsing strings (Caraballo and Charniak, 1998; Goldwater et al., 1998; Blaheta and Charniak, 1999; Charniak, 2000; Charniak, 2001); an obvious solution to parsing wordlattices is to use n–best list reranking. The n–best list reranking procedure, depicted in Figure 3, utilizes an external language model that selects a set of strings from the word-lattice. These strings are analyzed by the parser which computes a language model probability. This probability is combined 1To rescore a word-lattice, each arch is assigned a new score (probability) defined by a new model (in combination with the acoustic model). duh/1.385 1 6 the/0 3 2 man/0 is</context>
<context citStr="Caraballo and Charniak, 1998" endWordPosition="1512" position="9442" startWordPosition="1509">ice parsing. In Figure 4 we present the general overview of a multi-stage parsing technique (Goodman, 1997; Charniak, 2000; Charniak, 2001). This process 1. Parse word-lattice with PCFG parser 2. Overparse, generating additional candidates 3. Compute inside-outside probabilities 4. Prune candidates with probability threshold Table 1: First stage word-lattice parser is know as coarse-to-fine modeling, where coarse models are more efficient but less accurate than fine models, which are robust but computationally expensive. In this particular parsing model a PCFG best-first parser (Bobrow, 1990; Caraballo and Charniak, 1998) is used to search the unconstrained space of parses Π over a string. This first stage performs overparsing which effectively allows it to generate a set of high probability candidate parses π'. These parses are then rescored using a lexicalized syntactic model (Charniak, 2001). Although the coarse-to-fine model may include any number of intermediary stages, in this paper we consider this two-stage model. There is no guarantee that parses favored by the second stage will be generated by the first stage. In other words, because the first stage model prunes the space of parses from which the sec</context>
<context citStr="Caraballo and Charniak, 1998" endWordPosition="1778" position="11019" startWordPosition="1775">e parser to completion it would generate all parses for all strings described by the word-lattice. As with string parsing, we stop the first stage parser early, generating a subset of all parses. Only the strings covered by complete Π PCFG Parser π' ⊂ Π Lexicalized Parser parses are passed on to the second stage parser. This indirectly prunes the word-lattice of all word-arcs that were not covered by complete parses in the first stage. We use a first stage PCFG parser that performs a best-first search over the space of parses, which means that it depends on a heuristic “figure-ofmerit” (FOM) (Caraballo and Charniak, 1998). A good FOM attempts to model the true probability of a chart edge2 P(Nij,k). Generally, this probability is impossible to compute during the parsing process as it requires knowing both the inside and outside probabilities (Charniak, 1993; Manning and Sch¨utze, 1999). The FOM we describe is an approximation to the edge probability and is computed using an estimate of the inside probability times an approximation to the outside probability 3. The inside probability β(Nij,k) can be computed incrementally during bottom-up parsing. The normalized acoustic probabilities from the acoustic recognize</context>
</contexts>
<marker>Caraballo, Charniak, 1998</marker>
<rawString>Sharon Caraballo and Eugene Charniak. 1998. New figures of merit for best-first probabilistic chart parsing. Computational Linguistics, 24(2):275–298, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Don Blaheta</author>
<author>Niyu Ge</author>
<author>Keith Hall</author>
<author>John Hale</author>
<author>Mark Johnson</author>
</authors>
<date>1999</date>
<booktitle>BLLIP 1987–89 wsj corpus release 1. LDC corpus LDC2000T43.</booktitle>
<contexts>
<context citStr="Charniak et al., 1999" endWordPosition="3169" position="19563" startWordPosition="3166">e pop6 multiplicative factor. The parser records the number of edge pops required to reach the first complete parse. The parser continues to parse a until multiple of the number of edge pops required for the first parse are popped off the agenda. The second stage parser used is a modified version of the Charniak language modeling parser described in (Charniak, 2001). We trained this parser 5Brian Roark of AT&amp;T provided a tool to perform the speech normalization. 6An edge pop is the process of the parser removing an edge from the agenda and placing it in the parse chart. on the BLLIP99 corpus (Charniak et al., 1999); a corpus of 30million words automatically parsed using the Charniak parser (Charniak, 2000). In order to compare the work done by the n–best reranking technique to the word-lattice parser, we generated a set of n–best lattices. 50–best lists were extracted using the Chelba A* decoder7. A 50– best lattice is a sublattice of the acoustic lattice that generates only the strings found in the 50–best list. Additionally, we provide the results for parsing the full acoustic lattices (although these work measurements should not be compared to those of n–best reranking). We report the amount of work,</context>
</contexts>
<marker>Charniak, Blaheta, Ge, Hall, Hale, Johnson, 1999</marker>
<rawString>Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall, John Hale, and Mark Johnson. 1999. BLLIP 1987–89 wsj corpus release 1. LDC corpus LDC2000T43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical Language Learning.</title>
<date>1993</date>
<publisher>MIT Press.</publisher>
<contexts>
<context citStr="Charniak, 1993" endWordPosition="1816" position="11258" startWordPosition="1815">Lexicalized Parser parses are passed on to the second stage parser. This indirectly prunes the word-lattice of all word-arcs that were not covered by complete parses in the first stage. We use a first stage PCFG parser that performs a best-first search over the space of parses, which means that it depends on a heuristic “figure-ofmerit” (FOM) (Caraballo and Charniak, 1998). A good FOM attempts to model the true probability of a chart edge2 P(Nij,k). Generally, this probability is impossible to compute during the parsing process as it requires knowing both the inside and outside probabilities (Charniak, 1993; Manning and Sch¨utze, 1999). The FOM we describe is an approximation to the edge probability and is computed using an estimate of the inside probability times an approximation to the outside probability 3. The inside probability β(Nij,k) can be computed incrementally during bottom-up parsing. The normalized acoustic probabilities from the acoustic recognizer are included in this calculation. ˆα(Nij,k) (4) �= fwd(Tqi,j)p(Ni|T q)p(Tr|Ni)bkwd(Trk,l) i,l,q,r The outside probability is approximated with a bitag model and the standard tag/category boundary model (Caraballo and Charniak, 1998; Hall</context>
</contexts>
<marker>Charniak, 1993</marker>
<rawString>Eugene Charniak. 1993. Statistical Language Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 Conference of the North American Chapter of the Association for Computational Linguistics., ACL,</booktitle>
<location>New Brunswick, NJ.</location>
<contexts>
<context citStr="Charniak, 2000" endWordPosition="1150" position="7186" startWordPosition="1149">o find the optimal string given P(A|W), the acoustic model, and P(W), the language model. Although the language model can be used to rescore1 the word-lattice, it is typically used to select a single hypothesis. We focus our attention in this paper to syntactic language modeling techniques that perform complete parsing, meaning that parse trees are built upon the strings in the word-lattice. 2.1 n–best list reranking Much effort has been put forth in developing efficient probabilistic models for parsing strings (Caraballo and Charniak, 1998; Goldwater et al., 1998; Blaheta and Charniak, 1999; Charniak, 2000; Charniak, 2001); an obvious solution to parsing wordlattices is to use n–best list reranking. The n–best list reranking procedure, depicted in Figure 3, utilizes an external language model that selects a set of strings from the word-lattice. These strings are analyzed by the parser which computes a language model probability. This probability is combined 1To rescore a word-lattice, each arch is assigned a new score (probability) defined by a new model (in combination with the acoustic model). duh/1.385 1 6 the/0 3 2 man/0 is/0 mans/1.385 man's/1.385 man/0 4 7 10 is/0 5 early/0 early/0 surly/</context>
<context citStr="Charniak, 2000" endWordPosition="1442" position="8935" startWordPosition="1441">select n paths through the lattice generating at most n unique strings. The maximum performance that can be achieved is limited by the performance of this extractor model. Second, of the strings that are analyzed by the parser, many will share common substrings. Much of the work performed by the parser is duplicated for these substrings. This second point is the primary motivation behind parsing word-lattices (Hall and Johnson, 2003). 2.2 Multi-stage parsing Figure 4: Coarse-to-fine lattice parsing. In Figure 4 we present the general overview of a multi-stage parsing technique (Goodman, 1997; Charniak, 2000; Charniak, 2001). This process 1. Parse word-lattice with PCFG parser 2. Overparse, generating additional candidates 3. Compute inside-outside probabilities 4. Prune candidates with probability threshold Table 1: First stage word-lattice parser is know as coarse-to-fine modeling, where coarse models are more efficient but less accurate than fine models, which are robust but computationally expensive. In this particular parsing model a PCFG best-first parser (Bobrow, 1990; Caraballo and Charniak, 1998) is used to search the unconstrained space of parses Π over a string. This first stage perfor</context>
<context citStr="Charniak, 2000" endWordPosition="3183" position="19656" startWordPosition="3182"> complete parse. The parser continues to parse a until multiple of the number of edge pops required for the first parse are popped off the agenda. The second stage parser used is a modified version of the Charniak language modeling parser described in (Charniak, 2001). We trained this parser 5Brian Roark of AT&amp;T provided a tool to perform the speech normalization. 6An edge pop is the process of the parser removing an edge from the agenda and placing it in the parse chart. on the BLLIP99 corpus (Charniak et al., 1999); a corpus of 30million words automatically parsed using the Charniak parser (Charniak, 2000). In order to compare the work done by the n–best reranking technique to the word-lattice parser, we generated a set of n–best lattices. 50–best lists were extracted using the Chelba A* decoder7. A 50– best lattice is a sublattice of the acoustic lattice that generates only the strings found in the 50–best list. Additionally, we provide the results for parsing the full acoustic lattices (although these work measurements should not be compared to those of n–best reranking). We report the amount of work, shown as the cumulative # edge pops, the oracle WER for the word-lattices after first stage </context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proceedings of the 2000 Conference of the North American Chapter of the Association for Computational Linguistics., ACL, New Brunswick, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Immediate-head parsing for language models.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context citStr="Charniak, 2001" endWordPosition="174" position="1171" startWordPosition="173">d towards word-lattice subsets for which there are few or no syntactic analyses posited. This attention-shifting technique provides a six-times increase in speed (measured as the number of parser analyses evaluated) while performing equivalently when used as the first-stage of a multi-stage parsing-based language model. 1 Introduction Success in language modeling has been dominated by the linear n-gram for the past few decades. A number of syntactic language models have proven to be competitive with the n-gram and better than the most popular n-gram, the trigram (Roark, 2001; Xu et al., 2002; Charniak, 2001; Hall and Johnson, 2003). Language modeling for speech could well be the first real problem for which syntactic techniques are useful. VP:ate VB NP IN NP:plate IN NP:fork John ate the pizza on a plate with a fork . Figure 1: An incomplete parse tree with head-word annotations. One reason that we expect syntactic models to perform well is that they are capable of modeling long-distance dependencies that simple n-gram ∗ This research was supported in part by NSF grants 9870676 and 0085940. models cannot. For example, the model presented by Chelba and Jelinek (Chelba and Jelinek, 1998; Xu et al.</context>
<context citStr="Charniak, 2001" endWordPosition="708" position="4554" startWordPosition="707">0 yesterday/0 0 1 and/4.004 in/14.73 14 3 tuesday/0 to/0 5 2 tuesday/0 4 two/8.769 6 to/0.000 outlaw/83.57 outlines/10.71 8 outlined/8.027 outline/0 in/0 10 outlined/12.58 efficiency while incurring no loss of language modeling performance (measured as Word Error Rate – WER). In (Hall and Johnson, 2003) we presented a modular lattice parsing process that operates in two stages. The first stage is a PCFG word-lattice parser that generates a set of candidate parses over strings in a word-lattice, while the second stage rescores these candidate edges using a lexicalized syntactic language model (Charniak, 2001). Under this paradigm, the first stage is not only responsible for selecting candidate parses, but also for selecting paths in the word-lattice. Due to computational and memory requirements of the lexicalized model, the second stage parser is capable of rescoring only a small subset of all parser analyses. For this reason, the PCFG prunes the set of parser analyses, thereby indirectly pruning paths in the word lattice. We propose adding a meta-process to the firststage that effectively shifts the selection of wordlattice paths to the second stage (where lexical information is available). We ac</context>
<context citStr="Charniak, 2001" endWordPosition="1153" position="7203" startWordPosition="1151">al string given P(A|W), the acoustic model, and P(W), the language model. Although the language model can be used to rescore1 the word-lattice, it is typically used to select a single hypothesis. We focus our attention in this paper to syntactic language modeling techniques that perform complete parsing, meaning that parse trees are built upon the strings in the word-lattice. 2.1 n–best list reranking Much effort has been put forth in developing efficient probabilistic models for parsing strings (Caraballo and Charniak, 1998; Goldwater et al., 1998; Blaheta and Charniak, 1999; Charniak, 2000; Charniak, 2001); an obvious solution to parsing wordlattices is to use n–best list reranking. The n–best list reranking procedure, depicted in Figure 3, utilizes an external language model that selects a set of strings from the word-lattice. These strings are analyzed by the parser which computes a language model probability. This probability is combined 1To rescore a word-lattice, each arch is assigned a new score (probability) defined by a new model (in combination with the acoustic model). duh/1.385 1 6 the/0 3 2 man/0 is/0 mans/1.385 man's/1.385 man/0 4 7 10 is/0 5 early/0 early/0 surly/0 surly/0.692 sur</context>
<context citStr="Charniak, 2001" endWordPosition="1444" position="8952" startWordPosition="1443">hrough the lattice generating at most n unique strings. The maximum performance that can be achieved is limited by the performance of this extractor model. Second, of the strings that are analyzed by the parser, many will share common substrings. Much of the work performed by the parser is duplicated for these substrings. This second point is the primary motivation behind parsing word-lattices (Hall and Johnson, 2003). 2.2 Multi-stage parsing Figure 4: Coarse-to-fine lattice parsing. In Figure 4 we present the general overview of a multi-stage parsing technique (Goodman, 1997; Charniak, 2000; Charniak, 2001). This process 1. Parse word-lattice with PCFG parser 2. Overparse, generating additional candidates 3. Compute inside-outside probabilities 4. Prune candidates with probability threshold Table 1: First stage word-lattice parser is know as coarse-to-fine modeling, where coarse models are more efficient but less accurate than fine models, which are robust but computationally expensive. In this particular parsing model a PCFG best-first parser (Bobrow, 1990; Caraballo and Charniak, 1998) is used to search the unconstrained space of parses Π over a string. This first stage performs overparsing wh</context>
<context citStr="Charniak, 2001" endWordPosition="2824" position="17504" startWordPosition="2823">tional pruning based on the probability of the parse edges (the product of the inside and outside probabilities). This is necessary in order to constrain the size of the hypothesis set passed on to the second stage parsing model. The Charniak lexicalized syntactic language model effectively splits the number of parse states (an edges in a PCFG parser) by the number of unique contexts in which the state is found. These contexts include syntactic structure such as parent and grandparent category labels as well as lexical items such as the head of the parent or the head of a sibling constituent (Charniak, 2001). State splitting on this level causes the memory requirement of the lexicalized parser to grow rapidly. Ideally, we would pass all edges on to the second stage, but due to memory limitations, pruning is necessary. It is likely that edges recently discovered by the attention shifting procedure are pruned. However, the true PCFG probability model is used to prune these edges rather than the approximation used in the FOM. We believe that by considering parses which have a relatively high probability according to the combined PCFG and acoustic models that we will include most of the analyses for </context>
<context citStr="Charniak, 2001" endWordPosition="3123" position="19309" startWordPosition="3122">trained on sections 2 through 22, and 24 of the Penn WSJ treebank (Marcus et al., 1993). Prior to training, the treebank is transformed into speech-like text, removing punctuation and expanding numerals, etc.5 Overparsing is performed using an edge pop6 multiplicative factor. The parser records the number of edge pops required to reach the first complete parse. The parser continues to parse a until multiple of the number of edge pops required for the first parse are popped off the agenda. The second stage parser used is a modified version of the Charniak language modeling parser described in (Charniak, 2001). We trained this parser 5Brian Roark of AT&amp;T provided a tool to perform the speech normalization. 6An edge pop is the process of the parser removing an edge from the agenda and placing it in the parse chart. on the BLLIP99 corpus (Charniak et al., 1999); a corpus of 30million words automatically parsed using the Charniak parser (Charniak, 2000). In order to compare the work done by the n–best reranking technique to the word-lattice parser, we generated a set of n–best lattices. 50–best lists were extracted using the Chelba A* decoder7. A 50– best lattice is a sublattice of the acoustic lattic</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>Eugene Charniak. 2001. Immediate-head parsing for language models. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>A study on richer syntactic dependencies for structured language modeling.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<pages>225--231</pages>
<contexts>
<context citStr="Chelba and Jelinek, 1998" endWordPosition="274" position="1760" startWordPosition="271">01; Xu et al., 2002; Charniak, 2001; Hall and Johnson, 2003). Language modeling for speech could well be the first real problem for which syntactic techniques are useful. VP:ate VB NP IN NP:plate IN NP:fork John ate the pizza on a plate with a fork . Figure 1: An incomplete parse tree with head-word annotations. One reason that we expect syntactic models to perform well is that they are capable of modeling long-distance dependencies that simple n-gram ∗ This research was supported in part by NSF grants 9870676 and 0085940. models cannot. For example, the model presented by Chelba and Jelinek (Chelba and Jelinek, 1998; Xu et al., 2002) uses syntactic structure to identify lexical items in the left-context which are then modeled as an n-gram process. The model presented by Charniak (Charniak, 2001) identifies both syntactic structural and lexical dependencies that aid in language modeling. While there are n-gram models that attempt to extend the left-context window through the use of caching and skip models (Goodman, 2001), we believe that linguistically motivated models, such as these lexical-syntactic models, are more robust. Figure 1 presents a simple example to illustrate the nature of long-distance dep</context>
</contexts>
<marker>Chelba, Jelinek, 1998</marker>
<rawString>Ciprian Chelba and Frederick Jelinek. 1998. A study on richer syntactic dependencies for structured language modeling. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, pages 225–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Best-first edge-based chart parsing.</title>
<date>1998</date>
<booktitle>In 6th Annual Workshop for Very Large Corpora,</booktitle>
<pages>127--133</pages>
<contexts>
<context citStr="Goldwater et al., 1998" endWordPosition="1144" position="7142" startWordPosition="1141">rg max P(A|W)P(W) In Equation 3 we use Bayes’ rule to find the optimal string given P(A|W), the acoustic model, and P(W), the language model. Although the language model can be used to rescore1 the word-lattice, it is typically used to select a single hypothesis. We focus our attention in this paper to syntactic language modeling techniques that perform complete parsing, meaning that parse trees are built upon the strings in the word-lattice. 2.1 n–best list reranking Much effort has been put forth in developing efficient probabilistic models for parsing strings (Caraballo and Charniak, 1998; Goldwater et al., 1998; Blaheta and Charniak, 1999; Charniak, 2000; Charniak, 2001); an obvious solution to parsing wordlattices is to use n–best list reranking. The n–best list reranking procedure, depicted in Figure 3, utilizes an external language model that selects a set of strings from the word-lattice. These strings are analyzed by the parser which computes a language model probability. This probability is combined 1To rescore a word-lattice, each arch is assigned a new score (probability) defined by a new model (in combination with the acoustic model). duh/1.385 1 6 the/0 3 2 man/0 is/0 mans/1.385 man's/1.38</context>
<context citStr="Goldwater et al., 1998" endWordPosition="2080" position="12958" startWordPosition="2077">statistics which are estimated from training data (details of this model can be found in (Hall and Johnson, 2003)). FOM(Nij,k) = ˆα(Nij,k)β(Nij,k)ηC(j, k) (5) The best-first search employed by the first stage parser uses the FOM defined in Equation 5, where η is a normalization factor based on path length C(j, k). The normalization factor prevents small constituents from consistently being assigned a 2A chart edge Nzj,k indicates a grammar category Nz can be constructed from nodes j to k. 3An alternative to the inside and outside probabilities are the Viterbi inside and outside probabilities (Goldwater et al., 1998; Hall and Johnson, 2003). higher probability than larger constituents (Goldwater et al., 1998). Although this heuristic works well for directing the parser towards likely parses over a string, it is not an ideal model for pruning the word-lattice. First, the outside approximation of this FOM is based on a linear part-of-speech tag model (the bitag). Such a simple syntactic model is unlikely to provide realistic information when choosing a word-lattice path to consider. Second, the model is prone to favoring subsets of the word-lattice causing it to posit additional parse trees for the favored</context>
</contexts>
<marker>Goldwater, Charniak, Johnson, 1998</marker>
<rawString>Sharon Goldwater, Eugene Charniak, and Mark Johnson. 1998. Best-first edge-based chart parsing. In 6th Annual Workshop for Very Large Corpora, pages 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Global thresholding and multiple-pass parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>11--25</pages>
<contexts>
<context citStr="Goodman, 1997" endWordPosition="1440" position="8919" startWordPosition="1439">del is used to select n paths through the lattice generating at most n unique strings. The maximum performance that can be achieved is limited by the performance of this extractor model. Second, of the strings that are analyzed by the parser, many will share common substrings. Much of the work performed by the parser is duplicated for these substrings. This second point is the primary motivation behind parsing word-lattices (Hall and Johnson, 2003). 2.2 Multi-stage parsing Figure 4: Coarse-to-fine lattice parsing. In Figure 4 we present the general overview of a multi-stage parsing technique (Goodman, 1997; Charniak, 2000; Charniak, 2001). This process 1. Parse word-lattice with PCFG parser 2. Overparse, generating additional candidates 3. Compute inside-outside probabilities 4. Prune candidates with probability threshold Table 1: First stage word-lattice parser is know as coarse-to-fine modeling, where coarse models are more efficient but less accurate than fine models, which are robust but computationally expensive. In this particular parsing model a PCFG best-first parser (Bobrow, 1990; Caraballo and Charniak, 1998) is used to search the unconstrained space of parses Π over a string. This fi</context>
</contexts>
<marker>Goodman, 1997</marker>
<rawString>Joshua Goodman. 1997. Global thresholding and multiple-pass parsing. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, pages 11–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>A bit of progress in language modeling, extendend version.</title>
<date>2001</date>
<booktitle>In Microsoft Research</booktitle>
<tech>Technical Report MSR-TR-2001-72.</tech>
<contexts>
<context citStr="Goodman, 2001" endWordPosition="341" position="2172" startWordPosition="339">e dependencies that simple n-gram ∗ This research was supported in part by NSF grants 9870676 and 0085940. models cannot. For example, the model presented by Chelba and Jelinek (Chelba and Jelinek, 1998; Xu et al., 2002) uses syntactic structure to identify lexical items in the left-context which are then modeled as an n-gram process. The model presented by Charniak (Charniak, 2001) identifies both syntactic structural and lexical dependencies that aid in language modeling. While there are n-gram models that attempt to extend the left-context window through the use of caching and skip models (Goodman, 2001), we believe that linguistically motivated models, such as these lexical-syntactic models, are more robust. Figure 1 presents a simple example to illustrate the nature of long-distance dependencies. Using a syntactic model such as the the Structured Language Model (Chelba and Jelinek, 1998), we predict the word fork given the context {ate, with} where a trigram model uses the context {with, a}. Consider the problem of disambiguating between ... plate with a fork and ... plate with effort. The syntactic model captures the semantic relationship between the words ate and fork. The syntactic struc</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>Joshua Goodman. 2001. A bit of progress in language modeling, extendend version. In Microsoft Research Technical Report MSR-TR-2001-72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Hall</author>
<author>Mark Johnson</author>
</authors>
<title>Language modeling using efficient best-first bottom-up parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of IEEE Automated Speech Recognition and Understanding Workshop.</booktitle>
<contexts>
<context citStr="Hall and Johnson, 2003" endWordPosition="178" position="1196" startWordPosition="175">attice subsets for which there are few or no syntactic analyses posited. This attention-shifting technique provides a six-times increase in speed (measured as the number of parser analyses evaluated) while performing equivalently when used as the first-stage of a multi-stage parsing-based language model. 1 Introduction Success in language modeling has been dominated by the linear n-gram for the past few decades. A number of syntactic language models have proven to be competitive with the n-gram and better than the most popular n-gram, the trigram (Roark, 2001; Xu et al., 2002; Charniak, 2001; Hall and Johnson, 2003). Language modeling for speech could well be the first real problem for which syntactic techniques are useful. VP:ate VB NP IN NP:plate IN NP:fork John ate the pizza on a plate with a fork . Figure 1: An incomplete parse tree with head-word annotations. One reason that we expect syntactic models to perform well is that they are capable of modeling long-distance dependencies that simple n-gram ∗ This research was supported in part by NSF grants 9870676 and 0085940. models cannot. For example, the model presented by Chelba and Jelinek (Chelba and Jelinek, 1998; Xu et al., 2002) uses syntactic st</context>
<context citStr="Hall and Johnson, 2003" endWordPosition="660" position="4243" startWordPosition="657"> the word-lattice. In this paper we propose a variation of a probabilistic word-lattice parsing technique that increases PP:with PP:on Figure 2: A partial word-lattice from the NIST HUB-1 dataset. outline/0 of/115.4 to/0 13 it/51.59 9 7 outline/2.573 strategy/0 a/71.30 the/115.3 strategy/0 11 &lt;/s&gt;/0 12/0 outlines/7.140 yesterday/0 0 1 and/4.004 in/14.73 14 3 tuesday/0 to/0 5 2 tuesday/0 4 two/8.769 6 to/0.000 outlaw/83.57 outlines/10.71 8 outlined/8.027 outline/0 in/0 10 outlined/12.58 efficiency while incurring no loss of language modeling performance (measured as Word Error Rate – WER). In (Hall and Johnson, 2003) we presented a modular lattice parsing process that operates in two stages. The first stage is a PCFG word-lattice parser that generates a set of candidate parses over strings in a word-lattice, while the second stage rescores these candidate edges using a lexicalized syntactic language model (Charniak, 2001). Under this paradigm, the first stage is not only responsible for selecting candidate parses, but also for selecting paths in the word-lattice. Due to computational and memory requirements of the lexicalized model, the second stage parser is capable of rescoring only a small subset of al</context>
<context citStr="Hall and Johnson, 2003" endWordPosition="1417" position="8758" startWordPosition="1414">are two significant disadvantages to this approach. First, we are limited by the performance of the language model used to select the n–best lists. Usually, the trigram model is used to select n paths through the lattice generating at most n unique strings. The maximum performance that can be achieved is limited by the performance of this extractor model. Second, of the strings that are analyzed by the parser, many will share common substrings. Much of the work performed by the parser is duplicated for these substrings. This second point is the primary motivation behind parsing word-lattices (Hall and Johnson, 2003). 2.2 Multi-stage parsing Figure 4: Coarse-to-fine lattice parsing. In Figure 4 we present the general overview of a multi-stage parsing technique (Goodman, 1997; Charniak, 2000; Charniak, 2001). This process 1. Parse word-lattice with PCFG parser 2. Overparse, generating additional candidates 3. Compute inside-outside probabilities 4. Prune candidates with probability threshold Table 1: First stage word-lattice parser is know as coarse-to-fine modeling, where coarse models are more efficient but less accurate than fine models, which are robust but computationally expensive. In this particular</context>
<context citStr="Hall and Johnson, 2003" endWordPosition="1639" position="10194" startWordPosition="1636">ows it to generate a set of high probability candidate parses π'. These parses are then rescored using a lexicalized syntactic model (Charniak, 2001). Although the coarse-to-fine model may include any number of intermediary stages, in this paper we consider this two-stage model. There is no guarantee that parses favored by the second stage will be generated by the first stage. In other words, because the first stage model prunes the space of parses from which the second stage rescores, the first stage model may remove solutions that the second stage would have assigned a high probability. In (Hall and Johnson, 2003), we extended the multi-stage parsing model to work on word-lattices. The first-stage parser, Table 1, is responsible for positing a set of candidate parses over the wordlattice. Were we to run the parser to completion it would generate all parses for all strings described by the word-lattice. As with string parsing, we stop the first stage parser early, generating a subset of all parses. Only the strings covered by complete Π PCFG Parser π' ⊂ Π Lexicalized Parser parses are passed on to the second stage parser. This indirectly prunes the word-lattice of all word-arcs that were not covered by </context>
<context citStr="Hall and Johnson, 2003" endWordPosition="1906" position="11877" startWordPosition="1903">1993; Manning and Sch¨utze, 1999). The FOM we describe is an approximation to the edge probability and is computed using an estimate of the inside probability times an approximation to the outside probability 3. The inside probability β(Nij,k) can be computed incrementally during bottom-up parsing. The normalized acoustic probabilities from the acoustic recognizer are included in this calculation. ˆα(Nij,k) (4) �= fwd(Tqi,j)p(Ni|T q)p(Tr|Ni)bkwd(Trk,l) i,l,q,r The outside probability is approximated with a bitag model and the standard tag/category boundary model (Caraballo and Charniak, 1998; Hall and Johnson, 2003). Equation 4 presents the approximation to the outside probability. Part-of-speech tags Tq and Tr are the candidate tags to the left and right of the constituent Nij,k. The fwd() and bkwd() functions are the HMM forward and backward probabilities calculated over a lattice containing the part-of-speech tag, the word, and the acoustic scores from the word-lattice to the left and right of the constituent, respectively. p(Ni|Tq) and p(Tr|Ni) are the boundary statistics which are estimated from training data (details of this model can be found in (Hall and Johnson, 2003)). FOM(Nij,k) = ˆα(Nij,k)β(N</context>
</contexts>
<marker>Hall, Johnson, 2003</marker>
<rawString>Keith Hall and Mark Johnson. 2003. Language modeling using efficient best-first bottom-up parsing. In Proceedings of IEEE Automated Speech Recognition and Understanding Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFG models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--617</pages>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 24:617–636.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Meeting of the Association for Computational Linguistics (ACL-03).</booktitle>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Meeting of the Association for Computational Linguistics (ACL-03).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Foundations of statistical natural language processing.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. 1999. Foundations of statistical natural language processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context citStr="Marcus et al., 1993" endWordPosition="3035" position="18781" startWordPosition="3032">4 Experiments The purpose of attention shifting is to reduce the amount of work exerted by the first stage PCFG parser while maintaining the same quality of language modeling (in the multi-stage system). We have performed a set of experiments on the NIST ’93 HUB–1 word-lattices. The HUB–1 is a collection of 213 word-lattices resulting from an acoustic recognizer’s analysis of speech utterances. Professional readers reading Wall Street Journal articles generated the utterances. The first stage parser is a best-first PCFG parser trained on sections 2 through 22, and 24 of the Penn WSJ treebank (Marcus et al., 1993). Prior to training, the treebank is transformed into speech-like text, removing punctuation and expanding numerals, etc.5 Overparsing is performed using an edge pop6 multiplicative factor. The parser records the number of edge pops required to reach the first complete parse. The parser continues to parse a until multiple of the number of edge pops required for the first parse are popped off the agenda. The second stage parser used is a modified version of the Charniak language modeling parser described in (Charniak, 2001). We trained this parser 5Brian Roark of AT&amp;T provided a tool to perform</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context citStr="Roark, 2001" endWordPosition="168" position="1138" startWordPosition="167">e parser’s attention is shifted towards word-lattice subsets for which there are few or no syntactic analyses posited. This attention-shifting technique provides a six-times increase in speed (measured as the number of parser analyses evaluated) while performing equivalently when used as the first-stage of a multi-stage parsing-based language model. 1 Introduction Success in language modeling has been dominated by the linear n-gram for the past few decades. A number of syntactic language models have proven to be competitive with the n-gram and better than the most popular n-gram, the trigram (Roark, 2001; Xu et al., 2002; Charniak, 2001; Hall and Johnson, 2003). Language modeling for speech could well be the first real problem for which syntactic techniques are useful. VP:ate VB NP IN NP:plate IN NP:fork John ate the pizza on a plate with a fork . Figure 1: An incomplete parse tree with head-word annotations. One reason that we expect syntactic models to perform well is that they are capable of modeling long-distance dependencies that simple n-gram ∗ This research was supported in part by NSF grants 9870676 and 0085940. models cannot. For example, the model presented by Chelba and Jelinek (Ch</context>
<context citStr="Roark, 2001" endWordPosition="3548" position="21901" startWordPosition="3547">r 6.5 million states when provided with 30,000 local trees. Model # edge pops O-WER WER n–best (Charniak) 2.5 million 7.75 11.8 100x LatParse 3.4 million 8.18 12.0 10x AttShift 564,895 7.78 11.9 Table 2: Results for n–best lists and n–best lattices. Table 2 shows the results for n–best list reranking and word-lattice parsing of n–best lattices. We recreated the results of the Charniak language model parser used for reranking in order to measure the amount of work required. We ran the first stage parser with 4-times overparsing for each string in 7The n–best lists were provided by Brian Roark (Roark, 2001) 8A local-tree is an explicit expansion of an edge and its children. An example local tree is NP3,8 → DT3,4 NN4,8. the n–best list. The LatParse result represents running the word-lattice parser on the n–best lattices performing 100–times overparsing in the first stage. The AttShift model is the attention shifting parser described in this paper. We used 10–times overparsing for both the initial parse and each of the attention shifting iterations. When run on the n–best lattice, this model achieves a comparable WER, while reducing the amount of parser work sixfold (as compared to the regular wo</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Brian Roark. 2001. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27(3):249–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>A study on richer syntactic dependencies for structured language modeling.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>191--198</pages>
<contexts>
<context citStr="Xu et al., 2002" endWordPosition="172" position="1155" startWordPosition="169">tention is shifted towards word-lattice subsets for which there are few or no syntactic analyses posited. This attention-shifting technique provides a six-times increase in speed (measured as the number of parser analyses evaluated) while performing equivalently when used as the first-stage of a multi-stage parsing-based language model. 1 Introduction Success in language modeling has been dominated by the linear n-gram for the past few decades. A number of syntactic language models have proven to be competitive with the n-gram and better than the most popular n-gram, the trigram (Roark, 2001; Xu et al., 2002; Charniak, 2001; Hall and Johnson, 2003). Language modeling for speech could well be the first real problem for which syntactic techniques are useful. VP:ate VB NP IN NP:plate IN NP:fork John ate the pizza on a plate with a fork . Figure 1: An incomplete parse tree with head-word annotations. One reason that we expect syntactic models to perform well is that they are capable of modeling long-distance dependencies that simple n-gram ∗ This research was supported in part by NSF grants 9870676 and 0085940. models cannot. For example, the model presented by Chelba and Jelinek (Chelba and Jelinek,</context>
</contexts>
<marker>Xu, Chelba, Jelinek, 2002</marker>
<rawString>Peng Xu, Ciprian Chelba, and Frederick Jelinek. 2002. A study on richer syntactic dependencies for structured language modeling. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 191– 198.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>