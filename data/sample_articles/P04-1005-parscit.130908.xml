<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant confidence="0.029793" no="0">
<title confidence="0.997206">
A TAG-based noisy channel model of speech repairs
</title>
<author confidence="0.999419">
Mark Johnson Eugene Charniak
</author>
<affiliation confidence="0.999467">
Brown University Brown University
</affiliation>
<address confidence="0.949729">
Providence, RI 02912 Providence, RI 02912
</address>
<email confidence="0.998967">
mj@cs.brown.edu ec@cs.brown.edu
</email>
<sectionHeader confidence="0.993891" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999082444444444">This paper describes a noisy channel model of speech repairs, which can identify and correct repairs in speech transcripts. A syntactic parser is used as the source model, and a novel type of TAG-based transducer is the channel model. The use of TAG is motivated by the intuition that the reparandum is a “rough copy” of the repair. The model is trained and tested on the Switchboard disfluency-annotated corpus.</bodyText>
<sectionHeader confidence="0.998793" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99993282">Most spontaneous speech contains disfluencies such as partial words, filled pauses (e.g., “uh”, “um”, “huh”), explicit editing terms (e.g., “I mean”), parenthetical asides and repairs. Of these repairs pose particularly difficult problems for parsing and related NLP tasks. This paper presents an explicit generative model of speech repairs and shows how it can eliminate this kind of disfluency. While speech repairs have been studied by psycholinguists for some time, as far as we know this is the first time a probabilistic model of speech repairs based on a model of syntactic structure has been described in the literature. Probabilistic models have the advantage over other kinds of models that they can in principle be integrated with other probabilistic models to produce a combined model that uses all available evidence to select the globally optimal analysis. Shriberg and Stolcke (1998) studied the location and distribution of repairs in the Switchboard corpus, but did not propose an actual model of repairs. Heeman and Allen (1999) describe a noisy channel model of speech repairs, but leave “extending the model to incorporate higher level syntactic ... processing” to future work. The previous work most closely related to the current work is Charniak and Johnson (2001), who used a boosted decision stub classifier to classify words as edited or not on a word by word basis, but do not identify or assign a probability to a repair as a whole. There are two innovations in this paper. First, we demonstrate that using a syntactic parser-based language model Charniak (2001) instead of bi/trigram language models significantly improves the accuracy of repair detection and correction. Second, we show how Tree Adjoining Grammars (TAGs) can be used to provide a precise formal description and probabilistic model of the crossed dependencies occurring in speech repairs. The rest of this paper is structured as follows. The next section describes the noisy channel model of speech repairs and the section after that explains how it can be applied to detect and repair speech repairs. Section 4 evaluates this model on the Penn 3 disfluency-tagged Switchboard corpus, and section 5 concludes and discusses future work.</bodyText>
<sectionHeader confidence="0.85361" genericHeader="method">
2 A noisy channel model of repairs
</sectionHeader>
<bodyText confidence="0.997113272727273">We follow Shriberg (1994) and most other work on speech repairs by dividing a repair into three parts: the reparandum (the material repaired), the interregnum that is typically either empty or consists of a filler, and the repair. Figure 1 shows these three parts for a typical repair. Most current probabilistic language models are based on HMMs or PCFGs, which induce linear or tree-structured dependencies between words. The relationship between reparandum and repair seems to be quite different: the repair is a “rough copy” of the reparandum, often incorporating the same or very similar words in roughly the same word order. That is, they seem to involve “crossed” dependencies between the reparandum and the repair, shown in Figure 1. Languages with an unbounded number of crossed dependencies cannot be described by a context-free or finitestate grammar, and crossed dependencies like these have been used to argue natural languages ... a flight to Boston, uh, I mean, to Denver on Friday ...</bodyText>
<note confidence="0.6464385">
� Y J � Y J � Y J
Reparandum Interregnum Repair
</note>
<figureCaption confidence="0.99547775">
Figure 1: The structure of a typical repair, with crossing dependencies between reparandum and
repair.
Figure 2: The “helical” dependency structure induced by the generative model of speech repairs
for the repair depicted in Figure 1.
</figureCaption>
<figure confidence="0.978045333333333">
a flight
to Denver on Friday
to Boston
</figure>
<bodyText confidence="0.999227878048781">are not context-free Shieber (1985). Mildly context-sensitive grammars, such as Tree Adjoining Grammars (TAGs) and Combinatory Categorial Grammars, can describe such crossing dependencies, and that is why TAGs are used here. Figure 2 shows the combined model’s dependency structure for the repair of Figure 1. Interestingly, if we trace the temporal word string through this dependency structure, aligning words next to the words they are dependent on, we obtain a “helical” type of structure familiar from genome models, and in fact TAGs are being used to model genomes for very similar reasons. The noisy channel model described here involves two components. A language model defines a probability distribution P(X) over the source sentences X, which do not contain repairs. The channel model defines a conditional probability distribution P(YIX) of surface sentences Y , which may contain repairs, given source sentences. In the work reported here, X is a word string and Y is a speech transcription not containing punctuation or partial words. We use two language models here: a bigram language model, which is used in the search process, and a syntactic parser-based language model Charniak (2001), which is used to rescore a set of the most likely analysis obtained using the bigram model. Because the language model is responsible for generating the well-formed sentence X, it is reasonable to expect that a language model that can model more global properties of sentences will lead to better performance, and the results presented here show that this is the case. The channel model is a stochastic TAG-based transducer; it is responsible for generating the repairs in the transcript Y , and it uses the ability of TAGs to straightforwardly model crossed dependencies.</bodyText>
<subsectionHeader confidence="0.959956">
2.1 Informal description
</subsectionHeader>
<bodyText confidence="0.9994915">Given an observed sentence Y we wish to find the most likely source sentence X, where:</bodyText>
<equation confidence="0.999825">
X = argmax P(XIY) = argmaxP(YIX)P(Y).
X X
</equation>
<bodyText confidence="0.999895538461539">This is the same general setup that is used in statistical speech recognition and machine translation, and in these applications syntaxbased language models P(Y ) yield state-of-theart performance, so we use one such model here. The channel model P(YIX) generates sentences Y given a source X. A repair can potentially begin before any word of X. When a repair has begun, the channel model incrementally processes the succeeding words from the start of the repair. Before each succeeding word either the repair can end or else a sequence of words can be inserted in the reparandum. At the end of each repair, a (possibly null) interregnum is appended to the reparandum. The intuition motivating the channel model design is that the words inserted into the reparandum are very closely related those in the repair. Indeed, in our training data over 60% of the words in the reparandum are exact copies of words in the repair; this similarity is strong evidence of a repair. The channel model is designed so that exact copy reparandum words will have high probability. We assume that X is a substring of Y , i.e., that the source sentence can be obtained by deleting words from Y , so for a fixed observed sentence there are only a finite number of possible source sentences. However, the number of source sentences grows exponentially with the length of Y , so exhaustive search is probably infeasible. TAGs provide a systematic way of formalizing the channel model, and their polynomialtime dynamic programming parsing algorithms can be used to search for likely repairs, at least when used with simple language models like a bigram language model. In this paper we first identify the 20 most likely analysis of each sentence using the TAG channel model together with a bigram language model. Then each of these analysis is rescored using the TAG channel model and a syntactic parser based language model. The TAG channel model’s analysis do not reflect the syntactic structure of the sentence being analyzed; instead they encode the crossed dependencies of the speech repairs. If we want to use TAG dynamic programming algorithms to efficiently search for repairs, it is necessary that the intersection (in language terms) of the TAG channel model and the language model itself be describable by a TAG. One way to guarantee this is to use a finite state language model; this motivates our use of a bigram language model. On the other hand, it seems desirable to use a language model that is sensitive to more global properties of the sentence, and we do this by reranking the initial analysis, replacing the bigram language model with a syntactic parser based model. We do not need to intersect this parser based language model with our TAG channel model since we evaluate each analysis separately.</bodyText>
<subsectionHeader confidence="0.998712">
2.2 The TAG channel model
</subsectionHeader>
<bodyText confidence="0.999703789473684">The TAG channel model defines a stochastic mapping of source sentences X into observed sentences Y . There are several ways to define transducers using TAGs such as Shieber and Schabes (1990), but the following simple method, inspired by finite-state transducers, suffices for the application here. The TAG defines a language whose vocabulary is the set of pairs (EU101)x(EU101), where E is the vocabulary of the observed sentences Y . A string Z in this language can be interpreted as a pair of strings (Y, X), where Y is the concatenation of the projection of the first components of Z and X is the concatenation of the projection of the second components. For example, the string Z = a:a flight:flight to:0 Boston:0 uh:0 I:0 mean:0 to:to Denver:Denver on:on Friday:Friday corresponds to the observed string Y = a flight to Boston uh I mean to Denver on Friday and the source string X = a flight to Denver on Friday. Figure 3 shows the TAG rules used to generate this example. The nonterminals in this grammar are of the form N,,,x, R,,,y:,,,x and I, where w,, is a word appearing in the source string and wy is a word appearing in the observed string. Informally, the N,,,x nonterminals indicate that the preceding word w,, was analyzed as not being part of a repair, while the R,,,y:,,,x that the preceding words wy and w,, were part of a repair. The nonterminal I generates words in the interregnum of a repair. Encoding the preceding words in the TAGs nonterminals permits the channel model to be sensitive to lexical properties of the preceding words. The start symbol is N$, where ‘$’ is a distinguished symbol used to indicate the beginning and end of sentences.</bodyText>
<subsectionHeader confidence="0.915059">
2.3 Estimating the repair channel
model from data
</subsectionHeader>
<bodyText confidence="0.999924785714286">The model is trained from the disfluency and POS tagged Switchboard corpus on the LDC Penn tree bank III CD-ROM (specifically, the files under dysfl/dps/swbd). This version of the corpus annotates the beginning and ending positions of repairs as well as fillers, editing terms, asides, etc., which might serve as the interregnum in a repair. The corpus also includes punctuation and partial words, which are ignored in both training and evaluation here since we felt that in realistic applications these would not be available in speech recognizer output. The transcript of the example of Figure 1 would look something like the following:</bodyText>
<construct confidence="0.531175">
a/DT flight/NN [to/IN Boston/NNP +
{F uh/UH} {E I/PRP mean/VBP} to/IN
Denver/NNP] on/IN Friday/NNP
</construct>
<bodyText confidence="0.999106857142857">In this transcription the repair is the string from the opening bracket “[” to the interruption point “+”; the interregnum is the sequence of braced strings following the interregnum, and the repair is the string that begins at the end of the interregnum and ends at the closing bracket “]”. The interregnum consists of the braced expressions immediately following the interruption point.</bodyText>
<figure confidence="0.999821611111111">
(a1) Nwant
a:a Na j
1 − Pn(repair|a)
(a2) Na
flight:flight Rflight:flight
Ij
Pn(repair|flight)
(a3) NDenver
on:on Non j
1 − Pn(repair|on)
(a5) I
uh I
I mean
PZ(uh I mean)
R*Boston,Denver tomorrow:tomorrow
Pr(del|Boston, Denver)
Nwant
a:a Na
R*
Boston,Denver
Pr(ins|Boston, Denver)
Pr(tomorrow|ins, Boston, Denver)
flight:flight Rflight:flight
to:0 Rto:to
Boston:0 RBoston:Denver
RBoston:Denver
Denver:Denver on:on Non
Friday:Friday NFriday
. . .
NDenver
Rto:to
Rflight:flight
I
uh:0 I
I:0 mean:0
to:to
</figure>
<figureCaption confidence="0.991814">
Figure 3: The TAG rules used to generate the example shown in Figure 1 and their respective
weights, and the corresponding derivation and derived trees.
</figureCaption>
<bodyText confidence="0.974858875">We used the disfluency tagged version of the corpus for training rather than the parsed version because the parsed version does not mark the interregnum, but we need this information for training our repair channel model. Testing was performed using data from the parsed version since this data is cleaner, and it enables a direct comparison with earlier work.</bodyText>
<figure confidence="0.997860592592593">
(03) RBoston:Denver
(02) Rto:to
Boston:0 RBoston:Denver
R Denver:Denver
'to:to
Pr(subst|to, to)Pr(Boston|subst, to, Denver)
(01) Rflight:flight
to:0 Rto:to
R*
`flight:flight to:to
Pr(copy|flight, flight)
R*
Boston:Denver NDenver j
Pr(nonrep|Boston, Denver)
(04) RBoston,Denver
RBoston,tomorrow
(05) RBoston,Denver
tomorrow:0 Rtomorrow,Denver
. . .
a1
a2
a5 01
02
03
a3
a4
. . .
</figure>
<bodyText confidence="0.993002278409091">We followed Charniak and Johnson (2001) and split the corpus into main training data, heldout training data and test data as follows: main training consisted of all sw[23]*.dps files, heldout training consisted of all sw4[5-9]*.dps files and test consisted of all sw4[0-1]*.mrg files. We now describe how the weights on the TAG productions described in subsection 2.2 are estimated from this training data. In order to estimate these weights we need to know the TAG derivation of each sentence in the training data. In order to uniquely determine this we need the not just the locations of each reparandum, interregnum and repair (which are annotated in the corpus) but also the crossing dependencies between the reparandum and repair words, as indicated in Figure 1. We obtain these by aligning the reparandum and repair strings of each repair using a minimum-edit distance string aligner with the following alignment costs: aligning identical words costs 0, aligning words with the same POS tag costs 2, an insertion or a deletion costs 4, aligning words with POS tags that begin with the same letter costs 5, and an arbitrary substitution costs 7. These costs were chosen so that a substitution will be selected over an insertion followed by a deletion, and the lower cost for substitutions involving POS tags beginning with the same letter is a rough and easy way of establishing a preference for aligning words whose POS tags come from the same broad class, e.g., it results in aligning singular and plural nouns, present and past participles, etc. While we did not evaluate the quality of the alignments since they are not in themselves the object of this exercise, they seem to be fairly good. From our training data we estimate a number of conditional probability distributions. These estimated probability distributions are the linear interpolation of the corresponding empirical distributions from the main sub-corpus using various subsets of conditioning variables (e.g., bigram models are mixed with unigram models, etc.)using Chen’s bucketing scheme Chen and Goodman (1998). As is commonly done in language modelling, the interpolation coefficients are determined by maximizing the likelihood of the held out data counts using EM. Special care was taken to ensure that all distributions over words ranged over (and assigned non-zero probability to) every word that occurred in the training corpora; this turns out to be important as the size of the training data for the different distributions varies greatly. The first distribution is defined over the words in source sentences (i.e., that do not contain reparandums or interregnums). Pn(repair|W) is the probability of a repair beginning after a word W in the source sentence X; it is estimated from the training sentences with reparandums and interregnums removed. Here and in what follows, W ranges over E  {$}, where ‘$’ is a distinguished beginning-ofsentence marker. For example, Pn(repair|flight) is the probability of a repair beginning after the word flight. Note that repairs are relatively rare; in our training data Pn(repair)  0.02, which is a fairly strong bias against repairs. The other distributions are defined over aligned reparandum/repair strings, and are estimated from the aligned repairs extracted from the training data. In training we ignored all overlapping repairs (i.e., cases where the reparandum of one repair is the repair of another).(Naturally, in testing we have no such freedom.) We analyze each repair as consisting of n aligned word pairs (we describe the interregnum model later). Mi is the ith reparandum word and Ri is the corresponding repair word, so both of these range over E  {}. We define M0 and R0 to be source sentence word that preceded the repair (which is ‘$’ if the repair begins at the beginning of a sentence). We define M i and R i to be the last non- reparandum and repair words respectively, i.e., M i = Mi if Mi =  and M i = Mi−1 otherwise. Finally, Ti, i = 1... n + 1, which indicates the type of repair that occurs at position i, ranges over {copy, subst, ins, del, nonrep}, where Tn+1 = nonrep (indicating that the repair has ended), and for i = 1... n, Ti = copy if Mi = Ri, Ti = ins if Ri = , Ti = del if Mi =  and Ti = subst otherwise. The distributions we estimate from the aligned repair data are the following. Pr(Ti|Mi−1, Ri−1) is the probability of seeing repair type Ti following the reparandum word Mi−1 and repair word Ri−1; e.g., Pr(nonrep|Boston, Denver) is the probability of the repair ending when Boston is the last reparandum word and Denver is the last repair word. Pr(Mi|Ti = ins,Mi−1,Ri) is the probability that Mi is the word that is inserted into the reparandum (i.e., Ri = ) given that some word is substituted, and that the preceding reparandum and repair words are M i−1 and R i. For example Pr(tomorrow|ins, Boston, Denver) is the probability that the word tomorrow is inserted into the reparandum after the words Boston and Denver, given that some word is inserted. Pr(Mi|Ti = subst, Mi−1, Ri) is the probability that Mi is the word that is substituted in the reparandum for Ri, given that some word is substituted. For example, Pr(Boston|subst, to, Denver) is the probability that Boston is substituted for Denver, given that some word is substituted. Finally, we also estimated a probability distribution Pi(W) over interregnum strings as follows. Our training corpus annotates what we call interregnum expressions, such as uh and I mean. We estimated a simple unigram distribution over all of the interregnum expressions observed in our training corpus, and also extracted the empirical distribution of the number of interregnum expressions in each repair. Interregnums are generated as follows. First, the number k of interregnum expressions is chosen using the empirical distribution. Then k interregnum expressions are independently generated from the unigram distribution of interregnum expressions, and appended to yield the interregnum string W. The weighted TAG that constitutes the channel model is straight forward to define using these conditional probability distributions. Note that the language model generates the source string X. Thus the weights of the TAG rules condition on the words in X, but do not generate them. There are three different schema defining the initial trees of the TAG. These correspond to analyzing a source word as not beginning a repair (e.g., a1 and a3 in Figure 3), analyzing a source word as beginning a repair (e.g., a2), and generating an interregnum (e.g., a5). Auxiliary trees generate the paired reparandum/repair words of a repair. There are five different schema defining the auxiliary trees corresponding to the five different values that Ti can take. Note that the nonterminal Rm,r expanded by the auxiliary trees is annotated with the last reparandum and repair words Mi−1 and Ri−1 respectively, which makes it possible to condition the rule’s weight on these words. Auxiliary trees of the form (01) generate reparandum words that are copies of the corresponding repair words; the weight on such trees is Pr(copy|Mi−1, Ri−1). Trees of the form (02) substitute a reparandum word for a repair word; their weight is Pr(subst|Mi−1, Ri−1)Pr(Mi|subst, Mi−1, Ri). Trees of the form (03) end a repair; their weight is Pr(nonrep|, Mi−1, Ri−1). Auxiliary trees of the form (03) end a repair; they are weighted Pr(nonrep|Mi−1, Ri−1). Auxiliary trees of the form (04) permit the repair word Ri−1 to be deleted in the reparandum; the weight of such a tree is Pr(del|Mi−1,Ri−1). Finally, auxiliary trees of the form (05) generate a reparandum word Mi is inserted; the weight of such a tree is Pr(ins|Mi−1, Ri−1)Pr(Mi|ins, Mi−1, Ri−1).</bodyText>
<sectionHeader confidence="0.967599" genericHeader="method">
3 Detecting and repairing speech
repairs
</sectionHeader>
<bodyText confidence="0.9999848125">The TAG just described is not probabilistic; informally, it does not include the probability costs for generating the source words. However, it is easy to modify the TAG so it does include a bigram model that does generate the source words, since each nonterminal encodes the preceding source word. That is, we multiply the weights of each TAG production given earlier that introduces a source word Ri by Pn(Ri|Ri−1). The resulting stochastic TAG is in fact exactly the intersection of the channel model TAG with a bigram language model. The standard n5 bottom-up dynamic programming parsing algorithm can be used with this stochastic TAG. Each different parse of the observed string Y with this grammar corresponds to a way of analyzing Y in terms of a hypothetical underlying sentence X and a number of different repairs. In our experiments below we extract the 20 most likely parses for each sentence. Since the weighted grammar just given does not generate the source string X, the score of the parse using the weighted TAG is P(Y |X). This score multiplied by the probability P(X) of the source string using the syntactic parser based language model, is our best estimate of the probability of an analysis. However, there is one additional complication that makes a marked improvement to the model’s performance. Recall that we use the standard bottom-up dynamic programming TAG parsing algorithm to search for candidate parses. This algorithm has n5 running time, where n is the length of the string. Even though our sentences are often long, it is extremely unlikely that any repair will be longer than, say, 12 words. So to increase processing speed we only compute analyses for strings of length 12 or less. For every such substring that can be analyzed as a repair we calculate the repair odds, i.e., the probability of generating this substring as a repair divided by the probability of generating this substring via the non-repair rules, or equivalently, the odds that this substring constitutes a repair. The substrings with high repair odds are likely to be repairs. This more local approach has a number of advantages over computing a global analysis. First, as just noted it is much more efficient to compute these partial analyses rather than to compute global analyses of the entire sentence. Second, there are rare cases in which the same substring functions as both repair and reparandum (i.e., the repair string is itself repaired again). A single global analysis would not be able to capture this (since the TAG channel model does not permit the same substring to be both a reparandum and a repair), but we combine these overlapping repair substring analyses in a post-processing operation to yield an analysis of the whole sentence. (We do insist that the reparandum and interregnum of a repair do not overlap with those of any other repairs in the same analysis).</bodyText>
<sectionHeader confidence="0.99947" genericHeader="evaluation and result">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999757421052632">This section describes how we evaluate our noisy model. As mentioned earlier, following Charniak and Johnson (2001) our test data consisted of all Penn III Switchboard tree-bank sw4[01]*.mrg files. However, our test data differs from theirs in that in this test we deleted all partial words and punctuation from the data, as this results in a more realistic test situation. Since the immediate goal of this work is to produce a program that identifies the words of a sentence that belong to the reparandum of a repair construction (to a first approximation these words can be ignored in later processing), our evaluation focuses on the model’s performance in recovering the words in a reparandum. That is, the model is used to classify each word in the sentence as belonging to a reparandum or not, and all other additional structure produced by the model is ignored. We measure model performance using standard precision p, recall r and f-score f, measures. If n, is the number of reparandum words the model correctly classified, nt is the number of true reparandum words given by the manual annotations and nm is the number of words the model predicts to be reparandum words, then the precision is n,/nm, recall is n,/nt, and f is 2pr/(p + r). For comparison we include the results of running the word-by-word classifier described in Charniak and Johnson (2001), but where partial words and punctuation have been removed from the training and test data. We also provide results for our noisy channel model using a bigram language model and a second trigram model where the twenty most likely analyses are rescored. Finally we show the results using the parser language model.</bodyText>
<table confidence="0.777109">
CJ01' Bigram Trigram Parser
Precision 0.951 0.776 0.774 0.820
Recall 0.631 0.736 0.763 0.778
F-score 0.759 0.756 0.768 0.797
</table>
<bodyText confidence="0.999848666666667">The noisy channel model using a bigram language model does a slightly worse job at identifying reparandum and interregnum words than the classifier proposed in Charniak and Johnson (2001). Replacing the bigram language model with a trigram model helps slightly, and parserbased language model results in a significant performance improvement over all of the others.</bodyText>
<sectionHeader confidence="0.963121" genericHeader="conclusion">
5 Conclusion and further work
</sectionHeader>
<bodyText confidence="0.999985771428571">This paper has proposed a novel noisy channel model of speech repairs and has used it to identify reparandum words. One of the advantages of probabilistic models is that they can be integrated with other probabilistic models in a principled way, and it would be interesting to investigate how to integrate this kind of model of speech repairs with probabilistic speech recognizers. There are other kinds of joint models of reparandum and repair that may produce a better reparandum detection system. We have experimented with versions of the models described above based on POS bi-tag dependencies rather than word bigram dependencies, but with results very close to those presented here.<FR>Still, more sophisticated models may yield bet-
ter performance.
It would also be interesting to combine this
probabilistic model of speech repairs with the
word classifier approach of Charniak and John-
son (2001). </FR>That approach may do so well be-
cause many speech repairs are very short, in-
volving only one or two words Shriberg and
Stolcke (1998), so the reparandum, interregnum
and repair are all contained in the surround-
ing word window used as features by the classi-
fier. On the other hand, the probabilistic model
of repairs explored here seems to be most suc-
cessful in identifying long repairs in which the
reparandum and repair are similar enough to be
unlikely to have been generated independently.
Since the two approaches seem to have different
strengths, a combined model may outperform
both of them.
</bodyText>
<sectionHeader confidence="0.999045" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998514358974359">
Eugene Charniak and Mark Johnson. 2001.
Edit detection and parsing for transcribed
speech. In Proceedings of the 2nd Meeting
of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages
118–126. The Association for Computational
Linguistics.
Eugene Charniak. 2001. Immediate-head pars-
ing for language models. In Proceedings of the
39th Annual Meeting of the Association for
Computational Linguistics. The Association
for Computational Linguistics.
Stanley F. Chen and Joshua Goodman. 1998.
An empirical study of smoothing techniques
for language modeling. Technical Report TR-
10-98, Center for Research in Computing
Technology, Harvard University.
Peter A. Heeman and James F. Allen. 1999.
Speech repairs, intonational phrases, and dis-
course markers: Modeling speaker’s utter-
ances in spoken dialogue. Computational
Linguistics, 25(4):527–571.
Stuart M. Shieber and Yves Schabes. 1990.
Synchronous tree-adjoining grammars. In
Proceedings of the 13th International Confer-
ence on Computational Linguistics (COLING
1990), pages 253–258.
Stuart M. Shieber. 1985. Evidence against the
Context-Freeness of natural language. Lin-
guistics and Philosophy, 8(3):333–344.
Elizabeth Shriberg and Andreas Stolcke. 1998.
How far do speakers back up in repairs? a
quantitative model. In Proceedings of the In-
ternational Conference on Spoken Language
Processing, volume 5, pages 2183–2186, Syd-
ney, Australia.
Elizabeth Shriberg. 1994. Preliminaries to a
Theory of Speech Disfluencies. Ph.D. thesis,
University of California, Berkeley.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant confidence="0.837525" no="0">
<title confidence="0.999187">A TAG-based noisy channel model of speech repairs</title>
<author confidence="0.999953">Mark Johnson Eugene Charniak</author>
<affiliation confidence="0.999996">Brown University Brown University</affiliation>
<address confidence="0.996963">Providence, RI 02912 Providence, RI 02912</address>
<email confidence="0.999699">mj@cs.brown.eduec@cs.brown.edu</email>
<abstract confidence="0.9831793">This paper describes a noisy channel model of speech repairs, which can identify and correct repairs in speech transcripts. A syntactic parser is used as the source model, and a novel type of TAG-based transducer is the channel model. The use of TAG is motivated by the intuition that the reparandum is a “rough copy” of the repair. The model is trained and tested on the Switchboard disfluency-annotated corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Edit detection and parsing for transcribed speech.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>118--126</pages>
<contexts>
<context citStr="Charniak and Johnson (2001)" endWordPosition="301" position="1911" startWordPosition="298">dvantage over other kinds of models that they can in principle be integrated with other probabilistic models to produce a combined model that uses all available evidence to select the globally optimal analysis. Shriberg and Stolcke (1998) studied the location and distribution of repairs in the Switchboard corpus, but did not propose an actual model of repairs. Heeman and Allen (1999) describe a noisy channel model of speech repairs, but leave “extending the model to incorporate higher level syntactic ... processing” to future work. The previous work most closely related to the current work is Charniak and Johnson (2001), who used a boosted decision stub classifier to classify words as edited or not on a word by word basis, but do not identify or assign a probability to a repair as a whole. There are two innovations in this paper. First, we demonstrate that using a syntactic parser-based language model Charniak (2001) instead of bi/trigram language models significantly improves the accuracy of repair detection and correction. Second, we show how Tree Adjoining Grammars (TAGs) can be used to provide a precise formal description and probabilistic model of the crossed dependencies occurring in speech repairs. Th</context>
<context citStr="Charniak and Johnson (2001)" endWordPosition="2206" position="13251" startWordPosition="2203">m, but we need this information for training our repair channel model. Testing was performed using data from the parsed version since this data is cleaner, and (03) RBoston:Denver (02) Rto:to Boston:0 RBoston:Denver R Denver:Denver 'to:to Pr(subst|to, to)Pr(Boston|subst, to, Denver) (01) Rflight:flight to:0 Rto:to R* `flight:flight to:to Pr(copy|flight, flight) R* Boston:Denver NDenver j Pr(nonrep|Boston, Denver) (04) RBoston,Denver RBoston,tomorrow (05) RBoston,Denver tomorrow:0 Rtomorrow,Denver . . . a1 a2 a5 01 02 03 a3 a4 . . . it enables a direct comparison with earlier work. We followed Charniak and Johnson (2001) and split the corpus into main training data, heldout training data and test data as follows: main training consisted of all sw[23]*.dps files, heldout training consisted of all sw4[5, 6, 7, 8, 9]*.dps files and test consisted of all sw4[0, 1]*.mrg files. We now describe how the weights on the TAG productions described in subsection 2.2 are estimated from this training data. In order to estimate these weights we need to know the TAG derivation of each sentence in the training data. In order to uniquely determine this we need the not just the locations of each reparandum, interregnum and repai</context>
<context citStr="Charniak and Johnson (2001)" endWordPosition="4043" position="24057" startWordPosition="4039">air and reparandum (i.e., the repair string is itself repaired again). A single global analysis would not be able to capture this (since the TAG channel model does not permit the same substring to be both a reparandum and a repair), but we combine these overlapping repair substring analyses in a post-processing operation to yield an analysis of the whole sentence. (We do insist that the reparandum and interregnum of a repair do not overlap with those of any other repairs in the same analysis). 4 Evaluation This section describes how we evaluate our noisy model. As mentioned earlier, following Charniak and Johnson (2001) our test data consisted of all Penn III Switchboard tree-bank sw4[0- 1]*.mrg files. However, our test data differs from theirs in that in this test we deleted all partial words and punctuation from the data, as this results in a more realistic test situation. Since the immediate goal of this work is to produce a program that identifies the words of a sentence that belong to the reparandum of a repair construction (to a first approximation these words can be ignored in later processing), our evaluation focuses on the model’s performance in recovering the words in a reparandum. That is, the mod</context>
<context citStr="Charniak and Johnson (2001)" endWordPosition="4260" position="25307" startWordPosition="4257">each word in the sentence as belonging to a reparandum or not, and all other additional structure produced by the model is ignored. We measure model performance using standard precision p, recall r and f-score f, measures. If n, is the number of reparandum words the model correctly classified, nt is the number of true reparandum words given by the manual annotations and nm is the number of words the model predicts to be reparandum words, then the precision is n,/nm, recall is n,/nt, and f is 2pr/(p + r). For comparison we include the results of running the word-by-word classifier described in Charniak and Johnson (2001), but where partial words and punctuation have been removed from the training and test data. We also provide results for our noisy channel model using a bigram language model and a second trigram model where the twenty most likely analyses are rescored. Finally we show the results using the parser language model. CJ01' Bigram Trigram Parser Precision 0.951 0.776 0.774 0.820 Recall 0.631 0.736 0.763 0.778 F-score 0.759 0.756 0.768 0.797 The noisy channel model using a bigram language model does a slightly worse job at identifying reparandum and interregnum words than the classifier proposed in </context>
<context citStr="Charniak and Johnson (2001)" endWordPosition="4546" position="27042" startWordPosition="4542">g to investigate how to integrate this kind of model of speech repairs with probabilistic speech recognizers. There are other kinds of joint models of reparandum and repair that may produce a better reparandum detection system. We have experimented with versions of the models described above based on POS bi-tag dependencies rather than word bigram dependencies, but with results very close to those presented here. Still, more sophisticated models may yield better performance. It would also be interesting to combine this probabilistic model of speech repairs with the word classifier approach of Charniak and Johnson (2001). That approach may do so well because many speech repairs are very short, involving only one or two words Shriberg and Stolcke (1998), so the reparandum, interregnum and repair are all contained in the surrounding word window used as features by the classifier. On the other hand, the probabilistic model of repairs explored here seems to be most successful in identifying long repairs in which the reparandum and repair are similar enough to be unlikely to have been generated independently. Since the two approaches seem to have different strengths, a combined model may outperform both of them. R</context>
</contexts>
<marker>Charniak, Johnson, 2001</marker>
<rawString>Eugene Charniak and Mark Johnson. 2001. Edit detection and parsing for transcribed speech. In Proceedings of the 2nd Meeting of the North American Chapter of the Association for Computational Linguistics, pages 118–126. The Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Immediate-head parsing for language models.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics. The Association for Computational Linguistics.</booktitle>
<contexts>
<context citStr="Charniak (2001)" endWordPosition="355" position="2214" startWordPosition="354">but did not propose an actual model of repairs. Heeman and Allen (1999) describe a noisy channel model of speech repairs, but leave “extending the model to incorporate higher level syntactic ... processing” to future work. The previous work most closely related to the current work is Charniak and Johnson (2001), who used a boosted decision stub classifier to classify words as edited or not on a word by word basis, but do not identify or assign a probability to a repair as a whole. There are two innovations in this paper. First, we demonstrate that using a syntactic parser-based language model Charniak (2001) instead of bi/trigram language models significantly improves the accuracy of repair detection and correction. Second, we show how Tree Adjoining Grammars (TAGs) can be used to provide a precise formal description and probabilistic model of the crossed dependencies occurring in speech repairs. The rest of this paper is structured as follows. The next section describes the noisy channel model of speech repairs and the section after that explains how it can be applied to detect and repair speech repairs. Section 4 evaluates this model on the Penn 3 disfluency-tagged Switchboard corpus, and secti</context>
<context citStr="Charniak (2001)" endWordPosition="889" position="5416" startWordPosition="888"> reasons. The noisy channel model described here involves two components. A language model defines a probability distribution P(X) over the source sentences X, which do not contain repairs. The channel model defines a conditional probability distribution P(YIX) of surface sentences Y , which may contain repairs, given source sentences. In the work reported here, X is a word string and Y is a speech transcription not containing punctuation or partial words. We use two language models here: a bigram language model, which is used in the search process, and a syntactic parser-based language model Charniak (2001), which is used to rescore a set of the most likely analysis obtained using the bigram model. Because the language model is responsible for generating the well-formed sentence X, it is reasonable to expect that a language model that can model more global properties of sentences will lead to better performance, and the results presented here show that this is the case. The channel model is a stochastic TAG-based transducer; it is responsible for generating the repairs in the transcript Y , and it uses the ability of TAGs to straightforwardly model crossed dependencies. 2.1 Informal description </context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>Eugene Charniak. 2001. Immediate-head parsing for language models. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics. The Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR10-98,</tech>
<institution>Center for Research in Computing Technology, Harvard University.</institution>
<contexts>
<context citStr="Chen and Goodman (1998)" endWordPosition="2547" position="15310" startWordPosition="2544">.g., it results in aligning singular and plural nouns, present and past participles, etc. While we did not evaluate the quality of the alignments since they are not in themselves the object of this exercise, they seem to be fairly good. From our training data we estimate a number of conditional probability distributions. These estimated probability distributions are the linear interpolation of the corresponding empirical distributions from the main sub-corpus using various subsets of conditioning variables (e.g., bigram models are mixed with unigram models, etc.) using Chen’s bucketing scheme Chen and Goodman (1998). As is commonly done in language modelling, the interpolation coefficients are determined by maximizing the likelihood of the held out data counts using EM. Special care was taken to ensure that all distributions over words ranged over (and assigned non-zero probability to) every word that occurred in the training corpora; this turns out to be important as the size of the training data for the different distributions varies greatly. The first distribution is defined over the words in source sentences (i.e., that do not contain reparandums or interregnums). Pn(repair|W) is the probability of a</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR10-98, Center for Research in Computing Technology, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter A Heeman</author>
<author>James F Allen</author>
</authors>
<title>Speech repairs, intonational phrases, and discourse markers: Modeling speaker’s utterances in spoken dialogue.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context citStr="Heeman and Allen (1999)" endWordPosition="262" position="1670" startWordPosition="259">been studied by psycholinguists for some time, as far as we know this is the first time a probabilistic model of speech repairs based on a model of syntactic structure has been described in the literature. Probabilistic models have the advantage over other kinds of models that they can in principle be integrated with other probabilistic models to produce a combined model that uses all available evidence to select the globally optimal analysis. Shriberg and Stolcke (1998) studied the location and distribution of repairs in the Switchboard corpus, but did not propose an actual model of repairs. Heeman and Allen (1999) describe a noisy channel model of speech repairs, but leave “extending the model to incorporate higher level syntactic ... processing” to future work. The previous work most closely related to the current work is Charniak and Johnson (2001), who used a boosted decision stub classifier to classify words as edited or not on a word by word basis, but do not identify or assign a probability to a repair as a whole. There are two innovations in this paper. First, we demonstrate that using a syntactic parser-based language model Charniak (2001) instead of bi/trigram language models significantly imp</context>
</contexts>
<marker>Heeman, Allen, 1999</marker>
<rawString>Peter A. Heeman and James F. Allen. 1999. Speech repairs, intonational phrases, and discourse markers: Modeling speaker’s utterances in spoken dialogue. Computational Linguistics, 25(4):527–571.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Yves Schabes</author>
</authors>
<title>Synchronous tree-adjoining grammars.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th International Conference on Computational Linguistics (COLING</booktitle>
<pages>253--258</pages>
<contexts>
<context citStr="Shieber and Schabes (1990)" endWordPosition="1540" position="9160" startWordPosition="1537">se of a bigram language model. On the other hand, it seems desirable to use a language model that is sensitive to more global properties of the sentence, and we do this by reranking the initial analysis, replacing the bigram language model with a syntactic parser based model. We do not need to intersect this parser based language model with our TAG channel model since we evaluate each analysis separately. 2.2 The TAG channel model The TAG channel model defines a stochastic mapping of source sentences X into observed sentences Y . There are several ways to define transducers using TAGs such as Shieber and Schabes (1990), but the following simple method, inspired by finite-state transducers, suffices for the application here. The TAG defines a language whose vocabulary is the set of pairs (EU101)x(EU101), where E is the vocabulary of the observed sentences Y . A string Z in this language can be interpreted as a pair of strings (Y, X), where Y is the concatenation of the projection of the first components of Z and X is the concatenation of the projection of the second components. For example, the string Z = a:a flight:flight to:0 Boston:0 uh:0 I:0 mean:0 to:to Denver:Denver on:on Friday:Friday corresponds to t</context>
</contexts>
<marker>Shieber, Schabes, 1990</marker>
<rawString>Stuart M. Shieber and Yves Schabes. 1990. Synchronous tree-adjoining grammars. In Proceedings of the 13th International Conference on Computational Linguistics (COLING 1990), pages 253–258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>Evidence against the Context-Freeness of natural language.</title>
<date>1985</date>
<journal>Linguistics and Philosophy,</journal>
<volume>8</volume>
<issue>3</issue>
<contexts>
<context citStr="Shieber (1985)" endWordPosition="696" position="4249" startWordPosition="695">nguages with an unbounded number of crossed dependencies cannot be described by a context-free or finitestate grammar, and crossed dependencies like these have been used to argue natural languages ... a flight to Boston, uh, I mean, to Denver on Friday ... � Y J � Y J � Y J Reparandum Interregnum Repair Figure 1: The structure of a typical repair, with crossing dependencies between reparandum and repair. Figure 2: The “helical” dependency structure induced by the generative model of speech repairs for the repair depicted in Figure 1. a flight to Denver on Friday to Boston are not context-free Shieber (1985). Mildly context-sensitive grammars, such as Tree Adjoining Grammars (TAGs) and Combinatory Categorial Grammars, can describe such crossing dependencies, and that is why TAGs are used here. Figure 2 shows the combined model’s dependency structure for the repair of Figure 1. Interestingly, if we trace the temporal word string through this dependency structure, aligning words next to the words they are dependent on, we obtain a “helical” type of structure familiar from genome models, and in fact TAGs are being used to model genomes for very similar reasons. The noisy channel model described here</context>
</contexts>
<marker>Shieber, 1985</marker>
<rawString>Stuart M. Shieber. 1985. Evidence against the Context-Freeness of natural language. Linguistics and Philosophy, 8(3):333–344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Shriberg</author>
<author>Andreas Stolcke</author>
</authors>
<title>How far do speakers back up in repairs? a quantitative model.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<volume>5</volume>
<pages>2183--2186</pages>
<location>Sydney, Australia.</location>
<contexts>
<context citStr="Shriberg and Stolcke (1998)" endWordPosition="236" position="1522" startWordPosition="233">s. This paper presents an explicit generative model of speech repairs and shows how it can eliminate this kind of disfluency. While speech repairs have been studied by psycholinguists for some time, as far as we know this is the first time a probabilistic model of speech repairs based on a model of syntactic structure has been described in the literature. Probabilistic models have the advantage over other kinds of models that they can in principle be integrated with other probabilistic models to produce a combined model that uses all available evidence to select the globally optimal analysis. Shriberg and Stolcke (1998) studied the location and distribution of repairs in the Switchboard corpus, but did not propose an actual model of repairs. Heeman and Allen (1999) describe a noisy channel model of speech repairs, but leave “extending the model to incorporate higher level syntactic ... processing” to future work. The previous work most closely related to the current work is Charniak and Johnson (2001), who used a boosted decision stub classifier to classify words as edited or not on a word by word basis, but do not identify or assign a probability to a repair as a whole. There are two innovations in this pap</context>
</contexts>
<marker>Shriberg, Stolcke, 1998</marker>
<rawString>Elizabeth Shriberg and Andreas Stolcke. 1998. How far do speakers back up in repairs? a quantitative model. In Proceedings of the International Conference on Spoken Language Processing, volume 5, pages 2183–2186, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Shriberg</author>
</authors>
<title>Preliminaries to a Theory of Speech Disfluencies.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California, Berkeley.</institution>
<contexts>
<context citStr="Shriberg (1994)" endWordPosition="474" position="2916" startWordPosition="473">tection and correction. Second, we show how Tree Adjoining Grammars (TAGs) can be used to provide a precise formal description and probabilistic model of the crossed dependencies occurring in speech repairs. The rest of this paper is structured as follows. The next section describes the noisy channel model of speech repairs and the section after that explains how it can be applied to detect and repair speech repairs. Section 4 evaluates this model on the Penn 3 disfluency-tagged Switchboard corpus, and section 5 concludes and discusses future work. 2 A noisy channel model of repairs We follow Shriberg (1994) and most other work on speech repairs by dividing a repair into three parts: the reparandum (the material repaired), the interregnum that is typically either empty or consists of a filler, and the repair. Figure 1 shows these three parts for a typical repair. Most current probabilistic language models are based on HMMs or PCFGs, which induce linear or tree-structured dependencies between words. The relationship between reparandum and repair seems to be quite different: the repair is a “rough copy” of the reparandum, often incorporating the same or very similar words in roughly the same word o</context>
</contexts>
<marker>Shriberg, 1994</marker>
<rawString>Elizabeth Shriberg. 1994. Preliminaries to a Theory of Speech Disfluencies. Ph.D. thesis, University of California, Berkeley.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>