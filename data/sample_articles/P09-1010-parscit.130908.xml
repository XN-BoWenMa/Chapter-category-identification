<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant confidence="0.014312" no="0">
<title confidence="0.995877">
Reinforcement Learning for Mapping Instructions to Actions
</title>
<author confidence="0.998661">
S.R.K. Branavan, Harr Chen, Luke S. Zettlemoyer, Regina Barzilay
</author>
<affiliation confidence="0.9981495">
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
</affiliation>
<email confidence="0.988324">
{branavan, harr, lsz, regina}@csail.mit.edu
</email>
<sectionHeader confidence="0.993588" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999872157894737">In this paper, we present a reinforcement learning approach for mapping natural language instructions to sequences of executable actions. We assume access to a reward function that defines the quality of the executed actions. During training, the learner repeatedly constructs action sequences for a set of documents, executes those actions, and observes the resulting reward. We use a policy gradient algorithm to estimate the parameters of a log-linear model for action selection. We apply our method to interpret instructions in two domains — Windows troubleshooting guides and game tutorials. Our results demonstrate that this method can rival supervised learning techniques while requiring few or no annotated training examples.1</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999840866666667">The problem of interpreting instructions written in natural language has been widely studied since the early days of artificial intelligence (Winograd, 1972; Di Eugenio, 1992). Mapping instructions to a sequence of executable actions would enable the automation of tasks that currently require human participation. Examples include configuring software based on how-to guides and operating simulators using instruction manuals. In this paper, we present a reinforcement learning framework for inducing mappings from text to actions without the need for annotated training examples. For concreteness, consider instructions from a Windows troubleshooting guide on deleting temporary folders, shown in Figure 1. We aim to map this text to the corresponding low-level commands and parameters.</bodyText>
<footnote confidence="0.982809">
1Code, data, and annotations used in this work are avail-
able at http://groups.csail.mit.edu/rbg/code/rl/
</footnote>
<figureCaption confidence="0.986414">
Figure 1: A Windows troubleshooting article de-
scribing how to remove the “msdownld.tmp” tem-
porary folder.
</figureCaption>
<bodyText confidence="0.999881555555556">For example, properly interpreting the third instruction requires clicking on a tab, finding the appropriate option in a tree control, and clearing its associated checkbox. In this and many other applications, the validity of a mapping can be verified by executing the induced actions in the corresponding environment and observing their effects. For instance, in the example above we can assess whether the goal described in the instructions is achieved, i.e., the folder is deleted. The key idea of our approach is to leverage the validation process as the main source of supervision to guide learning. This form of supervision allows us to learn interpretations of natural language instructions when standard supervised techniques are not applicable, due to the lack of human-created annotations. Reinforcement learning is a natural framework for building models using validation from an environment (Sutton and Barto, 1998). We assume that supervision is provided in the form of a reward function that defines the quality of executed actions. During training, the learner repeatedly constructs action sequences for a set of given documents, executes those actions, and observes the resulting reward. The learner’s goal is to estimate a policy — a distribution over actions given instruction text and environment state — that maximizes future expected reward.</bodyText>
<page confidence="0.988102">
82
</page>
<note confidence="0.9996105">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 82–90,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999673">Our policy is modeled in a log-linear fashion, allowing us to incorporate features of both the instruction text and the environment. We employ a policy gradient algorithm to estimate the parameters of this model. We evaluate our method on two distinct applications: Windows troubleshooting guides and puzzle game tutorials. The key findings of our experiments are twofold. First, models trained only with simple reward signals achieve surprisingly high results, coming within 11% of a fully supervised method in the Windows domain. Second, augmenting unlabeled documents with even a small fraction of annotated examples greatly reduces this performance gap, to within 4% in that domain. These results indicate the power of learning from this new form of automated supervision.</bodyText>
<sectionHeader confidence="0.999782" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998654175">Grounded Language Acquisition Our work fits into a broader class of approaches that aim to learn language from a situated context (Mooney, 2008a; Mooney, 2008b; Fleischman and Roy, 2005; Yu and Ballard, 2004; Siskind, 2001; Oates, 2001). Instances of such approaches include work on inferring the meaning of words from video data (Roy and Pentland, 2002; Barnard and Forsyth, 2001), and interpreting the commentary of a simulated soccer game (Chen and Mooney, 2008). Most of these approaches assume some form of parallel data, and learn perceptual cooccurrence patterns. In contrast, our emphasis is on learning language by proactively interacting with an external environment. Reinforcement Learning for Language Processing Reinforcement learning has been previously applied to the problem of dialogue management (Scheffler and Young, 2002; Roy et al., 2000; Litman et al., 2000; Singh et al., 1999). These systems converse with a human user by taking actions that emit natural language utterances. The reinforcement learning state space encodes information about the goals of the user and what they say at each time step. The learning problem is to find an optimal policy that maps states to actions, through a trial-and-error process of repeated interaction with the user. Reinforcement learning is applied very differently in dialogue systems compared to our setup. In some respects, our task is more easily amenable to reinforcement learning. For instance, we are not interacting with a human user, so the cost of interaction is lower. However, while the state space can be designed to be relatively small in the dialogue management task, our state space is determined by the underlying environment and is typically quite large. We address this complexity by developing a policy gradient algorithm that learns efficiently while exploring a small subset of the states.</bodyText>
<sectionHeader confidence="0.981818" genericHeader="method">
3 Problem Formulation
</sectionHeader>
<bodyText confidence="0.9999208">Our task is to learn a mapping between documents and the sequence of actions they express. Figure 2 shows how one example sentence is mapped to three actions. Mapping Text to Actions As input, we are given a document d, comprising a sequence of sentences (u1, ... , ut), where each ui is a sequence of words. Our goal is to map d to a sequence of actions a� = (a0, ... , a,1). Actions are predicted and executed sequentially.2 An action a = (c, R, W') encompasses a command c, the command’s parameters R, and the words W' specifying c and R. Elements of R refer to objects available in the environment state, as described below. Some parameters can also refer to words in document d. Additionally, to account for words that do not describe any actions, c can be a null command. The Environment The environment state £ specifies the set of objects available for interaction, and their properties. In Figure 2, £ is shown on the right. The environment state £ changes in response to the execution of command c with parameters R according to a transition distribution p(£'J£, c, R). This distribution is a priori unknown to the learner. As we will see in Section 5, our approach avoids having to directly estimate this distribution. State To predict actions sequentially, we need to track the state of the document-to-actions mapping over time. A mapping state s is a tuple (£, d, j, W), where £ refers to the current environment state; j is the index of the sentence currently being interpreted in document d; and W contains words that were mapped by previous actions for the same sentence.</bodyText>
<footnote confidence="0.98649">
2That is, action ai is executed before ai+1 is predicted.
</footnote>
<page confidence="0.999182">
83
</page>
<figureCaption confidence="0.9576425">
Figure 2: A three-step mapping from an instruction sentence to a sequence of actions in Windows 2000.
For each step, the figure shows the words selected by the action, along with the corresponding system
command and its parameters. The words of W' are underlined, and the words of W are highlighted in
grey.
</figureCaption>
<bodyText confidence="0.997306758620689">The mapping state s is observed after each action. The initial mapping state s0 for document d is (£d, d, 0, 0); £d is the unique starting environment state for d. Performing action a in state s = (£, d, j, W) leads to a new state s' according to distribution p(s'|s, a), defined as follows: £ transitions according to p(£'|£, c, R), W is updated with a’s selected words, and j is incremented if all words of the sentence have been mapped. For the applications we consider in this work, environment state transitions, and consequently mapping state transitions, are deterministic. Training During training, we are provided with a set D of documents, the ability to sample from the transition distribution, and a reward function r(h). Here, h = (s0, a0, ... , sn−1, an−1, sn) is a history of states and actions visited while interpreting one document.r(h) outputs a realvalued score that correlates with correct action selection.3 We consider both immediate reward, which is available after each action, and delayed reward, which does not provide feedback until the last action. For example, task completion is a delayed reward that produces a positive value after the final action only if the task was completed successfully. We will also demonstrate how manually annotated action sequences can be incorporated into the reward.</bodyText>
<footnote confidence="0.9942665">
3In most reinforcement learning problems, the reward
function is defined over state-action pairs, as r(s, a) — in this
case, r(h) _ Et r(st, at), and our formulation becomes a
standard finite-horizon Markov decision process. Policy gra-
dient approaches allow us to learn using the more general
case of history-based reward.
</footnote>
<bodyText confidence="0.9993076">The goal of training is to estimate parameters 0 of the action selection distribution p(a|s, 0), called the policy. Since the reward correlates with action sequence correctness, the 0 that maximizes expected reward will yield the best actions.</bodyText>
<sectionHeader confidence="0.995426" genericHeader="method">
4 A Log-Linear Model for Actions
</sectionHeader>
<bodyText confidence="0.999987133333333">Our goal is to predict a sequence of actions. We construct this sequence by repeatedly choosing an action given the current mapping state, and applying that action to advance to a new state. Given a state s = (£, d, j, W), the space of possible next actions is defined by enumerating subspans of unused words in the current sentence (i.e., subspans of the jth sentence of d not in W), and the possible commands and parameters in environment state £.4 We model the policy distribution p(a|s; 0) over this action space in a log-linear fashion (Della Pietra et al., 1997; Lafferty et al., 2001), giving us the flexibility to incorporate a diverse range of features. Under this representation, the policy distribution is:</bodyText>
<equation confidence="0.9905">
ee-�(s,a)
p(a|s; 0) = 1: ee-0(s,a') , (1)
a/
</equation>
<bodyText confidence="0.999964333333333">where 0(s, a) E Rn is an n-dimensional feature representation. During test, actions are selected according to the mode of this distribution.</bodyText>
<footnote confidence="0.9980725">
4For parameters that refer to words, the space of possible
values is defined by the unused words in the current sentence.
</footnote>
<page confidence="0.999133">
84
</page>
<sectionHeader confidence="0.990473" genericHeader="method">
5 Reinforcement Learning
</sectionHeader>
<bodyText confidence="0.999725833333333">During training, our goal is to find the optimal policy p(a|s; θ). Since reward correlates with correct action selection, a natural objective is to maximize expected future reward — that is, the reward we expect while acting according to that policy from state s. Formally, we maximize the value function:</bodyText>
<equation confidence="0.999174">
Vθ(s) = Ep(h|θ) [r(h)] , (2)
</equation>
<bodyText confidence="0.992348666666667">where the history h is the sequence of states and actions encountered while interpreting a single document d E D. This expectation is averaged over all documents in D. The distribution p(h|θ) returns the probability of seeing history h when starting from state s and acting according to a policy with parameters θ. This distribution can be decomposed into a product over time steps: Input: A document set D, Feature representation φ, Reward function r(h), Number of iterations T</bodyText>
<figure confidence="0.792341333333333">
Initialization: Set θ to small random values.
1 for i = 1 ... T do
2 foreach d ∈ D do
3 Sample history h ∼ p(h|θ) where
h = (s0, a0, ... , an−1, sn) as follows:
3a fort = 0 ... n − 1 do
3b Sample action at ∼ p(a|st; θ)
3c Execute at on state st: st+1 ∼ p(s|st, at)
end
A ← E �φ(st, at) − Ea, φ(st, a�)p(a�|st; θ)�
4 t
5 θ ← θ + r(h)A
end
end
Output: Estimate of parameters θ
</figure>
<figureCaption confidence="0.575715">
Algorithm 1: A policy gradient algorithm.
</figureCaption>
<equation confidence="0.978055">
p(h|θ) = n−1H p(at|st; θ)p(st+1|st, at). (3)
t=0
</equation>
<subsectionHeader confidence="0.993994">
5.1 A Policy Gradient Algorithm
</subsectionHeader>
<bodyText confidence="0.9999962">Our reinforcement learning problem is to find the parameters θ that maximize Vθ from equation 2. Although there is no closed form solution, policy gradient algorithms (Sutton et al., 2000) estimate the parameters θ by performing stochastic gradient ascent. The gradient of Vθ is approximated by interacting with the environment, and the resulting reward is used to update the estimate of θ. Policy gradient algorithms optimize a non-convex objective and are only guaranteed to find a local optimum. However, as we will see, they scale to large state spaces and can perform well in practice. To find the parameters θ that maximize the objective, we first compute the derivative of Vθ. Expanding according to the product rule, we have:</bodyText>
<equation confidence="0.998555333333333">
� ∂I: ∂
∂θVθ(s) = Ep(h|θ) r(h) ∂θ log p(at|st; θ) ,
t
</equation>
<bodyText confidence="0.999988666666667">where the inner sum is over all time steps t in the current history h. Expanding the inner partial derivative we observe that:</bodyText>
<equation confidence="0.883454">
∂ I:
∂θ log p(a|s; θ) = φ(s, a)−
a/
</equation>
<bodyText confidence="0.998913485714286">which is the derivative of a log-linear distribution. Equation 5 is easy to compute directly. However, the complete derivative of Vθ in equation 4 is intractable, because computing the expectation would require summing over all possible histories. Instead, policy gradient algorithms employ stochastic gradient ascent by computing a noisy estimate of the expectation using just a subset of the histories. Specifically, we draw samples from p(h|θ) by acting in the target environment, and use these samples to approximate the expectation in equation 4. In practice, it is often sufficient to sample a single history h for this approximation. Algorithm 1 details the complete policy gradient algorithm. It performs T iterations over the set of documents D. Step 3 samples a history that maps each document to actions. This is done by repeatedly selecting actions according to the current policy, and updating the state by executing the selected actions. Steps 4 and 5 compute the empirical gradient and update the parameters θ. In many domains, interacting with the environment is expensive. Therefore, we use two techniques that allow us to take maximum advantage of each environment interaction. First, a history h = (s0, a0, ... , sn) contains subsequences (si, ai,... sn) for i = 1 to n − 1, each with its own reward value given by the environment as a side effect of executing h. We apply the update from equation 5 for each subsequence. Second, for a sampled history h, we can propose alternative histories h' that result in the same commands and parameters with different word spans. We can again apply equation 5 for each h', weighted by its probability under the current policy, p(h�|θ)</bodyText>
<equation confidence="0.9526205">
p(h|θ) .
φ(s, a')p(a'|s; θ),
</equation>
<page confidence="0.985481">
85
</page>
<bodyText confidence="0.99994">The algorithm we have presented belongs to a family of policy gradient algorithms that have been successfully used for complex tasks such as robot control (Ng et al., 2003). Our formulation is unique in how it represents natural language in the reinforcement learning framework.</bodyText>
<subsectionHeader confidence="0.999727">
5.2 Reward Functions and ML Estimation
</subsectionHeader>
<bodyText confidence="0.999975466666667">We can design a range of reward functions to guide learning, depending on the availability of annotated data and environment feedback. Consider the case when every training document d E D is annotated with its correct sequence of actions, and state transitions are deterministic. Given these examples, it is straightforward to construct a reward function that connects policy gradient to maximum likelihood. Specifically, define a reward function r(h) that returns one when h matches the annotation for the document being analyzed, and zero otherwise. Policy gradient performs stochastic gradient ascent on the objective from equation 2, performing one update per document. For document d, this objective becomes:</bodyText>
<equation confidence="0.997082">
Ep(h|e)[r(h)] = � r(h)p(h|e) = p(hd|e),
h
</equation>
<bodyText confidence="0.9999694">where hd is the history corresponding to the annotated action sequence. Thus, with this reward policy gradient is equivalent to stochastic gradient ascent with a maximum likelihood objective. At the other extreme, when annotations are completely unavailable, learning is still possible given informative feedback from the environment. Crucially, this feedback only needs to correlate with action sequence quality. We detail environment-based reward functions in the next section. As our results will show, reward functions built using this kind of feedback can provide strong guidance for learning. We will also consider reward functions that combine annotated supervision with environment feedback.</bodyText>
<sectionHeader confidence="0.996134" genericHeader="method">
6 Applying the Model
</sectionHeader>
<bodyText confidence="0.99997">We study two applications of our model: following instructions to perform software tasks, and solving a puzzle game using tutorial guides.</bodyText>
<subsectionHeader confidence="0.993068">
6.1 Microsoft Windows Help and Support
</subsectionHeader>
<bodyText confidence="0.53819">On its Help and Support website,5 Microsoft publishes a number of articles describing how to perform tasks and troubleshoot problems in the Windows operating systems.</bodyText>
<footnote confidence="0.693947">
5support.microsoft.com
</footnote>
<table confidence="0.988745571428571">
Notation
o Parameter referring to an environment object
L Set of object class names (e.g. “button”)
V Vocabulary
Features on W and object o
Test if o is visible in s
Test if o has input focus
Test if o is in the foreground
Test if o was previously interacted with
Test if o came into existence since last action
Min. edit distance between w E W and object labels in s
Features on words in W, command c, and object o
`dc' E C, w E V : test if c' = c and w E W
`dc' E C, l E L: test if c' = c and l is the class of o
</table>
<tableCaption confidence="0.999887333333333">
Table 1: Example features in the Windows do-
main. All features are binary, except for the nor-
malized edit distance which is real-valued.
</tableCaption>
<bodyText confidence="0.999838333333333">Examples of such tasks include installing patches and changing security settings. Figure 1 shows one such article. Our goal is to automatically execute these support articles in the Windows 2000 environment. Here, the environment state is the set of visible user interface (UI) objects, and object properties such as label, location, and parent window. Possible commands include left-click, right-click, double-click, and type-into, all of which take a UI object as a parameter; type-into additionally requires a parameter for the input text. Table 1 lists some of the features we use for this domain. These features capture various aspects of the action under consideration, the current Windows UI state, and the input instructions. For example, one lexical feature measures the similarity of a word in the sentence to the UI labels of objects in the environment. Environment-specific features, such as whether an object is currently in focus, are useful when selecting the object to manipulate. In total, there are 4,438 features. Reward Function Environment feedback can be used as a reward function in this domain. An obvious reward would be task completion (e.g., whether the stated computer problem was fixed). Unfortunately, verifying task completion is a challenging system issue in its own right. Instead, we rely on a noisy method of checking whether execution can proceed from one sentence to the next: at least one word in each sentence has to correspond to an object in the environment.6 For instance, in the sentence from Figure 2 the word “Run” matches the Run... menu item.</bodyText>
<page confidence="0.997109">
86
</page>
<figureCaption confidence="0.99993125">
Figure 3: Crossblock puzzle with tutorial. For this
level, four squares in a row or column must be re-
moved at once. The first move specified by the
tutorial is greyed in the puzzle.
</figureCaption>
<bodyText confidence="0.999962875">If no words in a sentence match a current environment object, then one of the previous sentences was analyzed incorrectly. In this case, we assign the history a reward of -1. This reward is not guaranteed to penalize all incorrect histories, because there may be false positive matches between the sentence and the environment. When at least one word matches, we assign a positive reward that linearly increases with the percentage of words assigned to non-null commands, and linearly decreases with the number of output actions. This reward signal encourages analyses that interpret all of the words without producing spurious actions.</bodyText>
<subsectionHeader confidence="0.997566">
6.2 Crossblock: A Puzzle Game
</subsectionHeader>
<bodyText confidence="0.999953625">Our second application is to a puzzle game called Crossblock, available online as a Flash game.7 Each of 50 puzzles is played on a grid, where some grid positions are filled with squares. The object of the game is to clear the grid by drawing vertical or horizontal line segments that remove groups of squares. Each segment must exactly cross a specific number of squares, ranging from two to seven depending on the puzzle. Humans players have found this game challenging and engaging enough to warrant posting textual tutorials.8 A sample puzzle and tutorial are shown in Figure 3. The environment is defined by the state of the grid. The only command is clear, which takes a parameter specifying the orientation (row or column) and grid location of the line segment to be removed.</bodyText>
<footnote confidence="0.9943106">
6We assume that a word maps to an environment object if
the edit distance between the word and the object’s name is
below a threshold value.
7hexaditidom.deviantart.com/art/Crossblock-108669149
8www.jayisgames.com/archives/2009/01/crossblock.php
</footnote>
<bodyText confidence="0.99988928">The challenge in this domain is to segment the text into the phrases describing each action, and then correctly identify the line segments from references such as “the bottom four from the second column from the left.” For this domain, we use two sets of binary features on state-action pairs (s, a). First, for each vocabulary word w, we define a feature that is one if w is the last word of a’s consumed words W'. These features help identify the proper text segmentation points between actions. Second, we introduce features for pairs of vocabulary word w and attributes of action a, e.g., the line orientation and grid locations of the squares that a would remove. This set of features enables us to match words (e.g., “row”) with objects in the environment (e.g., a move that removes a horizontal series of squares). In total, there are 8,094 features. Reward Function For Crossblock it is easy to directly verify task completion, which we use as the basis of our reward function. The reward r(h) is -1 if h ends in a state where the puzzle cannot be completed. For solved puzzles, the reward is a positive value proportional to the percentage of words assigned to non-null commands.</bodyText>
<sectionHeader confidence="0.991965" genericHeader="evaluation">
7 Experimental Setup
</sectionHeader>
<bodyText confidence="0.9998925">Datasets For the Windows domain, our dataset consists of 128 documents, divided into 70 for training, 18 for development, and 40 for test. In the puzzle game domain, we use 50 tutorials, divided into 40 for training and 10 for test.9 Statistics for the datasets are shown below.</bodyText>
<table confidence="0.998719142857143">
Windows Puzzle
Total # of documents 128 50
Total # of words 5562 994
Vocabulary size 610 46
Avg. words per sentence 9.93 19.88
Avg. sentences per document 4.38 1.00
Avg. actions per document 10.37 5.86
</table>
<bodyText confidence="0.999305166666667">The data exhibits certain qualities that make for a challenging learning problem. For instance, there are a surprising variety of linguistic constructs — as Figure 4 shows, in the Windows domain even a simple command is expressed in at least six different ways.</bodyText>
<footnote confidence="0.999704333333333">
9For Crossblock, because the number of puzzles is lim-
ited, we did not hold out a separate development set, and re-
port averaged results over five training/test splits.
</footnote>
<page confidence="0.998757">
87
</page>
<figureCaption confidence="0.999792">
Figure 4: Variations of “click internet options on
the tools menu” present in the Windows corpus.
</figureCaption>
<bodyText confidence="0.999800972222222">Experimental Framework To apply our algorithm to the Windows domain, we use the Win32 application programming interface to simulate human interactions with the user interface, and to gather environment state information. The operating system environment is hosted within a virtual machine,10 allowing us to rapidly save and reset system state snapshots. For the puzzle game domain, we replicated the game with an implementation that facilitates automatic play. As is commonly done in reinforcement learning, we use a softmax temperature parameter to smooth the policy distribution (Sutton and Barto, 1998), set to 0.1 in our experiments. For Windows, the development set is used to select the best parameters. For Crossblock, we choose the parameters that produce the highest reward during training. During evaluation, we use these parameters to predict mappings for the test documents. Evaluation Metrics For evaluation, we compare the results to manually constructed sequences of actions. We measure the number of correct actions, sentences, and documents. An action is correct if it matches the annotations in terms of command and parameters. A sentence is correct if all of its actions are correctly identified, and analogously for documents.11 Statistical significance is measured with the sign test. Additionally, we compute a word alignment score to investigate the extent to which the input text is used to construct correct analyses. This score measures the percentage of words that are aligned to the corresponding annotated actions in correctly analyzed documents. Baselines We consider the following baselines to characterize the performance of our approach.</bodyText>
<footnote confidence="0.991414">
10VMware Workstation, available at www.vmware.com
11In these tasks, each action depends on the correct execu-
tion of all previous actions, so a single error can render the
remainder of that document’s mapping incorrect. In addition,
due to variability in document lengths, overall action accu-
racy is not guaranteed to be higher than document accuracy.
</footnote>
<listItem confidence="0.986882">• Full Supervision Sequence prediction problems like ours are typically addressed using supervised techniques.</listItem>
<bodyText confidence="0.996797222222222">We measure how a standard supervised approach would perform on this task by using a reward signal based on manual annotations of output action sequences, as defined in Section 5.2. As shown there, policy gradient with this reward is equivalent to stochastic gradient ascent with a maximum likelihood objective.</bodyText>
<listItem confidence="0.999394">• Partial Supervision We consider the case when only a subset of training documents is annotated, and environment reward is used for the remainder.</listItem>
<bodyText confidence="0.99894125">Our method seamlessly combines these two kinds of rewards.</bodyText>
<listItem confidence="0.995976">• Random and Majority (Windows) We consider two naive baselines.</listItem>
<bodyText confidence="0.99977925">Both scan through each sentence from left to right. A command c is executed on the object whose name is encountered first in the sentence. This command c is either selected randomly, or set to the majority command, which is leftclick. This procedure is repeated until no more words match environment objects.</bodyText>
<listItem confidence="0.998057">• Random (Puzzle) We consider a baseline that randomly selects among the actions that are valid in the current game state.12</listItem>
<sectionHeader confidence="0.999805" genericHeader="result">
8 Results
</sectionHeader>
<bodyText confidence="0.999944888888889">Table 2 presents evaluation results on the test sets. There are several indicators of the difficulty of this task. The random and majority baselines’ poor performance in both domains indicates that naive approaches are inadequate for these tasks. The performance of the fully supervised approach provides further evidence that the task is challenging. This difficulty can be attributed in part to the large branching factor of possible actions at each step — on average, there are 27.14 choices per action in the Windows domain, and 9.78 in the Crossblock domain. In both domains, the learners relying only on environment reward perform well. Although the fully supervised approach performs the best, adding just a few annotated training examples to the environment-based learner significantly reduces the performance gap.</bodyText>
<footnote confidence="0.931098">
12Since action selection is among objects, there is no natu-
ral majority baseline for the puzzle.
</footnote>
<page confidence="0.996145">
88
</page>
<table confidence="0.999852571428572">
Windows Puzzle
Action Sent. Doc. Word Action Doc. Word
Random baseline 0.128 0.101 0.000 —– 0.081 0.111 —–
Majority baseline 0.287 0.197 0.100 —– —– —– —–
Environment reward ∗ 0.647 ∗ 0.590 ∗ 0.375 0.819 ∗ 0.428 ∗ 0.453 0.686
Partial supervision *0.723 ∗ 0.702 0.475 0.989 0.575 ∗ 0.523 0.850
Full supervision *0.756 0.714 0.525 0.991 0.632 0.630 0.869
</table>
<tableCaption confidence="0.982859857142857">
Table 2: Performance on the test set with different reward signals and baselines. Our evaluation measures
the proportion of correct actions, sentences, and documents. We also report the percentage of correct
word alignments for the successfully completed documents. Note the puzzle domain has only single-
sentence documents, so its sentence and document scores are identical. The partial supervision line
refers to 20 out of 70 annotated training documents for Windows, and 10 out of 40 for the puzzle. Each
result marked with ∗ or o is a statistically significant improvement over the result immediately above it;
∗ indicates p &lt; 0.01 and o indicates p &lt; 0.05.
</tableCaption>
<figureCaption confidence="0.999419">
Figure 5: Comparison of two training scenarios where training is done using a subset of annotated
documents, with and without environment reward for the remaining unannotated documents.
</figureCaption>
<bodyText confidence="0.994158809523809">Figure 5 shows the overall tradeoff between annotation effort and system performance for the two domains. The ability to make this tradeoff is one of the advantages of our approach. The figure also shows that augmenting annotated documents with additional environment-reward documents invariably improves performance. The word alignment results from Table 2 indicate that the learners are mapping the correct words to actions for documents that are successfully completed. For example, the models that perform best in the Windows domain achieve nearly perfect word alignment scores. To further assess the contribution of the instruction text, we train a variant of our model without access to text features. This is possible in the game domain, where all of the puzzles share a single goal state that is independent of the instructions. This variant solves 34% of the puzzles, suggesting that access to the instructions significantly improves performance.</bodyText>
<sectionHeader confidence="0.998869" genericHeader="conclusion">
9 Conclusions
</sectionHeader>
<bodyText confidence="0.999974714285714">In this paper, we presented a reinforcement learning approach for inducing a mapping between instructions and actions. This approach is able to use environment-based rewards, such as task completion, to learn to analyze text. We showed that having access to a suitable reward function can significantly reduce the need for annotations.</bodyText>
<sectionHeader confidence="0.998361" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999924090909091">The authors acknowledge the support of the NSF (CAREER grant IIS-0448168, grant IIS-0835445, grant IIS-0835652, and a Graduate Research Fellowship) and the ONR. Thanks to Michael Collins, Amir Globerson, Tommi Jaakkola, Leslie Pack Kaelbling, Dina Katabi, Martin Rinard, and members of the MIT NLP group for their suggestions and comments. Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors, and do not necessarily reflect the views of the funding organizations.</bodyText>
<page confidence="0.999205">
89
</page>
<bodyText confidence="0.998977">Jeffrey Mark Siskind. 2001. Grounding the lexical semantics of verbs in visual perception using force dynamics and event logic. J. Artif. Intell. Res. (JAIR), 15:31–90.</bodyText>
<sectionHeader confidence="0.94337" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997651596774194">
Kobus Barnard and David A. Forsyth. 2001. Learning
the semantics of words and pictures. In Proceedings
of ICCV.
David L. Chen and Raymond J. Mooney. 2008. Learn-
ing to sportscast: a test of grounded language acqui-
sition. In Proceedings of ICML.
Stephen Della Pietra, Vincent J. Della Pietra, and
John D. Lafferty. 1997. Inducing features of ran-
dom fields. IEEE Trans. Pattern Anal. Mach. Intell.,
19(4):380–393.
Barbara Di Eugenio. 1992. Understanding natural lan-
guage instructions: the case of purpose clauses. In
Proceedings of ACL.
Michael Fleischman and Deb Roy. 2005. Intentional
context in situated language learning. In Proceed-
ings of CoNLL.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML.
Diane J. Litman, Michael S. Kearns, Satinder Singh,
and Marilyn A. Walker. 2000. Automatic optimiza-
tion of dialogue management. In Proceedings of
COLING.
Raymond J. Mooney. 2008a. Learning language
from its perceptual context. In Proceedings of
ECML/PKDD.
Raymond J. Mooney. 2008b. Learning to connect lan-
guage and perception. In Proceedings ofAAAI.
Andrew Y. Ng, H. Jin Kim, Michael I. Jordan, and
Shankar Sastry. 2003. Autonomous helicopter flight
via reinforcement learning. In Advances in NIPS.
James Timothy Oates. 2001. Grounding knowledge
in sensors: Unsupervised learning for language and
planning. Ph.D. thesis, University of Massachusetts
Amherst.
Deb K. Roy and Alex P. Pentland. 2002. Learn-
ing words from sights and sounds: a computational
model. Cognitive Science 26, pages 113–146.
Nicholas Roy, Joelle Pineau, and Sebastian Thrun.
2000. Spoken dialogue management using proba-
bilistic reasoning. In Proceedings of ACL.
Konrad Scheffler and Steve Young. 2002. Automatic
learning of dialogue strategy using dialogue simula-
tion and reinforcement learning. In Proceedings of
HLT.
Satinder P. Singh, Michael J. Kearns, Diane J. Litman,
and Marilyn A. Walker. 1999. Reinforcement learn-
ing for spoken dialogue systems. In Advances in
NIPS.
Richard S. Sutton and Andrew G. Barto. 1998. Re-
inforcement Learning: An Introduction. The MIT
Press.
Richard S. Sutton, David McAllester, Satinder Singh,
and Yishay Mansour. 2000. Policy gradient meth-
ods for reinforcement learning with function approx-
imation. In Advances in NIPS.
Terry Winograd. 1972. Understanding Natural Lan-
guage. Academic Press.
Chen Yu and Dana H. Ballard. 2004. On the integra-
tion of grounding language and learning objects. In
Proceedings ofAAAI.
</reference>
<page confidence="0.998636">
90
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant confidence="0.678906" no="0">
<title confidence="0.999363">Reinforcement Learning for Mapping Instructions to Actions</title>
<author confidence="0.999251">S R K Branavan</author>
<author confidence="0.999251">Harr Chen</author>
<author confidence="0.999251">Luke S Zettlemoyer</author>
<author confidence="0.999251">Regina Barzilay</author>
<affiliation confidence="0.999985">Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology</affiliation>
<email confidence="0.818822">harr,lsz,</email>
<abstract confidence="0.990990578947368">In this paper, we present a reinforcement learning approach for mapping natural language instructions to sequences of executable actions. We assume access to a reward function that defines the quality of the executed actions. During training, the learner repeatedly constructs action sequences for a set of documents, executes those actions, and observes the resulting reward. We use a policy gradient algorithm to estimate the parameters of a log-linear model for action selection. We apply our method to interpret instructions in two domains — Windows troubleshooting guides and game tutorials. Our results demonstrate that this method can rival supervised learning techniques while requiring few or no annotated training exam-</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kobus Barnard</author>
<author>David A Forsyth</author>
</authors>
<title>Learning the semantics of words and pictures.</title>
<date>2001</date>
<booktitle>In Proceedings of ICCV.</booktitle>
<contexts>
<context citStr="Barnard and Forsyth, 2001" endWordPosition="730" position="4713" startWordPosition="727">ing unlabeled documents with even a small fraction of annotated examples greatly reduces this performance gap, to within 4% in that domain. These results indicate the power of learning from this new form of automated supervision. 2 Related Work Grounded Language Acquisition Our work fits into a broader class of approaches that aim to learn language from a situated context (Mooney, 2008a; Mooney, 2008b; Fleischman and Roy, 2005; Yu and Ballard, 2004; Siskind, 2001; Oates, 2001). Instances of such approaches include work on inferring the meaning of words from video data (Roy and Pentland, 2002; Barnard and Forsyth, 2001), and interpreting the commentary of a simulated soccer game (Chen and Mooney, 2008). Most of these approaches assume some form of parallel data, and learn perceptual cooccurrence patterns. In contrast, our emphasis is on learning language by proactively interacting with an external environment. Reinforcement Learning for Language Processing Reinforcement learning has been previously applied to the problem of dialogue management (Scheffler and Young, 2002; Roy et al., 2000; Litman et al., 2000; Singh et al., 1999). These systems converse with a human user by taking actions that emit natural la</context>
</contexts>
<marker>Barnard, Forsyth, 2001</marker>
<rawString>Kobus Barnard and David A. Forsyth. 2001. Learning the semantics of words and pictures. In Proceedings of ICCV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to sportscast: a test of grounded language acquisition.</title>
<date>2008</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context citStr="Chen and Mooney, 2008" endWordPosition="743" position="4797" startWordPosition="740"> this performance gap, to within 4% in that domain. These results indicate the power of learning from this new form of automated supervision. 2 Related Work Grounded Language Acquisition Our work fits into a broader class of approaches that aim to learn language from a situated context (Mooney, 2008a; Mooney, 2008b; Fleischman and Roy, 2005; Yu and Ballard, 2004; Siskind, 2001; Oates, 2001). Instances of such approaches include work on inferring the meaning of words from video data (Roy and Pentland, 2002; Barnard and Forsyth, 2001), and interpreting the commentary of a simulated soccer game (Chen and Mooney, 2008). Most of these approaches assume some form of parallel data, and learn perceptual cooccurrence patterns. In contrast, our emphasis is on learning language by proactively interacting with an external environment. Reinforcement Learning for Language Processing Reinforcement learning has been previously applied to the problem of dialogue management (Scheffler and Young, 2002; Roy et al., 2000; Litman et al., 2000; Singh et al., 1999). These systems converse with a human user by taking actions that emit natural language utterances. The reinforcement learning state space encodes information about </context>
</contexts>
<marker>Chen, Mooney, 2008</marker>
<rawString>David L. Chen and Raymond J. Mooney. 2008. Learning to sportscast: a test of grounded language acquisition. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>John D Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<journal>IEEE Trans. Pattern Anal. Mach. Intell.,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context citStr="Pietra et al., 1997" endWordPosition="1758" position="10681" startWordPosition="1755"> the best actions. 4 A Log-Linear Model for Actions Our goal is to predict a sequence of actions. We construct this sequence by repeatedly choosing an action given the current mapping state, and applying that action to advance to a new state. Given a state s = (£, d, j, W), the space of possible next actions is defined by enumerating subspans of unused words in the current sentence (i.e., subspans of the jth sentence of d not in W), and the possible commands and parameters in environment state £.4 We model the policy distribution p(a|s; 0) over this action space in a log-linear fashion (Della Pietra et al., 1997; Lafferty et al., 2001), giving us the flexibility to incorporate a diverse range of features. Under this representation, the policy distribution is: ee-�(s,a) p(a|s; 0) = 1: ee-0(s,a') , (1) a/ where 0(s, a) E Rn is an n-dimensional feature representation. During test, actions are selected according to the mode of this distribution. 4For parameters that refer to words, the space of possible values is defined by the unused words in the current sentence. 84 5 Reinforcement Learning During training, our goal is to find the optimal policy p(a|s; θ). Since reward correlates with correct action se</context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1997</marker>
<rawString>Stephen Della Pietra, Vincent J. Della Pietra, and John D. Lafferty. 1997. Inducing features of random fields. IEEE Trans. Pattern Anal. Mach. Intell., 19(4):380–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Di Eugenio</author>
</authors>
<title>Understanding natural language instructions: the case of purpose clauses.</title>
<date>1992</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Di Eugenio, 1992</marker>
<rawString>Barbara Di Eugenio. 1992. Understanding natural language instructions: the case of purpose clauses. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Fleischman</author>
<author>Deb Roy</author>
</authors>
<title>Intentional context in situated language learning.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context citStr="Fleischman and Roy, 2005" endWordPosition="699" position="4517" startWordPosition="696">iments are twofold. First, models trained only with simple reward signals achieve surprisingly high results, coming within 11% of a fully supervised method in the Windows domain. Second, augmenting unlabeled documents with even a small fraction of annotated examples greatly reduces this performance gap, to within 4% in that domain. These results indicate the power of learning from this new form of automated supervision. 2 Related Work Grounded Language Acquisition Our work fits into a broader class of approaches that aim to learn language from a situated context (Mooney, 2008a; Mooney, 2008b; Fleischman and Roy, 2005; Yu and Ballard, 2004; Siskind, 2001; Oates, 2001). Instances of such approaches include work on inferring the meaning of words from video data (Roy and Pentland, 2002; Barnard and Forsyth, 2001), and interpreting the commentary of a simulated soccer game (Chen and Mooney, 2008). Most of these approaches assume some form of parallel data, and learn perceptual cooccurrence patterns. In contrast, our emphasis is on learning language by proactively interacting with an external environment. Reinforcement Learning for Language Processing Reinforcement learning has been previously applied to the pr</context>
</contexts>
<marker>Fleischman, Roy, 2005</marker>
<rawString>Michael Fleischman and Deb Roy. 2005. Intentional context in situated language learning. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context citStr="Lafferty et al., 2001" endWordPosition="1762" position="10705" startWordPosition="1759">A Log-Linear Model for Actions Our goal is to predict a sequence of actions. We construct this sequence by repeatedly choosing an action given the current mapping state, and applying that action to advance to a new state. Given a state s = (£, d, j, W), the space of possible next actions is defined by enumerating subspans of unused words in the current sentence (i.e., subspans of the jth sentence of d not in W), and the possible commands and parameters in environment state £.4 We model the policy distribution p(a|s; 0) over this action space in a log-linear fashion (Della Pietra et al., 1997; Lafferty et al., 2001), giving us the flexibility to incorporate a diverse range of features. Under this representation, the policy distribution is: ee-�(s,a) p(a|s; 0) = 1: ee-0(s,a') , (1) a/ where 0(s, a) E Rn is an n-dimensional feature representation. During test, actions are selected according to the mode of this distribution. 4For parameters that refer to words, the space of possible values is defined by the unused words in the current sentence. 84 5 Reinforcement Learning During training, our goal is to find the optimal policy p(a|s; θ). Since reward correlates with correct action selection, a natural objec</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane J Litman</author>
<author>Michael S Kearns</author>
<author>Satinder Singh</author>
<author>Marilyn A Walker</author>
</authors>
<title>Automatic optimization of dialogue management.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context citStr="Litman et al., 2000" endWordPosition="806" position="5211" startWordPosition="803">oaches include work on inferring the meaning of words from video data (Roy and Pentland, 2002; Barnard and Forsyth, 2001), and interpreting the commentary of a simulated soccer game (Chen and Mooney, 2008). Most of these approaches assume some form of parallel data, and learn perceptual cooccurrence patterns. In contrast, our emphasis is on learning language by proactively interacting with an external environment. Reinforcement Learning for Language Processing Reinforcement learning has been previously applied to the problem of dialogue management (Scheffler and Young, 2002; Roy et al., 2000; Litman et al., 2000; Singh et al., 1999). These systems converse with a human user by taking actions that emit natural language utterances. The reinforcement learning state space encodes information about the goals of the user and what they say at each time step. The learning problem is to find an optimal policy that maps states to actions, through a trial-and-error process of repeated interaction with the user. Reinforcement learning is applied very differently in dialogue systems compared to our setup. In some respects, our task is more easily amenable to reinforcement learning. For instance, we are not intera</context>
</contexts>
<marker>Litman, Kearns, Singh, Walker, 2000</marker>
<rawString>Diane J. Litman, Michael S. Kearns, Satinder Singh, and Marilyn A. Walker. 2000. Automatic optimization of dialogue management. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond J Mooney</author>
</authors>
<title>Learning language from its perceptual context.</title>
<date>2008</date>
<booktitle>In Proceedings of ECML/PKDD.</booktitle>
<contexts>
<context citStr="Mooney, 2008" endWordPosition="693" position="4475" startWordPosition="692"> The key findings of our experiments are twofold. First, models trained only with simple reward signals achieve surprisingly high results, coming within 11% of a fully supervised method in the Windows domain. Second, augmenting unlabeled documents with even a small fraction of annotated examples greatly reduces this performance gap, to within 4% in that domain. These results indicate the power of learning from this new form of automated supervision. 2 Related Work Grounded Language Acquisition Our work fits into a broader class of approaches that aim to learn language from a situated context (Mooney, 2008a; Mooney, 2008b; Fleischman and Roy, 2005; Yu and Ballard, 2004; Siskind, 2001; Oates, 2001). Instances of such approaches include work on inferring the meaning of words from video data (Roy and Pentland, 2002; Barnard and Forsyth, 2001), and interpreting the commentary of a simulated soccer game (Chen and Mooney, 2008). Most of these approaches assume some form of parallel data, and learn perceptual cooccurrence patterns. In contrast, our emphasis is on learning language by proactively interacting with an external environment. Reinforcement Learning for Language Processing Reinforcement lear</context>
</contexts>
<marker>Mooney, 2008</marker>
<rawString>Raymond J. Mooney. 2008a. Learning language from its perceptual context. In Proceedings of ECML/PKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to connect language and perception.</title>
<date>2008</date>
<booktitle>In Proceedings ofAAAI.</booktitle>
<contexts>
<context citStr="Mooney, 2008" endWordPosition="693" position="4475" startWordPosition="692"> The key findings of our experiments are twofold. First, models trained only with simple reward signals achieve surprisingly high results, coming within 11% of a fully supervised method in the Windows domain. Second, augmenting unlabeled documents with even a small fraction of annotated examples greatly reduces this performance gap, to within 4% in that domain. These results indicate the power of learning from this new form of automated supervision. 2 Related Work Grounded Language Acquisition Our work fits into a broader class of approaches that aim to learn language from a situated context (Mooney, 2008a; Mooney, 2008b; Fleischman and Roy, 2005; Yu and Ballard, 2004; Siskind, 2001; Oates, 2001). Instances of such approaches include work on inferring the meaning of words from video data (Roy and Pentland, 2002; Barnard and Forsyth, 2001), and interpreting the commentary of a simulated soccer game (Chen and Mooney, 2008). Most of these approaches assume some form of parallel data, and learn perceptual cooccurrence patterns. In contrast, our emphasis is on learning language by proactively interacting with an external environment. Reinforcement Learning for Language Processing Reinforcement lear</context>
</contexts>
<marker>Mooney, 2008</marker>
<rawString>Raymond J. Mooney. 2008b. Learning to connect language and perception. In Proceedings ofAAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Y Ng</author>
<author>H Jin Kim</author>
<author>Michael I Jordan</author>
<author>Shankar Sastry</author>
</authors>
<title>Autonomous helicopter flight via reinforcement learning.</title>
<date>2003</date>
<booktitle>In Advances in NIPS.</booktitle>
<contexts>
<context citStr="Ng et al., 2003" endWordPosition="2576" position="15329" startWordPosition="2573">1 to n − 1, each with its own reward value given by the environment as a side effect of executing h. We apply the update from equation 5 for each subsequence. Second, for a sampled history h, we can propose alternative histories h' that result in the same commands and parameters with different word spans. We can again apply equation 5 for each h', weighted by its probability under the current policy, p(h�|θ) p(h|θ) . φ(s, a')p(a'|s; θ), 85 The algorithm we have presented belongs to a family of policy gradient algorithms that have been successfully used for complex tasks such as robot control (Ng et al., 2003). Our formulation is unique in how it represents natural language in the reinforcement learning framework. 5.2 Reward Functions and ML Estimation We can design a range of reward functions to guide learning, depending on the availability of annotated data and environment feedback. Consider the case when every training document d E D is annotated with its correct sequence of actions, and state transitions are deterministic. Given these examples, it is straightforward to construct a reward function that connects policy gradient to maximum likelihood. Specifically, define a reward function r(h) th</context>
</contexts>
<marker>Ng, Kim, Jordan, Sastry, 2003</marker>
<rawString>Andrew Y. Ng, H. Jin Kim, Michael I. Jordan, and Shankar Sastry. 2003. Autonomous helicopter flight via reinforcement learning. In Advances in NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Timothy Oates</author>
</authors>
<title>Grounding knowledge in sensors: Unsupervised learning for language and planning.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Massachusetts Amherst.</institution>
<contexts>
<context citStr="Oates, 2001" endWordPosition="707" position="4568" startWordPosition="706">rd signals achieve surprisingly high results, coming within 11% of a fully supervised method in the Windows domain. Second, augmenting unlabeled documents with even a small fraction of annotated examples greatly reduces this performance gap, to within 4% in that domain. These results indicate the power of learning from this new form of automated supervision. 2 Related Work Grounded Language Acquisition Our work fits into a broader class of approaches that aim to learn language from a situated context (Mooney, 2008a; Mooney, 2008b; Fleischman and Roy, 2005; Yu and Ballard, 2004; Siskind, 2001; Oates, 2001). Instances of such approaches include work on inferring the meaning of words from video data (Roy and Pentland, 2002; Barnard and Forsyth, 2001), and interpreting the commentary of a simulated soccer game (Chen and Mooney, 2008). Most of these approaches assume some form of parallel data, and learn perceptual cooccurrence patterns. In contrast, our emphasis is on learning language by proactively interacting with an external environment. Reinforcement Learning for Language Processing Reinforcement learning has been previously applied to the problem of dialogue management (Scheffler and Young, </context>
</contexts>
<marker>Oates, 2001</marker>
<rawString>James Timothy Oates. 2001. Grounding knowledge in sensors: Unsupervised learning for language and planning. Ph.D. thesis, University of Massachusetts Amherst.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deb K Roy</author>
<author>Alex P Pentland</author>
</authors>
<title>Learning words from sights and sounds: a computational model.</title>
<date>2002</date>
<journal>Cognitive Science</journal>
<volume>26</volume>
<pages>113--146</pages>
<contexts>
<context citStr="Roy and Pentland, 2002" endWordPosition="726" position="4685" startWordPosition="723"> domain. Second, augmenting unlabeled documents with even a small fraction of annotated examples greatly reduces this performance gap, to within 4% in that domain. These results indicate the power of learning from this new form of automated supervision. 2 Related Work Grounded Language Acquisition Our work fits into a broader class of approaches that aim to learn language from a situated context (Mooney, 2008a; Mooney, 2008b; Fleischman and Roy, 2005; Yu and Ballard, 2004; Siskind, 2001; Oates, 2001). Instances of such approaches include work on inferring the meaning of words from video data (Roy and Pentland, 2002; Barnard and Forsyth, 2001), and interpreting the commentary of a simulated soccer game (Chen and Mooney, 2008). Most of these approaches assume some form of parallel data, and learn perceptual cooccurrence patterns. In contrast, our emphasis is on learning language by proactively interacting with an external environment. Reinforcement Learning for Language Processing Reinforcement learning has been previously applied to the problem of dialogue management (Scheffler and Young, 2002; Roy et al., 2000; Litman et al., 2000; Singh et al., 1999). These systems converse with a human user by taking </context>
</contexts>
<marker>Roy, Pentland, 2002</marker>
<rawString>Deb K. Roy and Alex P. Pentland. 2002. Learning words from sights and sounds: a computational model. Cognitive Science 26, pages 113–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Roy</author>
<author>Joelle Pineau</author>
<author>Sebastian Thrun</author>
</authors>
<title>Spoken dialogue management using probabilistic reasoning.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context citStr="Roy et al., 2000" endWordPosition="802" position="5190" startWordPosition="799">ances of such approaches include work on inferring the meaning of words from video data (Roy and Pentland, 2002; Barnard and Forsyth, 2001), and interpreting the commentary of a simulated soccer game (Chen and Mooney, 2008). Most of these approaches assume some form of parallel data, and learn perceptual cooccurrence patterns. In contrast, our emphasis is on learning language by proactively interacting with an external environment. Reinforcement Learning for Language Processing Reinforcement learning has been previously applied to the problem of dialogue management (Scheffler and Young, 2002; Roy et al., 2000; Litman et al., 2000; Singh et al., 1999). These systems converse with a human user by taking actions that emit natural language utterances. The reinforcement learning state space encodes information about the goals of the user and what they say at each time step. The learning problem is to find an optimal policy that maps states to actions, through a trial-and-error process of repeated interaction with the user. Reinforcement learning is applied very differently in dialogue systems compared to our setup. In some respects, our task is more easily amenable to reinforcement learning. For instan</context>
</contexts>
<marker>Roy, Pineau, Thrun, 2000</marker>
<rawString>Nicholas Roy, Joelle Pineau, and Sebastian Thrun. 2000. Spoken dialogue management using probabilistic reasoning. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Konrad Scheffler</author>
<author>Steve Young</author>
</authors>
<title>Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning.</title>
<date>2002</date>
<booktitle>In Proceedings of HLT.</booktitle>
<contexts>
<context citStr="Scheffler and Young, 2002" endWordPosition="798" position="5172" startWordPosition="795">d, 2001; Oates, 2001). Instances of such approaches include work on inferring the meaning of words from video data (Roy and Pentland, 2002; Barnard and Forsyth, 2001), and interpreting the commentary of a simulated soccer game (Chen and Mooney, 2008). Most of these approaches assume some form of parallel data, and learn perceptual cooccurrence patterns. In contrast, our emphasis is on learning language by proactively interacting with an external environment. Reinforcement Learning for Language Processing Reinforcement learning has been previously applied to the problem of dialogue management (Scheffler and Young, 2002; Roy et al., 2000; Litman et al., 2000; Singh et al., 1999). These systems converse with a human user by taking actions that emit natural language utterances. The reinforcement learning state space encodes information about the goals of the user and what they say at each time step. The learning problem is to find an optimal policy that maps states to actions, through a trial-and-error process of repeated interaction with the user. Reinforcement learning is applied very differently in dialogue systems compared to our setup. In some respects, our task is more easily amenable to reinforcement le</context>
</contexts>
<marker>Scheffler, Young, 2002</marker>
<rawString>Konrad Scheffler and Steve Young. 2002. Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning. In Proceedings of HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satinder P Singh</author>
<author>Michael J Kearns</author>
<author>Diane J Litman</author>
<author>Marilyn A Walker</author>
</authors>
<title>Reinforcement learning for spoken dialogue systems.</title>
<date>1999</date>
<booktitle>In Advances in NIPS.</booktitle>
<contexts>
<context citStr="Singh et al., 1999" endWordPosition="810" position="5232" startWordPosition="807">n inferring the meaning of words from video data (Roy and Pentland, 2002; Barnard and Forsyth, 2001), and interpreting the commentary of a simulated soccer game (Chen and Mooney, 2008). Most of these approaches assume some form of parallel data, and learn perceptual cooccurrence patterns. In contrast, our emphasis is on learning language by proactively interacting with an external environment. Reinforcement Learning for Language Processing Reinforcement learning has been previously applied to the problem of dialogue management (Scheffler and Young, 2002; Roy et al., 2000; Litman et al., 2000; Singh et al., 1999). These systems converse with a human user by taking actions that emit natural language utterances. The reinforcement learning state space encodes information about the goals of the user and what they say at each time step. The learning problem is to find an optimal policy that maps states to actions, through a trial-and-error process of repeated interaction with the user. Reinforcement learning is applied very differently in dialogue systems compared to our setup. In some respects, our task is more easily amenable to reinforcement learning. For instance, we are not interacting with a human us</context>
</contexts>
<marker>Singh, Kearns, Litman, Walker, 1999</marker>
<rawString>Satinder P. Singh, Michael J. Kearns, Diane J. Litman, and Marilyn A. Walker. 1999. Reinforcement learning for spoken dialogue systems. In Advances in NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard S Sutton</author>
<author>Andrew G Barto</author>
</authors>
<title>Reinforcement Learning: An Introduction.</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context citStr="Sutton and Barto, 1998" endWordPosition="440" position="2948" startWordPosition="437">g environment and observing their effects. For instance, in the example above we can assess whether the goal described in the instructions is achieved, i.e., the folder is deleted. The key idea of our approach is to leverage the validation process as the main source of supervision to guide learning. This form of supervision allows us to learn interpretations of natural language instructions when standard supervised techniques are not applicable, due to the lack of human-created annotations. Reinforcement learning is a natural framework for building models using validation from an environment (Sutton and Barto, 1998). We assume that supervision is provided in the form of a reward function that defines the quality of executed actions. During training, the learner repeatedly constructs action sequences for a set of given documents, executes those actions, and observes the resulting reward. The learner’s goal is to estimate a 82 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 82–90, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP policy — a distribution over actions given instruction text and environment state — that maximizes future expected reward. Our poli</context>
<context citStr="Sutton and Barto, 1998" endWordPosition="4084" position="24268" startWordPosition="4081">e Windows corpus. Experimental Framework To apply our algorithm to the Windows domain, we use the Win32 application programming interface to simulate human interactions with the user interface, and to gather environment state information. The operating system environment is hosted within a virtual machine,10 allowing us to rapidly save and reset system state snapshots. For the puzzle game domain, we replicated the game with an implementation that facilitates automatic play. As is commonly done in reinforcement learning, we use a softmax temperature parameter to smooth the policy distribution (Sutton and Barto, 1998), set to 0.1 in our experiments. For Windows, the development set is used to select the best parameters. For Crossblock, we choose the parameters that produce the highest reward during training. During evaluation, we use these parameters to predict mappings for the test documents. Evaluation Metrics For evaluation, we compare the results to manually constructed sequences of actions. We measure the number of correct actions, sentences, and documents. An action is correct if it matches the annotations in terms of command and parameters. A sentence is correct if all of its actions are correctly i</context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>Richard S. Sutton and Andrew G. Barto. 1998. Reinforcement Learning: An Introduction. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard S Sutton</author>
<author>David McAllester</author>
<author>Satinder Singh</author>
<author>Yishay Mansour</author>
</authors>
<title>Policy gradient methods for reinforcement learning with function approximation.</title>
<date>2000</date>
<booktitle>In Advances in NIPS.</booktitle>
<contexts>
<context citStr="Sutton et al., 2000" endWordPosition="2114" position="12668" startWordPosition="2111"> 2 foreach d ∈ D do 3 Sample history h ∼ p(h|θ) where h = (s0, a0, ... , an−1, sn) as follows: 3a fort = 0 ... n − 1 do 3b Sample action at ∼ p(a|st; θ) 3c Execute at on state st: st+1 ∼ p(s|st, at) end A ← E �φ(st, at) − Ea, φ(st, a�)p(a�|st; θ)� 4 t 5 θ ← θ + r(h)A end end Output: Estimate of parameters θ Algorithm 1: A policy gradient algorithm. p(h|θ) = n−1H p(at|st; θ)p(st+1|st, at). (3) t=0 5.1 A Policy Gradient Algorithm Our reinforcement learning problem is to find the parameters θ that maximize Vθ from equation 2. Although there is no closed form solution, policy gradient algorithms (Sutton et al., 2000) estimate the parameters θ by performing stochastic gradient ascent. The gradient of Vθ is approximated by interacting with the environment, and the resulting reward is used to update the estimate of θ. Policy gradient algorithms optimize a non-convex objective and are only guaranteed to find a local optimum. However, as we will see, they scale to large state spaces and can perform well in practice. To find the parameters θ that maximize the objective, we first compute the derivative of Vθ. Expanding according to the product rule, we have: � ∂I: ∂ ∂θVθ(s) = Ep(h|θ) r(h) ∂θ log p(at|st; θ) , t </context>
</contexts>
<marker>Sutton, McAllester, Singh, Mansour, 2000</marker>
<rawString>Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. 2000. Policy gradient methods for reinforcement learning with function approximation. In Advances in NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Winograd</author>
</authors>
<title>Understanding Natural Language.</title>
<date>1972</date>
<publisher>Academic Press.</publisher>
<contexts>
<context citStr="Winograd, 1972" endWordPosition="173" position="1177" startWordPosition="172">s for a set of documents, executes those actions, and observes the resulting reward. We use a policy gradient algorithm to estimate the parameters of a log-linear model for action selection. We apply our method to interpret instructions in two domains — Windows troubleshooting guides and game tutorials. Our results demonstrate that this method can rival supervised learning techniques while requiring few or no annotated training examples.1 1 Introduction The problem of interpreting instructions written in natural language has been widely studied since the early days of artificial intelligence (Winograd, 1972; Di Eugenio, 1992). Mapping instructions to a sequence of executable actions would enable the automation of tasks that currently require human participation. Examples include configuring software based on how-to guides and operating simulators using instruction manuals. In this paper, we present a reinforcement learning framework for inducing mappings from text to actions without the need for annotated training examples. For concreteness, consider instructions from a Windows troubleshooting guide on deleting temporary folders, shown in Figure 1. We aim to map 1Code, data, and annotations used</context>
</contexts>
<marker>Winograd, 1972</marker>
<rawString>Terry Winograd. 1972. Understanding Natural Language. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Yu</author>
<author>Dana H Ballard</author>
</authors>
<title>On the integration of grounding language and learning objects.</title>
<date>2004</date>
<booktitle>In Proceedings ofAAAI.</booktitle>
<contexts>
<context citStr="Yu and Ballard, 2004" endWordPosition="703" position="4539" startWordPosition="700"> models trained only with simple reward signals achieve surprisingly high results, coming within 11% of a fully supervised method in the Windows domain. Second, augmenting unlabeled documents with even a small fraction of annotated examples greatly reduces this performance gap, to within 4% in that domain. These results indicate the power of learning from this new form of automated supervision. 2 Related Work Grounded Language Acquisition Our work fits into a broader class of approaches that aim to learn language from a situated context (Mooney, 2008a; Mooney, 2008b; Fleischman and Roy, 2005; Yu and Ballard, 2004; Siskind, 2001; Oates, 2001). Instances of such approaches include work on inferring the meaning of words from video data (Roy and Pentland, 2002; Barnard and Forsyth, 2001), and interpreting the commentary of a simulated soccer game (Chen and Mooney, 2008). Most of these approaches assume some form of parallel data, and learn perceptual cooccurrence patterns. In contrast, our emphasis is on learning language by proactively interacting with an external environment. Reinforcement Learning for Language Processing Reinforcement learning has been previously applied to the problem of dialogue mana</context>
</contexts>
<marker>Yu, Ballard, 2004</marker>
<rawString>Chen Yu and Dana H. Ballard. 2004. On the integration of grounding language and learning objects. In Proceedings ofAAAI.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>