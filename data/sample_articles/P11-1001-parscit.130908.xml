<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant confidence="0.003061" no="0">
<title confidence="0.987614">
A Word-Class Approach to Labeling PSCFG Rules for Machine Translation
</title>
<author confidence="0.995091">
Andreas Zollmann and Stephan Vogel
</author>
<affiliation confidence="0.91742025">
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.999525">
{zollmann,vogel+}@cs.cmu.edu
</email>
<sectionHeader confidence="0.997398" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999767">In this work we propose methods to label probabilistic synchronous context-free grammar (PSCFG) rules using only word tags, generated by either part-of-speech analysis or unsupervised word class induction. The proposals range from simple tag-combination schemes to a phrase clustering model that can incorporate an arbitrary number of features. Our models improve translation quality over the single generic label approach of Chiang (2005) and perform on par with the syntactically motivated approach from Zollmann and Venugopal (2006) on the NIST large Chineseto-English translation task. These results persist when using automatically learned word tags, suggesting broad applicability of our technique across diverse language pairs for which syntactic resources are not available.</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996610672727273">The Probabilistic Synchronous Context Free Grammar (PSCFG) formalism suggests an intuitive approach to model the long-distance and lexically sensitive reordering phenomena that often occur across language pairs considered for statistical machine translation. As in monolingual parsing, nonterminal symbols in translation rules are used to generalize beyond purely lexical operations. Labels on these nonterminal symbols are often used to enforce syntactic constraints in the generation of bilingual sentences and imply conditional independence assumptions in the translation model. Several techniques have been recently proposed to automatically identify and estimate parameters for PSCFGs (or related synchronous grammars) from parallel corpora (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006; Marcu et al., 2006). 1 While all of these techniques rely on wordalignments to suggest lexical relationships, they differ in the way in which they assign labels to nonterminal symbols of PSCFG rules. Chiang (2005) describes a procedure to extract PSCFG rules from word-aligned (Brown et al., 1993) corpora, where all nonterminals share the same generic label X. In Galley et al. (2004) and Marcu et al. (2006), target language parse trees are used to identify rules and label their nonterminal symbols, while Liu et al. (2006) use source language parse trees instead. Zollmann and Venugopal (2006) directly extend the rule extraction procedure from Chiang (2005) to heuristically label any phrase pair based on target language parse trees. Label-based approaches have resulted in improvements in translation quality over the single X label approach (Zollmann et al., 2008; Mi and Huang, 2008); however, all the works cited here rely on stochastic parsers that have been trained on manually created syntactic treebanks. These treebanks are difficult and expensive to produce and exist for a limited set of languages only. In this work, we propose a labeling approach that is based merely on part-of-speech analysis of the source or target language (or even both). Towards the ultimate goal of building end-to-end machine translation systems without any human annotations, we also experiment with automatically inferred word classes using distributional clustering (Kneser and Ney, 1993). Since the number of classes is a parameter of the clustering method and the resulting nonterminal size of our grammar is a function of the number of word classes, the PSCFG grammar complexity can be adjusted to the specific translation task at hand. Finally, we introduce a more flexible labeling approach based on K-means clustering, which allows the incorporation of an arbitrary number of wordclass based features, including phrasal contexts, canmake use of multiple tagging schemes, and also allows non-class features such as phrase sizes.</bodyText>
<note confidence="0.963666">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1–11,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.999516" genericHeader="method">
2 PSCFG-based translation
</sectionHeader>
<bodyText confidence="0.939008636363636">In this work we experiment with PSCFGs that have been automatically learned from word-aligned parallel corpora. PSCFGs are defined by a source terminal set (source vocabulary) TS, a target terminal set (target vocabulary) TT , a shared nonterminal set N and rules of the form: A → hγ, α, wi where</bodyText>
<listItem confidence="0.9992916">• A E N is a labeled nonterminal referred to as the Consider the target-tagged example sentence pair: left-hand-side of the rule, Ich habe ihn gesehen  |I/PRP saw/VBD him/PRP • ry E (N U TS)* is the source side of the rule, Then (depending on the extracted phrase pairs), the • α E (N U TT)* is the target side of the rule, resulting initial rules could be: • w E [0, oc) is a non-negative real-valued weight 1: PRP-PRP → Ich  |I</listItem>
<bodyText confidence="0.99580206329114">Chiang (2005) learns a single-nonterminal PSCFG from a bilingual corpus by first identifying initial phrase pairs using the technique from Koehn et al. (2003), and then performing a generalization operation to generate phrase pairs with gaps, which can be viewed as PSCFG rules with generic ‘X’ nonterminal left-hand-sides and substitution sites. Bilingual features φi that judge the quality of each rule are estimated based on rule extraction frequency counts.</bodyText>
<sectionHeader confidence="0.999516" genericHeader="method">
3 Hard rule labeling from word classes
</sectionHeader>
<bodyText confidence="0.99580206329114">ble for substitution in a PSCFG derivation: The left We now describe a simple method of inducing a and right boundary word tags of the inserted rule’s multi-nonterminal PSCFG from a parallel corpus target side have to match the respective boundary with word-tagged target side sentences. The same word tags of the phrase pair that was replaced by procedure can straightforwardly be applied to a cora nonterminal when the complex rule was created pus with tagged source side sentences. We use the from a training sentence pair. Since consecutive simple term ‘tag’ to stand for any kind of word-level words within a rule stem from consecutive words in analysis—a syntactic, statistical, or other means of the training corpus and thus are already consistent, grouping word types or tokens into classes, possibly the boundary word tags are more informative than based on their position and context in the sentence, tags of words between the boundaries for the task POS tagging being the most obvious example. of combining different rules in a derivation, and are As in Chiang’s hierarchical system, we rely on therefore a more appropriate choice for the creation an external phrase-extraction procedure such as the of grammar labels than tags of inside words. one of Koehn et al. (2003) to provide us with a set Accounting for phrase size A drawback of the of phrase pairs for each sentence pair in the traincurrent approach is that a single-word rule such as ing corpus, annotated with their respective start and PRP-PRP —* Ich  |I end positions in the source and target sentences. Let f = fi • • • fm be the current source sentence, e = el • • • en the current target sentence, and t = 2 can have the same left-hand-side nonterminal as a Consider again our example sentence pair (now long rule with identical left and right boundary tags, also annotated with source-side part-of-speech tags): such as (when using target-side tags): Ich/PRP habe/AUX ihn/PRP gesehen/VBN PRP-PRP → Ich habe ihn gesehen  |I saw him I/PRP saw/VBD him/PRP We therefore introduce a means of distinguishing Given the same phrase extraction method as before, between one-word, two-word, and multiple-word the resulting initial rules for our bilingual model, phrases as follows: Each one-word phrase with tag when also accounting for phrase size, are as follows: T simply receives the label T, instead of T-T. Two1: PRP+PRP → Ich  |I word phrases with tag sequence T1T2 are labeled 2: PRP+PRP → ihn  |him T1-T2 as before. Phrases of length greater two with 3: VBN+VBD → gesehen  |saw tag sequence T1 · · · Tn are labeled T1..Tn to denote 4: AUX..VBN+VBD-PRP → habe ihn that tags were omitted from the phrase’s tag segesehen  |saw him quence. The resulting number of grammar nonter5: PRP..VBN+PRP..PRP → Ich habe ihn minals based on a tag vocabulary of size t is thus gesehen  |I saw him given by 2t2 + t. Abstracting-out rule 2 from rule 4, for instance, An alternative way of accounting for phrase size leads to the complex rule: is presented by Chiang et al. (2008), who introAUX..VBN+VBD-PRP → habe PRP+PRP, duce structural distortion features into a hierarchigesehen  |saw PRP+PRP, cal phrase-based model, aimed at modeling nonterUnsupervised word class assignment by clusterminal reordering given source span length. Our ing As an alternative to POS tags, we experiment approach instead uses distinct grammar rules and with unsupervised word clustering methods based labels to discriminate phrase size, with the advanon the exchange algorithm (Kneser and Ney, 1993). tage of enabling all translation models to estimate Its objective function is maximizing the likelihood distinct weights for distinct size classes and avoidn ing the need of additional models in the log-linear P(wi|w1, ... , wi−1) framework; however, the increase in the number of i=1 labels and thus grammar rules decreases the reliaof the training data w = w1, ... , wn given a parbility of estimated models for rare events due to intially class-based bigram model of the form creased data sparseness. P(wi|w1,...,wi−1) ≈ Xc(wi)|wi−1)·Awi|c(wi)) Extension to a bilingually tagged corpus While where c : V → {1, ... , N} maps a word (type, not the availability of syntactic annotations for both token) w to its class c(w), V is the vocabulary, and source and target language is unlikely in most transN the fixed number of classes, which has to be cholation scenarios, some form of word tags, be it partsen a priori. We use the publicly available impleof-speech tags or learned word clusters (cf. Secmentation MKCLS (Och, 1999) to train this model. tion 3) might be available on both sides. In this case, As training data we use the respective side of the our grammar extraction procedure can be easily exparallel training data for the translation system. tended to impose both source and target constraints We also experiment with the extension of this on the eligible substitutions simultaneously. model by Clark (2003), who incorporated morphoLet Nf be the nonterminal label that would be logical information by imposing a Bayesian prior assigned to a given initial rule when utilizing the on the class mapping c, based on N individual dissource-side tag sequence, and Ne the assigned latributions over strings, one for each word class. bel according to the target-side tag sequence. Then Each such distribution is a character-based hidden our bilingual tag-based model assigns ‘Nf + Ne’ Markov model, thus encouraging the grouping of to the initial rule. The extraction of complex rules morphologically similar words into the same class. proceeds as before. The number of nonterminals in this model, based on a source tag vocabulary of size s and a target tag vocabulary of size t, is thus given by s2t2 for the regular labeling method and the K-means algorithm</bodyText>
<sectionHeader confidence="0.999516" genericHeader="method">
4 Clustering phrase pairs directly using
the K-means algorithm
</sectionHeader>
<bodyText confidence="0.999866471698114">Even though we have only made use of the first and last words’ classes in the labeling methods described so far, the number of resulting grammar nonterminals quickly explodes. Using a scheme based on source and target phrases with accounting for phrase size, with 36 word classes (the size of the Penn English POS tag set) for both languages, yields a grammar with (36+2*362)2 = 6.9m nonterminal labels. Quite plausibly, phrase labeling should be informed by more than just the classes of the first and last words of the phrase. Taking phrase context into account, for example, can aid the learning of syntactic properties: a phrase beginning with a determiner and ending with a noun, with a verb as right context, is more likely to be a noun phrase than the same phrase with another noun as right context. In the current scheme, there is no way of distinguishing between these two cases. Similarly, it is conceivable that using non-boundary words inside the phrase might aid the labeling process. When relying on unsupervised learning of the word classes, we are forced to chose a fixed number of classes. A smaller number of word clusters will result in smaller number of grammar nonterminals, and thus more reliable feature estimation, while a larger number has the potential to discover more subtle syntactic properties. Using multiple word clusterings simultaneously, each based on a different number of classes, could turn this global, hard trade-off into a local, soft one, informed by the number of phrase pair instances available for a given granularity. Lastly, our method of accounting for phrase size is somewhat displeasing: While there is a hard partitioning of one-word and two-word phrases, no distinction is made between phrases of length greater than two. Marking phrase sizes greater than two explicitly by length, however, would create many sparse, low-frequency rules, and one of the strengths of PSCFG-based translation is the ability to substitute flexible-length spans into nonterminals of a derivation. A partitioning where phrase size is instead merely a feature informing the labeling process seems more desirable. We thus propose to represent each phrase pair instance (including its bilingual one-word contexts) as feature vectors, i.e., points of a vector space. We then use these data points to partition the space into clusters, and subsequently assign each phrase pair instance the cluster of its corresponding feature vector as label. The feature mapping Consider the phrase pair instance</bodyText>
<equation confidence="0.983181">
(f0)f1 ··· fm(fm+1) I (e0)e1 ··· en(en+1)
</equation>
<bodyText confidence="0.996782538461539">(where f0, fm+1, e0, en+1 are the left and right, source and target side contexts, respectively). We begin with the case of only a single, target-side word class scheme (either a tagger or an unsupervised word clustering/POS induction method). Let C = {c1, ... , cNI be its set of word classes. Further, let c0 be a short-hand for the result of looking up the class of a word that is out of bounds (e.g., the left context of the first word of a sentence, or the second word of a one-word phrase). We now map our phrase pair instance to the real-valued vector (where ✶[P] is the indicator function defined as 1 if property P is true, and 0 otherwise):</bodyText>
<equation confidence="0.988302777777778">
C✶[e1=c0], ... , ✶[e1=cN], ✶[en=c0], ... , ✶[en=cN],
αsec✶[e2=c0], ... , αsec✶[e2=cN],
αsec✶[en−1=c0], ... , αsec✶[en−1=cN],
En
αins �i=1 ✶[ei=cN]
αcntxt✶[e0=c0], . . . , αcntxt✶[e0=cN],
αcntxt✶[en+1=c0], ... , αcntxt✶[en+1=cN],
✓ &gt;
αphrsize N + 1 log10(n)
</equation>
<bodyText confidence="0.883201904761905">The α parameters determine the influence of the different types of information. The elements in the first line represent the phrase boundary word classes, the next two lines the classes of the second and penultimate word, followed by a line representing the accumulated contents of the whole phrase, followed by two lines pertaining to the context word classes. The final element of the vector is proportional to the logarithm of the phrase length.' We chose the logarithm assuming that length deviation of syntactic phrasal units is not constant, but proportional to the average length. Thus, all other features being equal, the distance between a two-word and a four-word phrase is 'The \/N + 1 factor serves to make the feature’s influence independent of the number of word classes by yielding the same distance (under L2) as N + 1 identical copies of the feature. , n En αins �i=1 ✶[ei=c0] n , ... , the same as the distance between a four-word and an eight-word phrase.</bodyText>
<page confidence="0.953528">
4
</page>
<bodyText confidence="0.999963461538462">We will mainly use the Euclidean (L2) distance to compare points for clustering purposes. Our feature space is thus the Euclidean vector space ][87�'+s. To additionally make use of source-side word classes, we append elements analogous to the ones above to the vector, all further multiplied by a parameter αgrc that allows trading off the relevance of source-side and target-side information. In the same fashion, we can incorporate multiple tagging schemes (e.g., word clusterings of different granularities) into the same feature vector. As finergrained schemes have more elements in the feature vector than coarser-grained ones, and thus exert more influence, we set the α parameter for each scheme to 1/N (where N is the number of word classes of the scheme). The K-means algorithm To create the clusters, we chose the K-means algorithm (Steinhaus, 1956; MacQueen, 1967) for both its computational efficiency and ease of implementation and parallelization. Given an initial mapping from the data points to K clusters, the procedure alternates between (i) computing the centroid of each cluster and (ii) reallocating each data point to the closest cluster centroid, until convergence. We implemented two commonly used initialization methods: Forgy and Random Partition. The Forgy method randomly chooses K observations from the data set and uses these as the initial means. The Random Partition method first randomly assigns a cluster to each observation and then proceeds straight to step (ii). Forgy tends to spread the initial means out, while Random Partition places all of them close to the center of the data set. As the resulting clusters looked similar, and Random Partition sometimes led to a high rate of empty clusters, we settled for Forgy.</bodyText>
<sectionHeader confidence="0.999832" genericHeader="evaluation and result">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999948153846154">We evaluate our approach by comparing translation quality, as evaluated by the IBM-BLEU (Papineni et al., 2002) metric on the NIST Chinese-to-English translation task using MT04 as development set to train the model parameters A, and MT05, MT06 and MT08 as test sets. Even though a key advantage of our method is its applicability to resource-poor languages, we used a language pair for which linguistic resources are available in order to determine how close translation performance can get to a fully syntax-based system. Accordingly, we use Chiang’s hierarchical phrase based translation model (Chiang, 2007) as a base line, and the syntax-augmented MT model (Zollmann and Venugopal, 2006) as a ‘target line’, a model that would not be applicable for language pairs without linguistic resources. We perform PSCFG rule extraction and decoding using the open-source “SAMT” system (Venugopal and Zollmann, 2009), using the provided implementations for the hierarchical and syntax-augmented grammars. Apart from the language model, the lexical, phrasal, and (for the syntax grammar) labelconditioned features, and the rule, target word, and glue operation counters, Venugopal and Zollmann (2009) also provide both the hierarchical and syntax-augmented grammars with a rareness penalty 1/ cnn(r), where cnn(r) is the occurrence count of rule r in the training corpus, allowing the system to learn penalization of low-frequency rules, as well as three indicator features firing if the rule has one, two unswapped, and two swapped nonterminal pairs, respectively.2 Further, to mitigate badly estimated PSCFG derivations based on low-frequency rules of the much sparser syntax model, the syntax grammar also contains the hierarchical grammar as a backbone (cf. Zollmann and Vogel (2010) for details and empirical analysis). We implemented our rule labeling approach within the SAMT rule extraction pipeline, resulting in comparable features across all systems. For all systems, we use the bottom-up chart parsing decoder implemented in the SAMT toolkit with a reordering limit of 15 source words, and correspondingly extract rules from initial phrase pairs of maximum source length 15. All rules have at most two nonterminal symbols, which must be non-consecutive on the source side, and rules must contain at least one source-side terminal symbol. The beam settings for the hierarchical system are 600 items per ‘X’ (generic rule) cell, and 600 per ‘S’ (glue) cell.3 Due to memory limitations, the multi-nonterminal grammars have to be pruned more harshly: We allow 100 ‘S’ items, and a total of 500 non-‘S’ items, but maximally 40 items per nonterminal.</bodyText>
<footnote confidence="0.989460857142857">
2Penalization or reward of purely-lexical rules can be indirectly
learned by trading off these features with the rule counter fea-
ture.
3For comparison, Chiang (2007) uses 30 and 15, respectively,
and further prunes items that deviate too much in score from
the best item. He extracts initial phrases of maximum length
10.
</footnote>
<page confidence="0.990778">
5
</page>
<bodyText confidence="0.9944281875">For all systems, we further discard non-initial rules occurring only once.4 For the multi-nonterminal systems, we generally further discard all non-generic non-initial rules occurring less than 6 times, but we additionally give results for a ‘slow’ version of the Syntax targetline system and our best word class based systems, where only single-occurrences were removed. For parameter tuning, we use the L0-regularized minimum-error-rate training tool provided by the SAMT toolkit. Each system is trained separately to adapt the parameters to its specific properties (size of nonterminal set, grammar complexity, features sparseness, reliance on the language model, etc.). The parallel training data comprises of 9.6M sentence pairs (206M Chinese and 228M English words). The source and target language parses for the syntax-augmented grammar, as well as the POS tags for our POS-based grammars were generated by the Stanford parser (Klein and Manning, 2003). The results are given in Table 1. Results for the Syntax system are consistent with previous results (Zollmann et al., 2008), indicating improvements over the hierarchical system. Our approach, using target POS tags (‘POS-tgt (no phr.s.)’), outperforms the hierarchical system on all three tests sets, and gains further improvements when accounting for phrase size (‘POS-tgt’). The latter approach is roughly on par with the corresponding Syntax system, slightly outperforming it on average, but not consistently across all test sets. The same is true for the ‘slow’ version (‘POS-tgt-slow’). The model based on bilingually tagged training instances (‘POS-src&amp;tgt’) does not gain further improvements over the merely target-based one, but actually performs worse. We assume this is due to the huge number of nonterminals of ‘POS-src&amp;tgt’ ((2 * 332 + 33)(2 * 362 + 36) = 5.8M in principle) compared to ‘POS-tgt’ (2 * 362 + 36 = 2628), increasing the sparseness of the grammar and thus leading to less reliable statistical estimates. We also experimented with a source-tag based model (‘POS-src’). In line with previous findings for syntax-augmented grammars (Zollmann and Vogel, 2010), the source-side-based grammar does not reach the translation quality of its target-based counterpart; however, the model still outperforms the hi4As shown in Zollmann et al.(2008), the impact of these rules on translation quality is negligible.erarchical system on all test sets. Further, decoding is much faster than for ‘POS-ext-tgt’ and even slightly faster than ‘Hierarchical’. This is due to the fact that for the source-tag based approach, a given chart cell in the CYK decoder, represented by a start and end position in the source sentence, almost uniquely determines the nonterminal any hypothesis in this cell can have: Disregarding partof-speech tag ambiguity and phrase size accounting, that nonterminal will be the composition of the tags of the start and end source words spanned by that cell. At the same time, this demonstrates that there is hence less of a role for the nonterminal labels to resolve translational ambiguity in the source based model than in the target based model. Performance of the word-clustering based models To empirically validate the unsupervised clustering approaches, we first need to decide how to determine the number of word classes, N. A straightforward approach is to run experiments and report test set results for many different N. While this would allow us to reliably conclude the optimal number N, a comparison of that best-performing clustering method to the hierarchical, syntax, and POS systems would be tainted by the fact that N was effectively tuned on the test sets. We therefore choose N merely based on development set performance. Unfortunately, variance in development set BLEU scores tends to be higher than test set scores, despite of SAMT MERT’s inbuilt algorithms to overcome local optima, such as random restarts and zeroing-out. We have noticed that using an L0penalized BLEU score5 as MERT’s objective on the merged n-best lists over all iterations is more stable and will therefore use this score to determine N. Figure 1 (left) shows the performance of the distributional clustering model (‘Clust’) and its morphology-sensitive extension (‘Clust-morph’) according to this score for varying values of N = 1, ... , 36 (the number Penn treebank POS tags, used for the ‘POS’ models, is 36).6 For ‘Clust’, we see a comfortably wide plateau of nearly-identical scores from N = 7, ... ,15. Scores for ‘Clust-morph’ are lower throughout, and peak at N = 7. Looking back at Table 1, we now compare the clustering models chosen by the procedure above— resulting in N = 7 for the morphology-unaware model (‘Clust-7-tgt’) as well as the morphologyaware model (‘Clust-7-morph-tgt’)—to the other systems.‘Clust-7-tgt’ improves over the hierarchical base line on all three test sets and is on par with the corresponding Syntax and POS target lines.</bodyText>
<footnote confidence="0.99481075">
5Given by: BLEU −,Q x |{i E {1, ... , K}|Ai # 0}|, where
Al, ... , Ax are the feature weights and the constant ,3 (which
we set to 0.00001) is the regularization penalty.
6All these models account for phrase size.
</footnote>
<page confidence="0.998784">
6
</page>
<table confidence="0.999967818181818">
Dev (MT04) MT05 MT06 MT08 TestAvg Time
Hierarchical 38.63 36.51 33.26 25.77 31.85 14.3
Syntax 39.39 37.09 34.01 26.53 32.54 18.1
Syntax-slow 39.69 37.56 34.66 26.93 33.05 34.6
POS-tgt (no phr. s.) 39.31 37.29 33.79 26.13 32.40 27.7
POS-tgt 39.14 37.29 33.97 26.77 32.68 19.2
POS-src 38.74 36.75 33.85 26.76 32.45 12.2
POS-src&amp;tgt 38.78 36.71 33.65 26.52 32.29 18.8
POS-tgt-slow 39.86 37.78 34.37 27.14 33.10 44.6
Clust-7-tgt 39.24 36.74 34.00 26.93 32.56 24.3
Clust-7-morph-tgt 39.08 36.57 33.81 26.40 32.26 23.6
Clust-7-src 38.68 36.17 33.23 26.55 31.98 11.1
Clust-7-src&amp;tgt 38.71 36.49 33.65 26.33 32.16 15.8
Clust-7-tgt-slow 39.48 37.70 34.31 27.24 33.08 45.2
kmeans-POS-src&amp;tgt 39.11 37.23 33.92 26.80 32.65 18.5
kmeans-POS-src&amp;tgt-L1 39.33 36.92 33.81 26.59 32.44 17.6
kmeans-POS-src&amp;tgt-cosine 39.15 37.07 33.98 26.68 32.58 17.7
kmeans-POS-src&amp;tgt (αins = .5) 39.07 36.88 33.71 26.26 32.28 16.5
kmeans-Clust-7-src&amp;tgt 39.19 36.96 34.26 26.97 32.73 19.3
kmeans-Clust-7..36-src&amp;tgt 39.09 36.93 34.24 26.92 32.70 17.3
kmeans-POS-src&amp;tgt-slow 39.28 37.16 34.38 27.11 32.88 36.3
kmeans-Clust-7..36-s&amp;t-slow 39.18 37.12 34.13 27.35 32.87 34.3
</table>
<tableCaption confidence="0.8139005">
Table 1: Translation quality in % case-insensitive IBM-BLEU (i.e., brevity penalty based on closest reference length)
for Chinese-English NIST-large translation tasks, comparing baseline Hierarchical and Syntax systems with POS and
clustering based approaches proposed in this work. ‘TestAvg’ shows the average score over the three test sets. ‘Time’
is the average decoding time per sentence in seconds on one CPU.
</tableCaption>
<bodyText confidence="0.999972065217392">The same holds for the ‘Clust-7-tgt-slow’ version. We also experimented with a model variant based on seven source and seven target language clusters (‘Clust-7-src&amp;tgt’) and a source-only labeled model (‘Clust-7-src’)—both performing worse. Surprisingly, the morphology-sensitive clustering model (‘Clust-7-morph-tgt’), while still improving over the hierarchical system, performs worse than the morphology-unaware model. An inspection of the trained word clusters showed that the model, while far superior to the morphologyunaware model in e.g.mapping all numbers to the same class, is overzealous in discovering morphological regularities (such as the ‘-ed’ suffix) to partition functionally only slightly dissimilar words (such present-tense and past-tense verbs) into different classes. While these subtle distinctions make for good partitionings when the number of clusters is large, they appear to lead to inferior results for our task that relies on coarse-grained partitionings of the vocabulary. Note that there are no ‘src’ or ‘src&amp;tgt’ systems for ‘Clust-morph’, as Chinese, being a monosyllabic writing system, does not lend itself to morphology-sensitive clustering. K-means clustering based models To establish suitable values for the α parameters and investigate the impact of the number of clusters, we looked at the development performance over various parameter combinations for a K-means model based on source and/or target part-of-speech tags.7 As can be seen from Figure 1 (right), our method reaches its peak performance at around 50 clusters and then levels off slightly. Encouragingly, in contrast to the hard labeling procedure, K-means actually improves when adding source-side information. The optimal ratio of weighting source and target classes is 0.5:1, corresponding to αuc = .5. Incorporating context information also helps, and does best for αcntxt = 0.25, i.e.when giving contexts 1/4 the influence of the phrase boundary words.</bodyText>
<footnote confidence="0.767746">
7We set αsec = .25, αins = 0, and αphrsize = .5 throughout.
</footnote>
<page confidence="0.999042">
7
</page>
<figureCaption confidence="0.98699">
Figure 1: Left: Performance of the distributional clustering model ‘Clust’ and its morphology-sensitive extension
</figureCaption>
<bodyText confidence="0.97276672972973">‘Clust-morph’ according to L0-penalized development set BLEU score for varying numbers N of word classes. For each data point N, its corresponding n.o. nonterminals of the induced grammar is stated in parentheses. Right: Dev. set performance of K-means for various n.o. labels and values of αur and αrnt,t. Entry ‘kmeans-POS-src&amp;tgt’ in Table 1 shows the test set results for the development-set best Kmeans configuration (i.e., αuc = .5, αcnt,,t = 0.25, and using 500 clusters). While beating the hierarchical baseline, it is only minimally better than the much simpler target-based hard labeling method ‘POS-tgt’. We also tried K-means variants in which the Euclidean distance metric is replaced by the city block distance L1 and the cosine dissimilarity, respectively, with slightly worse outcomes. Configuration ‘kmeans-POS-src&amp;tgt (αin� = .5)’ investigates the incorporation of non-boundary word tags inside the phrase. Unfortunately, these features appear to deteriorate performance, presumably because given a fixed number of clusters, accounting for contents inside the phrase comes at the cost of neglect of boundary words, which are more relevant to producing correctly reordered translations. The two completely unsupervised systems ‘kmeans-Clust-7-src&amp;tgt’ (based on 7-class MKCLS distributional word clustering) and ‘kmeans-Clust-7..36-src&amp;tgt’ (using six different word clustering models simultaneously: all the MKCLS models from Figure 1 (left) except for the two-, threeand five-class models) have the best results, outperforming the other K-means models as well as ‘Syntax’ and ‘POS-tgt’ on average, but not on all test sets. Lastly, we give results for ‘slow’ K-means configurations (‘kmeans-POS-src&amp;tgt-slow’ and ‘kmeansClust-7..36-s&amp;t-slow’). Unfortunately (or fortunately, from a pragmatic viewpoint), the models are outperformed by the much simpler ‘POS-tgt-slow’ and ‘Clust-7-tgt-slow’ models.</bodyText>
<sectionHeader confidence="0.999951" genericHeader="related work">
6 Related work
</sectionHeader>
<bodyText confidence="0.9999816">Hassan et al. (2007) improve the statistical phrasebased MT model by injecting supertags, lexical information such as the POS tag of the word and its subcategorization information, into the phrase table, resulting in generalized phrases with placeholders in them. The supertags are also injected into the language model. Our approach also generates phrase labels and placeholders based on word tags (albeit in a different manner and without the use of subcategorization information), but produces PSCFG rules for use in a parsing-based decoding system. Unsupervised synchronous grammar induction, apart from the contribution of Chiang (2005) discussed earlier, has been proposed by Wu (1997) for inversion transduction grammars, but as Chiang’s model only uses a single generic nonterminal label. Blunsom et al. (2009) present a nonparametric PSCFG translation model that directly induces a grammar from parallel sentences without the use of or constraints from a word-alignment model, and</bodyText>
<page confidence="0.939223">
8
</page>
<bodyText confidence="0.999678461538462">Cohn and Blunsom (2009) achieve the same for with the model of Zollmann and Venugopal (2006), tree-to-string grammars, with encouraging results using heuristically generated labels from parse trees. on small data. Our more humble approach treats Using automatically obtained word clusters instead the training sentences’ word alignments and phrase of POS tags yields essentially the same results, thus pairs, obtained from external modules, as ground making our methods applicable to all languages truth and employs a straight-forward generalization pairs with parallel corpora, whether syntactic reof Chiang’s popular rule extraction approach to lasources are available for them or not. beled phrase pairs, resulting in a PSCFG with mulWe also propose a more flexible way of obtaining tiple nonterminal labels. the phrase labels from word classes using K-means Our phrase pair clustering approach is similar in clustering. While currently the simple hard-labeling spirit to the work of Lin and Wu (2009), who use Kmethods perform just as well, we hope that the ease means to cluster (monolingual) phrases and use the of incorporating new features into the K-means laresulting clusters as features in discriminative clasbeling method will spur interesting future research. sifiers for a named-entity-recognition and a query When considering the constraints and indepenclassification task. Phrases are represented in terms dence relationships implied by each labeling apof their contexts, which can be more than one word proach, we can distinguish between approaches that long; words within the phrase are not considered. label rules differently within the context of the senFurther, each context contributes one dimension per tence that they were extracted from, and those that vocabulary word (not per word class as in our apdo not. The Syntax system from Zollmann and proach) to the feature space, allowing for the disVenugopal (2006) is at one end of this extreme. A covery of subtle semantic similarities in the phrases, given target span might be labeled differently debut at much greater computational expense. Another pending on the syntactic analysis of the sentence distinction is that Lin and Wu (2009) work with that it is a part of. On the other extreme, the clusphrase types instead of phrase instances, obtaining tering based approach labels phrases based on the a phrase type’s contexts by averaging the contexts contained words alone.8 The POS grammar repreof all its phrase instances. sents an intermediate point on this spectrum, since Nagata et al. (2006) present a reordering model POS tags can change based on surrounding words in for machine translation, and make use of clustered the sentence; and the position of the K-means model phrase pairs to cope with data sparseness in the depends on the influence of the phrase contexts on model. They achieve the clustering by reducing the clustering process. Context insensitive labeling phrases to their head words and then applying the has the advantage that there are less alternative leftMKCLS tool to these pseudo-words. hand-side labels for initial rules, producing gramKuhn et al. (2010) cluster the phrase pairs of mars with less rules, whose weights can be more an SMT phrase table based on their co-occurrence accurately estimated. This could explain the strong counts and edit distances in order to arrive at semanperformance of the word-clustering based labeling tically similar phrases for the purpose of phrase table approach. smoothing. The clustering proceeds in a bottom-up All source code underlying this work is available fashion, gradually merging similar phrases while alunder the GNU Lesser General Public License as ternating back and forth between the two languages. part of the Hadoop-based ‘SAMT’ system at: 7 Conclusion and discussion www.cs.cmu.edu/˜zollmann/samt In this work we proposed methods of labeling phrase Acknowledgments pairs to create automatically learned PSCFG rules We thank Jakob Uszkoreit and Ashish Venugopal for for machine translation. Crucially, our methods only helpful comments and suggestions and Yahoo! for rely on “shallow” lexical tags, either generated by the access to the M45 supercomputing cluster. POS taggers or by automatic clustering of words into classes. Evaluated on a Chinese-to-English translation task, our approach improves translation quality over a popular PSCFG baseline—the hierarchical model of Chiang (2005) —and performs on par 9 8Note, however, that the creation of clusters itself did take the context of the clustered words into account.</bodyText>
<sectionHeader confidence="0.996062" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999601209523809">
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of ACL,
Singapore, August.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2).
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, Honolulu, Hawaii, October.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics (ACL).
David Chiang. 2007. Hierarchical phrase based transla-
tion. Computational Linguistics, 33(2).
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of the European chapter of the
Association for Computational Linguistics (EACL),
pages 59–66.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian model
of syntax-directed tree to string grammar induction.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
Singapore.
Michael Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Proceedings of the Human Language Technology
Conference of the North American Chapter of the As-
sociation for Computational Linguistics Conference
(HLT/NAACL).
Hany Hassan, Khalil Sima’an, and Andy Way. 2007. Su-
pertagged phrase-based statistical machine translation.
In Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, Prague, Czech
Republic, June.
Dan Klein and Christoper Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics (ACL).
Reinhard Kneser and Hermann Ney. 1993. Improved
clustering techniques for class-based statistical lan-
guage modelling. In Proceedings of the 3rd European
Conference on Speech Communication and Technol-
ogy, pages 973–976, Berlin, Germany.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics Conference (HLT/NAACL).
Roland Kuhn, Boxing Chen, George Foster, and Evan
Stratford. 2010. Phrase clustering for smoothing
TM probabilities - or, how to extract paraphrases from
phrase tables. In Proceedings of the 23rd Interna-
tional Conference on Computational Linguistics (Col-
ing 2010), pages 608–616, Beijing, China, August.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proceedings of the 47th
Annual Meeting of the Association for Computational
Linguistics (ACL).
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics.
J. B. MacQueen. 1967. Some methods for classification
and analysis of multivariate observations. In L. M. Le
Cam and J. Neyman, editors, Proc. of the fifth Berkeley
Symposium on Mathematical Statistics and Probabil-
ity, volume 1, pages 281–297. University of California
Press.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), Syd-
ney, Australia.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP).
Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto,
and Kazuteru Ohashi. 2006. A clustered global phrase
reordering model for statistical machine translation. In
Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meet-
ing of the Association for Computational Linguistics,
ACL-44, pages 713–720.
Franz Josef Och. 1999. An efficient method for de-
termining bilingual word classes. In Proceedings of
the European chapter of the Association for Computa-
tional Linguistics (EACL), pages 71–76.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (ACL).
Hugo Steinhaus. 1956. Sur la division des corps
mat´eriels en parties. Bull. Acad. Polon. Sci. Cl. III.
4, pages 801–804.
</reference>
<page confidence="0.949445">
10
</page>
<reference confidence="0.999188428571429">
Ashish Venugopal and Andreas Zollmann. 2009. Gram-
mar based statistical MT on Hadoop: An end-to-end
toolkit for large scale PSCFG based MT. The Prague
Bulletin of Mathematical Linguistics, 91:67–78.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3).
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, HLT/NAACL.
Andreas Zollmann and Stephan Vogel. 2010. New
parameterizations and features for PSCFG-based ma-
chine translation. In Proceedings of the 4th Work-
shop on Syntax and Structure in Statistical Translation
(SSST), Beijing, China.
Andreas Zollmann, Ashish Venugopal, Franz J. Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
MT. In Proceedings of the Conference on Computa-
tional Linguistics (COLING).
</reference>
<page confidence="0.999485">
11
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant confidence="0.715415" no="0">
<title confidence="0.853744666666667">A Word-Class Approach to Labeling PSCFG Rules for Machine Translation Zollmann Language Technologies</title>
<affiliation confidence="0.9962185">School of Computer Carnegie Mellon</affiliation>
<address confidence="0.993021">Pittsburgh, PA 15213,</address>
<abstract confidence="0.999151">In this work we propose methods to label probabilistic synchronous context-free grammar (PSCFG) rules using only word tags, generated by either part-of-speech analysis or unsupervised word class induction. The proposals range from simple tag-combination schemes to a phrase clustering model that can incorporate an arbitrary number of features. Our models improve translation quality over the single generic label approach of Chiang (2005) and perform on par with the syntactically motivated approach from Zollmann and Venugopal (2006) on the NIST large Chineseto-English translation task. These results persist when using automatically learned word tags, suggesting broad applicability of our technique across diverse language pairs for which syntactic resources are not available.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Chris Dyer</author>
<author>Miles Osborne</author>
</authors>
<title>A Gibbs sampler for phrasal synchronous grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL, Singapore,</booktitle>
<contexts>
<context citStr="Blunsom et al. (2009)" endWordPosition="5336" position="33173" startWordPosition="5333">table, resulting in generalized phrases with placeholders in them. The supertags are also injected into the language model. Our approach also generates phrase labels and placeholders based on word tags (albeit in a different manner and without the use of subcategorization information), but produces PSCFG rules for use in a parsing-based decoding system. Unsupervised synchronous grammar induction, apart from the contribution of Chiang (2005) discussed earlier, has been proposed by Wu (1997) for inversion transduction grammars, but as Chiang’s model only uses a single generic nonterminal label. Blunsom et al. (2009) present a nonparametric PSCFG translation model that directly induces a grammar from parallel sentences without the use of or constraints from a word-alignment model, and 8 Cohn and Blunsom (2009) achieve the same for with the model of Zollmann and Venugopal (2006), tree-to-string grammars, with encouraging results using heuristically generated labels from parse trees. on small data. Our more humble approach treats Using automatically obtained word clusters instead the training sentences’ word alignments and phrase of POS tags yields essentially the same results, thus pairs, obtained from ext</context>
</contexts>
<marker>Blunsom, Cohn, Dyer, Osborne, 2009</marker>
<rawString>Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Osborne. 2009. A Gibbs sampler for phrasal synchronous grammar induction. In Proceedings of ACL, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context citStr="Brown et al., 1993" endWordPosition="316" position="2180" startWordPosition="313"> and imply conditional independence assumptions in the translation model. Several techniques have been recently proposed to automatically identify and estimate parameters for PSCFGs (or related synchronous grammars) from parallel corpora (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006; Marcu et al., 2006). 1 While all of these techniques rely on wordalignments to suggest lexical relationships, they differ in the way in which they assign labels to nonterminal symbols of PSCFG rules. Chiang (2005) describes a procedure to extract PSCFG rules from word-aligned (Brown et al., 1993) corpora, where all nonterminals share the same generic label X. In Galley et al. (2004) and Marcu et al. (2006), target language parse trees are used to identify rules and label their nonterminal symbols, while Liu et al. (2006) use source language parse trees instead. Zollmann and Venugopal (2006) directly extend the rule extraction procedure from Chiang (2005) to heuristically label any phrase pair based on target language parse trees. Label-based approaches have resulted in improvements in translation quality over the single X label approach (Zollmann et al., 2008; Mi and Huang, 2008); how</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Honolulu, Hawaii,</location>
<contexts>
<context citStr="Chiang et al. (2008)" endWordPosition="1549" position="9462" startWordPosition="1546">with tag sequence T1T2 are labeled 2: PRP+PRP → ihn |him T1-T2 as before. Phrases of length greater two with 3: VBN+VBD → gesehen |saw tag sequence T1 · · · Tn are labeled T1..Tn to denote 4: AUX..VBN+VBD-PRP → habe ihn that tags were omitted from the phrase’s tag se- gesehen |saw him quence. The resulting number of grammar nonter- 5: PRP..VBN+PRP..PRP → Ich habe ihn minals based on a tag vocabulary of size t is thus gesehen |I saw him given by 2t2 + t. Abstracting-out rule 2 from rule 4, for instance, An alternative way of accounting for phrase size leads to the complex rule: is presented by Chiang et al. (2008), who intro- AUX..VBN+VBD-PRP → habe PRP+PRP, duce structural distortion features into a hierarchi- gesehen |saw PRP+PRP, cal phrase-based model, aimed at modeling nonter- Unsupervised word class assignment by clusterminal reordering given source span length. Our ing As an alternative to POS tags, we experiment approach instead uses distinct grammar rules and with unsupervised word clustering methods based labels to discriminate phrase size, with the advan- on the exchange algorithm (Kneser and Ney, 1993). tage of enabling all translation models to estimate Its objective function is maximizing</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, Honolulu, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context citStr="Chiang (2005)" endWordPosition="93" position="694" startWordPosition="92">lmann and Stephan Vogel Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA {zollmann,vogel+}@cs.cmu.edu Abstract In this work we propose methods to label probabilistic synchronous context-free grammar (PSCFG) rules using only word tags, generated by either part-of-speech analysis or unsupervised word class induction. The proposals range from simple tag-combination schemes to a phrase clustering model that can incorporate an arbitrary number of features. Our models improve translation quality over the single generic label approach of Chiang (2005) and perform on par with the syntactically motivated approach from Zollmann and Venugopal (2006) on the NIST large Chineseto-English translation task. These results persist when using automatically learned word tags, suggesting broad applicability of our technique across diverse language pairs for which syntactic resources are not available. 1 Introduction The Probabilistic Synchronous Context Free Grammar (PSCFG) formalism suggests an intuitive approach to model the long-distance and lexically sensitive reordering phenomena that often occur across language pairs considered for statistical mac</context>
<context citStr="Chiang (2005)" endWordPosition="303" position="2096" startWordPosition="302">used to enforce syntactic constraints in the generation of bilingual sentences and imply conditional independence assumptions in the translation model. Several techniques have been recently proposed to automatically identify and estimate parameters for PSCFGs (or related synchronous grammars) from parallel corpora (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006; Marcu et al., 2006). 1 While all of these techniques rely on wordalignments to suggest lexical relationships, they differ in the way in which they assign labels to nonterminal symbols of PSCFG rules. Chiang (2005) describes a procedure to extract PSCFG rules from word-aligned (Brown et al., 1993) corpora, where all nonterminals share the same generic label X. In Galley et al. (2004) and Marcu et al. (2006), target language parse trees are used to identify rules and label their nonterminal symbols, while Liu et al. (2006) use source language parse trees instead. Zollmann and Venugopal (2006) directly extend the rule extraction procedure from Chiang (2005) to heuristically label any phrase pair based on target language parse trees. Label-based approaches have resulted in improvements in translation quali</context>
<context citStr="Chiang (2005)" endWordPosition="880" position="5505" startWordPosition="879">as in Chiang’s model. • A E N is a labeled nonterminal referred to as the Consider the target-tagged example sentence pair: left-hand-side of the rule, Ich habe ihn gesehen |I/PRP saw/VBD him/PRP • ry E (N U TS)* is the source side of the rule, Then (depending on the extracted phrase pairs), the • α E (N U TT)* is the target side of the rule, resulting initial rules could be: • w E [0, oc) is a non-negative real-valued weight 1: PRP-PRP → Ich |I assigned to the rule; in our model, w is the product 2: PRP-PRP → ihn |him of features Oi raised to the power of weight Ai. 3: VBD-VBD → gesehen |saw Chiang (2005) learns a single-nonterminal PSCFG 4: VBD-PRP → habe ihn gesehen |saw him from a bilingual corpus by first identifying initial 5: PRP-PRP → Ich habe ihn gesehen |I saw him phrase pairs using the technique from Koehn et al. Now, by abstracting-out initial rule 2 from initial (2003), and then performing a generalization opera- rule 4, we obtain the complex rule: tion to generate phrase pairs with gaps, which can be VBD-PRP → habe PRP-PRPI gesehen |saw PRP-PRPI viewed as PSCFG rules with generic ‘X’ nontermi- Intuitively, the labeling of initial rules with tags nal left-hand-sides and substitutio</context>
<context citStr="Chiang (2005)" endWordPosition="5307" position="32996" startWordPosition="5306"> the statistical phrasebased MT model by injecting supertags, lexical information such as the POS tag of the word and its subcategorization information, into the phrase table, resulting in generalized phrases with placeholders in them. The supertags are also injected into the language model. Our approach also generates phrase labels and placeholders based on word tags (albeit in a different manner and without the use of subcategorization information), but produces PSCFG rules for use in a parsing-based decoding system. Unsupervised synchronous grammar induction, apart from the contribution of Chiang (2005) discussed earlier, has been proposed by Wu (1997) for inversion transduction grammars, but as Chiang’s model only uses a single generic nonterminal label. Blunsom et al. (2009) present a nonparametric PSCFG translation model that directly induces a grammar from parallel sentences without the use of or constraints from a word-alignment model, and 8 Cohn and Blunsom (2009) achieve the same for with the model of Zollmann and Venugopal (2006), tree-to-string grammars, with encouraging results using heuristically generated labels from parse trees. on small data. Our more humble approach treats Usi</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context citStr="Chiang, 2007" endWordPosition="3174" position="19183" startWordPosition="3173">s We evaluate our approach by comparing translation quality, as evaluated by the IBM-BLEU (Papineni et al., 2002) metric on the NIST Chinese-to-English translation task using MT04 as development set to train the model parameters A, and MT05, MT06 and MT08 as test sets. Even though a key advantage of our method is its applicability to resource-poor languages, we used a language pair for which linguistic resources are available in order to determine how close translation performance can get to a fully syntax-based system. Accordingly, we use Chiang’s hierarchical phrase based translation model (Chiang, 2007) as a base line, and the syntax-augmented MT model (Zollmann and Venugopal, 2006) as a ‘target line’, a model that would not be applicable for language pairs without linguistic resources. We perform PSCFG rule extraction and decoding using the open-source “SAMT” system (Venugopal and Zollmann, 2009), using the provided implementations for the hierarchical and syntax-augmented grammars. Apart from the language model, the lexical, phrasal, and (for the syntax grammar) labelconditioned features, and the rule, target word, and glue operation counters, Venugopal and Zollmann (2009) also provide bot</context>
<context citStr="Chiang (2007)" endWordPosition="3504" position="21295" startWordPosition="3503">y extract rules from initial phrase pairs of maximum source length 15. All rules have at most two nonterminal symbols, which must be non-consecutive on the source side, and rules must contain at least one source-side terminal symbol. The beam settings for the hierarchical system are 600 items per ‘X’ (generic rule) cell, and 600 per ‘S’ (glue) cell.3 Due to memory limitations, the multi-nonterminal grammars have to be pruned more harshly: We al2Penalization or reward of purely-lexical rules can be indirectly learned by trading off these features with the rule counter feature. 3For comparison, Chiang (2007) uses 30 and 15, respectively, and further prunes items that deviate too much in score from the best item. He extracts initial phrases of maximum length 10. 5 low 100 ‘S’ items, and a total of 500 non-‘S’ items, but maximally 40 items per nonterminal. For all systems, we further discard non-initial rules occurring only once.4 For the multi-nonterminal systems, we generally further discard all non-generic non-initial rules occurring less than 6 times, but we additionally give results for a ‘slow’ version of the Syntax targetline system and our best word class based systems, where only single-oc</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase based translation. Computational Linguistics, 33(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Combining distributional and morphological information for part of speech induction.</title>
<date>2003</date>
<booktitle>In Proceedings of the European chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>59--66</pages>
<contexts>
<context citStr="Clark (2003)" endWordPosition="1868" position="11409" startWordPosition="1867">mber of classes, which has to be cholation scenarios, some form of word tags, be it part- sen a priori. We use the publicly available impleof-speech tags or learned word clusters (cf. Sec- mentation MKCLS (Och, 1999) to train this model. tion 3) might be available on both sides. In this case, As training data we use the respective side of the our grammar extraction procedure can be easily ex- parallel training data for the translation system. tended to impose both source and target constraints We also experiment with the extension of this on the eligible substitutions simultaneously. model by Clark (2003), who incorporated morphoLet Nf be the nonterminal label that would be logical information by imposing a Bayesian prior assigned to a given initial rule when utilizing the on the class mapping c, based on N individual dissource-side tag sequence, and Ne the assigned la- tributions over strings, one for each word class. bel according to the target-side tag sequence. Then Each such distribution is a character-based hidden our bilingual tag-based model assigns ‘Nf + Ne’ Markov model, thus encouraging the grouping of to the initial rule. The extraction of complex rules morphologically similar word</context>
</contexts>
<marker>Clark, 2003</marker>
<rawString>Alexander Clark. 2003. Combining distributional and morphological information for part of speech induction. In Proceedings of the European chapter of the Association for Computational Linguistics (EACL), pages 59–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
</authors>
<title>A Bayesian model of syntax-directed tree to string grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<contexts>
<context citStr="Cohn and Blunsom (2009)" endWordPosition="5367" position="33370" startWordPosition="5364">d tags (albeit in a different manner and without the use of subcategorization information), but produces PSCFG rules for use in a parsing-based decoding system. Unsupervised synchronous grammar induction, apart from the contribution of Chiang (2005) discussed earlier, has been proposed by Wu (1997) for inversion transduction grammars, but as Chiang’s model only uses a single generic nonterminal label. Blunsom et al. (2009) present a nonparametric PSCFG translation model that directly induces a grammar from parallel sentences without the use of or constraints from a word-alignment model, and 8 Cohn and Blunsom (2009) achieve the same for with the model of Zollmann and Venugopal (2006), tree-to-string grammars, with encouraging results using heuristically generated labels from parse trees. on small data. Our more humble approach treats Using automatically obtained word clusters instead the training sentences’ word alignments and phrase of POS tags yields essentially the same results, thus pairs, obtained from external modules, as ground making our methods applicable to all languages truth and employs a straight-forward generalization pairs with parallel corpora, whether syntactic reof Chiang’s popular rule</context>
</contexts>
<marker>Cohn, Blunsom, 2009</marker>
<rawString>Trevor Cohn and Phil Blunsom. 2009. A Bayesian model of syntax-directed tree to string grammar induction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP), Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics Conference (HLT/NAACL).</booktitle>
<contexts>
<context citStr="Galley et al., 2004" endWordPosition="254" position="1819" startWordPosition="250">ive reordering phenomena that often occur across language pairs considered for statistical machine translation. As in monolingual parsing, nonterminal symbols in translation rules are used to generalize beyond purely lexical operations. Labels on these nonterminal symbols are often used to enforce syntactic constraints in the generation of bilingual sentences and imply conditional independence assumptions in the translation model. Several techniques have been recently proposed to automatically identify and estimate parameters for PSCFGs (or related synchronous grammars) from parallel corpora (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006; Marcu et al., 2006). 1 While all of these techniques rely on wordalignments to suggest lexical relationships, they differ in the way in which they assign labels to nonterminal symbols of PSCFG rules. Chiang (2005) describes a procedure to extract PSCFG rules from word-aligned (Brown et al., 1993) corpora, where all nonterminals share the same generic label X. In Galley et al. (2004) and Marcu et al. (2006), target language parse trees are used to identify rules and label their nonterminal symbols, while Liu et al. (2006) use sourc</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michael Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics Conference (HLT/NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hany Hassan</author>
<author>Khalil Sima’an</author>
<author>Andy Way</author>
</authors>
<title>Supertagged phrase-based statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<location>Prague, Czech Republic,</location>
<marker>Hassan, Sima’an, Way, 2007</marker>
<rawString>Hany Hassan, Khalil Sima’an, and Andy Way. 2007. Supertagged phrase-based statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christoper Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context citStr="Klein and Manning, 2003" endWordPosition="3692" position="22506" startWordPosition="3689">nly single-occurrences were removed. For parameter tuning, we use the L0-regularized minimum-error-rate training tool provided by the SAMT toolkit. Each system is trained separately to adapt the parameters to its specific properties (size of nonterminal set, grammar complexity, features sparseness, reliance on the language model, etc.). The parallel training data comprises of 9.6M sentence pairs (206M Chinese and 228M English words). The source and target language parses for the syntax-augmented grammar, as well as the POS tags for our POS-based grammars were generated by the Stanford parser (Klein and Manning, 2003). The results are given in Table 1. Results for the Syntax system are consistent with previous results (Zollmann et al., 2008), indicating improvements over the hierarchical system. Our approach, using target POS tags (‘POS-tgt (no phr. s.)’), outperforms the hierarchical system on all three tests sets, and gains further improvements when accounting for phrase size (‘POS-tgt’). The latter approach is roughly on par with the corresponding Syntax system, slightly outperforming it on average, but not consistently across all test sets. The same is true for the ‘slow’ version (‘POS-tgt-slow’). The </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christoper Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved clustering techniques for class-based statistical language modelling.</title>
<date>1993</date>
<booktitle>In Proceedings of the 3rd European Conference on Speech Communication and Technology,</booktitle>
<pages>973--976</pages>
<location>Berlin, Germany.</location>
<contexts>
<context citStr="Kneser and Ney, 1993" endWordPosition="507" position="3368" startWordPosition="504">., 2008; Mi and Huang, 2008); however, all the works cited here rely on stochastic parsers that have been trained on manually created syntactic treebanks. These treebanks are difficult and expensive to produce and exist for a limited set of languages only. In this work, we propose a labeling approach that is based merely on part-of-speech analysis of the source or target language (or even both). Towards the ultimate goal of building end-to-end machine translation systems without any human annotations, we also experiment with automatically inferred word classes using distributional clustering (Kneser and Ney, 1993). Since the number of classes is a parameter of the clustering method and the resulting nonterminal size of our grammar is a function of the number of word classes, the PSCFG grammar complexity can be adjusted to the specific translation task at hand. Finally, we introduce a more flexible labeling approach based on K-means clustering, which allows Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1–11, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics the incorporation of an arbitrary number of word- ti • • • tn it</context>
<context citStr="Kneser and Ney, 1993" endWordPosition="1623" position="9972" startWordPosition="1620">n alternative way of accounting for phrase size leads to the complex rule: is presented by Chiang et al. (2008), who intro- AUX..VBN+VBD-PRP → habe PRP+PRP, duce structural distortion features into a hierarchi- gesehen |saw PRP+PRP, cal phrase-based model, aimed at modeling nonter- Unsupervised word class assignment by clusterminal reordering given source span length. Our ing As an alternative to POS tags, we experiment approach instead uses distinct grammar rules and with unsupervised word clustering methods based labels to discriminate phrase size, with the advan- on the exchange algorithm (Kneser and Ney, 1993). tage of enabling all translation models to estimate Its objective function is maximizing the likelihood distinct weights for distinct size classes and avoid- n ing the need of additional models in the log-linear P(wi|w1, ... , wi−1) framework; however, the increase in the number of i=1 labels and thus grammar rules decreases the relia- of the training data w = w1, ... , wn given a parbility of estimated models for rare events due to in- tially class-based bigram model of the form creased data sparseness. P(wi|w1,...,wi−1) ≈ Xc(wi)|wi−1)·Awi|c(wi)) Extension to a bilingually tagged corpus Whi</context>
</contexts>
<marker>Kneser, Ney, 1993</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1993. Improved clustering techniques for class-based statistical language modelling. In Proceedings of the 3rd European Conference on Speech Communication and Technology, pages 973–976, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics Conference (HLT/NAACL).</booktitle>
<contexts>
<context citStr="Koehn et al. (2003)" endWordPosition="1239" position="7704" startWordPosition="1236">ntactic, statistical, or other means of the training corpus and thus are already consistent, grouping word types or tokens into classes, possibly the boundary word tags are more informative than based on their position and context in the sentence, tags of words between the boundaries for the task POS tagging being the most obvious example. of combining different rules in a derivation, and are As in Chiang’s hierarchical system, we rely on therefore a more appropriate choice for the creation an external phrase-extraction procedure such as the of grammar labels than tags of inside words. one of Koehn et al. (2003) to provide us with a set Accounting for phrase size A drawback of the of phrase pairs for each sentence pair in the train- current approach is that a single-word rule such as ing corpus, annotated with their respective start and PRP-PRP —* Ich |I end positions in the source and target sentences. Let f = fi • • • fm be the current source sentence, e = el • • • en the current target sentence, and t = 2 can have the same left-hand-side nonterminal as a Consider again our example sentence pair (now long rule with identical left and right boundary tags, also annotated with source-side part-of-spee</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics Conference (HLT/NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Kuhn</author>
<author>Boxing Chen</author>
<author>George Foster</author>
<author>Evan Stratford</author>
</authors>
<title>Phrase clustering for smoothing TM probabilities - or, how to extract paraphrases from phrase tables.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>608--616</pages>
<location>Beijing, China,</location>
<contexts>
<context citStr="Kuhn et al. (2010)" endWordPosition="5866" position="36520" startWordPosition="5862">rum, since Nagata et al. (2006) present a reordering model POS tags can change based on surrounding words in for machine translation, and make use of clustered the sentence; and the position of the K-means model phrase pairs to cope with data sparseness in the depends on the influence of the phrase contexts on model. They achieve the clustering by reducing the clustering process. Context insensitive labeling phrases to their head words and then applying the has the advantage that there are less alternative leftMKCLS tool to these pseudo-words. hand-side labels for initial rules, producing gramKuhn et al. (2010) cluster the phrase pairs of mars with less rules, whose weights can be more an SMT phrase table based on their co-occurrence accurately estimated. This could explain the strong counts and edit distances in order to arrive at seman- performance of the word-clustering based labeling tically similar phrases for the purpose of phrase table approach. smoothing. The clustering proceeds in a bottom-up All source code underlying this work is available fashion, gradually merging similar phrases while al- under the GNU Lesser General Public License as ternating back and forth between the two languages.</context>
</contexts>
<marker>Kuhn, Chen, Foster, Stratford, 2010</marker>
<rawString>Roland Kuhn, Boxing Chen, George Foster, and Evan Stratford. 2010. Phrase clustering for smoothing TM probabilities - or, how to extract paraphrases from phrase tables. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 608–616, Beijing, China, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Xiaoyun Wu</author>
</authors>
<title>Phrase clustering for discriminative learning.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context citStr="Lin and Wu (2009)" endWordPosition="5515" position="34355" startWordPosition="5512">ults, thus pairs, obtained from external modules, as ground making our methods applicable to all languages truth and employs a straight-forward generalization pairs with parallel corpora, whether syntactic reof Chiang’s popular rule extraction approach to la- sources are available for them or not. beled phrase pairs, resulting in a PSCFG with mul- We also propose a more flexible way of obtaining tiple nonterminal labels. the phrase labels from word classes using K-means Our phrase pair clustering approach is similar in clustering. While currently the simple hard-labeling spirit to the work of Lin and Wu (2009), who use K- methods perform just as well, we hope that the ease means to cluster (monolingual) phrases and use the of incorporating new features into the K-means laresulting clusters as features in discriminative clas- beling method will spur interesting future research. sifiers for a named-entity-recognition and a query When considering the constraints and indepenclassification task. Phrases are represented in terms dence relationships implied by each labeling apof their contexts, which can be more than one word proach, we can distinguish between approaches that long; words within the phrase</context>
</contexts>
<marker>Lin, Wu, 2009</marker>
<rawString>Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering for discriminative learning. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-tostring alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context citStr="Liu et al., 2006" endWordPosition="265" position="1881" startWordPosition="262">onsidered for statistical machine translation. As in monolingual parsing, nonterminal symbols in translation rules are used to generalize beyond purely lexical operations. Labels on these nonterminal symbols are often used to enforce syntactic constraints in the generation of bilingual sentences and imply conditional independence assumptions in the translation model. Several techniques have been recently proposed to automatically identify and estimate parameters for PSCFGs (or related synchronous grammars) from parallel corpora (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006; Marcu et al., 2006). 1 While all of these techniques rely on wordalignments to suggest lexical relationships, they differ in the way in which they assign labels to nonterminal symbols of PSCFG rules. Chiang (2005) describes a procedure to extract PSCFG rules from word-aligned (Brown et al., 1993) corpora, where all nonterminals share the same generic label X. In Galley et al. (2004) and Marcu et al. (2006), target language parse trees are used to identify rules and label their nonterminal symbols, while Liu et al. (2006) use source language parse trees instead. Zollmann and Venugopal (2006) </context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-tostring alignment template for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B MacQueen</author>
</authors>
<title>Some methods for classification and analysis of multivariate observations.</title>
<date>1967</date>
<booktitle>Proc. of the fifth Berkeley Symposium on Mathematical Statistics and Probability,</booktitle>
<volume>1</volume>
<pages>281--297</pages>
<editor>In L. M. Le Cam and J. Neyman, editors,</editor>
<publisher>University of California Press.</publisher>
<contexts>
<context citStr="MacQueen, 1967" endWordPosition="2930" position="17676" startWordPosition="2929"> vector, all further multiplied by a parameter αgrc that allows trading off the relevance of source-side and target-side information. In the same fashion, we can incorporate multiple tagging schemes (e.g., word clusterings of different granularities) into the same feature vector. As finergrained schemes have more elements in the feature vector than coarser-grained ones, and thus exert more influence, we set the α parameter for each scheme to 1/N (where N is the number of word classes of the scheme). The K-means algorithm To create the clusters, we chose the K-means algorithm (Steinhaus, 1956; MacQueen, 1967) for both its computational efficiency and ease of implementation and parallelization. Given an initial mapping from the data points to K clusters, the procedure alternates between (i) computing the centroid of each cluster and (ii) reallocating each data point to the closest cluster centroid, until convergence. We implemented two commonly used initialization methods: Forgy and Random Partition. The Forgy method randomly chooses K observations from the data set and uses these as the initial means. The Random Partition method first randomly assigns a cluster to each observation and then proceed</context>
</contexts>
<marker>MacQueen, 1967</marker>
<rawString>J. B. MacQueen. 1967. Some methods for classification and analysis of multivariate observations. In L. M. Le Cam and J. Neyman, editors, Proc. of the fifth Berkeley Symposium on Mathematical Statistics and Probability, volume 1, pages 281–297. University of California Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Wei Wang</author>
<author>Abdessamad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>SPMT: Statistical machine translation with syntactified target language phrases.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context citStr="Marcu et al., 2006" endWordPosition="269" position="1902" startWordPosition="266">istical machine translation. As in monolingual parsing, nonterminal symbols in translation rules are used to generalize beyond purely lexical operations. Labels on these nonterminal symbols are often used to enforce syntactic constraints in the generation of bilingual sentences and imply conditional independence assumptions in the translation model. Several techniques have been recently proposed to automatically identify and estimate parameters for PSCFGs (or related synchronous grammars) from parallel corpora (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006; Marcu et al., 2006). 1 While all of these techniques rely on wordalignments to suggest lexical relationships, they differ in the way in which they assign labels to nonterminal symbols of PSCFG rules. Chiang (2005) describes a procedure to extract PSCFG rules from word-aligned (Brown et al., 1993) corpora, where all nonterminals share the same generic label X. In Galley et al. (2004) and Marcu et al. (2006), target language parse trees are used to identify rules and label their nonterminal symbols, while Liu et al. (2006) use source language parse trees instead. Zollmann and Venugopal (2006) directly extend the r</context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. SPMT: Statistical machine translation with syntactified target language phrases. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
</authors>
<title>Forest-based translation rule extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context citStr="Mi and Huang, 2008" endWordPosition="413" position="2775" startWordPosition="410">ned (Brown et al., 1993) corpora, where all nonterminals share the same generic label X. In Galley et al. (2004) and Marcu et al. (2006), target language parse trees are used to identify rules and label their nonterminal symbols, while Liu et al. (2006) use source language parse trees instead. Zollmann and Venugopal (2006) directly extend the rule extraction procedure from Chiang (2005) to heuristically label any phrase pair based on target language parse trees. Label-based approaches have resulted in improvements in translation quality over the single X label approach (Zollmann et al., 2008; Mi and Huang, 2008); however, all the works cited here rely on stochastic parsers that have been trained on manually created syntactic treebanks. These treebanks are difficult and expensive to produce and exist for a limited set of languages only. In this work, we propose a labeling approach that is based merely on part-of-speech analysis of the source or target language (or even both). Towards the ultimate goal of building end-to-end machine translation systems without any human annotations, we also experiment with automatically inferred word classes using distributional clustering (Kneser and Ney, 1993). Since</context>
</contexts>
<marker>Mi, Huang, 2008</marker>
<rawString>Haitao Mi and Liang Huang. 2008. Forest-based translation rule extraction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
<author>Kuniko Saito</author>
<author>Kazuhide Yamamoto</author>
<author>Kazuteru Ohashi</author>
</authors>
<title>A clustered global phrase reordering model for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>713--720</pages>
<contexts>
<context citStr="Nagata et al. (2006)" endWordPosition="5771" position="35933" startWordPosition="5768">this extreme. A covery of subtle semantic similarities in the phrases, given target span might be labeled differently debut at much greater computational expense. Another pending on the syntactic analysis of the sentence distinction is that Lin and Wu (2009) work with that it is a part of. On the other extreme, the clusphrase types instead of phrase instances, obtaining tering based approach labels phrases based on the a phrase type’s contexts by averaging the contexts contained words alone.8 The POS grammar repreof all its phrase instances. sents an intermediate point on this spectrum, since Nagata et al. (2006) present a reordering model POS tags can change based on surrounding words in for machine translation, and make use of clustered the sentence; and the position of the K-means model phrase pairs to cope with data sparseness in the depends on the influence of the phrase contexts on model. They achieve the clustering by reducing the clustering process. Context insensitive labeling phrases to their head words and then applying the has the advantage that there are less alternative leftMKCLS tool to these pseudo-words. hand-side labels for initial rules, producing gramKuhn et al. (2010) cluster the </context>
</contexts>
<marker>Nagata, Saito, Yamamoto, Ohashi, 2006</marker>
<rawString>Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto, and Kazuteru Ohashi. 2006. A clustered global phrase reordering model for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44, pages 713–720.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>An efficient method for determining bilingual word classes.</title>
<date>1999</date>
<booktitle>In Proceedings of the European chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>71--76</pages>
<contexts>
<context citStr="Och, 1999" endWordPosition="1803" position="11013" startWordPosition="1802"> due to in- tially class-based bigram model of the form creased data sparseness. P(wi|w1,...,wi−1) ≈ Xc(wi)|wi−1)·Awi|c(wi)) Extension to a bilingually tagged corpus While where c : V → {1, ... , N} maps a word (type, not the availability of syntactic annotations for both token) w to its class c(w), V is the vocabulary, and source and target language is unlikely in most trans- N the fixed number of classes, which has to be cholation scenarios, some form of word tags, be it part- sen a priori. We use the publicly available impleof-speech tags or learned word clusters (cf. Sec- mentation MKCLS (Och, 1999) to train this model. tion 3) might be available on both sides. In this case, As training data we use the respective side of the our grammar extraction procedure can be easily ex- parallel training data for the translation system. tended to impose both source and target constraints We also experiment with the extension of this on the eligible substitutions simultaneously. model by Clark (2003), who incorporated morphoLet Nf be the nonterminal label that would be logical information by imposing a Bayesian prior assigned to a given initial rule when utilizing the on the class mapping c, based on</context>
</contexts>
<marker>Och, 1999</marker>
<rawString>Franz Josef Och. 1999. An efficient method for determining bilingual word classes. In Proceedings of the European chapter of the Association for Computational Linguistics (EACL), pages 71–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context citStr="Papineni et al., 2002" endWordPosition="3096" position="18683" startWordPosition="3093">om Partition. The Forgy method randomly chooses K observations from the data set and uses these as the initial means. The Random Partition method first randomly assigns a cluster to each observation and then proceeds straight to step (ii). Forgy tends to spread the initial means out, while Random Partition places all of them close to the center of the data set. As the resulting clusters looked similar, and Random Partition sometimes led to a high rate of empty clusters, we settled for Forgy. 5 Experiments We evaluate our approach by comparing translation quality, as evaluated by the IBM-BLEU (Papineni et al., 2002) metric on the NIST Chinese-to-English translation task using MT04 as development set to train the model parameters A, and MT05, MT06 and MT08 as test sets. Even though a key advantage of our method is its applicability to resource-poor languages, we used a language pair for which linguistic resources are available in order to determine how close translation performance can get to a fully syntax-based system. Accordingly, we use Chiang’s hierarchical phrase based translation model (Chiang, 2007) as a base line, and the syntax-augmented MT model (Zollmann and Venugopal, 2006) as a ‘target line’</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Steinhaus</author>
</authors>
<title>Sur la division des corps mat´eriels en parties.</title>
<date>1956</date>
<journal>Bull. Acad. Polon. Sci. Cl. III.</journal>
<volume>4</volume>
<pages>801--804</pages>
<contexts>
<context citStr="Steinhaus, 1956" endWordPosition="2928" position="17659" startWordPosition="2927">ones above to the vector, all further multiplied by a parameter αgrc that allows trading off the relevance of source-side and target-side information. In the same fashion, we can incorporate multiple tagging schemes (e.g., word clusterings of different granularities) into the same feature vector. As finergrained schemes have more elements in the feature vector than coarser-grained ones, and thus exert more influence, we set the α parameter for each scheme to 1/N (where N is the number of word classes of the scheme). The K-means algorithm To create the clusters, we chose the K-means algorithm (Steinhaus, 1956; MacQueen, 1967) for both its computational efficiency and ease of implementation and parallelization. Given an initial mapping from the data points to K clusters, the procedure alternates between (i) computing the centroid of each cluster and (ii) reallocating each data point to the closest cluster centroid, until convergence. We implemented two commonly used initialization methods: Forgy and Random Partition. The Forgy method randomly chooses K observations from the data set and uses these as the initial means. The Random Partition method first randomly assigns a cluster to each observation</context>
</contexts>
<marker>Steinhaus, 1956</marker>
<rawString>Hugo Steinhaus. 1956. Sur la division des corps mat´eriels en parties. Bull. Acad. Polon. Sci. Cl. III. 4, pages 801–804.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollmann</author>
</authors>
<title>Grammar based statistical MT on Hadoop: An end-to-end toolkit for large scale PSCFG based MT. The Prague Bulletin of Mathematical Linguistics,</title>
<date>2009</date>
<pages>91--67</pages>
<contexts>
<context citStr="Venugopal and Zollmann, 2009" endWordPosition="3221" position="19483" startWordPosition="3218">key advantage of our method is its applicability to resource-poor languages, we used a language pair for which linguistic resources are available in order to determine how close translation performance can get to a fully syntax-based system. Accordingly, we use Chiang’s hierarchical phrase based translation model (Chiang, 2007) as a base line, and the syntax-augmented MT model (Zollmann and Venugopal, 2006) as a ‘target line’, a model that would not be applicable for language pairs without linguistic resources. We perform PSCFG rule extraction and decoding using the open-source “SAMT” system (Venugopal and Zollmann, 2009), using the provided implementations for the hierarchical and syntax-augmented grammars. Apart from the language model, the lexical, phrasal, and (for the syntax grammar) labelconditioned features, and the rule, target word, and glue operation counters, Venugopal and Zollmann (2009) also provide both the hierarchical and syntax-augmented grammars with a rareness penalty 1/ cnn(r), where cnn(r) is the occurrence count of rule r in the training corpus, allowing the system to learn penalization of low-frequency rules, as well as three indicator features firing if the rule has one, two unswapped, </context>
</contexts>
<marker>Venugopal, Zollmann, 2009</marker>
<rawString>Ashish Venugopal and Andreas Zollmann. 2009. Grammar based statistical MT on Hadoop: An end-to-end toolkit for large scale PSCFG based MT. The Prague Bulletin of Mathematical Linguistics, 91:67–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context citStr="Wu (1997)" endWordPosition="5316" position="33046" startWordPosition="5315">ertags, lexical information such as the POS tag of the word and its subcategorization information, into the phrase table, resulting in generalized phrases with placeholders in them. The supertags are also injected into the language model. Our approach also generates phrase labels and placeholders based on word tags (albeit in a different manner and without the use of subcategorization information), but produces PSCFG rules for use in a parsing-based decoding system. Unsupervised synchronous grammar induction, apart from the contribution of Chiang (2005) discussed earlier, has been proposed by Wu (1997) for inversion transduction grammars, but as Chiang’s model only uses a single generic nonterminal label. Blunsom et al. (2009) present a nonparametric PSCFG translation model that directly induces a grammar from parallel sentences without the use of or constraints from a word-alignment model, and 8 Cohn and Blunsom (2009) achieve the same for with the model of Zollmann and Venugopal (2006), tree-to-string grammars, with encouraging results using heuristically generated labels from parse trees. on small data. Our more humble approach treats Using automatically obtained word clusters instead th</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation, HLT/NAACL.</booktitle>
<contexts>
<context citStr="Zollmann and Venugopal (2006)" endWordPosition="108" position="790" startWordPosition="105">ce Carnegie Mellon University Pittsburgh, PA 15213, USA {zollmann,vogel+}@cs.cmu.edu Abstract In this work we propose methods to label probabilistic synchronous context-free grammar (PSCFG) rules using only word tags, generated by either part-of-speech analysis or unsupervised word class induction. The proposals range from simple tag-combination schemes to a phrase clustering model that can incorporate an arbitrary number of features. Our models improve translation quality over the single generic label approach of Chiang (2005) and perform on par with the syntactically motivated approach from Zollmann and Venugopal (2006) on the NIST large Chineseto-English translation task. These results persist when using automatically learned word tags, suggesting broad applicability of our technique across diverse language pairs for which syntactic resources are not available. 1 Introduction The Probabilistic Synchronous Context Free Grammar (PSCFG) formalism suggests an intuitive approach to model the long-distance and lexically sensitive reordering phenomena that often occur across language pairs considered for statistical machine translation. As in monolingual parsing, nonterminal symbols in translation rules are used t</context>
<context citStr="Zollmann and Venugopal (2006)" endWordPosition="367" position="2480" startWordPosition="363">ugopal, 2006; Liu et al., 2006; Marcu et al., 2006). 1 While all of these techniques rely on wordalignments to suggest lexical relationships, they differ in the way in which they assign labels to nonterminal symbols of PSCFG rules. Chiang (2005) describes a procedure to extract PSCFG rules from word-aligned (Brown et al., 1993) corpora, where all nonterminals share the same generic label X. In Galley et al. (2004) and Marcu et al. (2006), target language parse trees are used to identify rules and label their nonterminal symbols, while Liu et al. (2006) use source language parse trees instead. Zollmann and Venugopal (2006) directly extend the rule extraction procedure from Chiang (2005) to heuristically label any phrase pair based on target language parse trees. Label-based approaches have resulted in improvements in translation quality over the single X label approach (Zollmann et al., 2008; Mi and Huang, 2008); however, all the works cited here rely on stochastic parsers that have been trained on manually created syntactic treebanks. These treebanks are difficult and expensive to produce and exist for a limited set of languages only. In this work, we propose a labeling approach that is based merely on part-of</context>
<context citStr="Zollmann and Venugopal, 2006" endWordPosition="3187" position="19264" startWordPosition="3184">valuated by the IBM-BLEU (Papineni et al., 2002) metric on the NIST Chinese-to-English translation task using MT04 as development set to train the model parameters A, and MT05, MT06 and MT08 as test sets. Even though a key advantage of our method is its applicability to resource-poor languages, we used a language pair for which linguistic resources are available in order to determine how close translation performance can get to a fully syntax-based system. Accordingly, we use Chiang’s hierarchical phrase based translation model (Chiang, 2007) as a base line, and the syntax-augmented MT model (Zollmann and Venugopal, 2006) as a ‘target line’, a model that would not be applicable for language pairs without linguistic resources. We perform PSCFG rule extraction and decoding using the open-source “SAMT” system (Venugopal and Zollmann, 2009), using the provided implementations for the hierarchical and syntax-augmented grammars. Apart from the language model, the lexical, phrasal, and (for the syntax grammar) labelconditioned features, and the rule, target word, and glue operation counters, Venugopal and Zollmann (2009) also provide both the hierarchical and syntax-augmented grammars with a rareness penalty 1/ cnn(r</context>
<context citStr="Zollmann and Venugopal (2006)" endWordPosition="5379" position="33439" startWordPosition="5376">categorization information), but produces PSCFG rules for use in a parsing-based decoding system. Unsupervised synchronous grammar induction, apart from the contribution of Chiang (2005) discussed earlier, has been proposed by Wu (1997) for inversion transduction grammars, but as Chiang’s model only uses a single generic nonterminal label. Blunsom et al. (2009) present a nonparametric PSCFG translation model that directly induces a grammar from parallel sentences without the use of or constraints from a word-alignment model, and 8 Cohn and Blunsom (2009) achieve the same for with the model of Zollmann and Venugopal (2006), tree-to-string grammars, with encouraging results using heuristically generated labels from parse trees. on small data. Our more humble approach treats Using automatically obtained word clusters instead the training sentences’ word alignments and phrase of POS tags yields essentially the same results, thus pairs, obtained from external modules, as ground making our methods applicable to all languages truth and employs a straight-forward generalization pairs with parallel corpora, whether syntactic reof Chiang’s popular rule extraction approach to la- sources are available for them or not. be</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings of the Workshop on Statistical Machine Translation, HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Stephan Vogel</author>
</authors>
<title>New parameterizations and features for PSCFG-based machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 4th Workshop on Syntax and Structure in Statistical Translation (SSST),</booktitle>
<location>Beijing, China.</location>
<contexts>
<context citStr="Zollmann and Vogel (2010)" endWordPosition="3354" position="20353" startWordPosition="3351">ounters, Venugopal and Zollmann (2009) also provide both the hierarchical and syntax-augmented grammars with a rareness penalty 1/ cnn(r), where cnn(r) is the occurrence count of rule r in the training corpus, allowing the system to learn penalization of low-frequency rules, as well as three indicator features firing if the rule has one, two unswapped, and two swapped nonterminal pairs, respectively.2 Further, to mitigate badly estimated PSCFG derivations based on low-frequency rules of the much sparser syntax model, the syntax grammar also contains the hierarchical grammar as a backbone (cf. Zollmann and Vogel (2010) for details and empirical analysis). We implemented our rule labeling approach within the SAMT rule extraction pipeline, resulting in comparable features across all systems. For all systems, we use the bottom-up chart parsing decoder implemented in the SAMT toolkit with a reordering limit of 15 source words, and correspondingly extract rules from initial phrase pairs of maximum source length 15. All rules have at most two nonterminal symbols, which must be non-consecutive on the source side, and rules must contain at least one source-side terminal symbol. The beam settings for the hierarchica</context>
<context citStr="Zollmann and Vogel, 2010" endWordPosition="3880" position="23692" startWordPosition="3876">e ‘slow’ version (‘POS-tgt-slow’). The model based on bilingually tagged training instances (‘POS-src&amp;tgt’) does not gain further improvements over the merely target-based one, but actually performs worse. We assume this is due to the huge number of nonterminals of ‘POS-src&amp;tgt’ ((2 * 332 + 33)(2 * 362 + 36) = 5.8M in principle) compared to ‘POS-tgt’ (2 * 362 + 36 = 2628), increasing the sparseness of the grammar and thus leading to less reliable statistical estimates. We also experimented with a source-tag based model (‘POS-src’). In line with previous findings for syntax-augmented grammars (Zollmann and Vogel, 2010), the source-side-based grammar does not reach the translation quality of its target-based counterpart; however, the model still outperforms the hi4As shown in Zollmann et al. (2008), the impact of these rules on translation quality is negligible. erarchical system on all test sets. Further, decoding is much faster than for ‘POS-ext-tgt’ and even slightly faster than ‘Hierarchical’. This is due to the fact that for the source-tag based approach, a given chart cell in the CYK decoder, represented by a start and end position in the source sentence, almost uniquely determines the nonterminal any </context>
</contexts>
<marker>Zollmann, Vogel, 2010</marker>
<rawString>Andreas Zollmann and Stephan Vogel. 2010. New parameterizations and features for PSCFG-based machine translation. In Proceedings of the 4th Workshop on Syntax and Structure in Statistical Translation (SSST), Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
<author>Franz J Och</author>
<author>Jay Ponte</author>
</authors>
<title>A systematic comparison of phrasebased, hierarchical and syntax-augmented statistical MT.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context citStr="Zollmann et al., 2008" endWordPosition="409" position="2754" startWordPosition="406">FG rules from word-aligned (Brown et al., 1993) corpora, where all nonterminals share the same generic label X. In Galley et al. (2004) and Marcu et al. (2006), target language parse trees are used to identify rules and label their nonterminal symbols, while Liu et al. (2006) use source language parse trees instead. Zollmann and Venugopal (2006) directly extend the rule extraction procedure from Chiang (2005) to heuristically label any phrase pair based on target language parse trees. Label-based approaches have resulted in improvements in translation quality over the single X label approach (Zollmann et al., 2008; Mi and Huang, 2008); however, all the works cited here rely on stochastic parsers that have been trained on manually created syntactic treebanks. These treebanks are difficult and expensive to produce and exist for a limited set of languages only. In this work, we propose a labeling approach that is based merely on part-of-speech analysis of the source or target language (or even both). Towards the ultimate goal of building end-to-end machine translation systems without any human annotations, we also experiment with automatically inferred word classes using distributional clustering (Kneser </context>
<context citStr="Zollmann et al., 2008" endWordPosition="3713" position="22632" startWordPosition="3710">y the SAMT toolkit. Each system is trained separately to adapt the parameters to its specific properties (size of nonterminal set, grammar complexity, features sparseness, reliance on the language model, etc.). The parallel training data comprises of 9.6M sentence pairs (206M Chinese and 228M English words). The source and target language parses for the syntax-augmented grammar, as well as the POS tags for our POS-based grammars were generated by the Stanford parser (Klein and Manning, 2003). The results are given in Table 1. Results for the Syntax system are consistent with previous results (Zollmann et al., 2008), indicating improvements over the hierarchical system. Our approach, using target POS tags (‘POS-tgt (no phr. s.)’), outperforms the hierarchical system on all three tests sets, and gains further improvements when accounting for phrase size (‘POS-tgt’). The latter approach is roughly on par with the corresponding Syntax system, slightly outperforming it on average, but not consistently across all test sets. The same is true for the ‘slow’ version (‘POS-tgt-slow’). The model based on bilingually tagged training instances (‘POS-src&amp;tgt’) does not gain further improvements over the merely target</context>
<context citStr="Zollmann et al. (2008)" endWordPosition="3908" position="23874" startWordPosition="3905">ally performs worse. We assume this is due to the huge number of nonterminals of ‘POS-src&amp;tgt’ ((2 * 332 + 33)(2 * 362 + 36) = 5.8M in principle) compared to ‘POS-tgt’ (2 * 362 + 36 = 2628), increasing the sparseness of the grammar and thus leading to less reliable statistical estimates. We also experimented with a source-tag based model (‘POS-src’). In line with previous findings for syntax-augmented grammars (Zollmann and Vogel, 2010), the source-side-based grammar does not reach the translation quality of its target-based counterpart; however, the model still outperforms the hi4As shown in Zollmann et al. (2008), the impact of these rules on translation quality is negligible. erarchical system on all test sets. Further, decoding is much faster than for ‘POS-ext-tgt’ and even slightly faster than ‘Hierarchical’. This is due to the fact that for the source-tag based approach, a given chart cell in the CYK decoder, represented by a start and end position in the source sentence, almost uniquely determines the nonterminal any hypothesis in this cell can have: Disregarding partof-speech tag ambiguity and phrase size accounting, that nonterminal will be the composition of the tags of the start and end sourc</context>
</contexts>
<marker>Zollmann, Venugopal, Och, Ponte, 2008</marker>
<rawString>Andreas Zollmann, Ashish Venugopal, Franz J. Och, and Jay Ponte. 2008. A systematic comparison of phrasebased, hierarchical and syntax-augmented statistical MT. In Proceedings of the Conference on Computational Linguistics (COLING).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>