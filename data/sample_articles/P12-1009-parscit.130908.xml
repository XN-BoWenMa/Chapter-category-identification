<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant confidence="0.000116" no="0">
<title confidence="0.997291">
SITS: A Hierarchical Nonparametric Model using Speaker Identity for
Topic Segmentation in Multiparty Conversations
</title>
<author confidence="0.996656">
Viet-An Nguyen
</author>
<affiliation confidence="0.977832333333333">
Department of Computer Science
and UMIACS
University of Maryland
</affiliation>
<address confidence="0.89085">
College Park, MD
</address>
<email confidence="0.998722">
vietan@cs.umd.edu
</email>
<author confidence="0.840059">
Jordan Boyd-Graber
</author>
<affiliation confidence="0.690607666666667">
iSchool
and UMIACS
University of Maryland
</affiliation>
<address confidence="0.882004">
College Park, MD
</address>
<email confidence="0.998825">
jbg@umiacs.umd.edu
</email>
<author confidence="0.993552">
Philip Resnik
</author>
<affiliation confidence="0.975259333333333">
Department of Linguistics
and UMIACS
University of Maryland
</affiliation>
<address confidence="0.891001">
College Park, MD
</address>
<email confidence="0.999321">
resnik@umd.edu
</email>
<sectionHeader confidence="0.993912" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999636857142857">One of the key tasks for analyzing conversational data is segmenting it into coherent topic segments. However, most models of topic segmentation ignore the social aspect of conversations, focusing only on the words used. We introduce a hierarchical Bayesian nonparametric model, Speaker Identity for Topic Segmentation (SITS), that discovers (1) the topics used in a conversation, (2) how these topics are shared across conversations, (3) when these topics shift, and (4) a person-specific tendency to introduce new topics. We evaluate against current unsupervised segmentation models to show that including personspecific information improves segmentation performance on meeting corpora and on political debates. Moreover, we provide evidence that SITS captures an individual’s tendency to introduce new topics in political contexts, via analysis of the 2008 US presidential debates and the television program Crossfire.</bodyText>
<sectionHeader confidence="0.726682" genericHeader="introduction">
1 Topic Segmentation as a Social Process
</sectionHeader>
<bodyText confidence="0.999648714285714">Conversation, interactive discussion between two or more people, is one of the most essential and common forms of communication. Whether in an informal situation or in more formal settings such as a political debate or business meeting, a conversation is often not about just one thing: topics evolve and are replaced as the conversation unfolds. Discovering this hidden structure in conversations is a key problem for conversational assistants (Tur et al., 2010) and tools that summarize (Murray et al., 2005) and display (Ehlen et al., 2007) conversational data. Topic segmentation also can illuminate individuals’ agendas (Boydstun et al., 2011), patterns of agreement and disagreement (Hawes et al., 2009; Abbott et al., 2011), and relationships among conversational participants (Ireland et al., 2011).</bodyText>
<page confidence="0.972271">
78
</page>
<bodyText confidence="0.999942272727273">One of the most natural ways to capture conversational structure is topic segmentation (Reynar, 1998; Purver, 2011). Topic segmentation approaches range from simple heuristic methods based on lexical similarity (Morris and Hirst, 1991; Hearst, 1997) to more intricate generative models and supervised methods (Georgescul et al., 2006; Purver et al., 2006; Gruber et al., 2007; Eisenstein and Barzilay, 2008), which have been shown to outperform the established heuristics. However, previous computational work on conversational structure, particularly in topic discovery and topic segmentation, focuses primarily on content, ignoring the speakers. We argue that, because conversation is a social process, we can understand conversational phenomena better by explicitly modeling behaviors of conversational participants. In Section 2, we incorporate participant identity in a new model we call Speaker Identity for Topic Segmentation (SITS), which discovers topical structure in conversation while jointly incorporating a participantlevel social component. Specifically, we explicitly model an individual’s tendency to introduce a topic. After outlining inference in Section 3 and introducing data in Section 4, we use SITS to improve state-ofthe-art-topic segmentation and topic identification models in Section 5. In addition, in Section 6, we also show that the per-speaker model is able to discover individuals who shape and influence the course of a conversation. Finally, we discuss related work and conclude the paper in Section 7.</bodyText>
<sectionHeader confidence="0.959557" genericHeader="method">
2 Modeling Multiparty Discussions
</sectionHeader>
<bodyText confidence="0.8096215">Data Properties We are interested in turn-taking, multiparty discussion. This is a broad category, including political debates, business meetings, and online chats.</bodyText>
<note confidence="0.978422">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 78–87,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.981797425925926">More formally, such datasets contain C conversations. A conversation c has Tc turns, each of which is a maximal uninterrupted utterance by one speaker.1 In each turn t E 11, Tc], a speaker ac,t utters N words {wc,t,n}. Each word is from a vocabulary of size V , and there are M distinct speakers. Modeling Approaches The key insight of topic segmentation is that segments evince lexical cohesion (Galley et al., 2003; Olney and Cai, 2005). Words within a segment will look more like their neighbors than other words. This insight has been used to tune supervised methods (Hsueh et al., 2006) and inspire unsupervised models of lexical cohesion using bags of words (Purver et al., 2006) and language models (Eisenstein and Barzilay, 2008). We too take the unsupervised statistical approach. It requires few resources and is applicable in many domains without extensive training. Like previous approaches, we consider each turn to be a bag of words generated from an admixture of topics. Topics—after the topic modeling literature (Blei and Lafferty, 2009)—are multinomial distributions over terms. These topics are part of a generative model posited to have produced a corpus. However, topic models alone cannot model the dynamics of a conversation. Topic models typically do not model the temporal dynamics of individual documents, and those that do (Wang et al., 2008; Gerrish and Blei, 2010) are designed for larger documents and are not applicable here because they assume that most topics appear in every time slice. Instead, we endow each turn with a binary latent variable lc,t, called the topic shift. This latent variable signifies whether the speaker changed the topic of the conversation. To capture the topic-controlling behavior of the speakers across different conversations, we further associate each speaker m with a latent topic shift tendency, 7rm. Informally, this variable is intended to capture the propensity of a speaker to effect a topic shift. Formally, it represents the probability that the speaker m will change the topic (distribution) of a conversation. We take a Bayesian nonparametric approach (M¨uller and Quintana, 2004). Unlike 1Note the distinction with phonetic utterances, which by definition are bounded by silence.parametric models, which a priori fix the number of topics, nonparametric models use a flexible number of topics to better represent data. Nonparametric distributions such as the Dirichlet process (Ferguson, 1973) share statistical strength among conversations using a hierarchical model, such as the hierarchical Dirichlet process (HDP) (Teh et al., 2006).</bodyText>
<subsectionHeader confidence="0.527719">
2.1 Generative Process
</subsectionHeader>
<bodyText confidence="0.999719666666667">In this section, we develop SITS, a generative model of multiparty discourse that jointly discovers topics and speaker-specific topic shifts from an unannotated corpus (Figure 1a). As in the hierarchical Dirichlet process (Teh et al., 2006), we allow an unbounded number of topics to be shared among the turns of the corpus. Topics are drawn from a base distribution H over multinomial distributions over the vocabulary, a finite Dirichlet with symmetric prior A. Unlike the HDP, where every document (here, every turn) draws a new multinomial distribution from a Dirichlet process, the social and temporal dynamics of a conversation, as specified by the binary topic shift indicator lc,t, determine when new draws happen. The full generative process is as follows:</bodyText>
<listItem confidence="0.940070846153846">1. For speaker m E [1, M], draw speaker shift probability arm — Beta(γ) 2. Draw global probability measure G0 — DP(a, H) 3. For each conversation c E [1, C] (a) Draw conversation distribution Gc — DP(a0, G0) (b) For each turn t E [1, Tc] with speaker ac,t i. If t = 1, set the topic shift lc,t = 1. Otherwise, draw lc,t — Bernoulli(ara�,t). ii. If lc,t = 1, draw Gc,t — DP(ac, Gc). Otherwise, set Gc,t =_ Gc,t_1. iii. For each word index n E [1, Nc,t] • Draw ?Pc,t,n — Gc,t • Draw wc,t,n — Multinomial(?Pc,t,n)</listItem>
<bodyText confidence="0.995070142857143">The hierarchy of Dirichlet processes allows statistical strength to be shared across contexts; within a conversation and across conversations. The perspeaker topic shift tendency 7rm allows speaker identity to influence the evolution of topics. To make notation concrete and aligned with the topic segmentation, we introduce notation for segments in a conversation. A segment s of conversation c is a sequence of turns IT, T'] such that lc,τ = lc,τ1+1 = 1 and lc,t = 0, dt E (T, T']. When lc,t = 0, Gc,t is the same as Gc,t_1 and all topics (i.e. multinomial distributions over words) {0c,t,n} that generate words in turn t and the topics {0c,t_1,n} that generate words in turn t −1 come from the same distribution.</bodyText>
<page confidence="0.991296">
79
</page>
<figure confidence="0.999719888888889">
(a)
α0 Gc
α H
ψc,1,n ψc,2,n
wc,1,n
Gc,1 Gc,2 Gc,T�
G0
N�,1 N,2
αc lc,2 lc,T�
wc,2,n wc,T�,n
ac,2 ac,T�
πm γ
M
ψc,T.,n
N�,T�
C
(b)
Bc,1
zc,1,n zc,2,n
wc,1,n wc,2,n
N,1
α
β φk
lc,2
Bc,2
Nc2
ac,2 ac,T�
K
πm γ
M
zc,T.,n
wc,T.,n
lc,T�
Bc,T�
N�,Ta
C
</figure>
<figureCaption confidence="0.997159">
Figure 1: Graphical model representations of our proposed models: (a) the nonparametric version; (b) the
parametric version. Nodes represent random variables (shaded ones are observed), lines are probabilistic
dependencies. Plates represent repetition. The innermost plates are turns, grouped in conversations.
</figureCaption>
<bodyText confidence="0.575844">Thus all topics used in a segment s are drawn from a single distribution, Gc,s,</bodyText>
<equation confidence="0.88988">
Gc,s  |lc,1, lc,2, ··· , lc,T�, αc, Gc — DP(αc, Gc) (1)
</equation>
<bodyText confidence="0.999909434782609">For notational convenience, Sc denotes the number of segments in conversation c, and st denotes the segment index of turn t. We emphasize that all segment-related notations are derived from the posterior over the topic shifts l and not part of the model itself. Parametric Version SITS is a generalization of a parametric model (Figure 1b) where each turn has a multinomial distribution over K topics. In the parametric case, the number of topics K is fixed. Each topic, as before, is a multinomial distribution O1 ... OK. In the parametric case, each turn t in conversation c has an explicit multinomial distribution over K topics 0c,t, identical for turns within a segment. A new topic distribution 0 is drawn from a Dirichlet distribution parameterized by α when the topic shift indicator l is 1. The parametric version does not share strength within or across conversations, unlike SITS. When applied on a single conversation without speaker identity (all speakers are identical) it is equivalent to (Purver et al., 2006). In our experiments (Section 5), we compare against both.</bodyText>
<sectionHeader confidence="0.999761" genericHeader="method">
3 Inference
</sectionHeader>
<bodyText confidence="0.999995888888889">To find the latent variables that best explain observed data, we use Gibbs sampling, a widely used Markov chain Monte Carlo inference technique (Neal, 2000; Resnik and Hardisty, 2010). The state space is latent variables for topic indices assigned to all tokens z = {zc,t,n} and topic shifts assigned to turns l = {lc,t}. We marginalize over all other latent variables. Here, we only present the conditional sampling equations; for more details, see our supplement.2</bodyText>
<subsectionHeader confidence="0.999915">
3.1 Sampling Topic Assignments
</subsectionHeader>
<bodyText confidence="0.999969727272727">To sample zc,t,n, the index of the shared topic assigned to token n of turn t in conversation c, we need to sample the path assigning each word token to a segment-specific topic, each segment-specific topic to a conversational topic and each conversational topic to a shared topic. For efficiency, we make use of the minimal path assumption (Wallach, 2008) to generate these assignments.3 Under the minimal path assumption, an observation is assumed to have been generated by using a new distribution if and only if there is no existing distribution with the same value.</bodyText>
<footnote confidence="0.910128">
2 http://www.cs.umd.edu/∼vietan/topicshift/appendix.pdf
3We also investigated using the maximal assumption and
fully sampling assignments. We found the minimal path assump-
tion worked as well as explicitly sampling seating assignments
and that the maximal path assumption worked less well.
</footnote>
<page confidence="0.995177">
80
</page>
<bodyText confidence="0.998401454545455">We use Nc,s,k to denote the number of tokens in segment s in conversation c assigned topic k; Nc,k denotes the total number of segment-specific topics in conversation c assigned topic k and Nk denotes the number of conversational topics assigned topic k. TWk,w denotes the number of times the shared topic kis assigned to word w in the vocabulary. Marginal counts are represented with · and * represents all hyperparameters. The conditional distribution for zc,t,n is P(zc,t,n = k  |wc,t,n = w, z−c,t,n , w−c,t,n ,l,*) a</bodyText>
<equation confidence="0.937138333333333">
N−c,t,n +α0
c,·
(2)
</equation>
<bodyText confidence="0.998934777777778">Here V is the size of the vocabulary, K is the current number of shared topics and the superscript −c,t,n denotes counts without considering wc,t,n. In Equation 2, the first factor is proportional to the probability of sampling a path according to the minimal path assumption; the second factor is proportional to the likelihood of observing w given the sampled topic. Since an uninformed prior is used, when a new topic is sampled, all tokens are equiprobable.</bodyText>
<subsectionHeader confidence="0.999957">
3.2 Sampling Topic Shifts
</subsectionHeader>
<bodyText confidence="0.989848866666667">Sampling the topic shift variable lc,t requires us to consider merging or splitting segments. We use kc,t to denote the shared topic indices of all tokens in turn t of conversation c; Sac,t,x to denote the number of times speaker ac,t is assigned the topic shift with value x E {0,1}; Jxc,s to denote the number of topics in segment s of conversation c if lc,t = x and Nxc,s,j to denote the number of tokens assigned to the segment-specific topic j when lc,t = x.4 Again, the superscript −c,t is used to denote exclusion of turn t of conversation c in the corresponding counts. Recall that the topic shift is a binary variable. We use 0 to represent the case that the topic distribution is identical to the previous turn. We sample this assignment P(lc,t = 0  |l−c,t, w, k, a, *) a</bodyText>
<equation confidence="0.9246106">
Sc,t
ac,t,0 + γ
×
S−c,t
ac,t,· + 2γ
</equation>
<bodyText confidence="0.998709928571428">4Deterministically knowing the path assignments is the primary efficiency motivation for using the minimal path assumption. The alternative is to explicitly sample the path assignments, which is more complicated (for both notation and computation). This option is spelled in full detail in the supplementary material. In Equation 3, the first factor is proportional to the probability of assigning a topic shift of value 0 to speaker ac,t and the second factor is proportional to the joint probability of all topics in segment st of conversation c when lc,t = 0. The other alternative is for the topic shift to be 1, which represents the introduction of a new distribution over topics inside an existing segment. We sample this as P(lc,t = 1  |l−c,t, w, k, a, *) a</bodyText>
<equation confidence="0.9991000625">
⎛ J1 QJ1
S−c,t c,(st−1)
ac,t,1 + γ j=1 (N1
c,(st−1)
c
αc,(st−1),j − 1)!
× ⎝
S−c,t N1
ac,t,· + 2γ Qx=1 (x − 1 + αc)
c,(st−1),·
QJ1
j=1 (N1
c,st c,stj − 1)!
QN1
x=1 (x − 1 + αc)
c,st,·
</equation>
<bodyText confidence="0.999981571428571">As above, the first factor in Equation 4 is proportional to the probability of assigning a topic shift of value 1 to speaker ac,t; the second factor in the big bracket is proportional to the joint distribution of the topics in segments st − 1 and st. In this case lc,t = 1 means splitting the current segment, which results in two joint probabilities for two segments.</bodyText>
<sectionHeader confidence="0.999279" genericHeader="method">
4 Datasets
</sectionHeader>
<bodyText confidence="0.996324347826087">This section introduces the three corpora we use. We preprocess the data to remove stopwords and remove turns containing fewer than five tokens. The ICSI Meeting Corpus: The ICSI Meeting Corpus (Janin et al., 2003) is 75 transcribed meetings. For evaluation, we used a standard set of reference segmentations (Galley et al., 2003) of 25 meetings. Segmentations are binary, i.e., each point of the document is either a segment boundary or not, and on average each meeting has 8 segment boundaries. After preprocessing, there are 60 unique speakers and the vocabulary contains 3346 non-stopword tokens. The 2008 Presidential Election Debates Our second dataset contains three annotated presidential debates (Boydstun et al., 2011) between Barack Obama and John McCain and a vice presidential debate between Joe Biden and Sarah Palin. Each turn is one of two types: questions (Q) from the moderator or responses (R) from a candidate. Each clause in a turn is coded with a Question Topic (TQ) and a Response Topic (TR). Thus, a turn has a list of TQ’s and TR’s both of length equal to the number of clauses in the turn. Topics are from the Policy Agendas Topics</bodyText>
<figure confidence="0.998996828571428">
N−c,t,n + α
k K
N−c,t,n+α
·
N k,t,n
c+α0
Nc,t,n
c,st,k + αc
Nc,t,n+ αc ×
c, st ,
TWTW−c,t,n
k,w + λ
−c,t,n
k,· + V λ,
1
⎧
⎨⎪⎪
⎪⎪⎩
k new.
V
0
αJc,st
c
. (3)
QN0
x=1 (x − 1 + αc)
c,st,·
QJ0
j=1 (N0
c,st c,st,j − 1)!
1
αJc,st
c
⎞
⎠ . (4)
</figure>
<page confidence="0.981611">
81
</page>
<table confidence="0.999724">
Speaker Type Turn clauses TQ TR
Brokaw Q Sen. Obama, [... ] Are you saying [... ] that the American economy is going to get much worse 1 N/A
before it gets better and they ought to be prepared for that?
Obama R No, I am confident about the American economy. 1 1
[... ] But most importantly, we’re going to have to help ordinary families be able to stay in their 1 14
homes, make sure that they can pay their bills [... ]
Brokaw Q Sen. McCain, in all candor, do you think the economy is going to get worse before it gets better? 1 N/A
McCain R [... ] I think if we act effectively, if we stabilize the housing market–which I believe we can, 1 14
if we go out and buy up these bad loans, so that people can have a new mortgage at the new value 1 14
of their home
I think if we get rid of the cronyism and special interest influence in Washington so we can act 1 20
more effectively. [... ]
</table>
<tableCaption confidence="0.981133333333333">
Table 1: Example turns from the annotated 2008 election debates. The topics (TQ and TR) are from the Policy
Agendas Topics Codebook which contains the following codes of topic: Macroeconomics (1), Housing &amp;
Community Development (14), Government Operations (20).
</tableCaption>
<bodyText confidence="0.9991788">Codebook, a manual inventory of 19 major topics and 225 subtopics.5 Table 1 shows an example annotation. To get reference segmentations, we assign each turn a real value from 0 to 1 indicating how much a turn changes the topic. For a question-typed turn, the score is the fraction of clause topics not appearing in the previous turn; for response-typed turns, the score is the fraction of clause topics that do not appear in the corresponding question. This results in a set of non-binary reference segmentations. For evaluation metrics that require binary segmentations, we create a binary segmentation by setting a turn as a segment boundary if the computed score is 1. This threshold is chosen to include only true segment boundaries. CNN’s Crossfire Crossfire was a weekly U.S. television “talking heads” program engineered to incite heated arguments (hence the name). Each episode features two recurring hosts, two guests, and clips from the week’s news. Our Crossfire dataset contains 1134 transcribed episodes aired between 2000 and 2004.6 There are 2567 unique speakers. Unlike the previous two datasets, Crossfire does not have explicit topic segmentations, so we use it to explore speaker-specific characteristics (Section 6).</bodyText>
<sectionHeader confidence="0.995878" genericHeader="evaluation">
5 Topic Segmentation Experiments
</sectionHeader>
<bodyText confidence="0.889512">In this section, we examine how well SITS can replicate annotations of when new topics are introduced.</bodyText>
<footnote confidence="0.996335">
5 http://www.policyagendas.org/page/topic-codebook
6 http://www.cs.umd.edu/∼vietan/topicshift/crossfire.zip
</footnote>
<bodyText confidence="0.99989">We discuss metrics for evaluating an algorithm’s segmentation against a gold annotation, describe our experimental setup, and report those results. Evaluation Metrics To evaluate segmentations, we use Pk (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002). Both metrics measure the probability that two points in a document will be incorrectly separated by a segment boundary. Both techniques consider all spans of length k in the document and count whether the two endpoints of the window are (im)properly segmented against the gold segmentation. However, these metrics have drawbacks. First, they require both hypothesized and reference segmentations to be binary. Many algorithms (e.g., probabilistic approaches) give non-binary segmentations where candidate boundaries have real-valued scores (e.g., probability or confidence). Thus, evaluation requires arbitrary thresholding to binarize soft scores. To be fair, thresholds are set so the number of segments are equal to a predefined value (Purver et al., 2006; Galley et al., 2003). To overcome these limitations, we also use Earth Mover’s Distance (EMD) (Rubner et al., 2000), a metric that measures the distance between two distributions. The EMD is the minimal cost to transform one distribution into the other. Each segmentation can be considered a multi-dimensional distribution where each candidate boundary is a dimension. In EMD, a distance function across features allows partial credit for “near miss” segment boundaries. In addition, because EMD operates on distributions, we can compute the distance between non-binary hypothesized segmentations with binary or real-valued reference segmentations.</bodyText>
<page confidence="0.997056">
82
</page>
<bodyText confidence="0.996681875">We use the FastEMD implementation (Pele and Werman, 2009). Experimental Methods We applied the following methods to discover topic segmentations in a document:</bodyText>
<listItem confidence="0.998543642857143">• TextTiling (Hearst, 1997) is one of the earliest generalpurpose topic segmentation algorithms, sliding a fixedwidth window to detect major changes in lexical similarity. • P-NoSpeaker-S: parametric version without speaker identity run on each conversation (Purver et al., 2006) • P-NoSpeaker-M: parametric version without speaker identity run on all conversations • P-SITS: the parametric version of SITS with speaker identity run on all conversations • NP-HMM: the HMM-based nonparametric model which a single topic per turn. This model can be considered a Sticky HDP-HMM (Fox et al., 2008) with speaker identity. • NP-SITS: the nonparametric version of SITS with speaker identity run on all conversations.</listItem>
<bodyText confidence="0.999053102564103">Parameter Settings and Implementations In our experiment, all parameters of TextTiling are the same as in (Hearst, 1997). For statistical models, Gibbs sampling with 10 randomly initialized chains is used. Initial hyperparameter values are sampled from U(0,1) to favor sparsity; statistics are collected after 500 burn-in iterations with a lag of 25 iterations over a total of 5000 iterations; and slice sampling (Neal, 2003) optimizes hyperparameters. Results and Analysis Table 2 shows the performance of various models on the topic segmentation problem, using the ICSI corpus and the 2008 debates. Consistent with previous results, probabilistic models outperform TextTiling. In addition, among the probabilistic models, the models that had access to speaker information consistently segment better than those lacking such information, supporting our assertion that there is benefit to modeling conversation as a social process. Furthermore, NP-SITS outperforms NP-HMM in both experiments, suggesting that using a distribution over topics to turns is better than using a single topic. This is consistent with parametric results reported in (Purver et al., 2006). The contribution of speaker identity seems more valuable in the debate setting. Debates are characterized by strong rewards for setting the agenda; dodging a question or moving the debate toward an opponent’s weakness can be useful strategies (Boydstun et al., 2011). In contrast, meetings (particularly lowstakes ICSI meetings) are characterized by pragmatic rather than strategic topic shifts. Second, agendasetting roles are clearer in formal debates; a moderator is tasked with setting the agenda and ensuring the conversation does not wander too much. The nonparametric model does best on the smaller debate dataset. We suspect that an evaluation that directly accessed the topic quality, either via prediction (Teh et al., 2006) or interpretability (Chang et al., 2009) would favor the nonparametric model more.</bodyText>
<sectionHeader confidence="0.989185" genericHeader="evaluation and result">
6 Evaluating Topic Shift Tendency
</sectionHeader>
<bodyText confidence="0.999945606060606">In this section, we focus on the ability of SITS to capture speaker-level attributes. Recall that SITS associates with each speaker a topic shift tendency 7r that represents the probability of asserting a new topic in the conversation. While topic segmentation is a well studied problem, there are no established quantitative measurements of an individual’s ability to control a conversation. To evaluate whether the tendency is capturing meaningful characteristics of speakers, we compare our inferred tendencies against insights from political science. 2008 Elections To obtain a posterior estimate of 7r (Figure 3) we create 10 chains with hyperparameters sampled from the uniform distribution U(0,1) and averaged 7r over 10 chains (as described in Section 5). In these debates, Ifill is the moderator of the debate between Biden and Palin; Brokaw, Lehrer and Schieffer are the three moderators of three debates between Obama and McCain. Here “Question” denotes questions from audiences in “town hall” debate. The role of this “speaker” can be considered equivalent to the debate moderator. The topic shift tendencies of moderators are much higher than for candidates. In the three debates between Obama and McCain, the moderators— Brokaw, Lehrer and Schieffer—have significantly higher scores than both candidates. This is a useful reality check, since in a debate the moderators are the ones asking questions and literally controlling the topical focus. Interestingly, in the vice-presidential debate, the score of moderator Ifill is only slightly higher than those of Palin and Biden; this is consistent with media commentary characterizing her as a weak moderator.7 Similarly, the “Question” speaker had a relatively high variance, consistent with an amalgamation of many distinct speakers.</bodyText>
<page confidence="0.993064">
83
</page>
<figure confidence="0.9629718">
0.4
0.1
0.2
0
0.3
</figure>
<table confidence="0.986010928571429">
Model EMD Pk WindowDiff
k = 5 10 15 k = 5 10 15
ICSI Dataset TextTiling 2.507 .289 .388 .451 .318 .477 .561
P-NoSpeaker-S 1.949 .222 .283 .342 .269 .393 .485
P-NoSpeaker-M 1.935 .207 .279 .335 .253 .371 .468
P-SITS 1.807 .211 .251 .289 .256 .363 .434
NP-HMM 2.189 .232 .257 .263 .267 .377 .444
NP-SITS 2.126 .228 .253 .259 .262 .372 .440
Debates Dataset TextTiling 2.821 .433 .548 .633 .534 .674 .760
P-NoSpeaker-S 2.822 .426 .543 .653 .482 .650 .756
P-NoSpeaker-M 2.712 .411 .522 .589 .479 .644 .745
P-SITS 2.269 .380 .405 .402 .482 .625 .719
NP-HMM 2.132 .362 .348 .323 .486 .629 .723
NP-SITS 1.813 .332 .269 .231 .470 .600 .692
</table>
<tableCaption confidence="0.749777285714286">
Table 2: Results on the topic segmentation task.
Lower is better. The parameter k is the window
size of the metrics Pk and WindowDiff chosen to
replicate previous results.
Table 3: Topic shift tendency 7r of speakers in the
2008 Presidential Election Debates (larger means
greater tendency)
</tableCaption>
<bodyText confidence="0.999230607142857">These topic shift tendencies suggest that all candidates manage to succeed at some points in setting and controlling the debate topics. Our model gives Obama a slightly higher score than McCain, consistent with social science claims (Boydstun et al., 2011) that Obama had the lead in setting the agenda over McCain. Table 4 shows of SITS-detected topic shifts. Crossfire Crossfire, unlike the debates, has many speakers. This allows us to examine more closely what we can learn about speakers’ topic shift tendency. We verified that SITS can segment topics, and assuming that changing the topic is useful for a speaker, how can we characterize who does so effectively? We examine the relationship between topic shift tendency, social roles, and political ideology. To focus on frequent speakers, we filter out speakers with fewer than 30 turns. Most speakers have relatively small 7r, with the mode around 0.3. There are, however, speakers with very high topic shift tendencies. Table 5 shows the speakers having the highest values according to SITS. We find that there are three general patterns for who influences the course of a conversation in Crossfire. First, there are structural “speakers” the show uses to frame and propose new topics.These are audience questions, news clips (e.g.many of Gore’s and Bush’s turns from 2000), and voice overs.</bodyText>
<footnote confidence="0.543883">
7 http://harpers.org/archive/2008/10/hbc-90003659
</footnote>
<bodyText confidence="0.9988588">That SITS is able to recover these is reassuring. Second, the stable of regular hosts receives high topic shift tendencies, which is reasonable given their experience with the format and ostensible moderation roles (in practice they also stoke lively discussion). The remaining class is more interesting. The remaining non-hosts with high topic shift tendency are relative moderates on the political spectrum:</bodyText>
<listItem confidence="0.9997062">• John Kasich, one of few Republicans to support the assault weapons ban and now governor of Ohio, a swing state • Christine Todd Whitman, former Republican governor of New Jersey, a very Democratic state • John McCain, who before 2008 was known as a “maverick” for working with Democrats (e.g.Russ Feingold)</listItem>
<bodyText confidence="0.9975409">This suggests that, despite Crossfire’s tendency to create highly partisan debates, those who are able to work across the political spectrum may best be able to influence the topic under discussion in highly polarized contexts. Table 4 shows detected topic shifts from these speakers; two of these examples (McCain and Whitman) show disagreement of Republicans with President Bush. In the other, Kasich is defending a Republican plan (school vouchers) popular with traditional Democratic constituencies.</bodyText>
<sectionHeader confidence="0.996702" genericHeader="other">
7 Related and Future Work
</sectionHeader>
<bodyText confidence="0.997488333333333">In the realm of statistical models, a number of techniques incorporate social connections and identity to explain content in social networks (Chang and Blei,</bodyText>
<page confidence="0.997096">
84
</page>
<table confidence="0.59739008">
Previous turn Turn detected as shifting topic
Debates Dataset PALIN: Your question to him was whether he sup- IFILL: Wonderful. You agree. On that note, let’s move to foreign policy. You
ported gay marriage and my answer is the same as both have sons who are in Iraq or on their way to Iraq. You, Governor Palin,
his and it is that I do not. have said that you would like to see a real clear plan for an exit strategy. [... ]
MCCAIN: I think that Joe Biden is qualified in SCHIEFFER: [... ] Let’s talk about energy and climate control. Every president
many respects.... since Nixon has said what both of you [... ]
IFILL: So, Governor, as vice president, there’s BIDEN: Again, let me–let’s talk about those tax breaks. [Obama] voted for an
nothing that you have promised [... ] that you energy bill because, for the first time, it had real support for alternative energy.
wouldn’t take off the table because of this finan- [... ] on eliminating the tax breaks for the oil companies, Barack Obama voted
cial crisis we’re in? to eliminate them. [... ]
Crossfire Dataset PRESS: But what do you say, governor, to Gov- WHITMAN: Well I disagree with them on this particular issues [... ] that’s
ernor Bush and [... ] your party who would let important to me that George Bush stands for education of our children [... ] I
politicians and not medical scientists decide what care about tax policy, I care about the environment. I care about all the issues
drugs are distributed [... ] where he has a proven record in Texas [... ]
WEXLER: [... ] They need a Medicare prescrip- KASICH: [... ] I want to talk about choice. [... ] George Bush believes that, if
tion drug plan [... ] Talk about schools, [... ] Al schools fail, parents ought to have a right to get their kids out of those schools
Gore has got a real plan. George Bush offers us and give them a chance and an opportunity for success. Gore says “no way” [... ]
vouchers. Talk about the environment. [... ] Al Social Security. George Bush says [... ] direct it the way federal employees do
Gore is right on in terms of the majority of Ameri- [... ] Al Gore says “No way” [... ] That’s real choice. That’s real bottom-up,
cans, but George Bush [... ] not a bureaucratic approach, the way we run this country.
PRESS: Senator, Senator Breaux mentioned that MCCAIN: After one of closest elections in our nation’s history, there is one
it’s President Bush’s aim to start on education [... ] thing the American people are unanimous about They want their government
[McCain] [... ] said he was going to do introduce back. We can do that by ridding politics of large, unregulated contributions that
the legislation the first day of the first week of the give special interests a seat at the table while average Americans are stuck in the
new administration. [... ] back of the room.
</table>
<tableCaption confidence="0.9955745">
Table 4: Example of turns designated as a topic shift by SITS. Turns were chosen with speakers to give
examples of those with high topic shift tendency 7r.
</tableCaption>
<table confidence="0.9996261">
Rank Speaker 7r Rank Speaker 7r
1 Announcer .884 10 Kasich .570
2 Male .876 11 Carville† .550
3 Question .755 12 Carlson† .550
4 G. W. Bush$ .751 13 Begala† .545
5 Press† .651 14 Whitman .533
6 Female .650 15 McAuliffe .529
7 Gore$ .650 16 Matalin† .527
8 Narrator .642 17 McCain .524
9 Novak† .587 18 Fleischer .522
</table>
<tableCaption confidence="0.98442">
Table 5: Top speakers by topic shift tendencies. We
mark hosts (†) and “speakers” who often (but not always) appeared in clips (�).</tableCaption>
<bodyText confidence="0.995916432432432">Apart from those groups, speakers with the highest tendency were political moderates. 2009) and scientific corpora (Rosen-Zvi et al., 2004). However, these models ignore the temporal evolution of content, treating documents as static. Models that do investigate the evolution of topics over time typically ignore the identify of the speaker. For example: models having sticky topics over ngrams (Johnson, 2010), sticky HDP-HMM (Fox et al., 2008); models that are an amalgam of sequential models and topic models (Griffiths et al., 2005; Wallach, 2006; Gruber et al., 2007; Ahmed and Xing, 2008; Boyd-Graber and Blei, 2008; Du et al., 2010); or explicit models of time or other relevant features as a distinct latent variable (Wang and McCallum, 2006; Eisenstein et al., 2010). In contrast, SITS jointly models topic and individuals’ tendency to control a conversation. Not only does SITS outperform other models using standard computational linguistics baselines, but it also proposes intriguing hypotheses for social scientists. Associating each speaker with a scalar that models their tendency to change the topic does improve performance on standard tasks, but it’s inadequate to fully describe an individual. Modeling individuals’ perspective (Paul and Girju, 2010), “side” (Thomas et al., 2006), or personal preferences for topics (Grimmer, 2009) would enrich the model and better illuminate the interaction of influence and topic. Statistical analysis of political discourse can help discover patterns that political scientists, who often work via a “close reading,” might otherwise miss. We plan to work with social scientists to validate our implicit hypothesis that our topic shift tendency correlates well with intuitive measures of “influence.”</bodyText>
<page confidence="0.999635">
85
</page>
<sectionHeader confidence="0.996545" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999972">This research was funded in part by the Army Research Laboratory through ARL Cooperative Agreement W911NF-09-2-0072 and by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), through the Army Research Laboratory. Jordan Boyd-Graber and Philip Resnik are also supported by US National Science Foundation Grant NSF grant #1018625. Any opinions, findings, conclusions, or recommendations expressed are the authors’ and do not necessarily reflect those of the sponsors.</bodyText>
<sectionHeader confidence="0.998806" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998120954545454">
[Abbott et al., 2011] Abbott, R., Walker, M., Anand, P.,
Fox Tree, J. E., Bowmani, R., and King, J. (2011). How
can you say such things?!?: Recognizing disagreement
in informal political argument. In Proceedings of the
Workshop on Language in Social Media (LSM 2011),
pages 2–11.
[Ahmed and Xing, 2008] Ahmed, A. and Xing, E. P.
(2008). Dynamic non-parametric mixture models and
the recurrent Chinese restaurant process: with applica-
tions to evolutionary clustering. In SDM, pages 219–
230.
[Beeferman et al., 1999] Beeferman, D., Berger, A., and
Lafferty, J. (1999). Statistical models for text segmen-
tation. Mach. Learn., 34:177–210.
[Blei and Lafferty, 2009] Blei, D. M. and Lafferty, J.
(2009). Text Mining: Theory and Applications, chapter
Topic Models. Taylor and Francis, London.
[Boyd-Graber and Blei, 2008] Boyd-Graber, J. and Blei,
D. M. (2008). Syntactic topic models. In Proceedings
ofAdvances in Neural Information Processing Systems.
[Boydstun et al., 2011] Boydstun, A. E., Phillips, C., and
Glazier, R. A. (2011). It’s the economy again, stupid:
Agenda control in the 2008 presidential debates. Forth-
coming.
[Chang and Blei, 2009] Chang, J. and Blei, D. M. (2009).
Relational topic models for document networks. In
Proceedings of Artificial Intelligence and Statistics.
[Chang et al., 2009] Chang, J., Boyd-Graber, J., Wang, C.,
Gerrish, S., and Blei, D. M. (2009). Reading tea leaves:
How humans interpret topic models. In Neural Infor-
mation Processing Systems.
[Du et al., 2010] Du, L., Buntine, W., and Jin, H. (2010).
Sequential latent dirichlet allocation: Discover underly-
ing topic structures within a document. In Data Mining
(ICDM), 2010 IEEE 10th International Conference on,
pages 148 –157.
[Ehlen et al., 2007] Ehlen, P., Purver, M., and Niekrasz, J.
(2007). A meeting browser that learns. In In: Pro-
ceedings of the AAAI Spring Symposium on Interaction
Challenges for Intelligent Assistants.
[Eisenstein and Barzilay, 2008] Eisenstein, J. and Barzi-
lay, R. (2008). Bayesian unsupervised topic segmenta-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, Proceedings
of Emperical Methods in Natural Language Processing.
[Eisenstein et al., 2010] Eisenstein, J., O’Connor, B.,
Smith, N. A., and Xing, E. P. (2010). A latent variable
model for geographic lexical variation. In EMNLP’10,
pages 1277–1287.
[Ferguson, 1973] Ferguson, T. S. (1973). A Bayesian anal-
ysis of some nonparametric problems. The Annals of
Statistics, 1(2):209–230.
[Fox et al., 2008] Fox, E. B., Sudderth, E. B., Jordan, M. I.,
and Willsky, A. S. (2008). An hdp-hmm for systems
with state persistence. In Proceedings of International
Conference of Machine Learning.
[Galley et al., 2003] Galley, M., McKeown, K., Fosler-
Lussier, E., and Jing, H. (2003). Discourse segmenta-
tion of multi-party conversation. In Proceedings of the
Association for Computational Linguistics.
[Georgescul et al., 2006] Georgescul, M., Clark, A., and
Armstrong, S. (2006). Word distributions for thematic
segmentation in a support vector machine approach.
In Conference on Computational Natural Language
Learning.
[Gerrish and Blei, 2010] Gerrish, S. and Blei, D. M.
(2010). A language-based approach to measuring schol-
arly impact. In Proceedings of International Confer-
ence of Machine Learning.
[Griffiths et al., 2005] Griffiths, T. L., Steyvers, M., Blei,
D. M., and Tenenbaum, J. B. (2005). Integrating topics
and syntax. In Proceedings of Advances in Neural
Information Processing Systems.
[Grimmer, 2009] Grimmer, J. (2009). A Bayesian Hier-
archical Topic Model for Political Texts: Measuring
Expressed Agendas in Senate Press Releases. Political
Analysis, 18:1–35.
[Gruber et al., 2007] Gruber, A., Rosen-Zvi, M., and
Weiss, Y. (2007). Hidden topic Markov models. In
Artificial Intelligence and Statistics.
[Hawes et al., 2009] Hawes, T., Lin, J., and Resnik, P.
(2009). Elements of a computational model for multi-
party discourse: The turn-taking behavior of Supreme
Court justices. Journal of the American Society for In-
formation Science and Technology, 60(8):1607–1615.
[Hearst, 1997] Hearst, M. A. (1997). TextTiling: Segment-
ing text into multi-paragraph subtopic passages. Com-
putational Linguistics, 23(1):33–64.
</reference>
<page confidence="0.969879">
86
</page>
<reference confidence="0.999597571428572">
[Hsueh et al., 2006] Hsueh, P.-y., Moore, J. D., and Renals,
S. (2006). Automatic segmentation of multiparty dia-
logue. In Proceedings of the European Chapter of the
Association for Computational Linguistics.
[Ireland et al., 2011] Ireland, M. E., Slatcher, R. B., East-
wick, P. W., Scissors, L. E., Finkel, E. J., and Pen-
nebaker, J. W. (2011). Language style matching pre-
dicts relationship initiation and stability. Psychological
Science, 22(1):39–44.
[Janin et al., 2003] Janin, A., Baron, D., Edwards, J., El-
lis, D., Gelbart, D., Morgan, N., Peskin, B., Pfau, T.,
Shriberg, E., Stolcke, A., and Wooters, C. (2003). The
ICSI meeting corpus. In IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing.
[Johnson, 2010] Johnson, M. (2010). PCFGs, topic mod-
els, adaptor grammars and learning topical collocations
and the structure of proper names. In Proceedings of
the Association for Computational Linguistics.
[Morris and Hirst, 1991] Morris, J. and Hirst, G. (1991).
Lexical cohesion computed by thesaural relations as
an indicator of the structure of text. Computational
Linguistics, 17:21–48.
[M¨uller and Quintana, 2004] M¨uller, P. and Quintana,
F. A. (2004). Nonparametric Bayesian data analysis.
Statistical Science, 19(1):95–110.
[Murray et al., 2005] Murray, G., Renals, S., and Carletta,
J. (2005). Extractive summarization of meeting record-
ings. In European Conference on Speech Communica-
tion and Technology.
[Neal, 2000] Neal, R. M. (2000). Markov chain sampling
methods for Dirichlet process mixture models. Journal
of Computational and Graphical Statistics, 9(2):249–
265.
[Neal, 2003] Neal, R. M. (2003). Slice sampling. Annals
of Statistics, 31:705–767.
[Olney and Cai, 2005] Olney, A. and Cai, Z. (2005). An
orthonormal basis for topic segmentation in tutorial di-
alogue. In Proceedings of the Human Language Tech-
nology Conference.
[Paul and Girju, 2010] Paul, M. and Girju, R. (2010). A
two-dimensional topic-aspect model for discovering
multi-faceted topics. In Association for the Advance-
ment of Artificial Intelligence.
[Pele and Werman, 2009] Pele, O. and Werman, M.
(2009). Fast and robust earth mover’s distances. In
International Conference on Computer Vision.
[Pevzner and Hearst, 2002] Pevzner, L. and Hearst, M. A.
(2002). A critique and improvement of an evaluation
metric for text segmentation. Computational Linguis-
tics, 28.
[Purver, 2011] Purver, M. (2011). Topic segmentation. In
Tur, G. and de Mori, R., editors, Spoken Language
Understanding: Systems for Extracting Semantic Infor-
mation from Speech, pages 291–317. Wiley.
[Purver et al., 2006] Purver, M., K¨ording, K., Griffiths,
T. L., and Tenenbaum, J. (2006). Unsupervised topic
modelling for multi-party spoken discourse. In Pro-
ceedings of the Association for Computational Linguis-
tics.
[Resnik and Hardisty, 2010] Resnik, P. and Hardisty, E.
(2010). Gibbs sampling for the uninitiated. Technical
Report UMIACS-TR-2010-04, University of Maryland.
http://www.lib.umd.edu/drum/handle/1903/10058.
[Reynar, 1998] Reynar, J. C. (1998). Topic Segmentation:
Algorithms and Applications. PhD thesis, University of
Pennsylvania.
[Rosen-Zvi et al., 2004] Rosen-Zvi, M., Griffiths, T. L.,
Steyvers, M., and Smyth, P. (2004). The author-topic
model for authors and documents. In Proceedings of
Uncertainty in Artificial Intelligence.
[Rubner et al., 2000] Rubner, Y., Tomasi, C., and Guibas,
L. J. (2000). The earth mover’s distance as a metric
for image retrieval. International Journal of Computer
Vision, 40:99–121.
[Teh et al., 2006] Teh, Y. W., Jordan, M. I., Beal, M. J.,
and Blei, D. M. (2006). Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Association,
101(476):1566–1581.
[Thomas et al., 2006] Thomas, M., Pang, B., and Lee, L.
(2006). Get out the vote: Determining support or op-
position from Congressional floor-debate transcripts.
In Proceedings of Emperical Methods in Natural Lan-
guage Processing.
[Tur et al., 2010] Tur, G., Stolcke, A., Voss, L., Peters, S.,
Hakkani-T¨ur, D., Dowding, J., Favre, B., Fern´andez,
R., Frampton, M., Frandsen, M., Frederickson, C., Gra-
ciarena, M., Kintzing, D., Leveque, K., Mason, S.,
Niekrasz, J., Purver, M., Riedhammer, K., Shriberg, E.,
Tien, J., Vergyri, D., and Yang, F. (2010). The CALO
meeting assistant system. Trans. Audio, Speech and
Lang. Proc., 18:1601–1611.
[Wallach, 2006] Wallach, H. M. (2006). Topic modeling:
Beyond bag-of-words. In Proceedings of International
Conference of Machine Learning.
[Wallach, 2008] Wallach, H. M. (2008). Structured Topic
Models for Language. PhD thesis, University of Cam-
bridge.
[Wang et al., 2008] Wang, C., Blei, D. M., and Heckerman,
D. (2008). Continuous time dynamic topic models. In
Proceedings of Uncertainty in Artificial Intelligence.
[Wang and McCallum, 2006] Wang, X. and McCallum, A.
(2006). Topics over time: a non-Markov continuous-
time model of topical trends. In Knowledge Discovery
and Data Mining, Knowledge Discovery and Data Min-
ing.
</reference>
<page confidence="0.999473">
87
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant confidence="0.016596" no="0">
<title confidence="0.9952805">SITS: A Hierarchical Nonparametric Model using Speaker Identity Topic Segmentation in Multiparty Conversations</title>
<author confidence="0.972577">Viet-An</author>
<affiliation confidence="0.998477333333333">Department of Computer and University of</affiliation>
<address confidence="0.586819">College Park,</address>
<email confidence="0.999506">vietan@cs.umd.edu</email>
<author confidence="0.988482">Jordan</author>
<affiliation confidence="0.831444">and University of</affiliation>
<address confidence="0.716526">College Park,</address>
<email confidence="0.99985">jbg@umiacs.umd.edu</email>
<author confidence="0.989386">Philip</author>
<affiliation confidence="0.998303">Department of and University of</affiliation>
<address confidence="0.641043">College Park,</address>
<email confidence="0.999865">resnik@umd.edu</email>
<abstract confidence="0.985473278745645">One of the key tasks for analyzing conversational data is segmenting it into coherent topic segments. However, most models of topic ignore the of conversations, focusing only on the words used. We introduce a hierarchical Bayesian nonparametric model, Speaker Identity for Topic Segmentation (SITS), that discovers (1) the topics used in a conversation, (2) how these topics are shared across conversations, (3) when these topics shift, and (4) a person-specific tendency to introduce new topics. We evaluate against current unsupervised segmentation models to show that including personspecific information improves segmentation performance on meeting corpora and on political debates. Moreover, we provide evidence that SITS captures an individual’s tendency to introduce new topics in political contexts, via analysis of the 2008 US presidential debates and the television program Crossfire. 1 Topic Segmentation as a Social Process Conversation, interactive discussion between two or more people, is one of the most essential and common forms of communication. Whether in an informal situation or in more formal settings such as a political debate or business meeting, a conversation is often not about just one thing: topics evolve and are replaced as the conversation unfolds. Discovering this hidden structure in conversations is a key problem for conversational assistants (Tur et al., 2010) and tools that summarize (Murray et al., 2005) and display (Ehlen et al., 2007) conversational data. Topic segmentation also can illuminate individuals’ agendas (Boydstun et al., 2011), patterns of agreement and disagreement (Hawes et al., 2009; Abbott 78 et al., 2011), and relationships among conversational participants (Ireland et al., 2011). One of the most natural ways to capture conversational structure is topic segmentation (Reynar, 1998; Purver, 2011). Topic segmentation approaches range from simple heuristic methods based on lexical similarity (Morris and Hirst, 1991; Hearst, 1997) to more intricate generative models and supervised methods (Georgescul et al., 2006; Purver et al., 2006; Gruber et al., 2007; Eisenstein and Barzilay, 2008), which have been shown to outperform the established heuristics. However, previous computational work on conversational structure, particularly in topic discovery and topic segmentation, focuses primarily on content, ignoring the speakers. We argue that, because is a we can understand conversational phenomena better by explicitly modeling behaviors of conversational participants. In Section 2, we incorporate participant identity in a new model we call Speaker Identity for Topic Segmentation (SITS), which discovers topical structure in conversation while jointly incorporating a participantlevel social component. Specifically, we explicitly model an individual’s tendency to introduce a topic. After outlining inference in Section 3 and introducing data in Section 4, we use SITS to improve state-ofthe-art-topic segmentation and topic identification models in Section 5. In addition, in Section 6, we also show that the per-speaker model is able to discover individuals who shape and influence the course of a conversation. Finally, we discuss related work and conclude the paper in Section 7. 2 Modeling Multiparty Discussions Properties are interested in turn-taking, discussion. This is a broad category, inof the 50th Annual Meeting of the Association for Computational pages 78–87, Republic of Korea, 8-14 July 2012. Association for Computational Linguistics cluding political debates, business meetings, and onchats. More formally, such datasets contain A conversation turns, each of which is a maximal uninterrupted utterance by one In each turn a speaker Each word is from a vocabulary size and there are speakers. Approaches key insight of topic segmentation is that segments evince lexical cohesion (Galley et al., 2003; Olney and Cai, 2005). Words within a segment will look more like their neighbors than other words. This insight has been used to tune supervised methods (Hsueh et al., 2006) and inspire unsupervised models of lexical cohesion using bags of words (Purver et al., 2006) and language models (Eisenstein and Barzilay, 2008). We too take the unsupervised statistical approach. It requires few resources and is applicable in many domains without extensive training. Like previous approaches, we consider each turn to be a bag of words generated from an admixture of topics. Topics—after the topic modeling literature (Blei and Lafferty, 2009)—are multinomial distributions over terms. These topics are part of a generative model posited to have produced a corpus. However, topic models alone cannot model the dynamics of a conversation. Topic models typically do not model the temporal dynamics of individual documents, and those that do (Wang et al., 2008; Gerrish and Blei, 2010) are designed for larger documents and are not applicable here because they assume that most topics appear in every time slice. Instead, we endow each turn with a binary latent called the This latent variable signifies whether the speaker changed the topic of the conversation. To capture the topic-controlling behavior of the speakers across different conversations, we associate each speaker a latent Informally, this variable is intended to capture the propensity of a speaker to effect a topic shift. Formally, it represents the probability the speaker change the topic (distribution) of a conversation. We take a Bayesian nonparametric approach (M¨uller and Quintana, 2004). Unlike the distinction with phonetic utterances, which by definition are bounded by silence. models, which priori the number of topics, nonparametric models use a flexible number of topics to better represent data. Nonparametric distributions such as the Dirichlet process (Ferguson, 1973) share statistical strength among conversations using a hierarchical model, such as the hierarchical Dirichlet process (HDP) (Teh et al., 2006). 2.1 Generative Process In this section, we develop SITS, a generative model of multiparty discourse that jointly discovers topics and speaker-specific topic shifts from an unannotated corpus (Figure 1a). As in the hierarchical Dirichlet process (Teh et al., 2006), we allow an unbounded number of topics to be shared among the turns of the corpus. Topics are drawn from a base distribution multinomial distributions over the vocabua finite Dirichlet with symmetric prior Unlike the HDP, where every document (here, every turn) draws a new multinomial distribution from a Dirichlet process, the social and temporal dynamics of a conversation, as specified by the binary topic shift determine when new draws happen. The full generative process is as follows: For speaker draw speaker shift probability — Draw global probability measure For each conversation Draw conversation distribution — For each turn with speaker If 1, set the topic shift 1. Otherwise, If 1, draw Otherset For each word index Draw Draw The hierarchy of Dirichlet processes allows statistical strength to be shared across contexts; within a conversation and across conversations. The pertopic shift tendency allows speaker ideninfluence the evolution of topics. To make notation concrete and aligned with the segmentation, we introduce notation for sega conversation. A segment convera sequence of turns that 1 When the same as all topics (i.e. distributions over words) words in turn the topics generate words in turn from the same 79 (a) α H γ M C (b) α K γ M C Figure 1: Graphical model representations of our proposed models: (a) the nonparametric version; (b) the parametric version. Nodes represent random variables (shaded ones are observed), lines are probabilistic dependencies. Plates represent repetition. The innermost plates are turns, grouped in conversations. Thus all topics used in a segment from a single distribution, — notational convenience, denotes the numof segments in conversation and segment index of turn We emphasize that all segment-related notations are derived from the posteover the topic shifts not part of the model itself. Version is a generalization of a parametric model (Figure 1b) where each turn has multinomial distribution over In the case, the number of topics fixed. Each topic, as before, is a multinomial distribution In the parametric case, each turn conan explicit multinomial distribution identical for turns within a seg- A new topic distribution drawn from a distribution parameterized by the shift indicator The parametric version does not share strength within or across conversations, unlike SITS. When applied on a single conversation without speaker identity (all speakers are identical) it is equivalent to (Purver et al., 2006). In our experiments (Section 5), we compare against both. 3 Inference To find the latent variables that best explain observed data, we use Gibbs sampling, a widely used Markov chain Monte Carlo inference technique (Neal, 2000; Resnik and Hardisty, 2010). The state space is latent for topic indices assigned to all tokens = topic shifts assigned to turns = We marginalize over all other latent variables. Here, we only present the conditional sampling equations; more details, see our 3.1 Sampling Topic Assignments sample the index of the shared topic asto token turn conversation we need sample the each word token to a segment-specific topic, each segment-specific topic to a conversational topic and each conversational topic to a shared topic. For efficiency, we make use the path assumption 2008) to these Under the minimal path assumption, an observation is assumed to have been generated by using a new distribution if and only if there is no existing distribution with the same value. also investigated using the maximal assumption and fully sampling assignments. We found the minimal path assumption worked as well as explicitly sampling seating assignments and that the maximal path assumption worked less well. 80 use denote the number of tokens in conversation topic denotes the total number of segment-specific topin conversation topic denotes the number of conversational topics assigned the number of times the topic assigned to word the vocab- Marginal counts are represented with all hyperparameters. The conditional for is (2) the size of the vocabulary, the current of shared topics and the superscript counts without considering In Equation 2, the first factor is proportional to the probability of sampling a path according to the minimal path assumption; the second factor is proportional to the of observing the sampled topic. Since an uninformed prior is used, when a new topic is sampled, all tokens are equiprobable. 3.2 Sampling Topic Shifts the topic shift variable us to merging or splitting segments. We use to denote the shared topic indices of all tokens in conversation denote the numof times speaker assigned the topic shift value denote the number of in segment conversation denote the number of tokens assigned to the topic Again, the is used to denote exclusion of turn conversation the corresponding counts. Recall that the topic shift is a binary variable. We represent the case that the topic distribution is identical to the previous turn. We sample this 0 × knowing the path assignments is the primary efficiency motivation for using the minimal path assumption. The alternative is to explicitly sample the path assignments, which is more complicated (for both notation and computation). This option is spelled in full detail in the supplementary material. In Equation 3, the first factor is proportional to the of assigning a topic shift of value the second factor is proportional to joint probability of all topics in segment The other alternative is for the topic shift to be</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Abbott</author>
<author>M Walker</author>
<author>P Anand</author>
<author>Fox Tree</author>
<author>J E</author>
<author>R Bowmani</author>
<author>J King</author>
</authors>
<title>How can you say such things?!?: Recognizing disagreement in informal political argument.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Language in Social Media (LSM</booktitle>
<pages>2--11</pages>
<marker>[Abbott et al., 2011]</marker>
<rawString>Abbott, R., Walker, M., Anand, P., Fox Tree, J. E., Bowmani, R., and King, J. (2011). How can you say such things?!?: Recognizing disagreement in informal political argument. In Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 2–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ahmed</author>
<author>E P Xing</author>
</authors>
<title>Dynamic non-parametric mixture models and the recurrent Chinese restaurant process: with applications to evolutionary clustering.</title>
<date>2008</date>
<booktitle>In SDM,</booktitle>
<pages>219--230</pages>
<marker>[Ahmed and Xing, 2008]</marker>
<rawString>Ahmed, A. and Xing, E. P. (2008). Dynamic non-parametric mixture models and the recurrent Chinese restaurant process: with applications to evolutionary clustering. In SDM, pages 219– 230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Beeferman</author>
<author>A Berger</author>
<author>J Lafferty</author>
</authors>
<title>Statistical models for text segmentation.</title>
<date>1999</date>
<pages>34--177</pages>
<location>Mach. Learn.,</location>
<marker>[Beeferman et al., 1999]</marker>
<rawString>Beeferman, D., Berger, A., and Lafferty, J. (1999). Statistical models for text segmentation. Mach. Learn., 34:177–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>J Lafferty</author>
</authors>
<title>Text Mining: Theory and Applications, chapter Topic Models.</title>
<date>2009</date>
<publisher>Taylor and Francis,</publisher>
<location>London.</location>
<marker>[Blei and Lafferty, 2009]</marker>
<rawString>Blei, D. M. and Lafferty, J. (2009). Text Mining: Theory and Applications, chapter Topic Models. Taylor and Francis, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Boyd-Graber</author>
<author>D M Blei</author>
</authors>
<title>Syntactic topic models.</title>
<date>2008</date>
<booktitle>In Proceedings ofAdvances in Neural Information Processing Systems.</booktitle>
<marker>[Boyd-Graber and Blei, 2008]</marker>
<rawString>Boyd-Graber, J. and Blei, D. M. (2008). Syntactic topic models. In Proceedings ofAdvances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A E Boydstun</author>
<author>C Phillips</author>
<author>R A Glazier</author>
</authors>
<title>It’s the economy again, stupid: Agenda control in the 2008 presidential debates.</title>
<date>2011</date>
<journal>Forthcoming.</journal>
<marker>[Boydstun et al., 2011]</marker>
<rawString>Boydstun, A. E., Phillips, C., and Glazier, R. A. (2011). It’s the economy again, stupid: Agenda control in the 2008 presidential debates. Forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chang</author>
<author>D M Blei</author>
</authors>
<title>Relational topic models for document networks.</title>
<date>2009</date>
<booktitle>In Proceedings of Artificial Intelligence and Statistics.</booktitle>
<marker>[Chang and Blei, 2009]</marker>
<rawString>Chang, J. and Blei, D. M. (2009). Relational topic models for document networks. In Proceedings of Artificial Intelligence and Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chang</author>
<author>J Boyd-Graber</author>
<author>C Wang</author>
<author>S Gerrish</author>
<author>D M Blei</author>
</authors>
<title>Reading tea leaves: How humans interpret topic models.</title>
<date>2009</date>
<booktitle>In Neural Information Processing Systems.</booktitle>
<marker>[Chang et al., 2009]</marker>
<rawString>Chang, J., Boyd-Graber, J., Wang, C., Gerrish, S., and Blei, D. M. (2009). Reading tea leaves: How humans interpret topic models. In Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Du</author>
<author>W Buntine</author>
<author>H Jin</author>
</authors>
<title>Sequential latent dirichlet allocation: Discover underlying topic structures within a document.</title>
<date>2010</date>
<booktitle>In Data Mining (ICDM), 2010 IEEE 10th International Conference on,</booktitle>
<pages>148--157</pages>
<marker>[Du et al., 2010]</marker>
<rawString>Du, L., Buntine, W., and Jin, H. (2010). Sequential latent dirichlet allocation: Discover underlying topic structures within a document. In Data Mining (ICDM), 2010 IEEE 10th International Conference on, pages 148 –157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ehlen</author>
<author>M Purver</author>
<author>J Niekrasz</author>
</authors>
<title>A meeting browser that learns. In In:</title>
<date>2007</date>
<booktitle>Proceedings of the AAAI Spring Symposium on Interaction Challenges for Intelligent Assistants.</booktitle>
<marker>[Ehlen et al., 2007]</marker>
<rawString>Ehlen, P., Purver, M., and Niekrasz, J. (2007). A meeting browser that learns. In In: Proceedings of the AAAI Spring Symposium on Interaction Challenges for Intelligent Assistants.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisenstein</author>
<author>R Barzilay</author>
</authors>
<title>Bayesian unsupervised topic segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Proceedings of Emperical Methods in Natural Language Processing.</booktitle>
<marker>[Eisenstein and Barzilay, 2008]</marker>
<rawString>Eisenstein, J. and Barzilay, R. (2008). Bayesian unsupervised topic segmentation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Proceedings of Emperical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisenstein</author>
<author>B O’Connor</author>
<author>N A Smith</author>
<author>E P Xing</author>
</authors>
<title>A latent variable model for geographic lexical variation.</title>
<date>2010</date>
<booktitle>In EMNLP’10,</booktitle>
<pages>1277--1287</pages>
<marker>[Eisenstein et al., 2010]</marker>
<rawString>Eisenstein, J., O’Connor, B., Smith, N. A., and Xing, E. P. (2010). A latent variable model for geographic lexical variation. In EMNLP’10, pages 1277–1287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T S Ferguson</author>
</authors>
<title>A Bayesian analysis of some nonparametric problems.</title>
<date>1973</date>
<journal>The Annals of Statistics,</journal>
<volume>1</volume>
<issue>2</issue>
<marker>[Ferguson, 1973]</marker>
<rawString>Ferguson, T. S. (1973). A Bayesian analysis of some nonparametric problems. The Annals of Statistics, 1(2):209–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E B Fox</author>
<author>E B Sudderth</author>
<author>M I Jordan</author>
<author>A S Willsky</author>
</authors>
<title>An hdp-hmm for systems with state persistence.</title>
<date>2008</date>
<booktitle>In Proceedings of International Conference of Machine Learning.</booktitle>
<marker>[Fox et al., 2008]</marker>
<rawString>Fox, E. B., Sudderth, E. B., Jordan, M. I., and Willsky, A. S. (2008). An hdp-hmm for systems with state persistence. In Proceedings of International Conference of Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>K McKeown</author>
<author>E FoslerLussier</author>
<author>H Jing</author>
</authors>
<title>Discourse segmentation of multi-party conversation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<marker>[Galley et al., 2003]</marker>
<rawString>Galley, M., McKeown, K., FoslerLussier, E., and Jing, H. (2003). Discourse segmentation of multi-party conversation. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Georgescul</author>
<author>A Clark</author>
<author>S Armstrong</author>
</authors>
<title>Word distributions for thematic segmentation in a support vector machine approach.</title>
<date>2006</date>
<booktitle>In Conference on Computational Natural Language Learning.</booktitle>
<marker>[Georgescul et al., 2006]</marker>
<rawString>Georgescul, M., Clark, A., and Armstrong, S. (2006). Word distributions for thematic segmentation in a support vector machine approach. In Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gerrish</author>
<author>D M Blei</author>
</authors>
<title>A language-based approach to measuring scholarly impact.</title>
<date>2010</date>
<booktitle>In Proceedings of International Conference of Machine Learning.</booktitle>
<marker>[Gerrish and Blei, 2010]</marker>
<rawString>Gerrish, S. and Blei, D. M. (2010). A language-based approach to measuring scholarly impact. In Proceedings of International Conference of Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
<author>D M Blei</author>
<author>J B Tenenbaum</author>
</authors>
<title>Integrating topics and syntax.</title>
<date>2005</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems.</booktitle>
<marker>[Griffiths et al., 2005]</marker>
<rawString>Griffiths, T. L., Steyvers, M., Blei, D. M., and Tenenbaum, J. B. (2005). Integrating topics and syntax. In Proceedings of Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Grimmer</author>
</authors>
<title>A Bayesian Hierarchical Topic Model for Political Texts: Measuring Expressed Agendas in Senate Press Releases. Political Analysis,</title>
<date>2009</date>
<pages>18--1</pages>
<marker>[Grimmer, 2009]</marker>
<rawString>Grimmer, J. (2009). A Bayesian Hierarchical Topic Model for Political Texts: Measuring Expressed Agendas in Senate Press Releases. Political Analysis, 18:1–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gruber</author>
<author>M Rosen-Zvi</author>
<author>Y Weiss</author>
</authors>
<title>Hidden topic Markov models.</title>
<date>2007</date>
<booktitle>In Artificial Intelligence and Statistics.</booktitle>
<marker>[Gruber et al., 2007]</marker>
<rawString>Gruber, A., Rosen-Zvi, M., and Weiss, Y. (2007). Hidden topic Markov models. In Artificial Intelligence and Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hawes</author>
<author>J Lin</author>
<author>P Resnik</author>
</authors>
<title>Elements of a computational model for multiparty discourse: The turn-taking behavior of Supreme Court justices.</title>
<date>2009</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>60</volume>
<issue>8</issue>
<marker>[Hawes et al., 2009]</marker>
<rawString>Hawes, T., Lin, J., and Resnik, P. (2009). Elements of a computational model for multiparty discourse: The turn-taking behavior of Supreme Court justices. Journal of the American Society for Information Science and Technology, 60(8):1607–1615.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>TextTiling: Segmenting text into multi-paragraph subtopic passages.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<marker>[Hearst, 1997]</marker>
<rawString>Hearst, M. A. (1997). TextTiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics, 23(1):33–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P-y Hsueh</author>
<author>J D Moore</author>
<author>S Renals</author>
</authors>
<title>Automatic segmentation of multiparty dialogue.</title>
<date>2006</date>
<booktitle>In Proceedings of the European Chapter of the Association for Computational Linguistics.</booktitle>
<marker>[Hsueh et al., 2006]</marker>
<rawString>Hsueh, P.-y., Moore, J. D., and Renals, S. (2006). Automatic segmentation of multiparty dialogue. In Proceedings of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Ireland</author>
<author>R B Slatcher</author>
<author>P W Eastwick</author>
<author>L E Scissors</author>
<author>E J Finkel</author>
<author>J W Pennebaker</author>
</authors>
<title>Language style matching predicts relationship initiation and stability.</title>
<date>2011</date>
<journal>Psychological Science,</journal>
<volume>22</volume>
<issue>1</issue>
<marker>[Ireland et al., 2011]</marker>
<rawString>Ireland, M. E., Slatcher, R. B., Eastwick, P. W., Scissors, L. E., Finkel, E. J., and Pennebaker, J. W. (2011). Language style matching predicts relationship initiation and stability. Psychological Science, 22(1):39–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Janin</author>
<author>D Baron</author>
<author>J Edwards</author>
<author>D Ellis</author>
<author>D Gelbart</author>
<author>N Morgan</author>
<author>B Peskin</author>
<author>T Pfau</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
<author>C Wooters</author>
</authors>
<title>The ICSI meeting corpus.</title>
<date>2003</date>
<booktitle>In IEEE International Conference on Acoustics, Speech, and Signal Processing.</booktitle>
<marker>[Janin et al., 2003]</marker>
<rawString>Janin, A., Baron, D., Edwards, J., Ellis, D., Gelbart, D., Morgan, N., Peskin, B., Pfau, T., Shriberg, E., Stolcke, A., and Wooters, C. (2003). The ICSI meeting corpus. In IEEE International Conference on Acoustics, Speech, and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>PCFGs, topic models, adaptor grammars and learning topical collocations and the structure of proper names.</title>
<date>2010</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<marker>[Johnson, 2010]</marker>
<rawString>Johnson, M. (2010). PCFGs, topic models, adaptor grammars and learning topical collocations and the structure of proper names. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Morris</author>
<author>G Hirst</author>
</authors>
<title>Lexical cohesion computed by thesaural relations as an indicator of the structure of text.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<pages>17--21</pages>
<marker>[Morris and Hirst, 1991]</marker>
<rawString>Morris, J. and Hirst, G. (1991). Lexical cohesion computed by thesaural relations as an indicator of the structure of text. Computational Linguistics, 17:21–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P M¨uller</author>
<author>F A Quintana</author>
</authors>
<title>Nonparametric Bayesian data analysis.</title>
<date>2004</date>
<journal>Statistical Science,</journal>
<volume>19</volume>
<issue>1</issue>
<marker>[M¨uller and Quintana, 2004]</marker>
<rawString>M¨uller, P. and Quintana, F. A. (2004). Nonparametric Bayesian data analysis. Statistical Science, 19(1):95–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Murray</author>
<author>S Renals</author>
<author>J Carletta</author>
</authors>
<title>Extractive summarization of meeting recordings.</title>
<date>2005</date>
<booktitle>In European Conference on Speech Communication and Technology.</booktitle>
<marker>[Murray et al., 2005]</marker>
<rawString>Murray, G., Renals, S., and Carletta, J. (2005). Extractive summarization of meeting recordings. In European Conference on Speech Communication and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Neal</author>
</authors>
<title>Markov chain sampling methods for Dirichlet process mixture models.</title>
<date>2000</date>
<journal>Journal of Computational and Graphical Statistics,</journal>
<volume>9</volume>
<issue>2</issue>
<pages>265</pages>
<marker>[Neal, 2000]</marker>
<rawString>Neal, R. M. (2000). Markov chain sampling methods for Dirichlet process mixture models. Journal of Computational and Graphical Statistics, 9(2):249– 265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Neal</author>
</authors>
<title>Slice sampling.</title>
<date>2003</date>
<journal>Annals of Statistics,</journal>
<pages>31--705</pages>
<marker>[Neal, 2003]</marker>
<rawString>Neal, R. M. (2003). Slice sampling. Annals of Statistics, 31:705–767.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Olney</author>
<author>Z Cai</author>
</authors>
<title>An orthonormal basis for topic segmentation in tutorial dialogue.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference.</booktitle>
<marker>[Olney and Cai, 2005]</marker>
<rawString>Olney, A. and Cai, Z. (2005). An orthonormal basis for topic segmentation in tutorial dialogue. In Proceedings of the Human Language Technology Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Paul</author>
<author>R Girju</author>
</authors>
<title>A two-dimensional topic-aspect model for discovering multi-faceted topics.</title>
<date>2010</date>
<booktitle>In Association for the Advancement of Artificial Intelligence.</booktitle>
<marker>[Paul and Girju, 2010]</marker>
<rawString>Paul, M. and Girju, R. (2010). A two-dimensional topic-aspect model for discovering multi-faceted topics. In Association for the Advancement of Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Pele</author>
<author>M Werman</author>
</authors>
<title>Fast and robust earth mover’s distances.</title>
<date>2009</date>
<booktitle>In International Conference on Computer Vision.</booktitle>
<marker>[Pele and Werman, 2009]</marker>
<rawString>Pele, O. and Werman, M. (2009). Fast and robust earth mover’s distances. In International Conference on Computer Vision.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Pevzner</author>
<author>M A Hearst</author>
</authors>
<title>A critique and improvement of an evaluation metric for text segmentation.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<marker>[Pevzner and Hearst, 2002]</marker>
<rawString>Pevzner, L. and Hearst, M. A. (2002). A critique and improvement of an evaluation metric for text segmentation. Computational Linguistics, 28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Purver</author>
</authors>
<title>Topic segmentation.</title>
<date>2011</date>
<booktitle>Spoken Language Understanding: Systems for Extracting Semantic Information from Speech,</booktitle>
<pages>291--317</pages>
<editor>In Tur, G. and de Mori, R., editors,</editor>
<publisher>Wiley.</publisher>
<marker>[Purver, 2011]</marker>
<rawString>Purver, M. (2011). Topic segmentation. In Tur, G. and de Mori, R., editors, Spoken Language Understanding: Systems for Extracting Semantic Information from Speech, pages 291–317. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Purver</author>
<author>K K¨ording</author>
<author>T L Griffiths</author>
<author>J Tenenbaum</author>
</authors>
<title>Unsupervised topic modelling for multi-party spoken discourse.</title>
<date>2006</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<marker>[Purver et al., 2006]</marker>
<rawString>Purver, M., K¨ording, K., Griffiths, T. L., and Tenenbaum, J. (2006). Unsupervised topic modelling for multi-party spoken discourse. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
<author>E Hardisty</author>
</authors>
<title>Gibbs sampling for the uninitiated.</title>
<date>2010</date>
<tech>Technical Report UMIACS-TR-2010-04,</tech>
<institution>University of Maryland.</institution>
<note>http://www.lib.umd.edu/drum/handle/1903/10058.</note>
<marker>[Resnik and Hardisty, 2010]</marker>
<rawString>Resnik, P. and Hardisty, E. (2010). Gibbs sampling for the uninitiated. Technical Report UMIACS-TR-2010-04, University of Maryland. http://www.lib.umd.edu/drum/handle/1903/10058.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Reynar</author>
</authors>
<title>Topic Segmentation: Algorithms and Applications.</title>
<date>1998</date>
<tech>PhD thesis,</tech>
<institution>University of Pennsylvania.</institution>
<marker>[Reynar, 1998]</marker>
<rawString>Reynar, J. C. (1998). Topic Segmentation: Algorithms and Applications. PhD thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rosen-Zvi</author>
<author>T L Griffiths</author>
<author>M Steyvers</author>
<author>P Smyth</author>
</authors>
<title>The author-topic model for authors and documents.</title>
<date>2004</date>
<booktitle>In Proceedings of Uncertainty in Artificial Intelligence.</booktitle>
<marker>[Rosen-Zvi et al., 2004]</marker>
<rawString>Rosen-Zvi, M., Griffiths, T. L., Steyvers, M., and Smyth, P. (2004). The author-topic model for authors and documents. In Proceedings of Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Rubner</author>
<author>C Tomasi</author>
<author>L J Guibas</author>
</authors>
<title>The earth mover’s distance as a metric for image retrieval.</title>
<date>2000</date>
<journal>International Journal of Computer Vision,</journal>
<pages>40--99</pages>
<marker>[Rubner et al., 2000]</marker>
<rawString>Rubner, Y., Tomasi, C., and Guibas, L. J. (2000). The earth mover’s distance as a metric for image retrieval. International Journal of Computer Vision, 40:99–121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Teh</author>
<author>M I Jordan</author>
<author>M J Beal</author>
<author>D M Blei</author>
</authors>
<title>Hierarchical Dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>101</volume>
<issue>476</issue>
<marker>[Teh et al., 2006]</marker>
<rawString>Teh, Y. W., Jordan, M. I., Beal, M. J., and Blei, D. M. (2006). Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101(476):1566–1581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Thomas</author>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Get out the vote: Determining support or opposition from Congressional floor-debate transcripts.</title>
<date>2006</date>
<booktitle>In Proceedings of Emperical Methods in Natural Language Processing.</booktitle>
<marker>[Thomas et al., 2006]</marker>
<rawString>Thomas, M., Pang, B., and Lee, L. (2006). Get out the vote: Determining support or opposition from Congressional floor-debate transcripts. In Proceedings of Emperical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Tur</author>
<author>A Stolcke</author>
<author>L Voss</author>
<author>S Peters</author>
<author>D Hakkani-T¨ur</author>
<author>J Dowding</author>
<author>B Favre</author>
<author>R Fern´andez</author>
<author>M Frampton</author>
<author>M Frandsen</author>
<author>C Frederickson</author>
<author>M Graciarena</author>
<author>D Kintzing</author>
<author>K Leveque</author>
<author>S Mason</author>
<author>J Niekrasz</author>
<author>M Purver</author>
<author>K Riedhammer</author>
<author>E Shriberg</author>
<author>J Tien</author>
<author>D Vergyri</author>
<author>F Yang</author>
</authors>
<title>The CALO meeting assistant system.</title>
<date>2010</date>
<journal>Trans. Audio, Speech</journal>
<pages>18--1601</pages>
<marker>[Tur et al., 2010]</marker>
<rawString>Tur, G., Stolcke, A., Voss, L., Peters, S., Hakkani-T¨ur, D., Dowding, J., Favre, B., Fern´andez, R., Frampton, M., Frandsen, M., Frederickson, C., Graciarena, M., Kintzing, D., Leveque, K., Mason, S., Niekrasz, J., Purver, M., Riedhammer, K., Shriberg, E., Tien, J., Vergyri, D., and Yang, F. (2010). The CALO meeting assistant system. Trans. Audio, Speech and Lang. Proc., 18:1601–1611.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H M Wallach</author>
</authors>
<title>Topic modeling: Beyond bag-of-words.</title>
<date>2006</date>
<booktitle>In Proceedings of International Conference of Machine Learning.</booktitle>
<marker>[Wallach, 2006]</marker>
<rawString>Wallach, H. M. (2006). Topic modeling: Beyond bag-of-words. In Proceedings of International Conference of Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H M Wallach</author>
</authors>
<title>Structured Topic Models for Language.</title>
<date>2008</date>
<tech>PhD thesis,</tech>
<institution>University of Cambridge.</institution>
<marker>[Wallach, 2008]</marker>
<rawString>Wallach, H. M. (2008). Structured Topic Models for Language. PhD thesis, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wang</author>
<author>D M Blei</author>
<author>D Heckerman</author>
</authors>
<title>Continuous time dynamic topic models.</title>
<date>2008</date>
<booktitle>In Proceedings of Uncertainty in Artificial Intelligence.</booktitle>
<marker>[Wang et al., 2008]</marker>
<rawString>Wang, C., Blei, D. M., and Heckerman, D. (2008). Continuous time dynamic topic models. In Proceedings of Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wang</author>
<author>A McCallum</author>
</authors>
<title>Topics over time: a non-Markov continuoustime model of topical trends.</title>
<date>2006</date>
<booktitle>In Knowledge Discovery and Data Mining, Knowledge Discovery and Data Mining.</booktitle>
<marker>[Wang and McCallum, 2006]</marker>
<rawString>Wang, X. and McCallum, A. (2006). Topics over time: a non-Markov continuoustime model of topical trends. In Knowledge Discovery and Data Mining, Knowledge Discovery and Data Mining.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>