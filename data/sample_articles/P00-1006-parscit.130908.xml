<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant confidence="0.000066" no="0">
<note confidence="0.40287275">
A Maximum Entropy/Minimum Divergence Translation Model
George Foster
RALI, Universite de Montreal
fosterAiro.umontreaLea
</note>
<sectionHeader confidence="0.799513" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999808833333333">I present empirical comparisons between a linear combination of standard statistical language and translation models and an equivalent Maximum Entropy/Minimum Divergence (MEMD) model, using several different methods for automatic feature selection. The MEMD model significantly outperforms the standard model in test corpus perplexity, even though it has far fewer parameters.</bodyText>
<sectionHeader confidence="0.995551" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999388272727273">Statistical Machine Translation (SMT) systems use a model of p(t Is), the probability that a text s in the source language will translate into a text t in the target language, to determine the best translation for a given source text. The standard approach to modeling this distribution relies on a &amp;quot;noisy channel&amp;quot; decomposition into a language model p(t) and a translation model p (sit) , which correspond respectively to prior and likelihood components in a Bayesian formulation:</bodyText>
<equation confidence="0.955797">
p(t's) = p(t)p(slt)/Ep(t)p(slt)
P(t)p(slt),
</equation>
<bodyText confidence="0.999952875">where proportionality holds when searching for the optimum target text t for a given source text s. This equation has been called the &amp;quot;fundamental equation of SMT&amp;quot; (Brown et al., 1993). In this paper, I investigate an alternate technique for modeling p(t Is), based on a direct chain-rule expansion of the form:</bodyText>
<equation confidence="0.923610333333333">
It I
p(tis) = llp(tz It' tz_i, s), (1)
z=1
</equation>
<bodyText confidence="0.998877029411765">where tz denotes the ith token in t.1 The objects to be modeled in this case belong to the family of conditional distributions p(w lh,$), where w is a target word at a particular position in t, and h denotes the tokens which precede it in t. The main motivation for this approach is that it simplifies the &amp;quot;decoding&amp;quot; problem of finding the most likely target text according to the model. In particular, if h is known, the problem of finding the best word at the current position requires only a straightforward search through the target vocabulary, and simple and efficient dynamicprogramming based heuristics can be used to extend this to sequences of words. This is very important for applications such as TransType (Foster et al., 1997; Langlais et al., 2000), where the task is to make real-time predictions of the text a human translator will type next, based on the source text under translation and some prefix of the target text that has already been typed. The main drawback to modeling p(t Is) in terms of p(w I h, s) is that the latter distribution is conditioned on two very disparate sources of information which are difficult to combine in a complementary way. One simple strategy is to use a linear combination of This ignores the issue of normalization over target texts of all possible lengths, which can be easily enforced when desired by using a stop token or a prior distribution over lengths. language and translation components, of the form:</bodyText>
<equation confidence="0.996179">
p(w1h, s) = Ap(w1h) + (1— A)p(w 1s)• (2)
</equation>
<bodyText confidence="0.995926244444445">where A E [0,1] is a combining weight. However, this is a weak model because it averages over the relative strengths of its components; when p(wih) is likely to be a more accurate estimate than P(wls), it is obvious that the model should rely more heavily on p(w Ih), and vice versa, rather than using a fixed weight. In theory this could be partially remedied by making A depend on h and s, but in practice significant improvements with this technique have proven elusive (Langlais and Foster, 2000). The noisy channel model avoids this problem by making predictions based on h the responsibility of the language model p(t), and those based on s the responsibility of the translation model p(slt), and combining the two in an optimum way. But this comes at the cost of increased decoding complexity, because the chain rule can no longer be applied as in (1) due to the reversed direction of the translation model. Much recent research in SMT, eg (Garcia-Varea et al., 1998; Niessen et al., 1998; Och et al., 1999; Wang and Waibel, 1998) deals with the decoding problem, either directly or indirectly because of constraints imposed on the form of the translation model. A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence (MEMD) modeling (Berger et al., 1996). One of the main strengths of MEMD is that it allows information from different sources to be combined in a principled and effective way, so it is a natural choice for modeling p(wlh, s) In this paper, I describe a MEMD model for p(wlh, s) and compare its performance to that of an equivalent linear model. I also evaluate several different methods for MEMD feature selection, including a new algorithm due to Printz (1998). To my knowledge, this is the first application of MEMD to building a large-scale translation model, and one of the few direct comparisons between a MEMD model and an almost exactly equivalent linear mode1.2</bodyText>
<sectionHeader confidence="0.985354" genericHeader="method">
2 Models
</sectionHeader>
<subsectionHeader confidence="0.8681">
2.1 Linear Model
</subsectionHeader>
<bodyText confidence="0.998180714285714">The baseline model is a linear combination as in (2) of a standard interpolated trigram (Jelinek and Mercer, 1980) for p(will) and the IBM model 1 (IBM1) (Brown et al., 1993) for P(wls). As originally formulated, IBM1 models the distribution p(t Is), but since target text tokens are predicted independently, it can also be used for P(wls). The underlying generative process is as follows: 1) pick a token s at random in s, independent of the positions of w and s; 2) choose w according to a word-for-word translation probability p(w 18). Summing over all choices for s gives the complete model:</bodyText>
<equation confidence="0.99697">
p(wis) = Ep(wisi)/asi +1)
j=0
</equation>
<bodyText confidence="0.999982125">where si is the jth token in s for j &gt; 0, and so is a special null token prepended to each source sentence to account for target words which have no direct translations. The wordpair parameters P(wls) can be estimated from a bilingual corpus of aligned sentence pairs using the EM algorithm, as described in (Brown et al., 1993).</bodyText>
<subsectionHeader confidence="0.984184">
2.2 MEMD Model
</subsectionHeader>
<bodyText confidence="0.999236">A MEMD model for p(wlh , s) has the general form:</bodyText>
<equation confidence="0.982114333333333">
q(wlh, s) exp(5 f (w , h, s))
p(wlh,$) =
Z(h, s)
</equation>
<bodyText confidence="0.989171632653062">where q(w I h, s) is a reference distribution, f (w ,h, s) maps (w, h, s) into an ndimensional feature vector, is a corresponding vector of feature weights (the parameters of the model), and Z(h, s) = Ew q(w I h, s) exp(. f(w, h)) is a normalizing factor. 2Rosenfeld (1996) reports a greater perplexity reduction (23% versus 10%) over a baseline trigram language model due the use of ME versus linear word triggers. However, since the models tested apparently differed in other aspects, it is hard to determine how much of this gain can be attributed to the use of ME. It can be shown (Berger et al., 1996) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values.3 There is no requirement that the components of f represent disjoint or statistically independent events. This result motivates the use of MEMD models, but it offers only weak guidance on how to select q or f. In practice, q is usually chosen on the basis of efficiency considerations (when the information it captures would be computationally expensive to represent as components of f), and f is established using heuristics such as described in the next section. Once q and f have been chosen, the ITS algorithm (Della Pietra et al., 1995) can be used to find maximum likelihood parameter values. In the current context, since the aim was to compare equivalent linear and MEMD models, I used an interpolated trigram as the reference distribution q and boolean indicator functions over bilingual word pairs as features (ie, components of f). A pair of source,target words (8 ,t) has a corresponding feature function: { 1, s E s and t = w fst (w , h, s) = 0, else Using the notational convention that a,,t is 0 whenever the corresponding feature fst does not exist in the model, the final MEMD model can be written compactly as:</bodyText>
<equation confidence="0.9096685">
P(wlh, = q(wlh) exp(E asw)/Z(h, s).
s E s
</equation>
<bodyText confidence="0.8074265">This model is structurally quite similar to the one defined in the previous section:</bodyText>
<equation confidence="0.93991725">
Isl
1 — A
p(wlh,$) = Aq(wih) + 2p(wIsi)
Is' +1
</equation>
<bodyText confidence="0.980333264705883">3Another interpretation, which has been less well publicized in the NLP literature, is that of a singlelayer neural net with certain weight constraints and a &amp;quot;softmax&amp;quot; output function (Bishop, 1995). with the MEMD feature weights asw playing the role of the IBM1 probabilities P(wls) and the MEMD model summing over contributions from source sentence words rather than tokens for efficiency. If there are m free parameters in the trigram and n word pairs, the MEMD model will contain m + n free parameters and the linear model will contain 77 n ± 1 — 'Vs' IVtl 14 free parameters, so if the source and target vocabulary sizes IV,I and IVt I are equal the two models will contain precisely the same number of free parameters. One important practical difference between the two models is the requirement to calculate the MEMD normalizing factor Z(h, s) for each context in which this model is used. This makes the MEMD model much more computationally expensive than the linear model, so that it is not feasible to have it incorporate all available word-pair features (ie all bilingual pairs of words which cooccur in some aligned sentence pair in the training corpus). Moreover, since the empirical expectations of features are supposed to reflect their true values, having a feature for every cooccurring pair in the corpus would be theoretically inadvisable even if it were computationally feasible. Some method of selecting a subset of reliable features is therefore required, as described in the next section.</bodyText>
<sectionHeader confidence="0.992489" genericHeader="method">
3 Feature Selection
</sectionHeader>
<bodyText confidence="0.999807">I experimented with three methods for selecting bilingual word pairs for inclusion in the models. All methods assign scores to individual pairs, so feature subsets of any desired size can be extracted by taking the highest-ranked pairs.</bodyText>
<subsectionHeader confidence="0.981416">
3.1 Mutual Information
</subsectionHeader>
<bodyText confidence="0.999654">The simplest scoring method was mutual information (MI), defined for a pair (s,t) as:</bodyText>
<equation confidence="0.764342857142857">
p(x,y)
= E E 25(x, y) log
13(x)13(Y)'
xc{s,g} y Eft,q
40ne free combining weight, one normalization
constraint per source word, and IV
P(wlso) t I — 1 free parame-
</equation>
<bodyText confidence="0.9459294">ters from where P(s, t) is the probability that a randomly chosen pair of cooccurring source and target tokens in the corpus is (s, t); (s, t) is the probability that the source token is s and the target token is not t; etc; and P (x) and 25(y) are the left and right marginals off9(x, y). Mutual information measures the degree to which s and t are non-independent, so it is a reasonable choice for scoring pairs.</bodyText>
<subsectionHeader confidence="0.998267">
3.2 MEMD Gains
</subsectionHeader>
<bodyText confidence="0.999843">The second scoring method was an approximation of the MEMD gain for feature fst, defined as the log-likelihood difference between a MEMD model which includes this feature and one which does not:</bodyText>
<equation confidence="0.9959125">
1 Pst (T IS)
G8t = ITI log P(TIS)
</equation>
<bodyText confidence="0.999984416666667">where the training corpus (5, T) consists of a set of (statistically independent) sentence pairs (s, t), and pst is the model which includes ht. Since MEMD models are trained by finding the set of feature weights which maximizes the likelihood of the training corpus, it is natural to rate features according to how much they contribute to this likelihood. A powerful strategy for using gains is to build a model iteratively by adding at each step the feature which gives the highest gain with respect to those already added. Berger et al (1996) describe an efficient algorithm for accomplishing this in which approximations to Pst (TIS) are computed in parallel for all (new) features ft by holding all weights in the existing model fixed and optimizing only over a8t. However, this method requires many expensive passes over the corpus to optimize the weights for the set of features under consideration at each step, and it adds only one feature per step, so it is not practical for constructing models containing thousands of features or more. In a recent paper (Printz, 1998), Printz argues that it is usually sufficient to perform the iteration described in the previous paragraph only once, in other words that features can be ranked simply according to their gain with respect to some initial model. He also gives an algorithm for computing gains using a numerical approximation which requires only a single pass over the training corpus. I adopted Printz' method for computing MEMD gains, using the reference trigram as the initial model.</bodyText>
<subsectionHeader confidence="0.98959">
3.3 IBM1 Gains
</subsectionHeader>
<bodyText confidence="0.9998925">The final scoring method involved the gain of each word-pair parameter p(tis) within IBM1. Instead of taking gains with respect to an initial model as in the previous section, I computed them with respect to a &amp;quot;full&amp;quot; model which incorporated all available word pairs:</bodyText>
<equation confidence="0.993148666666667">
1 p(TIS)
G8 = log
TI (TIS)
</equation>
<bodyText confidence="0.9796855">where pwt denotes the full IBM1 model p with the parameter p(tis) set to zero and the resulting distribution /9(49) renormalized. The advantage of this method is that it gives a measure of each parameter's worth in the presence of other parameters. As is the previous section, this is an approximation because determining the true gain would require retraining pwt and not merely renormalizing. A problem with IBM1 gains is that they are not very robust. If the corpus contains a sentence pair (s, t) which consists only of a single word pair (8, t), then G,,t will contain the term 1p(t1.9)+p(t1.90) lo_ 1,7-1 6 p(tIso) , so if p(tIso) is close to zero (as is frequently the case), G,,t will be close to infinity, even though (s, t) may occur only once in the training corpus. To remedy this, I computed gains with respect to a linear combination of IBM1 and a smoothing model u, of the form Ap(w Is) ± (1— ).)u (w I h, s). In the experiments reported below, I used a uniform distribution for u, with A = .99.5 Smoothed IBM1 gains can be computed in parallel in a single pass over the training corpus using the algorithm in figure 1. The line marked with an asterisk takes into account the increase in p(tis) due to renormalizing the distribution /9(49) after setting p(tils) to zero, for each word 75’ 75 t in the vocabulary. To speed up the algorithm, I performed this step only for those 75’ such that p(t’ |s) Z .01. This causes the gains for pairs (5, t’) such that p(t’|s) &lt; .01 to be slightly overestimated, but since the gains of such pairs are low in any case, the ranking of the most valuable pairs is unlikely to be radically affected.</bodyText>
<footnote confidence="0.99696475">
5Another interesting choice for u would be the in-
terpolated trigram, which would make the method de-
scribed here more similar to the MEMD gain ranking
described in the previous section.
</footnote>
<page confidence="0.839819">
45
</page>
<figure confidence="0.950885962962963">
MI
MEMD
IBM1
40
test corpus perplexity
❯
35
30
25
20
0 5000 10000 15000 20000 25000 30000
number of features
trigram+IBM1
100 1000 10000 100000 1e+06 1e+07 1e+08
number of parameters
s
❴
test corpus perplexity
❴
58
56
54
52
50
48
46
44
</figure>
<page confidence="0.827553">
42
</page>
<sectionHeader confidence="0.943108" genericHeader="evaluation and result">
4 Experiments
</sectionHeader>
<bodyText confidence="0.98840504">I ran experiments on the Canadian Hansard corpus, with English as the source language and French as the target language. After sentence alignment using the method described in (Simard et al., 1992), the corpus was split into disjoint segments as shown in table 1. To evaluate performance, I used perplexity: p(7'|8)_1/|Tl, where p is the model being evaluated, and (8,T) is the test corpus. Perplexity is a good indicator of performance for the TransType application described in the introduction, and it has also been used in the evaluation of full-ﬂedged SMT systems (AlOnaizan et al., 1999). To ensure a fair comparison, all models used the same target vocabulary. To compare MEMD feature-selection methods, I ﬁrst ranked all 35 million bilingual word pairs cooccurring within aligned sentence pairs in the training corpus using the MI and IBM1 gains methods. Because the MEMD gains method was much more expensive, it was used to rank only a short list of approximately 160,000 pairs derived by merging the top 100,000 candidates from each of the other methods. As shown in table 2, the three methods give substantially different rankings, even among the top-ranked pairs. For each method, I trained MEMD models on a sequence of successively larger feature sets consisting of the top-ranked word pairs for that method. The results are shown in ﬁgure 2. Due to time constraints,6 20,000and 30,000feature models were trained only for the IBM1 feature sets, which outperformed the other methods by a small margin.</bodyText>
<bodyText confidence="0.98840504">Since the number of features in the MEMD models was much smaller than the number of parameters in the full IBMl, before comparing the MEMD and linear models I wanted to be sure that any performance difference was not due to IBMl overﬁtting the training corpus. To eliminate this possibility, I optimized the number of IBMl parameters by training linear models with various sizes of translation parameter sets obtained from the IBMl gain ranking. As shown in ﬁgure 3, the larger linear models do exhibit a very slight overtraining effect, with the optimum parameter set size around 1M, compared to 35M parameters in the full model. Table 3 presents ﬁnal results for various linear and MEMD models. The MEMD models give a striking improvement over the linear models, with a 1000-feature MEMD model performing better than the best linear model (despite containing 1000 times fewer wordpair parameters), and the best MEMD model yielding a perplexity reduction of more than 45% over the baseline linear model.</bodyText>
<sectionHeader confidence="0.943108" genericHeader="result">
5 Discussion
</sectionHeader>
<bodyText confidence="0.98840504">The main result of this paper is that the MEMD framework appears to be a much more effective way to combine information from different sources than linear interpolation, at least for the problem studied here. It is fairly easy to see intuitively why this should be the case: MEMD essentially multiplies predictive scores arising from different sources rather than averaging them.</bodyText>
<bodyText confidence="0.9999634">This gives information sources which assign either very high or very low scores much more influence over the final result. When such scores are based upon reliable evidence, this will lead to better models. One somewhat surprising result of these experiments was that the IBM1 gains feature selection method resulted in better models than the MEMD gains method, despite the fact that the latter is based on a much more direct measure of each feature's worth within the MEMD model. A possible explanation for this is that the gain over the reference trigram is not a good predictor of the gain in the presence of many other features; this is borne out by the fact that, for very small feature sets (on the order of 100 words and less), the MEMD method did outperform the IBM1 method. Another explanation is inaccuracies in the gain approximations computed by Printz' method, which involves many numerical parameters that require tuning. Further investigation is required into this and other techniques for finding valid word pairs, since all methods tested yielded significant quantities of noise beyond 30,000 pairs. Because the source vocabulary contains about 50,000 words this is obviously an unrealistically small number of translations. Although the main use for the model I have described in this paper is in applications like TransType which need to make rapid predictions of upcoming target text, it is interesting to speculate about whether a MEMD model for p(w I h, s) could also be useful for SMT. Compared to the standard noisy channel approach, this has the advantage of permitting much less complex search procedures; of allowing any information which is directly observable in the training corpus to be very easily incorporated into the model via boolean features; and of an estimation procedure where translation model parameters can be optimized for use with an existing language mode1.7 Disadvantages include the high cost of training MEMD models, the fact that p(w I h, s) is somewhat less general than P(slt) for building realistic translation models; and the lack of a mechanism equivalent to the EM algorithm for incorporating &amp;quot;hidden&amp;quot; variables into MEMD models (see (Foster, 2000) for a discussion of this problem).</bodyText>
<sectionHeader confidence="0.998179" genericHeader="conclusion">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999974863636363">The problem of searching for the best target text in statistical translation applications can be greatly simplified if the fundamental distribution p(t Is) is expanded directly in terms of the distribution p(w I h, s), rather than using the standard noisy-channel approach. I compared a simple linear model for p (w I h, s) based on IBM's model 1 with an equivalent MEMD model, and found that the MEMD model has over 45% lower test corpus perplexity, despite using two orders of magnitude fewer parameters. I also compared several methods for selecting MEMD word-pair features, and found that a simple method which ranks pairs according to their gain within model 1 offers slightly better performance and significantly lower computational cost than a more general MEMD feature-selection algorithm due to Printz. Finally, I suggest that it may be fruitful to explore the idea of using a MEMD model for p (w I h, s) as an alternative to the noisy-channel approach to SMT.</bodyText>
<sectionHeader confidence="0.993205" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999932142857143">This work was carried out as part of the TransType project at RALI, funded by the Natural Sciences and Engineering Research Council of Canada. I wish to thank Guy Lapalme and Andreas Eisele for comments on the paper, and Philippe Langlais for inspiring discussions.</bodyText>
<sectionHeader confidence="0.996409" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9769784">
Yaser Al-Onaizan, Jan Curin, Michael Jahr,
Kevin Knight, John Lafferty, Dan Melamed,
Franz-Josef Och, David Purdy, Noah A.
Smith, and David Yarowsky. 1999. Sta-
tistical machine translation: Final report,
</reference>
<footnote confidence="0.641268">
71n principle, both language and translation corn- ponents could be trained simultaneously.
</footnote>
<note confidence="0.462923">
JHU workshop 1999. Technical report,
The Center for Language and Speech Pro-
cessing, The Johns Hopkins University,
www.clsp.jhu.edu/ws99/projects/mt/final_report
</note>
<reference confidence="0.9990483">
Adam L. Berger, Stephen A. Della Pietra, and
Vincent J. Della Pietra. 1996. A Maximum En-
tropy approach to Natural Language Process-
ing. Computational Linguistics, 22(1):39-71.
Christopher M. Bishop. 1995. Neural Networks
for Pattern Recognition. Oxford.
Peter F. Brown, Stephen A. Della Pietra, Vincent
Della J. Pietra, and Robert L. Mercer. 1993.
The mathematics of Machine Translation: Pa-
rameter estimation. Computational Linguis-
tics, 19(2):263-312, June.
S. Della Pietra, V. Della Pietra, and J. Lafferty.
1995. Inducing features of random fields. Tech-
nical Report CMU-CS-95-144, CMU.
George Foster, Pierre Isabelle, and Pierre Plam-
ondon. 1997. Target-text Mediated Interac-
tive Machine Translation. Machine Transla-
tion, 12:175-194.
George Foster. 2000. Incorporating position in-
formation into a Maximum Entropy / Mini-
mum Divergence translation model. In Pro-
ceedings of the 4th Computational Natural Lan-
guage Learning Workshop (CoNLL), Lisbon,
Portugal, September. ACL SigNLL.
Ismael Garcia-Varea, Francisco Casacuberta, and
Hermann Ney. 1998. An iterative, DP-based
search algorithm for statistical machine trans-
lation. In ICSLP-98 (ICS, 1998), pages 1135-
1138.
1998. Proceedings of the 5th International Con-
ference on Spoken Language Processing (IC-
SLP) 1998, Sydney, Australia, December.
F. Jelinek and R. L. Mercer. 1980. Interpolated
estimation of Markov source parameters from
sparse data. In E. S. Gelsema and L. N. Kanal,
editors, Pattern Recognition in Practice. North-
Holland, Amsterdam.
Ph. Langlais and G. Foster. 2000. Using context-
dependent interpolation to combine statistical
language and translation models for interactive
MT. In Content-Based Multimedia Information
Access (RIAO), Paris, France, April.
Philippe Langlais, Sebastien Saul* George Fos-
ter, Elliott Macklovitch, and Guy Lapalme.
2000. A comparison of theoretical and user-
oriented evaluation procedures of a new type of
interactive MT. In Second International Con-
ference On Language Resources and Evaluation
(LREC), pages 641-648, Athens, Greece, June.
S. Niessen, S. Vogel, H. Ney, and C. Tillmann.
1998. A DP based search algorithm for sta-
tistical machine translation. In Proceedings
. of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL) and
17th International Conference on Computa-
tional Linguistics (COLING) 1998, pages 960-
967, Montréal, Canada, August.
Franz Josef Och, Christoph Tillmann, and Her-
mann Ney. 1999. Improved alignment models
for statistical machine translation. In Proceed-
ings of the 4nd Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
College Park, Maryland.
Harry Printz. 1998. Fast computation of Max-
imum Entropy/Minimum Divergence feature
gain. In ICSLP-98 (ICS, 1998), pages 2083-
2086.
Ronald Rosenfeld. 1996. A maximum entropy
approach to adaptive statistical language mod-
elling. Computer Speech and Language, 10:187-
228.
Michel Simard, George F. Foster, and Pierre Is-
abelle. 1992. Using cognates to align sen-
tences in bilingual corpora. In Proceedings of
the 4th Conference on Theoretical and Method-
ological Issues in Machine Translation (TMI),
Montréal, Québec.
Ye-yi Wang and Alex Waibel. 1998. Fast decoding
for statistical machine translation. In ICSLP-
98 (ICS, 1998), pages 2775-2778.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant confidence="0.882112" no="0">
<title confidence="0.997502">A Maximum Entropy/Minimum Divergence Translation Model</title>
<author confidence="0.998031">George Foster</author>
<affiliation confidence="0.974938">RALI, Universite de Montreal</affiliation>
<email confidence="0.991157">fosterAiro.umontreaLea</email>
<abstract confidence="0.993222692307692">empirical comparisons between a linear combination of standard statistical language and translation models and an equivalent Maximum Entropy/Minimum Divergence (MEMD) model, using several different methods for automatic feature selection. The MEMD model significantly outperforms the standard model in test corpus perplexity, even though it has far fewer parameters.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Jan Curin</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>John Lafferty</author>
<author>Dan Melamed</author>
<author>Franz-Josef Och</author>
<author>David Purdy</author>
<author>Noah A Smith</author>
<author>David Yarowsky</author>
</authors>
<date>1999</date>
<note>Statistical machine translation: Final report,</note>
<marker>Al-Onaizan, Curin, Jahr, Knight, Lafferty, Melamed, Och, Purdy, Smith, Yarowsky, 1999</marker>
<rawString>Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Franz-Josef Och, David Purdy, Noah A. Smith, and David Yarowsky. 1999. Statistical machine translation: Final report,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A Maximum Entropy approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context citStr="Berger et al., 1996" endWordPosition="723" position="4219" startWordPosition="720">p(slt), and combining the two in an optimum way. But this comes at the cost of increased decoding complexity, because the chain rule can no longer be applied as in (1) due to the reversed direction of the translation model. Much recent research in SMT, eg (Garcia-Varea et al., 1998; Niessen et al., 1998; Och et al., 1999; Wang and Waibel, 1998) deals with the decoding problem, either directly or indirectly because of constraints imposed on the form of the translation model. A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence (MEMD) modeling (Berger et al., 1996). One of the main strengths of MEMD is that it allows information from different sources to be combined in a principled and effective way, so it is a natural choice for modeling p(wlh, s) In this paper, I describe a MEMD model for p(wlh, s) and compare its performance to that of an equivalent linear model. I also evaluate several different methods for MEMD feature selection, including a new algorithm due to Printz (1998). To my knowledge, this is the first application of MEMD to building a large-scale translation model, and one of the few direct comparisons between a MEMD model and an almost e</context>
<context citStr="Berger et al., 1996" endWordPosition="1157" position="6554" startWordPosition="1154">,$) = Z(h, s) where q(w I h, s) is a reference distribution, f (w ,h, s) maps (w, h, s) into an ndimensional feature vector, is a corresponding vector of feature weights (the parameters of the model), and Z(h, s) = Ew q(w I h, s) exp(. f(w, h)) is a normalizing factor. 2Rosenfeld (1996) reports a greater perplexity reduction (23% versus 10%) over a baseline trigram language model due the use of ME versus linear word triggers. However, since the models tested apparently differed in other aspects, it is hard to determine how much of this gain can be attributed to the use of ME. It can be shown (Berger et al., 1996) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values.3 There is no requirement that the components of f represent disjoint or statistically independent events. This result motivates the use of MEMD models, but it offers only weak guidance on how to select q or f. In practice, q is usually chosen on the basis of efficiency considerations (when the information it ca</context>
<context citStr="Berger et al (1996)" endWordPosition="2012" position="11389" startWordPosition="2009">es this feature and one which does not: 1 Pst (T IS) G8t = ITI log P(TIS) where the training corpus (5, T) consists of a set of (statistically independent) sentence pairs (s, t), and pst is the model which includes ht. Since MEMD models are trained by finding the set of feature weights which maximizes the likelihood of the training corpus, it is natural to rate features according to how much they contribute to this likelihood. A powerful strategy for using gains is to build a model iteratively by adding at each step the feature which gives the highest gain with respect to those already added. Berger et al (1996) describe an efficient algorithm for accomplishing this in which approximations to Pst (TIS) are computed in parallel for all (new) features ft by holding all weights in the existing model fixed and optimizing only over a8t. However, this method requires many expensive passes over the corpus to optimize the weights for the set of features under consideration at each step, and it adds only one feature per step, so it is not practical for constructing models containing thousands of features or more. In a recent paper (Printz, 1998), Printz argues that it is usually sufficient to perform the iter</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A Maximum Entropy approach to Natural Language Processing. Computational Linguistics, 22(1):39-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<title>Neural Networks for Pattern Recognition.</title>
<date>1995</date>
<location>Oxford.</location>
<contexts>
<context citStr="Bishop, 1995" endWordPosition="1473" position="8343" startWordPosition="1472">rresponding feature function: { 1, s E s and t = w fst (w , h, s) = 0, else Using the notational convention that a,,t is 0 whenever the corresponding feature fst does not exist in the model, the final MEMD model can be written compactly as: P(wlh, = q(wlh) exp(E asw)/Z(h, s). s E s This model is structurally quite similar to the one defined in the previous section: Isl 1 — A p(wlh,$) = Aq(wih) + 2p(wIsi) Is' +1 3Another interpretation, which has been less well publicized in the NLP literature, is that of a singlelayer neural net with certain weight constraints and a &amp;quot;softmax&amp;quot; output function (Bishop, 1995). with the MEMD feature weights asw playing the role of the IBM1 probabilities P(wls) and the MEMD model summing over contributions from source sentence words rather than tokens for efficiency. If there are m free parameters in the trigram and n word pairs, the MEMD model will contain m + n free parameters and the linear model will contain 77 n ± 1 — 'Vs' IVtl 14 free parameters, so if the source and target vocabulary sizes IV,I and IVt I are equal the two models will contain precisely the same number of free parameters. One important practical difference between the two models is the requirem</context>
</contexts>
<marker>Bishop, 1995</marker>
<rawString>Christopher M. Bishop. 1995. Neural Networks for Pattern Recognition. Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent Della J Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of Machine Translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context citStr="Brown et al., 1993" endWordPosition="190" position="1232" startWordPosition="187">a text s in the source language will translate into a text t in the target language, to determine the best translation for a given source text. The standard approach to modeling this distribution relies on a &amp;quot;noisy channel&amp;quot; decomposition into a language model p(t) and a translation model p (sit) , which correspond respectively to prior and likelihood components in a Bayesian formulation: p(t's) = p(t)p(slt)/Ep(t)p(slt) P(t)p(slt), where proportionality holds when searching for the optimum target text t for a given source text s. This equation has been called the &amp;quot;fundamental equation of SMT&amp;quot; (Brown et al., 1993). In this paper, I investigate an alternate technique for modeling p(t Is), based on a direct chain-rule expansion of the form: It I p(tis) = llp(tz It' tz_i, s), (1) z=1 where tz denotes the ith token in t.1 The objects to be modeled in this case belong to the family of conditional distributions p(w lh,$), where w is a target word at a particular position in t, and h denotes the tokens which precede it in t. The main motivation for this approach is that it simplifies the &amp;quot;decoding&amp;quot; problem of finding the most likely target text according to the model. In particular, if h is known, the problem</context>
<context citStr="Brown et al., 1993" endWordPosition="872" position="5052" startWordPosition="869">a MEMD model for p(wlh, s) and compare its performance to that of an equivalent linear model. I also evaluate several different methods for MEMD feature selection, including a new algorithm due to Printz (1998). To my knowledge, this is the first application of MEMD to building a large-scale translation model, and one of the few direct comparisons between a MEMD model and an almost exactly equivalent linear mode1.2 2 Models 2.1 Linear Model The baseline model is a linear combination as in (2) of a standard interpolated trigram (Jelinek and Mercer, 1980) for p(will) and the IBM model 1 (IBM1) (Brown et al., 1993) for P(wls). As originally formulated, IBM1 models the distribution p(t Is), but since target text tokens are predicted independently, it can also be used for P(wls). The underlying generative process is as follows: 1) pick a token s at random in s, independent of the positions of w and s; 2) choose w according to a word-for-word translation probability p(w 18). Summing over all choices for s gives the complete model: p(wis) = Ep(wisi)/asi +1) j=0 where si is the jth token in s for j &gt; 0, and so is a special null token prepended to each source sentence to account for target words which have no</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent Della J. Pietra, and Robert L. Mercer. 1993. The mathematics of Machine Translation: Parameter estimation. Computational Linguistics, 19(2):263-312, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>J Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1995</date>
<tech>Technical Report CMU-CS-95-144, CMU.</tech>
<contexts>
<context citStr="Pietra et al., 1995" endWordPosition="1300" position="7383" startWordPosition="1297">alues of f in the training corpus are identical to their true expected values.3 There is no requirement that the components of f represent disjoint or statistically independent events. This result motivates the use of MEMD models, but it offers only weak guidance on how to select q or f. In practice, q is usually chosen on the basis of efficiency considerations (when the information it captures would be computationally expensive to represent as components of f), and f is established using heuristics such as described in the next section. Once q and f have been chosen, the ITS algorithm (Della Pietra et al., 1995) can be used to find maximum likelihood parameter values. In the current context, since the aim was to compare equivalent linear and MEMD models, I used an interpolated trigram as the reference distribution q and boolean indicator functions over bilingual word pairs as features (ie, components of f). A pair of source,target words (8 ,t) has a corresponding feature function: { 1, s E s and t = w fst (w , h, s) = 0, else Using the notational convention that a,,t is 0 whenever the corresponding feature fst does not exist in the model, the final MEMD model can be written compactly as: P(wlh, = q(w</context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1995</marker>
<rawString>S. Della Pietra, V. Della Pietra, and J. Lafferty. 1995. Inducing features of random fields. Technical Report CMU-CS-95-144, CMU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Pierre Isabelle</author>
<author>Pierre Plamondon</author>
</authors>
<date>1997</date>
<booktitle>Target-text Mediated Interactive Machine Translation. Machine Translation,</booktitle>
<pages>12--175</pages>
<contexts>
<context citStr="Foster et al., 1997" endWordPosition="354" position="2141" startWordPosition="351">butions p(w lh,$), where w is a target word at a particular position in t, and h denotes the tokens which precede it in t. The main motivation for this approach is that it simplifies the &amp;quot;decoding&amp;quot; problem of finding the most likely target text according to the model. In particular, if h is known, the problem of finding the best word at the current position requires only a straightforward search through the target vocabulary, and simple and efficient dynamicprogramming based heuristics can be used to extend this to sequences of words. This is very important for applications such as TransType (Foster et al., 1997; Langlais et al., 2000), where the task is to make real-time predictions of the text a human translator will type next, based on the source text under translation and some prefix of the target text that has already been typed. The main drawback to modeling p(t Is) in terms of p(w I h, s) is that the latter distribution is conditioned on two very disparate sources of information which are difficult to combine in a complementary way. One simple strategy is to use a linear combination of This ignores the issue of normalization over target texts of all possible lengths, which can be easily enforc</context>
</contexts>
<marker>Foster, Isabelle, Plamondon, 1997</marker>
<rawString>George Foster, Pierre Isabelle, and Pierre Plamondon. 1997. Target-text Mediated Interactive Machine Translation. Machine Translation, 12:175-194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
</authors>
<title>Incorporating position information into a Maximum Entropy / Minimum Divergence translation model.</title>
<date>2000</date>
<booktitle>In Proceedings of the 4th Computational Natural Language Learning Workshop (CoNLL),</booktitle>
<publisher>ACL SigNLL.</publisher>
<location>Lisbon, Portugal,</location>
<contexts>
<context citStr="Foster, 2000" endWordPosition="584" position="3408" startWordPosition="583">ibution over lengths. language and translation components, of the form: p(w1h, s) = Ap(w1h) + (1— A)p(w 1s)• (2) where A E [0,1] is a combining weight. However, this is a weak model because it averages over the relative strengths of its components; when p(wih) is likely to be a more accurate estimate than P(wls), it is obvious that the model should rely more heavily on p(w Ih), and vice versa, rather than using a fixed weight. In theory this could be partially remedied by making A depend on h and s, but in practice significant improvements with this technique have proven elusive (Langlais and Foster, 2000). The noisy channel model avoids this problem by making predictions based on h the responsibility of the language model p(t), and those based on s the responsibility of the translation model p(slt), and combining the two in an optimum way. But this comes at the cost of increased decoding complexity, because the chain rule can no longer be applied as in (1) due to the reversed direction of the translation model. Much recent research in SMT, eg (Garcia-Varea et al., 1998; Niessen et al., 1998; Och et al., 1999; Wang and Waibel, 1998) deals with the decoding problem, either directly or indirectly</context>
<context citStr="Foster, 2000" endWordPosition="2944" position="16731" startWordPosition="2942">ing much less complex search procedures; of allowing any information which is directly observable in the training corpus to be very easily incorporated into the model via boolean features; and of an estimation procedure where translation model parameters can be optimized for use with an existing language mode1.7 Disadvantages include the high cost of training MEMD models, the fact that p(w I h, s) is somewhat less general than P(slt) for building realistic translation models; and the lack of a mechanism equivalent to the EM algorithm for incorporating &amp;quot;hidden&amp;quot; variables into MEMD models (see (Foster, 2000) for a discussion of this problem). 6 Conclusion The problem of searching for the best target text in statistical translation applications can be greatly simplified if the fundamental distribution p(t Is) is expanded directly in terms of the distribution p(w I h, s), rather than using the standard noisy-channel approach. I compared a simple linear model for p (w I h, s) based on IBM's model 1 with an equivalent MEMD model, and found that the MEMD model has over 45% lower test corpus perplexity, despite using two orders of magnitude fewer parameters. I also compared several methods for selectin</context>
</contexts>
<marker>Foster, 2000</marker>
<rawString>George Foster. 2000. Incorporating position information into a Maximum Entropy / Minimum Divergence translation model. In Proceedings of the 4th Computational Natural Language Learning Workshop (CoNLL), Lisbon, Portugal, September. ACL SigNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ismael Garcia-Varea</author>
<author>Francisco Casacuberta</author>
<author>Hermann Ney</author>
</authors>
<title>An iterative, DP-based search algorithm for statistical machine translation.</title>
<date>1998</date>
<booktitle>In ICSLP-98 (ICS,</booktitle>
<pages>1135--1138</pages>
<contexts>
<context citStr="Garcia-Varea et al., 1998" endWordPosition="668" position="3881" startWordPosition="665">lly remedied by making A depend on h and s, but in practice significant improvements with this technique have proven elusive (Langlais and Foster, 2000). The noisy channel model avoids this problem by making predictions based on h the responsibility of the language model p(t), and those based on s the responsibility of the translation model p(slt), and combining the two in an optimum way. But this comes at the cost of increased decoding complexity, because the chain rule can no longer be applied as in (1) due to the reversed direction of the translation model. Much recent research in SMT, eg (Garcia-Varea et al., 1998; Niessen et al., 1998; Och et al., 1999; Wang and Waibel, 1998) deals with the decoding problem, either directly or indirectly because of constraints imposed on the form of the translation model. A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence (MEMD) modeling (Berger et al., 1996). One of the main strengths of MEMD is that it allows information from different sources to be combined in a principled and effective way, so it is a natural choice for modeling p(wlh, s) In this paper, I describe a MEMD model for p(wlh, s) and compare its perfo</context>
</contexts>
<marker>Garcia-Varea, Casacuberta, Ney, 1998</marker>
<rawString>Ismael Garcia-Varea, Francisco Casacuberta, and Hermann Ney. 1998. An iterative, DP-based search algorithm for statistical machine translation. In ICSLP-98 (ICS, 1998), pages 1135-1138.</rawString>
</citation>
<citation valid="true">
<date>1998</date>
<booktitle>Proceedings of the 5th International Conference on Spoken Language Processing (ICSLP) 1998,</booktitle>
<location>Sydney, Australia,</location>
<contexts>
<context citStr="(1998)" endWordPosition="800" position="4643" startWordPosition="800">ed on the form of the translation model. A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence (MEMD) modeling (Berger et al., 1996). One of the main strengths of MEMD is that it allows information from different sources to be combined in a principled and effective way, so it is a natural choice for modeling p(wlh, s) In this paper, I describe a MEMD model for p(wlh, s) and compare its performance to that of an equivalent linear model. I also evaluate several different methods for MEMD feature selection, including a new algorithm due to Printz (1998). To my knowledge, this is the first application of MEMD to building a large-scale translation model, and one of the few direct comparisons between a MEMD model and an almost exactly equivalent linear mode1.2 2 Models 2.1 Linear Model The baseline model is a linear combination as in (2) of a standard interpolated trigram (Jelinek and Mercer, 1980) for p(will) and the IBM model 1 (IBM1) (Brown et al., 1993) for P(wls). As originally formulated, IBM1 models the distribution p(t Is), but since target text tokens are predicted independently, it can also be used for P(wls). The underlying generativ</context>
</contexts>
<marker>1998</marker>
<rawString>1998. Proceedings of the 5th International Conference on Spoken Language Processing (ICSLP) 1998, Sydney, Australia, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>R L Mercer</author>
</authors>
<title>Interpolated estimation of Markov source parameters from sparse data.</title>
<date>1980</date>
<booktitle>Pattern Recognition in Practice. NorthHolland,</booktitle>
<editor>In E. S. Gelsema and L. N. Kanal, editors,</editor>
<location>Amsterdam.</location>
<contexts>
<context citStr="Jelinek and Mercer, 1980" endWordPosition="860" position="4992" startWordPosition="856">a natural choice for modeling p(wlh, s) In this paper, I describe a MEMD model for p(wlh, s) and compare its performance to that of an equivalent linear model. I also evaluate several different methods for MEMD feature selection, including a new algorithm due to Printz (1998). To my knowledge, this is the first application of MEMD to building a large-scale translation model, and one of the few direct comparisons between a MEMD model and an almost exactly equivalent linear mode1.2 2 Models 2.1 Linear Model The baseline model is a linear combination as in (2) of a standard interpolated trigram (Jelinek and Mercer, 1980) for p(will) and the IBM model 1 (IBM1) (Brown et al., 1993) for P(wls). As originally formulated, IBM1 models the distribution p(t Is), but since target text tokens are predicted independently, it can also be used for P(wls). The underlying generative process is as follows: 1) pick a token s at random in s, independent of the positions of w and s; 2) choose w according to a word-for-word translation probability p(w 18). Summing over all choices for s gives the complete model: p(wis) = Ep(wisi)/asi +1) j=0 where si is the jth token in s for j &gt; 0, and so is a special null token prepended to ea</context>
</contexts>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>F. Jelinek and R. L. Mercer. 1980. Interpolated estimation of Markov source parameters from sparse data. In E. S. Gelsema and L. N. Kanal, editors, Pattern Recognition in Practice. NorthHolland, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Langlais</author>
<author>G Foster</author>
</authors>
<title>Using contextdependent interpolation to combine statistical language and translation models for interactive MT.</title>
<date>2000</date>
<booktitle>In Content-Based Multimedia Information Access (RIAO),</booktitle>
<location>Paris, France,</location>
<contexts>
<context citStr="Langlais and Foster, 2000" endWordPosition="584" position="3408" startWordPosition="581">a prior distribution over lengths. language and translation components, of the form: p(w1h, s) = Ap(w1h) + (1— A)p(w 1s)• (2) where A E [0,1] is a combining weight. However, this is a weak model because it averages over the relative strengths of its components; when p(wih) is likely to be a more accurate estimate than P(wls), it is obvious that the model should rely more heavily on p(w Ih), and vice versa, rather than using a fixed weight. In theory this could be partially remedied by making A depend on h and s, but in practice significant improvements with this technique have proven elusive (Langlais and Foster, 2000). The noisy channel model avoids this problem by making predictions based on h the responsibility of the language model p(t), and those based on s the responsibility of the translation model p(slt), and combining the two in an optimum way. But this comes at the cost of increased decoding complexity, because the chain rule can no longer be applied as in (1) due to the reversed direction of the translation model. Much recent research in SMT, eg (Garcia-Varea et al., 1998; Niessen et al., 1998; Och et al., 1999; Wang and Waibel, 1998) deals with the decoding problem, either directly or indirectly</context>
</contexts>
<marker>Langlais, Foster, 2000</marker>
<rawString>Ph. Langlais and G. Foster. 2000. Using contextdependent interpolation to combine statistical language and translation models for interactive MT. In Content-Based Multimedia Information Access (RIAO), Paris, France, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philippe Langlais</author>
<author>Sebastien Saul George Foster</author>
<author>Elliott Macklovitch</author>
<author>Guy Lapalme</author>
</authors>
<title>A comparison of theoretical and useroriented evaluation procedures of a new type of interactive MT.</title>
<date>2000</date>
<booktitle>In Second International Conference On Language Resources and Evaluation (LREC),</booktitle>
<pages>641--648</pages>
<location>Athens, Greece,</location>
<contexts>
<context citStr="Langlais et al., 2000" endWordPosition="358" position="2165" startWordPosition="355">ere w is a target word at a particular position in t, and h denotes the tokens which precede it in t. The main motivation for this approach is that it simplifies the &amp;quot;decoding&amp;quot; problem of finding the most likely target text according to the model. In particular, if h is known, the problem of finding the best word at the current position requires only a straightforward search through the target vocabulary, and simple and efficient dynamicprogramming based heuristics can be used to extend this to sequences of words. This is very important for applications such as TransType (Foster et al., 1997; Langlais et al., 2000), where the task is to make real-time predictions of the text a human translator will type next, based on the source text under translation and some prefix of the target text that has already been typed. The main drawback to modeling p(t Is) in terms of p(w I h, s) is that the latter distribution is conditioned on two very disparate sources of information which are difficult to combine in a complementary way. One simple strategy is to use a linear combination of This ignores the issue of normalization over target texts of all possible lengths, which can be easily enforced when desired by using</context>
</contexts>
<marker>Langlais, Foster, Macklovitch, Lapalme, 2000</marker>
<rawString>Philippe Langlais, Sebastien Saul* George Foster, Elliott Macklovitch, and Guy Lapalme. 2000. A comparison of theoretical and useroriented evaluation procedures of a new type of interactive MT. In Second International Conference On Language Resources and Evaluation (LREC), pages 641-648, Athens, Greece, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Niessen</author>
<author>S Vogel</author>
<author>H Ney</author>
<author>C Tillmann</author>
</authors>
<title>A DP based search algorithm for statistical machine translation.</title>
<date>1998</date>
<booktitle>In Proceedings</booktitle>
<contexts>
<context citStr="Niessen et al., 1998" endWordPosition="672" position="3903" startWordPosition="669">pend on h and s, but in practice significant improvements with this technique have proven elusive (Langlais and Foster, 2000). The noisy channel model avoids this problem by making predictions based on h the responsibility of the language model p(t), and those based on s the responsibility of the translation model p(slt), and combining the two in an optimum way. But this comes at the cost of increased decoding complexity, because the chain rule can no longer be applied as in (1) due to the reversed direction of the translation model. Much recent research in SMT, eg (Garcia-Varea et al., 1998; Niessen et al., 1998; Och et al., 1999; Wang and Waibel, 1998) deals with the decoding problem, either directly or indirectly because of constraints imposed on the form of the translation model. A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence (MEMD) modeling (Berger et al., 1996). One of the main strengths of MEMD is that it allows information from different sources to be combined in a principled and effective way, so it is a natural choice for modeling p(wlh, s) In this paper, I describe a MEMD model for p(wlh, s) and compare its performance to that of an e</context>
</contexts>
<marker>Niessen, Vogel, Ney, Tillmann, 1998</marker>
<rawString>S. Niessen, S. Vogel, H. Ney, and C. Tillmann. 1998. A DP based search algorithm for statistical machine translation. In Proceedings</rawString>
</citation>
<citation valid="true">
<date>1998</date>
<booktitle>of the 36th Annual Meeting of the Association for Computational Linguistics (ACL) and 17th International Conference on Computational Linguistics (COLING)</booktitle>
<pages>960--967</pages>
<location>Montréal, Canada,</location>
<contexts>
<context citStr="(1998)" endWordPosition="800" position="4643" startWordPosition="800">ed on the form of the translation model. A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence (MEMD) modeling (Berger et al., 1996). One of the main strengths of MEMD is that it allows information from different sources to be combined in a principled and effective way, so it is a natural choice for modeling p(wlh, s) In this paper, I describe a MEMD model for p(wlh, s) and compare its performance to that of an equivalent linear model. I also evaluate several different methods for MEMD feature selection, including a new algorithm due to Printz (1998). To my knowledge, this is the first application of MEMD to building a large-scale translation model, and one of the few direct comparisons between a MEMD model and an almost exactly equivalent linear mode1.2 2 Models 2.1 Linear Model The baseline model is a linear combination as in (2) of a standard interpolated trigram (Jelinek and Mercer, 1980) for p(will) and the IBM model 1 (IBM1) (Brown et al., 1993) for P(wls). As originally formulated, IBM1 models the distribution p(t Is), but since target text tokens are predicted independently, it can also be used for P(wls). The underlying generativ</context>
</contexts>
<marker>1998</marker>
<rawString>. of the 36th Annual Meeting of the Association for Computational Linguistics (ACL) and 17th International Conference on Computational Linguistics (COLING) 1998, pages 960-967, Montréal, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In Proceedings of the 4nd Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>College Park, Maryland.</location>
<contexts>
<context citStr="Och et al., 1999" endWordPosition="676" position="3921" startWordPosition="673">n practice significant improvements with this technique have proven elusive (Langlais and Foster, 2000). The noisy channel model avoids this problem by making predictions based on h the responsibility of the language model p(t), and those based on s the responsibility of the translation model p(slt), and combining the two in an optimum way. But this comes at the cost of increased decoding complexity, because the chain rule can no longer be applied as in (1) due to the reversed direction of the translation model. Much recent research in SMT, eg (Garcia-Varea et al., 1998; Niessen et al., 1998; Och et al., 1999; Wang and Waibel, 1998) deals with the decoding problem, either directly or indirectly because of constraints imposed on the form of the translation model. A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence (MEMD) modeling (Berger et al., 1996). One of the main strengths of MEMD is that it allows information from different sources to be combined in a principled and effective way, so it is a natural choice for modeling p(wlh, s) In this paper, I describe a MEMD model for p(wlh, s) and compare its performance to that of an equivalent linear m</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Franz Josef Och, Christoph Tillmann, and Hermann Ney. 1999. Improved alignment models for statistical machine translation. In Proceedings of the 4nd Conference on Empirical Methods in Natural Language Processing (EMNLP), College Park, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harry Printz</author>
</authors>
<title>Fast computation of Maximum Entropy/Minimum Divergence feature gain.</title>
<date>1998</date>
<booktitle>In ICSLP-98 (ICS,</booktitle>
<pages>2083--2086</pages>
<contexts>
<context citStr="Printz (1998)" endWordPosition="800" position="4643" startWordPosition="799">s imposed on the form of the translation model. A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence (MEMD) modeling (Berger et al., 1996). One of the main strengths of MEMD is that it allows information from different sources to be combined in a principled and effective way, so it is a natural choice for modeling p(wlh, s) In this paper, I describe a MEMD model for p(wlh, s) and compare its performance to that of an equivalent linear model. I also evaluate several different methods for MEMD feature selection, including a new algorithm due to Printz (1998). To my knowledge, this is the first application of MEMD to building a large-scale translation model, and one of the few direct comparisons between a MEMD model and an almost exactly equivalent linear mode1.2 2 Models 2.1 Linear Model The baseline model is a linear combination as in (2) of a standard interpolated trigram (Jelinek and Mercer, 1980) for p(will) and the IBM model 1 (IBM1) (Brown et al., 1993) for P(wls). As originally formulated, IBM1 models the distribution p(t Is), but since target text tokens are predicted independently, it can also be used for P(wls). The underlying generativ</context>
<context citStr="Printz, 1998" endWordPosition="2103" position="11924" startWordPosition="2102">ives the highest gain with respect to those already added. Berger et al (1996) describe an efficient algorithm for accomplishing this in which approximations to Pst (TIS) are computed in parallel for all (new) features ft by holding all weights in the existing model fixed and optimizing only over a8t. However, this method requires many expensive passes over the corpus to optimize the weights for the set of features under consideration at each step, and it adds only one feature per step, so it is not practical for constructing models containing thousands of features or more. In a recent paper (Printz, 1998), Printz argues that it is usually sufficient to perform the iteration described in the previous paragraph only once, in other words that features can be ranked simply according to their gain with respect to some initial model. He also gives an algorithm for computing gains using a numerical approximation which requires only a single pass over the training corpus. I adopted Printz' method for computing MEMD gains, using the reference trigram as the initial model. 3.3 IBM1 Gains The final scoring method involved the gain of each word-pair parameter p(tis) within IBM1. Instead of taking gains wi</context>
</contexts>
<marker>Printz, 1998</marker>
<rawString>Harry Printz. 1998. Fast computation of Maximum Entropy/Minimum Divergence feature gain. In ICSLP-98 (ICS, 1998), pages 2083-2086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>A maximum entropy approach to adaptive statistical language modelling.</title>
<date>1996</date>
<journal>Computer Speech and Language,</journal>
<pages>10--187</pages>
<contexts>
<context citStr="Rosenfeld (1996)" endWordPosition="1096" position="6221" startWordPosition="1095">tence to account for target words which have no direct translations. The wordpair parameters P(wls) can be estimated from a bilingual corpus of aligned sentence pairs using the EM algorithm, as described in (Brown et al., 1993). 2.2 MEMD Model A MEMD model for p(wlh , s) has the general form: q(wlh, s) exp(5 f (w , h, s)) p(wlh,$) = Z(h, s) where q(w I h, s) is a reference distribution, f (w ,h, s) maps (w, h, s) into an ndimensional feature vector, is a corresponding vector of feature weights (the parameters of the model), and Z(h, s) = Ew q(w I h, s) exp(. f(w, h)) is a normalizing factor. 2Rosenfeld (1996) reports a greater perplexity reduction (23% versus 10%) over a baseline trigram language model due the use of ME versus linear word triggers. However, since the models tested apparently differed in other aspects, it is hard to determine how much of this gain can be attributed to the use of ME. It can be shown (Berger et al., 1996) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their t</context>
</contexts>
<marker>Rosenfeld, 1996</marker>
<rawString>Ronald Rosenfeld. 1996. A maximum entropy approach to adaptive statistical language modelling. Computer Speech and Language, 10:187-228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Simard</author>
<author>George F Foster</author>
<author>Pierre Isabelle</author>
</authors>
<title>Using cognates to align sentences in bilingual corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 4th Conference on Theoretical and Methodological Issues in Machine Translation (TMI),</booktitle>
<location>Montréal, Québec.</location>
<marker>Simard, Foster, Isabelle, 1992</marker>
<rawString>Michel Simard, George F. Foster, and Pierre Isabelle. 1992. Using cognates to align sentences in bilingual corpora. In Proceedings of the 4th Conference on Theoretical and Methodological Issues in Machine Translation (TMI), Montréal, Québec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ye-yi Wang</author>
<author>Alex Waibel</author>
</authors>
<title>Fast decoding for statistical machine translation.</title>
<date>1998</date>
<booktitle>In ICSLP98 (ICS,</booktitle>
<pages>2775--2778</pages>
<contexts>
<context citStr="Wang and Waibel, 1998" endWordPosition="680" position="3945" startWordPosition="677">cant improvements with this technique have proven elusive (Langlais and Foster, 2000). The noisy channel model avoids this problem by making predictions based on h the responsibility of the language model p(t), and those based on s the responsibility of the translation model p(slt), and combining the two in an optimum way. But this comes at the cost of increased decoding complexity, because the chain rule can no longer be applied as in (1) due to the reversed direction of the translation model. Much recent research in SMT, eg (Garcia-Varea et al., 1998; Niessen et al., 1998; Och et al., 1999; Wang and Waibel, 1998) deals with the decoding problem, either directly or indirectly because of constraints imposed on the form of the translation model. A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence (MEMD) modeling (Berger et al., 1996). One of the main strengths of MEMD is that it allows information from different sources to be combined in a principled and effective way, so it is a natural choice for modeling p(wlh, s) In this paper, I describe a MEMD model for p(wlh, s) and compare its performance to that of an equivalent linear model. I also evaluate se</context>
</contexts>
<marker>Wang, Waibel, 1998</marker>
<rawString>Ye-yi Wang and Alex Waibel. 1998. Fast decoding for statistical machine translation. In ICSLP98 (ICS, 1998), pages 2775-2778.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>