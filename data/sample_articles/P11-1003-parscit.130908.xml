<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant confidence="0.000086" no="0">
<title confidence="0.998362">
Effective Use of Function Words for Rule Generalization
in Forest-Based Translation
</title>
<author confidence="0.999525">
Xianchao Wu† Takuya Matsuzaki† Jun’ichi Tsujii†$*
</author>
<affiliation confidence="0.993933666666667">
†Department of Computer Science, The University of Tokyo
$School of Computer Science, University of Manchester
*National Centre for Text Mining (NaCTeM)
</affiliation>
<email confidence="0.974056">
{wxc, matuzaki, tsujii}@is.s.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.98682" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.802560954545454">In the present paper, we propose the effective usage of function words to generate generalized translation rules for forest-based translation. Given aligned forest-string pairs, we extract composed tree-to-string translation rules that account for multiple interpretations of both aligned and unaligned target function words. In order to constrain the exhaustive attachments of function words, we limit to bind them to the nearby syntactic chunks yielded by a target dependency parser. Therefore, the proposed approach can not only capture source-tree-to-target-chunk correspondences but can also use forest structures that compactly encode an exponential number of parse trees to properly generate target function words during decoding. Extensive experiments involving large-scale English-toJapanese translation revealed a significant improvement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system.</bodyText>
<sectionHeader confidence="0.999125" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997306571428572">Rule generalization remains a key challenge for current syntax-based statistical machine translation (SMT) systems. On the one hand, there is a tendency to integrate richer syntactic information into a translation rule in order to better express the translation phenomena. Thus, flat phrases (Koehn et al., 2003), hierarchical phrases (Chiang, 2005), and syntactic tree fragments (Galley et al., 2006; Mi and Huang, 2008; Wu et al., 2010) are gradually used in SMT. On the other hand, the use of syntactic phrases continues due to the requirement for phrase coverage in most syntax-based systems. For example, 22 Mi et al. (2008) achieved a 3.1-point improvement in BLEU score (Papineni et al., 2002) by including bilingual syntactic phrases in their forest-based system. Compared with flat phrases, syntactic rules are good at capturing global reordering, which has been reported to be essential for translating between languages with substantial structural differences, such as English and Japanese, which is a subject-objectverb language (Xu et al., 2009). Forest-based translation frameworks, which make use of packed parse forests on the source and/or target language side(s), are an increasingly promising approach to syntax-based SMT, being both algorithmically appealing (Mi et al., 2008) and empirically successful (Mi and Huang, 2008; Liu et al., 2009). However, forest-based translation systems, and, in general, most linguistically syntax-based SMT systems (Galley et al., 2004; Galley et al., 2006; Liu et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Chiang, 2010), are built upon word aligned parallel sentences and thus share a critical dependence on word alignments. For example, even a single spurious word alignment can invalidate a large number of otherwise extractable rules, and unaligned words can result in an exponentially large set of extractable rules for the interpretation of these unaligned words (Galley et al., 2006). What makes word alignment so fragile? In order to investigate this problem, we manually analyzed the alignments of the first 100 parallel sentences in our English-Japanese training data (to be shown in Table 2). The alignments were generated by running GIZA++ (Och and Ney, 2003) and the grow-diag-final-and symmetrizing strategy (Koehn et al., 2007) on the training set. Of the 1,324 word alignment pairs, there were 309 error pairs, among which there were 237 target function words, which account for 76.7% of the error pairs1 .</bodyText>
<note confidence="0.961893">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 22–31,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.997085471698113">This indicates that the alignments of the function words are more easily to be mistaken than content words. Moreover, we found that most Japanese function words tend to align to a few English words such as ‘of’ and ‘the’, which may appear anywhere in an English sentence. Following these problematic alignments, we are forced to make use of relatively large English tree fragments to construct translation rules that tend to be ill-formed and less generalized. This is the motivation of the present approach of re-aligning the target function words to source tree fragments, so that the influence of incorrect alignments is reduced and the function words can be generated by tree fragments on the fly. However, the current dominant research only uses 1-best trees for syntactic realignment (Galley et al., 2006; May and Knight, 2007; Wang et al., 2010), which adversely affects the rule set quality due to parsing errors. Therefore, we realign target function words to a packed forest that compactly encodes exponentially many parses. Given aligned forest-string pairs, we extract composed tree-to-string translation rules that account for multiple interpretations of both aligned and unaligned target function words. In order to constrain the exhaustive attachments of function words, we further limit the function words to bind to their surrounding chunks yielded by a dependency parser. Using the composed rules of the present study in a baseline forest-to-string translation system results in a 1.8-point improvement in the BLEU score for large-scale English-to-Japanese translation.</bodyText>
<sectionHeader confidence="0.999125" genericHeader="method">
2 Backgrounds
</sectionHeader>
<subsectionHeader confidence="0.999125" genericHeader="introduction">
2.1 Japanese function words
</subsectionHeader>
<bodyText confidence="0.979786">In the present paper, we limit our discussion on Japanese particles and auxiliary verbs (Martin, 1975). Particles are suffixes or tokens in Japanese grammar that immediately follow modified content words or sentences. There are eight types of Japanese function words, which are classified depending on what function they serve: case markers, parallel markers, sentence ending particles, interjec-tory particles, adverbial particles, binding particles, conjunctive particles, and phrasal particles. Japanese grammar also uses auxiliary verbs to give further semantic or syntactic information about the preceding main or full verb. Alike English, the extra meaning provided by a Japanese auxiliary verb alters the basic meaning of the main verb so that the main verb has one or more of the following functions: passive voice, progressive aspect, perfect aspect, modality, dummy, or emphasis.</bodyText>
<subsectionHeader confidence="0.999125" genericHeader="introduction">
2.2 HPSG forests
</subsectionHeader>
<bodyText confidence="0.979786">Following our precious work (Wu et al., 2010), we use head-drive phrase structure grammar (HPSG) forests generated by Enju2 (Miyao and Tsujii, 2008), which is a state-of-the-art HPSG parser for English. HPSG (Pollard and Sag, 1994; Sag et al., 2003) is a lexicalist grammar framework. In HPSG, linguistic entities such as words and phrases are represented by a data structure called a sign. A sign gives a factored representation of the syntactic features of a word/phrase, as well as a representation of their semantic content. Phrases and words represented by signs are collected into larger phrases by the applications of schemata. The semantic representation of the new phrase is calculated at the same time. As such, an HPSG parse forest can be considered to be a forest of signs. Making use of these signs instead of part-of-speech (POS)/phrasal tags in PCFG results in a fine-grained rule set integrated with deep syntactic information. For example, an aligned HPSG forest3 -string pair is shown in Figure 1. For simplicity, we only draw the identifiers for the signs of the nodes in the HPSG forest. Note that the identifiers that start with ‘c’ denote non-terminal nodes (e.g., c0, c1), and the identifiers that start with ‘t’ denote terminal nodes (e.g., t3, t1). In a complete HPSG forest given in (Wu et al., 2010), the terminal signs include features such as the POS tag, the tense, the auxiliary, the voice of a verb, etc.. The non-terminal signs include features such as the phrasal category, the name of the schema applied in the node, etc..</bodyText>
<equation confidence="0.784517043478261">
t3 t1 t4 t8 t1o t7 to t6 t5 t2 t9
c9 c1o c16 c22 c4 c21 c12 c18 c19 c14 c15
c7
2-best tree
1-best tree 3-best tree
co
c6
c8
c2o
c23
c11
c1
c3
c2
c5 c17
c13
this result was verified by the experiments
実験 o によって 1 この 2 結果 3 が 4 検証 5 さ 6 れ 7 た 8
experiments by this result verified
jikken niyotte kono kekka go kensyou so re to
this result was verified by the experiments
実験 o によって 1 この 2 結果 3 が 4 検証 5 さ 6 れ 7 た 8
C1 C2 C3 C4
</equation>
<figureCaption confidence="0.996189">
Figure 1: Illustration of an aligned HPSG forest-string pair for English-to-Japanese translation. The chunk-level
dependency tree for the Japanese sentence is shown as well.
</figureCaption>
<equation confidence="0.839619263157895">
Realign target function words
co
c1
c2
c6
c2o
c11
c8
c23
c3
c5
c17
c7
c13
c215-7 1 5-8
c9 c16 c22
c1o3 13-4 c45-7 1 5-8 c125-7 I 5-8 c18 c19 c14 c15
t7
t3 t1 t4 t8 t1o to t6 t5 t2 t9
</equation>
<sectionHeader confidence="0.991631" genericHeader="method">
3 Composed Rule Extraction
</sectionHeader>
<bodyText confidence="0.9963415">In this section, we first describe an algorithm that attaches function words to a packed forest guided by target chunk information. That is, given a triple ⟨FS, T, A⟩, namely an aligned (A) source forest (Fs) to target sentence (T) pair, we 1) tailor the alignment A by removing the alignments for target function words, 2) seek attachable nodes in the source forest Fs for each function word, and 3) construct a derivation forest by topologically traversing Fs. Then, we identify minimal and composed rules from the derivation forest and estimate the probabilities of rules and scores of derivations using the expectation-maximization (EM) (Dempster et al., 1977) algorithm.</bodyText>
<subsectionHeader confidence="0.99408">
3.1 Definitions
</subsectionHeader>
<bodyText confidence="0.998886333333333">In the proposed algorithm, we make use of the following definitions, which are similar to those described in (Galley et al., 2004; Mi and Huang, 2008):</bodyText>
<listItem confidence="0.994702">• s(·): the span of a (source) node v or a (target) chunk C, which is an index set of the words that v or C covers; the existence of these rules prevents the generaliza• t(v): the corresponding span of v, which is an tion ability of the final rule set that is extracted.index set of aligned words on another side; In order to address this problem, we tailor the • c(v): the complement span of v, which is the alignment by ignoring these three alignment pairs in union of corresponding spans of nodes v′ that dot lines.</listItem>
<page confidence="0.954205">
24
</page>
<listItem confidence="0.8456376">For example, by ignoring the ambiguous share an identical parse tree with v but are neialignments on the Japanese function words, we enther antecedents nor descendants of v; large the frontier set to include from 12 to 19 of the • PA: the frontier set of FS, which contains 24 non-terminal nodes.</listItem>
<bodyText confidence="0.972894222222223">Consequently, the number nodes that are consistent with an alignment A of extractable minimal rules increases from 12 (with (gray nodes in Figure 1), i.e., t(v) ≠ ∅ and three reordering rules rooted at c0, c1, and c2) to closure(t(v)) n c(v) = ∅.19 (with five reordering rules rooted at c0, c1, c2, c5, and c17). With more nodes included in the frontier set, we can extract more minimal and composed monotonic/reordering rules and avoid extracting the less generalized rules with extremely large tree fragments.3.2.2 Why chunking? In the proposed algorithm, we use a target chunk set to constrain the attachment explosion problem because we use a packed parse forest instead of a 1best tree, as in the case of (Galley et al., 2006). Multiple interpretations of unaligned function words for an aligned tree-string pair result in a derivation forest. Now, we have a packed parse forest in which each tree corresponds to a derivation forest. Thus, pruning free attachments of function words is practically important in order to extract composed rules from this “(derivation) forest of (parse) forest”. In the English-to-Japanese translation test case of the present study, the target chunk set is yielded by a state-of-the-art Japanese dependency parser, Cabocha v0.535 (Kudo and Matsumoto, 2002). The output of Cabocha is a list of chunks. A chunk contains roughly one content word (usually the head) and affixed function words, such as case markers (e.g., ga) and verbal morphemes (e.g., sa re ta, which indicate past tense and passive voice). For example, the Japanese sentence in Figure 1 is separated into four chunks, and the dependencies among these chunks are identified by arrows. These arrows point out the head chunk that the current chunk modifies. Moreover, we also hope to gain a fine-grained alignment among these syntactic chunks and source tree fragments. Thereby, during decoding, we are binding the generation of function words with the generation of target chunks. The function closure covers the gap(s) that may appear in the interval parameter. For example, closure(t(c3)) = closure({0-1, 4-7}) = {0-7}. Examples of the applications of these functions can be found in Table 1. Following (Galley et al., 2006), we distinguish between minimal and composed rules. The composed rules are generated by combining a sequence of minimal rules.3.2 Free attachment of target function words 3.2.1 Motivation We explain the motivation for the present research using an example that was extracted from our training data, as shown in Figure 1. In the alignment of this example, three lines (in dot lines) are used to align was and the with ga (subject particle), and was with ta (past tense auxiliary verb). Under this alignment, we are forced to extract rules with relatively large tree fragments. For example, by applying the GHKM algorithm (Galley et al., 2004), a rule rooted at c0 will take c7, t4, c4, c19, t2, and c15 as the leaves. The final tree fragment, with a height of 7, contains 13 nodes. In order to ensure that this rule is used during decoding, we must generate subtrees with a height of 7 for c0. Suppose that the input forest is binarized and that |E |is the average number of hyperedges of each node, then we must generate O(|E|26−1) subtrees4 for c0 in the worst case. Thus, 4For one (binarized) hyperedge e of a node, suppose there are x subtrees in the left tail node and y subtrees in the right tail node. Then the number of subtrees guided by e is (x + 1) x (y + 1). Thus, the recursive formula is Nh = |E|(Nh−1 +1)2, where h is the height of the hypergraph and Nh is the number of subtrees. When h = 1, we let Nh = 0.5http://chasen.org/∼taku/software/cabocha/ 25 Algorithm 1 Aligning function words to the forest Input: HPSG forest FS, target sentence T, word alignment A = {(i, j)}, target function word set {fw} appeared in T, and target chunk set {C} Output: a derivation forest DF</bodyText>
<listItem confidence="0.973153352941176">1: A′ ← A \ {(i, s(fw))} ◃ fw E {fw} 2: for each node v E PA′ in topological order do 3: Tv ← 0 ◃ store the corresponding spans of v 4: for each function word fw E {fw} do 5: if fw E C and t(v)n(C) ≠ 0 and fw are not attached to descendants of v then 6: append t(v) U {s(fw)} to Tv 7: end if 8: end for 9: for each corresponding span t(v) E Tv do 10: R ← IDENTIFYMINRULES(v, t(v), T) ◃ range over the hyperedges of v, and discount the factional count of each rule r E R by 1/|Tv| 11: create a node n in DF for each rule r E R 12: create a shared parent node ⊕ when |R |&gt; 1 13: end for 14: end for</listItem>
<subsectionHeader confidence="0.908111">
3.2.3 The algorithm
</subsectionHeader>
<bodyText confidence="0.999709769230769">Algorithm 1 outlines the proposed approach to constructing a derivation forest to include multiple interpretations of target function words. The derivation forest is a hypergraph as previously used in (Galley et al., 2006), to maintain the constraint that one unaligned target word be attached to some node v exactly once in one derivation tree. Starting from a triple ⟨Fs, T, A⟩, we first tailor the alignment A to A′ by removing the alignments for target function words. Then, we traverse the nodes v E PA′ in topological order. During the traversal, a function word fw will be attached to v if 1) t(v) overlaps with the span of the chunk to which fw belongs, and 2) fw has not been attached to the descendants of v. We identify translation rules that take v as the root of their tree fragments. Each tree fragment is a frontier tree that takes a node in the frontier set P4′ of Fs as the root node and non-lexicalized frontier nodes or lexicalized non-frontier nodes as the leaves. Also, a minimal frontier tree used in a minimal rule is limited to be a frontier tree such that all nodes other than the root and leaves are non-frontier nodes. We use Algorithm 1 described in (Mi and Huang, 2008) to collect minimal frontier trees rooted at v in Fs. That is, we range over each hyperedges headed at v and continue to expand downward until the current set of hyperedges forms a minimal frontier tree.</bodyText>
<table confidence="0.999423461538462">
node s(·) A → (A′) consistent
t(·) c(·)
c0 0-6 0-8(0-3,5-7) ∅ 1
c1 0-6 0-8(0-3,5-7) ∅ 1
c2 0-6 0-8(0-3,5-7) ∅ 1
c3 3-6 0-1,4-7(0-1, 5-7) 2,8 0
c4 3 5-7 0,8(0-3) 1
c5* 4-6 0,4(0-1) 2-8(2-3,5-7) 0(1)
c6* 0-3 2-8(2-3,5-7) 0,4(0-1) 0(1)
c7 0-1 2-3 0-1,4-8(0-1,5-7) 1
c8* 2-3 4-8(5-7) 0-4(0-3) 0(1)
c9 0 2 0-1,3-8(0-1,3,5-7) 1
c10 1 3 0-2,4-8(0-2,5-7) 1
c11 2-6 0-1,4-8(0-1,5-7) 2-3 0
c12 3 5-7 0,8(0-3) 1
c13* 5-6 0,4(0) 1-8(1-3,5-7) 0(1)
c14 5 4(∅) 0-8(0-3,5-7) 0
c15 6 0 1-8(1-3,5-7) 1
c16 2 4,8(∅) 0-7(0-3,5-7) 0
c17* 4-6 0,4(0-1) 2-8(2-3,5-7) 0(1)
c18 4 1 0,2-8(0,2-3,5-7) 1
c19 4 1 0,2-8(0,2-3,5-7) 1
c20* 0-3 2-8(2-3,5-7) 0,4(0-1) 0(1)
c21 3 5-7 0,8(0-3) 1
c22 2 4,8(∅) 0-7(0-3,5-7) 0
c23* 2-3 4-8(5-7) 0-4(0-3) 0(1)
</table>
<tableCaption confidence="0.948315">
Table 1: Change of node attributes after alignment modi-
fication from A to A′ of the example in Figure 1. Nodes
with * superscripts are consistent with A′ but not consis-
tent with A.
</tableCaption>
<bodyText confidence="0.97769075">In the derivation forest, we use ® nodes to manage minimal/composed rules that share the same node and the same corresponding span. Figure 2 shows some minimal rule and ® nodes derived from the example in Figure 1. Even though we bind function words to their nearby chunks, these function words may still be attached to relative large tree fragments, so that richer syntactic information can be used to predict the function words. For example, in Figure 2, the tree fragments rooted at node c0−8 0 can predict ga and/or ta. The syntactic foundation behind is that, whether to use ga as a subject particle or to use wo as an object particle depends on both the left-hand-side noun phrase (kekka) and the right-hand-side verb (kensyou sa re ta). This type of node v′ (such as c0−8 0 ) should satisfy the following two heuristic conditions:</bodyText>
<listItem confidence="0.985351333333333">• v′ is included in the frontier set P4′ of Fs, and • t(v′) covers the function word, or v′ is the root node of Fs if the function word is the beginning or ending word in the target sentence T. Starting from this derivation forest with minimal</listItem>
<page confidence="0.951438">
26
</page>
<figure confidence="0.999219194029851">
3 C11
�� �� ga �� ta
��
t4fl:was
coa8
C16
de +
����� ����� ��
��
Xo X1
Xo Cq2 C103 X1
C3
C7-3
t32: the
-3 C11
�� �� ga ��
��
C9z
t4f):was
Coa-8
C16
kono
xo ga X1
Xo C7�3 085-7 X1
�
�
C45-8 C~­
X1 Xa
C(i2-7
C3
W: result
�
C1o3
Xo
t4fl:was
�
kekka
Xo X1
Xo p714 085-7 X1
�
Coo-8
-4 C11 X2 Xo x, ta
C16
����� ����� ��
��
C(i2-7
�
t13: result
C3
Xo X1
Xo Cq2 ������ ��
�
C1a3-4
C7-4
�
kekka ga
Xo
�
t4fl:was
4 C11
Coa-8
C16
����� ����� ��
��
t
C3
X2 Xo X1
</figure>
<figureCaption confidence="0.9826255">
Figure 2: Illustration of a (partial) derivation forest. Gray nodes include some unaligned target function word(s).
Nodes annotated by “*” include ga, and nodes annotated by “+” include ta.
</figureCaption>
<bodyText confidence="0.999791333333333">rules as nodes, we can further combine two or more minimal rules to form composed rules nodes and can append these nodes to the derivation forest.</bodyText>
<subsectionHeader confidence="0.998146">
3.3 Estimating rule probabilities
</subsectionHeader>
<bodyText confidence="0.999972538461538">We use the EM algorithm to jointly estimate 1) the translation probabilities and fractional counts of rules and 2) the scores of derivations in the derivation forests. As reported in (May and Knight, 2007), EM, as has been used in (Galley et al., 2006) to estimate rule probabilities in derivation forests, is an iterative procedure and prefers shorter derivations containing large rules over longer derivations containing small rules. In order to overcome this bias problem, we discount the fractional count of a rule by the product of the probabilities of parse hyperedges that are included in the tree fragment of the rule.</bodyText>
<sectionHeader confidence="0.999651" genericHeader="evaluation and result">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.943623">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.999962833333333">We implemented the forest-to-string decoder described in (Mi et al., 2008) that makes use of forestbased translation rules (Mi and Huang, 2008) as the baseline system for translating English HPSG forests into Japanese sentences. We analyzed the performance of the proposed translation rule sets by using the same decoder.</bodyText>
<table confidence="0.999376142857143">
Train Dev. Test
# sentence pairs 994K 2K 2K
# En 1-best trees 987,401 1,982 1,984
# En forests 984,731 1,979 1,983
# En words 24.7M 50.3K 49.9K
# Jp words 28.2M 57.4K 57.1K
# Jp function words 8.0M 16.1K 16.1K
</table>
<tableCaption confidence="0.9643205">
Table 2: Statistics of the JST corpus. Here, En = English
and Jp = Japanese.
</tableCaption>
<bodyText confidence="0.9899398125">The JST Japanese-English paper abstract corpus6 (Utiyama and Isahara, 2007), which consists of one million parallel sentences, was used for training, tuning, and testing. Table 2 shows the statistics of this corpus. Note that Japanese function words occupy more than a quarter of the Japanese words. Making use of Enju 2.3.1, we generated 987,401 1-best trees and 984,731 parse forests for the English sentences in the training set, with successful parse rates of 99.3% and 99.1%, respectively. Using the pruning criteria expressed in (Mi and Huang, 2008), we continue to prune a parse forest by setting pe to be 8, 5, and 2, until there are no more than eio = 22, 026 trees in a forest. After pruning, there are an average of 82.3 trees in a parse forest.</bodyText>
<footnote confidence="0.969639">
6http://www.jst.go.jp
</footnote>
<page confidence="0.992882">
27
</page>
<table confidence="0.999289857142857">
C3-T M&amp;H-F Min-F C3-F
free fw Y N Y Y
alignment A′ A A′ A′
English side tree forest forest forest
# rule 86.30 96.52 144.91 228.59
# reorder rule 58.50 91.36 92.98 162.71
# tree types 21.62 93.55 72.98 120.08
# nodes/tree 14.2 42.1 26.3 18.6
extract time 30.2 52.2 58.6 130.7
EM time 9.4 - 11.2 29.0
# rules in dev. 0.77 1.22 1.37 2.18
# rules in test 0.77 1.23 1.37 2.15
DT(sec./sent.) 2.8 15.7 22.4 35.4
BLEU (%) 26.15 27.07 27.93 28.89
</table>
<tableCaption confidence="0.990624">
Table 3: Statistics and translation results for four types of
tree-to-string rules.</tableCaption>
<figureCaption confidence="0.4198108">With the exception of ‘# nodes/tree’,
the numbers in the table are in millions and the time is in
hours. Here, fw denotes function word, and DT denotes
the decoding time, and the BLEU scores were computed
on the test set.</figureCaption>
<bodyText confidence="0.999926470588235">We performed GIZA++ (Och and Ney, 2003) and the grow-diag-final-and symmetrizing strategy (Koehn et al., 2007) on the training set to obtain alignments. The SRI Language Modeling Toolkit (Stolcke, 2002) was employed to train a five-gram Japanese LM on the training set. We evaluated the translation quality using the BLEU-4 metric (Papineni et al., 2002). Joshua v1.3 (Li et al., 2009), which is a freely available decoder for hierarchical phrasebased SMT (Chiang, 2005), is used as an external baseline system for comparison. We extracted 4.5M translation rules from the training set for the 4K English sentences in the development and test sets. We used the default configuration of Joshua, with the exception of the maximum number of items/rules, and the value of k (of the k-best outputs) is set to be 200.</bodyText>
<subsectionHeader confidence="0.748474">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.791748888888889">Table 3 lists the statistics of the following translation rule sets: • C3-T: a composed rule set extracted from the derivation forests of 1-best HPSG trees that were constructed using the approach described in (Galley et al., 2006). The maximum number of internal nodes is set to be three when generating a composed rule. We free attach target function words to derivation forests;</bodyText>
<figureCaption confidence="0.99364375">
Figure 3: Distributions of the number of tree nodes in the
translation rule sets. Note that the curves of Min-F and
C3-F are duplicated when the number of tree nodes being
larger than 9.
</figureCaption>
<listItem confidence="0.956071642857143">• M&amp;H-F: a minimal rule set extracted from HPSG forests using the extracting algorithm of (Mi and Huang, 2008). Here, we make use of the original alignments. We use the two heuristic conditions described in Section 3.2.3 to attach unaligned words to some node(s) in the forest; • Min-F: a minimal rule set extracted from the derivation forests of HPSG forests that were constructed using Algorithm 1 (Section 3). • C3-F: a composed rule set extracted from the derivation forests of HPSG forests. Similar to C3-T, the maximum number of internal nodes during combination is three.</listItem>
<bodyText confidence="0.823424">We investigate the generalization ability of these rule sets through the following aspects:</bodyText>
<listItem confidence="0.993901571428571">1. the number of rules, the number of reordering rules, and the distributions of the number of tree nodes (Figure 3), i.e., more rules with relatively small tree fragments are preferred; 2. the number of rules that are applicable to the development and test sets (Table 3); and 3. the final translation accuracies.</listItem>
<bodyText confidence="0.9727324">Table 3 and Figure 3 reflect that the generalization abilities of these four rule sets increase in the order of C3-T &lt; M&amp;H-F &lt; Min-F &lt; C3-F. The advantage of using a packed forest for re-alignment is verified by comparing the statistics of the rules and the final BLEU scores of C3-T with Min-F and C3F.</bodyText>
<figure confidence="0.981605">
25
M&amp;H-F
Min-F
C3-T
C3-F
10
5
0
2 12 22 32 42 52 62 72 82 92
a of tWee nodes in Wile
a of Hiles M
20
15
</figure>
<page confidence="0.937986">
28
</page>
<figureCaption confidence="0.99743">
Figure 4: Comparison of decoding time and the number
of rules used for translating the test set.
</figureCaption>
<bodyText confidence="0.999927782608696">Using the composed rule set C3-F in our forestbased decoder, we achieved an optimal BLEU score of 28.89 (%). Taking M&amp;H-F as the baseline translation rule set, we achieved a significant improvement (p &lt; 0.01) of 1.81 points. In terms of decoding time, even though we used Algorithm 3 described in (Huang and Chiang, 2005), which lazily generated the N-best translation candidates, the decoding time tended to be increased because more rules were available during cubepruning. Figure 4 shows a comparison of decoding time (seconds per sentence) and the number of rules used for translating the test set. Easy to observe that, decoding time increases in a nearly linear way following the increase of the number of rules used during decoding. Finally, compared with Joshua, which achieved a BLEU score of 24.79 (%) on the test set with a decoding speed of 8.8 seconds per sentence, our forest-based decoder achieved a significantly better (p &lt; 0.01) BLEU score by using either of the four types of translation rules.</bodyText>
<sectionHeader confidence="0.999675" genericHeader="related work">
5 Related Research
</sectionHeader>
<bodyText confidence="0.999965454545455">Galley et al. (2006) first used derivation forests of aligned tree-string pairs to express multiple interpretations of unaligned target words. The EM algorithm was used to jointly estimate 1) the translation probabilities and fractional counts of rules and 2) the scores of derivations in the derivation forests. By dealing with the ambiguous word alignment instead of unaligned target words, syntaxbased re-alignment models were proposed by (May and Knight, 2007; Wang et al., 2010) for tree-based translations. Free attachment of the unaligned target word problem was ignored in (Mi and Huang, 2008), which was the first study on extracting tree-to-string rules from aligned forest-string pairs. This inspired the idea to re-align a packed forest and a target sentence. Specially, we observed that most incorrect or ambiguous word alignments are caused by function words rather than content words. Thus, we focus on the realignment of target function words to source tree fragments and use a dependency parser to limit the attachments of unaligned target words.</bodyText>
<sectionHeader confidence="0.999561" genericHeader="conclusion">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9966469375">We have proposed an effective use of target function words for extracting generalized transducer rules for forest-based translation. We extend the unaligned word approach described in (Galley et al., 2006) from the 1-best tree to the packed parse forest. A simple yet effective modification is that, during rule extraction, we account for multiple interpretations of both aligned and unaligned target function words. That is, we chose to loose the ambiguous alignments for all of the target function words. The consideration behind is in order to generate target function words in a robust manner. In order to avoid generating too large a derivation forest for a packed forest, we further used chunk-level information yielded by a target dependency parser. Extensive experiments on large-scale English-to-Japanese translation resulted in a significant improvement in BLEU score of 1.8 points (p &lt; 0.01), as compared with our implementation of a strong forest-to-string baseline system (Mi et al., 2008; Mi and Huang, 2008). The present work only re-aligns target function words to source tree fragments. It will be valuable to investigate the feasibility to re-align all the target words to source tree fragments. Also, it is interesting to automatically learn a word set for realigning'. Given source parse forests and a target word set for re-aligning beforehand, we argue our approach is generic and applicable to any language pairs. Finally, we intend to extend the proposed approach to tree-to-tree translation frameworks by This idea comes from one reviewer, we express our thankfulness here.</bodyText>
<figure confidence="0.998993352941176">
C3-T M&amp;H-F Min-F C3-F
# of rules (M)
0.0
2.0
0.5
2.5
1.0
1.5
# rules (M)
DT
50
40
30
20
10
0
Decoding time (sec./sent.)
</figure>
<page confidence="0.995987">
29
</page>
<bodyText confidence="0.9997832">re-aligning subtree pairs (Liu et al., 2009; Chiang, 2010) and consistency-to-dependency frameworks by re-aligning consistency-tree-to-dependency-tree pairs (Mi and Liu, 2010) in order to tackle the rulesparseness problem.</bodyText>
<sectionHeader confidence="0.995491" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999859545454545">The present study was supported in part by a Grantin-Aid for Specially Promoted Research (MEXT, Japan), by the Japanese/Chinese Machine Translation Project through Special Coordination Funds for Promoting Science and Technology (MEXT, Japan), and by Microsoft Research Asia Machine Translation Theme. Wu (wu.xianchao@lab.ntt.co.jp) has moved to NTT Communication Science Laboratories and Tsujii (junichi.tsujii@live.com) has moved to Microsoft Research Asia.</bodyText>
<sectionHeader confidence="0.99806" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999716759036144">
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL, pages 263–270, Ann Arbor, MI.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1443–1452, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the em
algorithm. Journal of the Royal Statistical Society,
39:1–38.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In Pro-
ceedings of HLT-NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING-ACL, pages 961–968, Sydney.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology and North
American Association for Computational Linguistics
Conference (HLT/NAACL), Edomonton, Canada, May
27-June 1.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the ACL 2007 Demo and Poster Sessions, pages
177–180.
Taku Kudo and Yuji Matsumoto. 2002. Japanese depen-
dency analysis using cascaded chunking. In Proceed-
ings of CoNLL-2002, pages 63–69. Taipei, Taiwan.
Zhifei Li, Chris Callison-Burch, Chris Dyery, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
N. G. Thornton, Jonathan Weese, and Omar F. Zaidan.
2009. Demonstration of joshua: An open source
toolkit for parsing-based machine translation. In Pro-
ceedings of the ACL-IJCNLP 2009 Software Demon-
strations, pages 25–28, August.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
transaltion. In Proceedings of COLING-ACL, pages
609–616, Sydney, Australia.
Yang Liu, Yajuan L¨u, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of ACL-IJCNLP, pages 558–566, August.
Samuel E. Martin. 1975. A Reference Grammar of
Japanese. New Haven, Conn.: Yale University Press.
Jonathan May and Kevin Knight. 2007. Syntactic re-
alignment models for machine translation. In Pro-
ceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 360–368, Prague, Czech Republic,
June. Association for Computational Linguistics.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of EMNLP, pages
206–214, October.
Haitao Mi and Qun Liu. 2010. Constituency to depen-
dency translation with forests. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1433–1442, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08:HLT,
pages 192–199, Columbus, Ohio.
Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature forest
models for probabilistic hpsg parsing. Computational
Lingustics, 34(1):35–80.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311–318.
</reference>
<page confidence="0.975397">
30
</page>
<reference confidence="0.9998053125">
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Ivan A. Sag, Thomas Wasow, and Emily M. Bender.
2003. Syntactic Theory: A Formal Introduction.
Number 152 in CSLI Lecture Notes. CSLI Publica-
tions.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proceedings of International
Conference on Spoken Language Processing, pages
901–904.
Masao Utiyama and Hitoshi Isahara. 2007. A japanese-
english patent parallel corpus. In Proceedings of MT
Summit XI, pages 475–482, Copenhagen.
Wei Wang, Jonathan May, Kevin Knight, and Daniel
Marcu. 2010. Re-structuring, re-labeling, and re-
aligning for syntax-based machine translation. Com-
putational Linguistics, 36(2):247–277.
Xianchao Wu, Takuya Matsuzaki, and Jun’ichi Tsujii.
2010. Fine-grained tree-to-string translation rule ex-
traction. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 325–334, Uppsala, Sweden, July. Association
for Computational Linguistics.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
smt for subject-object-verb languages. In Proceedings
of HLT-NAACL, pages 245–253.
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng Li,
and Chew Lim Tan. 2007. A tree-to-tree alignment-
based model for statistical machine translation. In
Proceedings of MT Summit XI, pages 535–542, Copen-
hagen, Denmark, September.
</reference>
<page confidence="0.999913">
31
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant confidence="0.141636" no="0">
<title confidence="0.9068525">Effective Use of Function Words for Rule in Forest-Based Translation</title>
<affiliation confidence="0.528845">of Computer Science, The University of of Computer Science, University of Centre for Text Mining</affiliation>
<email confidence="0.849332">matuzaki,</email>
<abstract confidence="0.998128826086957">In the present paper, we propose the effective usage of function words to generate generalized translation rules for forest-based translation. Given aligned forest-string pairs, we extract composed tree-to-string translation rules that account for multiple interpretations of both aligned and unaligned target function words. In order to constrain the exhaustive attachments of function words, we limit to bind them to the nearby syntactic chunks yielded by a target dependency parser. Therefore, the proposed approach can not only capture source-tree-to-target-chunk correspondences but can also use forest structures that compactly encode an exponential number of parse trees to properly generate target function words during decoding. Extensive experiments involving large-scale English-to- Japanese translation revealed a significant improvement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context citStr="Chiang, 2005" endWordPosition="229" position="1640" startWordPosition="228">y generate target function words during decoding. Extensive experiments involving large-scale English-toJapanese translation revealed a significant improvement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system. 1 Introduction Rule generalization remains a key challenge for current syntax-based statistical machine translation (SMT) systems. On the one hand, there is a tendency to integrate richer syntactic information into a translation rule in order to better express the translation phenomena. Thus, flat phrases (Koehn et al., 2003), hierarchical phrases (Chiang, 2005), and syntactic tree fragments (Galley et al., 2006; Mi and Huang, 2008; Wu et al., 2010) are gradually used in SMT. On the other hand, the use of syntactic phrases continues due to the requirement for phrase coverage in most syntax-based systems. For example, 22 Mi et al. (2008) achieved a 3.1-point improvement in BLEU score (Papineni et al., 2002) by including bilingual syntactic phrases in their forest-based system. Compared with flat phrases, syntactic rules are good at capturing global reordering, which has been reported to be essential for translating between languages with substantial s</context>
<context citStr="Chiang, 2005" endWordPosition="3989" position="23103" startWordPosition="3988">and the time is in hours. Here, fw denotes function word, and DT denotes the decoding time, and the BLEU scores were computed on the test set. We performed GIZA++ (Och and Ney, 2003) and the grow-diag-final-and symmetrizing strategy (Koehn et al., 2007) on the training set to obtain alignments. The SRI Language Modeling Toolkit (Stolcke, 2002) was employed to train a five-gram Japanese LM on the training set. We evaluated the translation quality using the BLEU-4 metric (Papineni et al., 2002). Joshua v1.3 (Li et al., 2009), which is a freely available decoder for hierarchical phrasebased SMT (Chiang, 2005), is used as an external baseline system for comparison. We extracted 4.5M translation rules from the training set for the 4K English sentences in the development and test sets. We used the default configuration of Joshua, with the exception of the maximum number of items/rules, and the value of k (of the k-best outputs) is set to be 200. 4.2 Results Table 3 lists the statistics of the following translation rule sets: • C3-T: a composed rule set extracted from the derivation forests of 1-best HPSG trees that were constructed using the approach described in (Galley et al., 2006). The maximum nu</context>
<context citStr="Chiang, 2005" endWordPosition="4480" position="25842" startWordPosition="4479">ing the statistics of the rules and 25 M&amp;H-F Min-F C3-T C3-F 10 5 0 2 12 22 32 42 52 62 72 82 92 a of tWee nodes in Wile a of Hiles M 20 15 28 Figure 4: Comparison of decoding time and the number of rules used for translating the test set. the final BLEU scores of C3-T with Min-F and C3- F. Using the composed rule set C3-F in our forestbased decoder, we achieved an optimal BLEU score of 28.89 (%). Taking M&amp;H-F as the baseline translation rule set, we achieved a significant improvement (p &lt; 0.01) of 1.81 points. In terms of decoding time, even though we used Algorithm 3 described in (Huang and Chiang, 2005), which lazily generated the N-best translation candidates, the decoding time tended to be increased because more rules were available during cubepruning. Figure 4 shows a comparison of decoding time (seconds per sentence) and the number of rules used for translating the test set. Easy to observe that, decoding time increases in a nearly linear way following the increase of the number of rules used during decoding. Finally, compared with Joshua, which achieved a BLEU score of 24.79 (%) on the test set with a decoding speed of 8.8 seconds per sentence, our forest-based decoder achieved a signif</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL, pages 263–270, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Learning to translate with source and target syntax.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1443--1452</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context citStr="Chiang, 2010" endWordPosition="429" position="2889" startWordPosition="428">sh and Japanese, which is a subject-objectverb language (Xu et al., 2009). Forest-based translation frameworks, which make use of packed parse forests on the source and/or target language side(s), are an increasingly promising approach to syntax-based SMT, being both algorithmically appealing (Mi et al., 2008) and empirically successful (Mi and Huang, 2008; Liu et al., 2009). However, forest-based translation systems, and, in general, most linguistically syntax-based SMT systems (Galley et al., 2004; Galley et al., 2006; Liu et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Chiang, 2010), are built upon word aligned parallel sentences and thus share a critical dependence on word alignments. For example, even a single spurious word alignment can invalidate a large number of otherwise extractable rules, and unaligned words can result in an exponentially large set of extractable rules for the interpretation of these unaligned words (Galley et al., 2006). What makes word alignment so fragile? In order to investigate this problem, we manually analyzed the alignments of the first 100 parallel sentences in our English-Japanese training data (to be shown in Table 2). The alignments w</context>
<context citStr="Chiang, 2010" endWordPosition="5063" position="29411" startWordPosition="5062">all the target words to source tree fragments. Also, it is interesting to automatically learn a word set for realigning'. Given source parse forests and a target word set for re-aligning beforehand, we argue our approach is generic and applicable to any language pairs. Finally, we intend to extend the proposed approach to tree-to-tree translation frameworks by This idea comes from one reviewer, we express our thankfulness here. C3-T M&amp;H-F Min-F C3-F # of rules (M) 0.0 2.0 0.5 2.5 1.0 1.5 # rules (M) DT 50 40 30 20 10 0 Decoding time (sec./sent.) 29 re-aligning subtree pairs (Liu et al., 2009; Chiang, 2010) and consistency-to-dependency frameworks by re-aligning consistency-tree-to-dependency-tree pairs (Mi and Liu, 2010) in order to tackle the rulesparseness problem. Acknowledgments The present study was supported in part by a Grantin-Aid for Specially Promoted Research (MEXT, Japan), by the Japanese/Chinese Machine Translation Project through Special Coordination Funds for Promoting Science and Technology (MEXT, Japan), and by Microsoft Research Asia Machine Translation Theme. Wu (wu.xianchao@lab.ntt.co.jp) has moved to NTT Communication Science Laboratories and Tsujii (junichi.tsujii@live.com</context>
</contexts>
<marker>Chiang, 2010</marker>
<rawString>David Chiang. 2010. Learning to translate with source and target syntax. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1443–1452, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the em algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<pages>39--1</pages>
<contexts>
<context citStr="Dempster et al., 1977" endWordPosition="1629" position="10092" startWordPosition="1626">rithm that attaches function words to a packed forest guided by target chunk information. That is, given a triple ⟨FS, T, A⟩, namely an aligned (A) source forest (Fs) to target sentence (T) pair, we 1) tailor the alignment A by removing the alignments for target function words, 2) seek attachable nodes in the source forest Fs for each function word, and 3) construct a derivation forest by topologically traversing Fs. Then, we identify minimal and composed rules from the derivation forest and estimate the probabilities of rules and scores of derivations using the expectation-maximization (EM) (Dempster et al., 1977) algorithm. 3.1 Definitions In the proposed algorithm, we make use of the following definitions, which are similar to those described in (Galley et al., 2004; Mi and Huang, 2008): • s(·): the span of a (source) node v or a (target) chunk C, which is an index set of the words that 24 v or C covers; the existence of these rules prevents the generaliza• t(v): the corresponding span of v, which is an tion ability of the final rule set that is extracted. index set of aligned words on another side; In order to address this problem, we tailor the • c(v): the complement span of v, which is the alignme</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society, 39:1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context citStr="Galley et al., 2004" endWordPosition="407" position="2780" startWordPosition="404">n reported to be essential for translating between languages with substantial structural differences, such as English and Japanese, which is a subject-objectverb language (Xu et al., 2009). Forest-based translation frameworks, which make use of packed parse forests on the source and/or target language side(s), are an increasingly promising approach to syntax-based SMT, being both algorithmically appealing (Mi et al., 2008) and empirically successful (Mi and Huang, 2008; Liu et al., 2009). However, forest-based translation systems, and, in general, most linguistically syntax-based SMT systems (Galley et al., 2004; Galley et al., 2006; Liu et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Chiang, 2010), are built upon word aligned parallel sentences and thus share a critical dependence on word alignments. For example, even a single spurious word alignment can invalidate a large number of otherwise extractable rules, and unaligned words can result in an exponentially large set of extractable rules for the interpretation of these unaligned words (Galley et al., 2006). What makes word alignment so fragile? In order to investigate this problem, we manually analyzed the alignments of the </context>
<context citStr="Galley et al., 2004" endWordPosition="1656" position="10249" startWordPosition="1653">t (Fs) to target sentence (T) pair, we 1) tailor the alignment A by removing the alignments for target function words, 2) seek attachable nodes in the source forest Fs for each function word, and 3) construct a derivation forest by topologically traversing Fs. Then, we identify minimal and composed rules from the derivation forest and estimate the probabilities of rules and scores of derivations using the expectation-maximization (EM) (Dempster et al., 1977) algorithm. 3.1 Definitions In the proposed algorithm, we make use of the following definitions, which are similar to those described in (Galley et al., 2004; Mi and Huang, 2008): • s(·): the span of a (source) node v or a (target) chunk C, which is an index set of the words that 24 v or C covers; the existence of these rules prevents the generaliza• t(v): the corresponding span of v, which is an tion ability of the final rule set that is extracted. index set of aligned words on another side; In order to address this problem, we tailor the • c(v): the complement span of v, which is the alignment by ignoring these three alignment pairs in union of corresponding spans of nodes v′ that dot lines. For example, by ignoring the ambiguous share an identi</context>
<context citStr="Galley et al., 2004" endWordPosition="2293" position="13969" startWordPosition="2290">n minimal and composed rules. The composed rules are generated by combining a sequence of minimal rules. 3.2 Free attachment of target function words 3.2.1 Motivation We explain the motivation for the present research using an example that was extracted from our training data, as shown in Figure 1. In the alignment of this example, three lines (in dot lines) are used to align was and the with ga (subject particle), and was with ta (past tense auxiliary verb). Under this alignment, we are forced to extract rules with relatively large tree fragments. For example, by applying the GHKM algorithm (Galley et al., 2004), a rule rooted at c0 will take c7, t4, c4, c19, t2, and c15 as the leaves. The final tree fragment, with a height of 7, contains 13 nodes. In order to ensure that this rule is used during decoding, we must generate subtrees with a height of 7 for c0. Suppose that the input forest is binarized and that |E |is the average number of hyperedges of each node, then we must generate O(|E|26−1) subtrees4 for c0 in the worst case. Thus, 4For one (binarized) hyperedge e of a node, suppose there are x subtrees in the left tail node and y subtrees in the right tail node. Then the number of subtrees guide</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>961--968</pages>
<location>Sydney.</location>
<contexts>
<context citStr="Galley et al., 2006" endWordPosition="238" position="1691" startWordPosition="235">ing. Extensive experiments involving large-scale English-toJapanese translation revealed a significant improvement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system. 1 Introduction Rule generalization remains a key challenge for current syntax-based statistical machine translation (SMT) systems. On the one hand, there is a tendency to integrate richer syntactic information into a translation rule in order to better express the translation phenomena. Thus, flat phrases (Koehn et al., 2003), hierarchical phrases (Chiang, 2005), and syntactic tree fragments (Galley et al., 2006; Mi and Huang, 2008; Wu et al., 2010) are gradually used in SMT. On the other hand, the use of syntactic phrases continues due to the requirement for phrase coverage in most syntax-based systems. For example, 22 Mi et al. (2008) achieved a 3.1-point improvement in BLEU score (Papineni et al., 2002) by including bilingual syntactic phrases in their forest-based system. Compared with flat phrases, syntactic rules are good at capturing global reordering, which has been reported to be essential for translating between languages with substantial structural differences, such as English and Japanese</context>
<context citStr="Galley et al., 2006" endWordPosition="487" position="3259" startWordPosition="484"> et al., 2009). However, forest-based translation systems, and, in general, most linguistically syntax-based SMT systems (Galley et al., 2004; Galley et al., 2006; Liu et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Chiang, 2010), are built upon word aligned parallel sentences and thus share a critical dependence on word alignments. For example, even a single spurious word alignment can invalidate a large number of otherwise extractable rules, and unaligned words can result in an exponentially large set of extractable rules for the interpretation of these unaligned words (Galley et al., 2006). What makes word alignment so fragile? In order to investigate this problem, we manually analyzed the alignments of the first 100 parallel sentences in our English-Japanese training data (to be shown in Table 2). The alignments were generated by running GIZA++ (Och and Ney, 2003) and the grow-diag-final-and symmetrizing strategy (Koehn et al., 2007) on the training set. Of the 1,324 word alignment pairs, there were 309 error pairs, among Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 22–31, Portland, Oregon, June 19-24, 2011. c�2011 Association </context>
<context citStr="Galley et al., 2006" endWordPosition="859" position="5610" startWordPosition="856">pproach of Following our precious work (Wu et al., 2010), we re-aligning the target function words to source tree use head-drive phrase structure grammar (HPSG) fragments, so that the influence of incorrect align- forests generated by Enju2 (Miyao and Tsujii, 2008), ments is reduced and the function words can be gen- which is a state-of-the-art HPSG parser for English. erated by tree fragments on the fly. However, the HPSG (Pollard and Sag, 1994; Sag et al., 2003) is a current dominant research only uses 1-best trees for lexicalist grammar framework. In HPSG, linguistic syntactic realignment (Galley et al., 2006; May and entities such as words and phrases are represented Knight, 2007; Wang et al., 2010), which adversely by a data structure called a sign. A sign gives a affects the rule set quality due to parsing errors. factored representation of the syntactic features of Therefore, we realign target function words to a a word/phrase, as well as a representation of their packed forest that compactly encodes exponentially semantic content. Phrases and words represented by many parses. Given aligned forest-string pairs, we signs are collected into larger phrases by the appliextract composed tree-to-str</context>
<context citStr="Galley et al., 2006" endWordPosition="1944" position="11830" startWordPosition="1941">2 (with (gray nodes in Figure 1), i.e., t(v) ≠ ∅ and three reordering rules rooted at c0, c1, and c2) to closure(t(v)) n c(v) = ∅. 19 (with five reordering rules rooted at c0, c1, c2, c5, and c17). With more nodes included in the frontier set, we can extract more minimal and composed monotonic/reordering rules and avoid extracting the less generalized rules with extremely large tree fragments. 3.2.2 Why chunking? In the proposed algorithm, we use a target chunk set to constrain the attachment explosion problem because we use a packed parse forest instead of a 1- best tree, as in the case of (Galley et al., 2006). Multiple interpretations of unaligned function words for an aligned tree-string pair result in a derivation forest. Now, we have a packed parse forest in which each tree corresponds to a derivation forest. Thus, pruning free attachments of function words is practically important in order to extract composed rules from this “(derivation) forest of (parse) forest”. In the English-to-Japanese translation test case of the present study, the target chunk set is yielded by a state-of-the-art Japanese dependency parser, Cabocha v0.535 (Kudo and Matsumoto, 2002). The output of Cabocha is a list of c</context>
<context citStr="Galley et al., 2006" endWordPosition="2184" position="13326" startWordPosition="2181"> four chunks, and the dependencies among these chunks are identified by arrows. These arrows point out the head chunk that the current chunk modifies. Moreover, we also hope to gain a fine-grained alignment among these syntactic chunks and source tree fragments. Thereby, during decoding, we are binding the generation of function words with the generation of target chunks. The function closure covers the gap(s) that may appear in the interval parameter. For example, closure(t(c3)) = closure({0-1, 4-7}) = {0-7}. Examples of the applications of these functions can be found in Table 1. Following (Galley et al., 2006), we distinguish between minimal and composed rules. The composed rules are generated by combining a sequence of minimal rules. 3.2 Free attachment of target function words 3.2.1 Motivation We explain the motivation for the present research using an example that was extracted from our training data, as shown in Figure 1. In the alignment of this example, three lines (in dot lines) are used to align was and the with ga (subject particle), and was with ta (past tense auxiliary verb). Under this alignment, we are forced to extract rules with relatively large tree fragments. For example, by applyi</context>
<context citStr="Galley et al., 2006" endWordPosition="2669" position="15858" startWordPosition="2666">tached to descendants of v then 6: append t(v) U {s(fw)} to Tv 7: end if 8: end for 9: for each corresponding span t(v) E Tv do 10: R ← IDENTIFYMINRULES(v, t(v), T) ◃ range over the hyperedges of v, and discount the factional count of each rule r E R by 1/|Tv| 11: create a node n in DF for each rule r E R 12: create a shared parent node ⊕ when |R |&gt; 1 13: end for 14: end for 3.2.3 The algorithm Algorithm 1 outlines the proposed approach to constructing a derivation forest to include multiple interpretations of target function words. The derivation forest is a hypergraph as previously used in (Galley et al., 2006), to maintain the constraint that one unaligned target word be attached to some node v exactly once in one derivation tree. Starting from a triple ⟨Fs, T, A⟩, we first tailor the alignment A to A′ by removing the alignments for target function words. Then, we traverse the nodes v E PA′ in topological order. During the traversal, a function word fw will be attached to v if 1) t(v) overlaps with the span of the chunk to which fw belongs, and 2) fw has not been attached to the descendants of v. We identify translation rules that take v as the root of their tree fragments. Each tree fragment is a </context>
<context citStr="Galley et al., 2006" endWordPosition="3467" position="20098" startWordPosition="3464">ure 2: Illustration of a (partial) derivation forest. Gray nodes include some unaligned target function word(s). Nodes annotated by “*” include ga, and nodes annotated by “+” include ta. rules as nodes, we can further combine two or more minimal rules to form composed rules nodes and can append these nodes to the derivation forest. 3.3 Estimating rule probabilities We use the EM algorithm to jointly estimate 1) the translation probabilities and fractional counts of rules and 2) the scores of derivations in the derivation forests. As reported in (May and Knight, 2007), EM, as has been used in (Galley et al., 2006) to estimate rule probabilities in derivation forests, is an iterative procedure and prefers shorter derivations containing large rules over longer derivations containing small rules. In order to overcome this bias problem, we discount the fractional count of a rule by the product of the probabilities of parse hyperedges that are included in the tree fragment of the rule. 4 Experiments 4.1 Setup We implemented the forest-to-string decoder described in (Mi et al., 2008) that makes use of forestbased translation rules (Mi and Huang, 2008) as the baseline system for translating English HPSG fores</context>
<context citStr="Galley et al., 2006" endWordPosition="4090" position="23687" startWordPosition="4087">hical phrasebased SMT (Chiang, 2005), is used as an external baseline system for comparison. We extracted 4.5M translation rules from the training set for the 4K English sentences in the development and test sets. We used the default configuration of Joshua, with the exception of the maximum number of items/rules, and the value of k (of the k-best outputs) is set to be 200. 4.2 Results Table 3 lists the statistics of the following translation rule sets: • C3-T: a composed rule set extracted from the derivation forests of 1-best HPSG trees that were constructed using the approach described in (Galley et al., 2006). The maximum number of internal nodes is set to be three when generating a composed rule. We free attach target function words to derivation forests; Figure 3: Distributions of the number of tree nodes in the translation rule sets. Note that the curves of Min-F and C3-F are duplicated when the number of tree nodes being larger than 9. • M&amp;H-F: a minimal rule set extracted from HPSG forests using the extracting algorithm of (Mi and Huang, 2008). Here, we make use of the original alignments. We use the two heuristic conditions described in Section 3.2.3 to attach unaligned words to some node(s)</context>
<context citStr="Galley et al. (2006)" endWordPosition="4604" position="26574" startWordPosition="4601">e rules were available during cubepruning. Figure 4 shows a comparison of decoding time (seconds per sentence) and the number of rules used for translating the test set. Easy to observe that, decoding time increases in a nearly linear way following the increase of the number of rules used during decoding. Finally, compared with Joshua, which achieved a BLEU score of 24.79 (%) on the test set with a decoding speed of 8.8 seconds per sentence, our forest-based decoder achieved a significantly better (p &lt; 0.01) BLEU score by using either of the four types of translation rules. 5 Related Research Galley et al. (2006) first used derivation forests of aligned tree-string pairs to express multiple interpretations of unaligned target words. The EM algorithm was used to jointly estimate 1) the translation probabilities and fractional counts of rules and 2) the scores of derivations in the derivation forests. By dealing with the ambiguous word alignment instead of unaligned target words, syntaxbased re-alignment models were proposed by (May and Knight, 2007; Wang et al., 2010) for tree-based translations. Free attachment of the unaligned target word problem was ignored in (Mi and Huang, 2008), which was the fir</context>
<context citStr="Galley et al., 2006" endWordPosition="4801" position="27836" startWordPosition="4798">s from aligned forest-string pairs. This inspired the idea to re-align a packed forest and a target sentence. Specially, we observed that most incorrect or ambiguous word alignments are caused by function words rather than content words. Thus, we focus on the realignment of target function words to source tree fragments and use a dependency parser to limit the attachments of unaligned target words. 6 Conclusion We have proposed an effective use of target function words for extracting generalized transducer rules for forest-based translation. We extend the unaligned word approach described in (Galley et al., 2006) from the 1-best tree to the packed parse forest. A simple yet effective modification is that, during rule extraction, we account for multiple interpretations of both aligned and unaligned target function words. That is, we chose to loose the ambiguous alignments for all of the target function words. The consideration behind is in order to generate target function words in a robust manner. In order to avoid generating too large a derivation forest for a packed forest, we further used chunk-level information yielded by a target dependency parser. Extensive experiments on large-scale English-to-</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of COLING-ACL, pages 961–968, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of IWPT.</booktitle>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proceedings of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference (HLT/NAACL),</booktitle>
<location>Edomonton, Canada,</location>
<contexts>
<context citStr="Koehn et al., 2003" endWordPosition="225" position="1603" startWordPosition="222">xponential number of parse trees to properly generate target function words during decoding. Extensive experiments involving large-scale English-toJapanese translation revealed a significant improvement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system. 1 Introduction Rule generalization remains a key challenge for current syntax-based statistical machine translation (SMT) systems. On the one hand, there is a tendency to integrate richer syntactic information into a translation rule in order to better express the translation phenomena. Thus, flat phrases (Koehn et al., 2003), hierarchical phrases (Chiang, 2005), and syntactic tree fragments (Galley et al., 2006; Mi and Huang, 2008; Wu et al., 2010) are gradually used in SMT. On the other hand, the use of syntactic phrases continues due to the requirement for phrase coverage in most syntax-based systems. For example, 22 Mi et al. (2008) achieved a 3.1-point improvement in BLEU score (Papineni et al., 2002) by including bilingual syntactic phrases in their forest-based system. Compared with flat phrases, syntactic rules are good at capturing global reordering, which has been reported to be essential for translating</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference (HLT/NAACL), Edomonton, Canada, May 27-June 1.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Hieu Hoang,</title>
<location>Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi,</location>
<marker>Koehn, </marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondˇrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL</booktitle>
<pages>177--180</pages>
<marker>Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese dependency analysis using cascaded chunking.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002,</booktitle>
<pages>63--69</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context citStr="Kudo and Matsumoto, 2002" endWordPosition="2030" position="12392" startWordPosition="2027">tead of a 1- best tree, as in the case of (Galley et al., 2006). Multiple interpretations of unaligned function words for an aligned tree-string pair result in a derivation forest. Now, we have a packed parse forest in which each tree corresponds to a derivation forest. Thus, pruning free attachments of function words is practically important in order to extract composed rules from this “(derivation) forest of (parse) forest”. In the English-to-Japanese translation test case of the present study, the target chunk set is yielded by a state-of-the-art Japanese dependency parser, Cabocha v0.535 (Kudo and Matsumoto, 2002). The output of Cabocha is a list of chunks. A chunk contains roughly one content word (usually the head) and affixed function words, such as case markers (e.g., ga) and verbal morphemes (e.g., sa re ta, which indicate past tense and passive voice). For example, the Japanese sentence in Figure 1 is separated into four chunks, and the dependencies among these chunks are identified by arrows. These arrows point out the head chunk that the current chunk modifies. Moreover, we also hope to gain a fine-grained alignment among these syntactic chunks and source tree fragments. Thereby, during decodin</context>
</contexts>
<marker>Kudo, Matsumoto, 2002</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2002. Japanese dependency analysis using cascaded chunking. In Proceedings of CoNLL-2002, pages 63–69. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Chris Callison-Burch</author>
<author>Chris Dyery</author>
<author>Juri Ganitkevitch</author>
<author>Sanjeev Khudanpur</author>
<author>Lane Schwartz</author>
<author>Wren N G Thornton</author>
<author>Jonathan Weese</author>
<author>Omar F Zaidan</author>
</authors>
<title>Demonstration of joshua: An open source toolkit for parsing-based machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Software Demonstrations,</booktitle>
<pages>25--28</pages>
<contexts>
<context citStr="Li et al., 2009" endWordPosition="3976" position="23018" startWordPosition="3973">g rules. With the exception of ‘# nodes/tree’, the numbers in the table are in millions and the time is in hours. Here, fw denotes function word, and DT denotes the decoding time, and the BLEU scores were computed on the test set. We performed GIZA++ (Och and Ney, 2003) and the grow-diag-final-and symmetrizing strategy (Koehn et al., 2007) on the training set to obtain alignments. The SRI Language Modeling Toolkit (Stolcke, 2002) was employed to train a five-gram Japanese LM on the training set. We evaluated the translation quality using the BLEU-4 metric (Papineni et al., 2002). Joshua v1.3 (Li et al., 2009), which is a freely available decoder for hierarchical phrasebased SMT (Chiang, 2005), is used as an external baseline system for comparison. We extracted 4.5M translation rules from the training set for the 4K English sentences in the development and test sets. We used the default configuration of Joshua, with the exception of the maximum number of items/rules, and the value of k (of the k-best outputs) is set to be 200. 4.2 Results Table 3 lists the statistics of the following translation rule sets: • C3-T: a composed rule set extracted from the derivation forests of 1-best HPSG trees that w</context>
</contexts>
<marker>Li, Callison-Burch, Dyery, Ganitkevitch, Khudanpur, Schwartz, Thornton, Weese, Zaidan, 2009</marker>
<rawString>Zhifei Li, Chris Callison-Burch, Chris Dyery, Juri Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren N. G. Thornton, Jonathan Weese, and Omar F. Zaidan. 2009. Demonstration of joshua: An open source toolkit for parsing-based machine translation. In Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 25–28, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Treeto-string alignment templates for statistical machine transaltion.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>609--616</pages>
<location>Sydney, Australia.</location>
<contexts>
<context citStr="Liu et al., 2006" endWordPosition="415" position="2819" startWordPosition="412"> between languages with substantial structural differences, such as English and Japanese, which is a subject-objectverb language (Xu et al., 2009). Forest-based translation frameworks, which make use of packed parse forests on the source and/or target language side(s), are an increasingly promising approach to syntax-based SMT, being both algorithmically appealing (Mi et al., 2008) and empirically successful (Mi and Huang, 2008; Liu et al., 2009). However, forest-based translation systems, and, in general, most linguistically syntax-based SMT systems (Galley et al., 2004; Galley et al., 2006; Liu et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Chiang, 2010), are built upon word aligned parallel sentences and thus share a critical dependence on word alignments. For example, even a single spurious word alignment can invalidate a large number of otherwise extractable rules, and unaligned words can result in an exponentially large set of extractable rules for the interpretation of these unaligned words (Galley et al., 2006). What makes word alignment so fragile? In order to investigate this problem, we manually analyzed the alignments of the first 100 parallel sentences in our Eng</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Treeto-string alignment templates for statistical machine transaltion. In Proceedings of COLING-ACL, pages 609–616, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yajuan L¨u</author>
<author>Qun Liu</author>
</authors>
<title>Improving tree-to-tree translation with packed forests.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP,</booktitle>
<pages>558--566</pages>
<marker>Liu, L¨u, Liu, 2009</marker>
<rawString>Yang Liu, Yajuan L¨u, and Qun Liu. 2009. Improving tree-to-tree translation with packed forests. In Proceedings of ACL-IJCNLP, pages 558–566, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel E Martin</author>
</authors>
<title>A Reference Grammar of Japanese.</title>
<date>1975</date>
<publisher>Yale University Press.</publisher>
<location>New Haven, Conn.:</location>
<marker>Martin, 1975</marker>
<rawString>Samuel E. Martin. 1975. A Reference Grammar of Japanese. New Haven, Conn.: Yale University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan May</author>
<author>Kevin Knight</author>
</authors>
<title>Syntactic realignment models for machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>360--368</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context citStr="May and Knight, 2007" endWordPosition="3457" position="20051" startWordPosition="3454">11 Coa-8 C16 ����� ����� �� �� t C3 X2 Xo X1 Figure 2: Illustration of a (partial) derivation forest. Gray nodes include some unaligned target function word(s). Nodes annotated by “*” include ga, and nodes annotated by “+” include ta. rules as nodes, we can further combine two or more minimal rules to form composed rules nodes and can append these nodes to the derivation forest. 3.3 Estimating rule probabilities We use the EM algorithm to jointly estimate 1) the translation probabilities and fractional counts of rules and 2) the scores of derivations in the derivation forests. As reported in (May and Knight, 2007), EM, as has been used in (Galley et al., 2006) to estimate rule probabilities in derivation forests, is an iterative procedure and prefers shorter derivations containing large rules over longer derivations containing small rules. In order to overcome this bias problem, we discount the fractional count of a rule by the product of the probabilities of parse hyperedges that are included in the tree fragment of the rule. 4 Experiments 4.1 Setup We implemented the forest-to-string decoder described in (Mi et al., 2008) that makes use of forestbased translation rules (Mi and Huang, 2008) as the bas</context>
<context citStr="May and Knight, 2007" endWordPosition="4674" position="27017" startWordPosition="4671">e, our forest-based decoder achieved a significantly better (p &lt; 0.01) BLEU score by using either of the four types of translation rules. 5 Related Research Galley et al. (2006) first used derivation forests of aligned tree-string pairs to express multiple interpretations of unaligned target words. The EM algorithm was used to jointly estimate 1) the translation probabilities and fractional counts of rules and 2) the scores of derivations in the derivation forests. By dealing with the ambiguous word alignment instead of unaligned target words, syntaxbased re-alignment models were proposed by (May and Knight, 2007; Wang et al., 2010) for tree-based translations. Free attachment of the unaligned target word problem was ignored in (Mi and Huang, 2008), which was the first study on extracting tree-to-string rules from aligned forest-string pairs. This inspired the idea to re-align a packed forest and a target sentence. Specially, we observed that most incorrect or ambiguous word alignments are caused by function words rather than content words. Thus, we focus on the realignment of target function words to source tree fragments and use a dependency parser to limit the attachments of unaligned target words.</context>
</contexts>
<marker>May, Knight, 2007</marker>
<rawString>Jonathan May and Kevin Knight. 2007. Syntactic realignment models for machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 360–368, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
</authors>
<title>Forest-based translation rule extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>206--214</pages>
<contexts>
<context citStr="Mi and Huang, 2008" endWordPosition="242" position="1711" startWordPosition="239">ments involving large-scale English-toJapanese translation revealed a significant improvement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system. 1 Introduction Rule generalization remains a key challenge for current syntax-based statistical machine translation (SMT) systems. On the one hand, there is a tendency to integrate richer syntactic information into a translation rule in order to better express the translation phenomena. Thus, flat phrases (Koehn et al., 2003), hierarchical phrases (Chiang, 2005), and syntactic tree fragments (Galley et al., 2006; Mi and Huang, 2008; Wu et al., 2010) are gradually used in SMT. On the other hand, the use of syntactic phrases continues due to the requirement for phrase coverage in most syntax-based systems. For example, 22 Mi et al. (2008) achieved a 3.1-point improvement in BLEU score (Papineni et al., 2002) by including bilingual syntactic phrases in their forest-based system. Compared with flat phrases, syntactic rules are good at capturing global reordering, which has been reported to be essential for translating between languages with substantial structural differences, such as English and Japanese, which is a subject</context>
<context citStr="Mi and Huang, 2008" endWordPosition="1660" position="10270" startWordPosition="1657">ence (T) pair, we 1) tailor the alignment A by removing the alignments for target function words, 2) seek attachable nodes in the source forest Fs for each function word, and 3) construct a derivation forest by topologically traversing Fs. Then, we identify minimal and composed rules from the derivation forest and estimate the probabilities of rules and scores of derivations using the expectation-maximization (EM) (Dempster et al., 1977) algorithm. 3.1 Definitions In the proposed algorithm, we make use of the following definitions, which are similar to those described in (Galley et al., 2004; Mi and Huang, 2008): • s(·): the span of a (source) node v or a (target) chunk C, which is an index set of the words that 24 v or C covers; the existence of these rules prevents the generaliza• t(v): the corresponding span of v, which is an tion ability of the final rule set that is extracted. index set of aligned words on another side; In order to address this problem, we tailor the • c(v): the complement span of v, which is the alignment by ignoring these three alignment pairs in union of corresponding spans of nodes v′ that dot lines. For example, by ignoring the ambiguous share an identical parse tree with v</context>
<context citStr="Mi and Huang, 2008" endWordPosition="2851" position="16834" startWordPosition="2848"> attached to v if 1) t(v) overlaps with the span of the chunk to which fw belongs, and 2) fw has not been attached to the descendants of v. We identify translation rules that take v as the root of their tree fragments. Each tree fragment is a frontier tree that takes a node in the frontier set P4′ of Fs as the root node and non-lexicalized frontier nodes or lexicalized non-frontier nodes as the leaves. Also, a minimal frontier tree used in a minimal rule is limited to be a frontier tree such that all nodes other than the root and leaves are non-frontier nodes. We use Algorithm 1 described in (Mi and Huang, 2008) to collect minimal frontier trees rooted at v in Fs. That is, we range over each hyperedges headed at v and continue to expand downward until the curnode s(·) A → (A′) consistent t(·) c(·) c0 0-6 0-8(0-3,5-7) ∅ 1 c1 0-6 0-8(0-3,5-7) ∅ 1 c2 0-6 0-8(0-3,5-7) ∅ 1 c3 3-6 0-1,4-7(0-1, 5-7) 2,8 0 c4 3 5-7 0,8(0-3) 1 c5* 4-6 0,4(0-1) 2-8(2-3,5-7) 0(1) c6* 0-3 2-8(2-3,5-7) 0,4(0-1) 0(1) c7 0-1 2-3 0-1,4-8(0-1,5-7) 1 c8* 2-3 4-8(5-7) 0-4(0-3) 0(1) c9 0 2 0-1,3-8(0-1,3,5-7) 1 c10 1 3 0-2,4-8(0-2,5-7) 1 c11 2-6 0-1,4-8(0-1,5-7) 2-3 0 c12 3 5-7 0,8(0-3) 1 c13* 5-6 0,4(0) 1-8(1-3,5-7) 0(1) c14 5 4(∅) 0-8(</context>
<context citStr="Mi and Huang, 2008" endWordPosition="3556" position="20640" startWordPosition="3553">rted in (May and Knight, 2007), EM, as has been used in (Galley et al., 2006) to estimate rule probabilities in derivation forests, is an iterative procedure and prefers shorter derivations containing large rules over longer derivations containing small rules. In order to overcome this bias problem, we discount the fractional count of a rule by the product of the probabilities of parse hyperedges that are included in the tree fragment of the rule. 4 Experiments 4.1 Setup We implemented the forest-to-string decoder described in (Mi et al., 2008) that makes use of forestbased translation rules (Mi and Huang, 2008) as the baseline system for translating English HPSG forests into Japanese sentences. We analyzed the performance of the proposed translation rule sets by Train Dev. Test # sentence pairs 994K 2K 2K # En 1-best trees 987,401 1,982 1,984 # En forests 984,731 1,979 1,983 # En words 24.7M 50.3K 49.9K # Jp words 28.2M 57.4K 57.1K # Jp function words 8.0M 16.1K 16.1K Table 2: Statistics of the JST corpus. Here, En = English and Jp = Japanese. using the same decoder. The JST Japanese-English paper abstract corpus6 (Utiyama and Isahara, 2007), which consists of one million parallel sentences, was use</context>
<context citStr="Mi and Huang, 2008" endWordPosition="4169" position="24135" startWordPosition="4166">le sets: • C3-T: a composed rule set extracted from the derivation forests of 1-best HPSG trees that were constructed using the approach described in (Galley et al., 2006). The maximum number of internal nodes is set to be three when generating a composed rule. We free attach target function words to derivation forests; Figure 3: Distributions of the number of tree nodes in the translation rule sets. Note that the curves of Min-F and C3-F are duplicated when the number of tree nodes being larger than 9. • M&amp;H-F: a minimal rule set extracted from HPSG forests using the extracting algorithm of (Mi and Huang, 2008). Here, we make use of the original alignments. We use the two heuristic conditions described in Section 3.2.3 to attach unaligned words to some node(s) in the forest; • Min-F: a minimal rule set extracted from the derivation forests of HPSG forests that were constructed using Algorithm 1 (Section 3). • C3-F: a composed rule set extracted from the derivation forests of HPSG forests. Similar to C3-T, the maximum number of internal nodes during combination is three. We investigate the generalization ability of these rule sets through the following aspects: 1. the number of rules, the number of r</context>
<context citStr="Mi and Huang, 2008" endWordPosition="4696" position="27155" startWordPosition="4693"> Related Research Galley et al. (2006) first used derivation forests of aligned tree-string pairs to express multiple interpretations of unaligned target words. The EM algorithm was used to jointly estimate 1) the translation probabilities and fractional counts of rules and 2) the scores of derivations in the derivation forests. By dealing with the ambiguous word alignment instead of unaligned target words, syntaxbased re-alignment models were proposed by (May and Knight, 2007; Wang et al., 2010) for tree-based translations. Free attachment of the unaligned target word problem was ignored in (Mi and Huang, 2008), which was the first study on extracting tree-to-string rules from aligned forest-string pairs. This inspired the idea to re-align a packed forest and a target sentence. Specially, we observed that most incorrect or ambiguous word alignments are caused by function words rather than content words. Thus, we focus on the realignment of target function words to source tree fragments and use a dependency parser to limit the attachments of unaligned target words. 6 Conclusion We have proposed an effective use of target function words for extracting generalized transducer rules for forest-based tran</context>
<context citStr="Mi and Huang, 2008" endWordPosition="4932" position="28653" startWordPosition="4929">nction words. That is, we chose to loose the ambiguous alignments for all of the target function words. The consideration behind is in order to generate target function words in a robust manner. In order to avoid generating too large a derivation forest for a packed forest, we further used chunk-level information yielded by a target dependency parser. Extensive experiments on large-scale English-to-Japanese translation resulted in a significant improvement in BLEU score of 1.8 points (p &lt; 0.01), as compared with our implementation of a strong forest-to-string baseline system (Mi et al., 2008; Mi and Huang, 2008). The present work only re-aligns target function words to source tree fragments. It will be valuable to investigate the feasibility to re-align all the target words to source tree fragments. Also, it is interesting to automatically learn a word set for realigning'. Given source parse forests and a target word set for re-aligning beforehand, we argue our approach is generic and applicable to any language pairs. Finally, we intend to extend the proposed approach to tree-to-tree translation frameworks by This idea comes from one reviewer, we express our thankfulness here. C3-T M&amp;H-F Min-F C3-F #</context>
</contexts>
<marker>Mi, Huang, 2008</marker>
<rawString>Haitao Mi and Liang Huang. 2008. Forest-based translation rule extraction. In Proceedings of EMNLP, pages 206–214, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Qun Liu</author>
</authors>
<title>Constituency to dependency translation with forests.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1433--1442</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<marker>Mi, Liu, 2010</marker>
<rawString>Haitao Mi and Qun Liu. 2010. Constituency to dependency translation with forests. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1433–1442, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08:HLT,</booktitle>
<pages>192--199</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context citStr="Mi et al. (2008)" endWordPosition="280" position="1920" startWordPosition="277">zation remains a key challenge for current syntax-based statistical machine translation (SMT) systems. On the one hand, there is a tendency to integrate richer syntactic information into a translation rule in order to better express the translation phenomena. Thus, flat phrases (Koehn et al., 2003), hierarchical phrases (Chiang, 2005), and syntactic tree fragments (Galley et al., 2006; Mi and Huang, 2008; Wu et al., 2010) are gradually used in SMT. On the other hand, the use of syntactic phrases continues due to the requirement for phrase coverage in most syntax-based systems. For example, 22 Mi et al. (2008) achieved a 3.1-point improvement in BLEU score (Papineni et al., 2002) by including bilingual syntactic phrases in their forest-based system. Compared with flat phrases, syntactic rules are good at capturing global reordering, which has been reported to be essential for translating between languages with substantial structural differences, such as English and Japanese, which is a subject-objectverb language (Xu et al., 2009). Forest-based translation frameworks, which make use of packed parse forests on the source and/or target language side(s), are an increasingly promising approach to synta</context>
<context citStr="Mi et al., 2008" endWordPosition="3544" position="20571" startWordPosition="3541">nd 2) the scores of derivations in the derivation forests. As reported in (May and Knight, 2007), EM, as has been used in (Galley et al., 2006) to estimate rule probabilities in derivation forests, is an iterative procedure and prefers shorter derivations containing large rules over longer derivations containing small rules. In order to overcome this bias problem, we discount the fractional count of a rule by the product of the probabilities of parse hyperedges that are included in the tree fragment of the rule. 4 Experiments 4.1 Setup We implemented the forest-to-string decoder described in (Mi et al., 2008) that makes use of forestbased translation rules (Mi and Huang, 2008) as the baseline system for translating English HPSG forests into Japanese sentences. We analyzed the performance of the proposed translation rule sets by Train Dev. Test # sentence pairs 994K 2K 2K # En 1-best trees 987,401 1,982 1,984 # En forests 984,731 1,979 1,983 # En words 24.7M 50.3K 49.9K # Jp words 28.2M 57.4K 57.1K # Jp function words 8.0M 16.1K 16.1K Table 2: Statistics of the JST corpus. Here, En = English and Jp = Japanese. using the same decoder. The JST Japanese-English paper abstract corpus6 (Utiyama and Isah</context>
<context citStr="Mi et al., 2008" endWordPosition="4928" position="28632" startWordPosition="4925">aligned target function words. That is, we chose to loose the ambiguous alignments for all of the target function words. The consideration behind is in order to generate target function words in a robust manner. In order to avoid generating too large a derivation forest for a packed forest, we further used chunk-level information yielded by a target dependency parser. Extensive experiments on large-scale English-to-Japanese translation resulted in a significant improvement in BLEU score of 1.8 points (p &lt; 0.01), as compared with our implementation of a strong forest-to-string baseline system (Mi et al., 2008; Mi and Huang, 2008). The present work only re-aligns target function words to source tree fragments. It will be valuable to investigate the feasibility to re-align all the target words to source tree fragments. Also, it is interesting to automatically learn a word set for realigning'. Given source parse forests and a target word set for re-aligning beforehand, we argue our approach is generic and applicable to any language pairs. Finally, we intend to extend the proposed approach to tree-to-tree translation frameworks by This idea comes from one reviewer, we express our thankfulness here. C3</context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In Proceedings of ACL-08:HLT, pages 192–199, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Feature forest models for probabilistic hpsg parsing.</title>
<date>2008</date>
<journal>Computational Lingustics,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context citStr="Miyao and Tsujii, 2008" endWordPosition="801" position="5256" startWordPosition="798">matic alignments, main verb has one or more of the following funcwe are forced to make use of relatively large English tions: passive voice, progressive aspect, perfect astree fragments to construct translation rules that tend pect, modality, dummy, or emphasis. to be ill-formed and less generalized. 2.2 HPSG forests This is the motivation of the present approach of Following our precious work (Wu et al., 2010), we re-aligning the target function words to source tree use head-drive phrase structure grammar (HPSG) fragments, so that the influence of incorrect align- forests generated by Enju2 (Miyao and Tsujii, 2008), ments is reduced and the function words can be gen- which is a state-of-the-art HPSG parser for English. erated by tree fragments on the fly. However, the HPSG (Pollard and Sag, 1994; Sag et al., 2003) is a current dominant research only uses 1-best trees for lexicalist grammar framework. In HPSG, linguistic syntactic realignment (Galley et al., 2006; May and entities such as words and phrases are represented Knight, 2007; Wang et al., 2010), which adversely by a data structure called a sign. A sign gives a affects the rule set quality due to parsing errors. factored representation of the sy</context>
</contexts>
<marker>Miyao, Tsujii, 2008</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature forest models for probabilistic hpsg parsing. Computational Lingustics, 34(1):35–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context citStr="Och and Ney, 2003" endWordPosition="535" position="3540" startWordPosition="532">allel sentences and thus share a critical dependence on word alignments. For example, even a single spurious word alignment can invalidate a large number of otherwise extractable rules, and unaligned words can result in an exponentially large set of extractable rules for the interpretation of these unaligned words (Galley et al., 2006). What makes word alignment so fragile? In order to investigate this problem, we manually analyzed the alignments of the first 100 parallel sentences in our English-Japanese training data (to be shown in Table 2). The alignments were generated by running GIZA++ (Och and Ney, 2003) and the grow-diag-final-and symmetrizing strategy (Koehn et al., 2007) on the training set. Of the 1,324 word alignment pairs, there were 309 error pairs, among Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 22–31, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics which there were 237 target function words, which tory particles, adverbial particles, binding particles, account for 76.7% of the error pairs1. This indicates conjunctive particles, and phrasal particles. that the alignments of the function words are</context>
<context citStr="Och and Ney, 2003" endWordPosition="3921" position="22672" startWordPosition="3918">2.71 # tree types 21.62 93.55 72.98 120.08 # nodes/tree 14.2 42.1 26.3 18.6 extract time 30.2 52.2 58.6 130.7 EM time 9.4 - 11.2 29.0 # rules in dev. 0.77 1.22 1.37 2.18 # rules in test 0.77 1.23 1.37 2.15 DT(sec./sent.) 2.8 15.7 22.4 35.4 BLEU (%) 26.15 27.07 27.93 28.89 Table 3: Statistics and translation results for four types of tree-to-string rules. With the exception of ‘# nodes/tree’, the numbers in the table are in millions and the time is in hours. Here, fw denotes function word, and DT denotes the decoding time, and the BLEU scores were computed on the test set. We performed GIZA++ (Och and Ney, 2003) and the grow-diag-final-and symmetrizing strategy (Koehn et al., 2007) on the training set to obtain alignments. The SRI Language Modeling Toolkit (Stolcke, 2002) was employed to train a five-gram Japanese LM on the training set. We evaluated the translation quality using the BLEU-4 metric (Papineni et al., 2002). Joshua v1.3 (Li et al., 2009), which is a freely available decoder for hierarchical phrasebased SMT (Chiang, 2005), is used as an external baseline system for comparison. We extracted 4.5M translation rules from the training set for the 4K English sentences in the development and te</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context citStr="Papineni et al., 2002" endWordPosition="291" position="1991" startWordPosition="288">al machine translation (SMT) systems. On the one hand, there is a tendency to integrate richer syntactic information into a translation rule in order to better express the translation phenomena. Thus, flat phrases (Koehn et al., 2003), hierarchical phrases (Chiang, 2005), and syntactic tree fragments (Galley et al., 2006; Mi and Huang, 2008; Wu et al., 2010) are gradually used in SMT. On the other hand, the use of syntactic phrases continues due to the requirement for phrase coverage in most syntax-based systems. For example, 22 Mi et al. (2008) achieved a 3.1-point improvement in BLEU score (Papineni et al., 2002) by including bilingual syntactic phrases in their forest-based system. Compared with flat phrases, syntactic rules are good at capturing global reordering, which has been reported to be essential for translating between languages with substantial structural differences, such as English and Japanese, which is a subject-objectverb language (Xu et al., 2009). Forest-based translation frameworks, which make use of packed parse forests on the source and/or target language side(s), are an increasingly promising approach to syntax-based SMT, being both algorithmically appealing (Mi et al., 2008) and</context>
<context citStr="Papineni et al., 2002" endWordPosition="3970" position="22987" startWordPosition="3966">sults for four types of tree-to-string rules. With the exception of ‘# nodes/tree’, the numbers in the table are in millions and the time is in hours. Here, fw denotes function word, and DT denotes the decoding time, and the BLEU scores were computed on the test set. We performed GIZA++ (Och and Ney, 2003) and the grow-diag-final-and symmetrizing strategy (Koehn et al., 2007) on the training set to obtain alignments. The SRI Language Modeling Toolkit (Stolcke, 2002) was employed to train a five-gram Japanese LM on the training set. We evaluated the translation quality using the BLEU-4 metric (Papineni et al., 2002). Joshua v1.3 (Li et al., 2009), which is a freely available decoder for hierarchical phrasebased SMT (Chiang, 2005), is used as an external baseline system for comparison. We extracted 4.5M translation rules from the training set for the 4K English sentences in the development and test sets. We used the default configuration of Joshua, with the exception of the maximum number of items/rules, and the value of k (of the k-best outputs) is set to be 200. 4.2 Results Table 3 lists the statistics of the following translation rule sets: • C3-T: a composed rule set extracted from the derivation fore</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context citStr="Pollard and Sag, 1994" endWordPosition="833" position="5440" startWordPosition="830">to construct translation rules that tend pect, modality, dummy, or emphasis. to be ill-formed and less generalized. 2.2 HPSG forests This is the motivation of the present approach of Following our precious work (Wu et al., 2010), we re-aligning the target function words to source tree use head-drive phrase structure grammar (HPSG) fragments, so that the influence of incorrect align- forests generated by Enju2 (Miyao and Tsujii, 2008), ments is reduced and the function words can be gen- which is a state-of-the-art HPSG parser for English. erated by tree fragments on the fly. However, the HPSG (Pollard and Sag, 1994; Sag et al., 2003) is a current dominant research only uses 1-best trees for lexicalist grammar framework. In HPSG, linguistic syntactic realignment (Galley et al., 2006; May and entities such as words and phrases are represented Knight, 2007; Wang et al., 2010), which adversely by a data structure called a sign. A sign gives a affects the rule set quality due to parsing errors. factored representation of the syntactic features of Therefore, we realign target function words to a a word/phrase, as well as a representation of their packed forest that compactly encodes exponentially semantic con</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan A Sag</author>
<author>Thomas Wasow</author>
<author>Emily M Bender</author>
</authors>
<title>Syntactic Theory: A Formal Introduction. Number 152 in CSLI Lecture Notes.</title>
<date>2003</date>
<publisher>CSLI Publications.</publisher>
<contexts>
<context citStr="Sag et al., 2003" endWordPosition="837" position="5459" startWordPosition="834">n rules that tend pect, modality, dummy, or emphasis. to be ill-formed and less generalized. 2.2 HPSG forests This is the motivation of the present approach of Following our precious work (Wu et al., 2010), we re-aligning the target function words to source tree use head-drive phrase structure grammar (HPSG) fragments, so that the influence of incorrect align- forests generated by Enju2 (Miyao and Tsujii, 2008), ments is reduced and the function words can be gen- which is a state-of-the-art HPSG parser for English. erated by tree fragments on the fly. However, the HPSG (Pollard and Sag, 1994; Sag et al., 2003) is a current dominant research only uses 1-best trees for lexicalist grammar framework. In HPSG, linguistic syntactic realignment (Galley et al., 2006; May and entities such as words and phrases are represented Knight, 2007; Wang et al., 2010), which adversely by a data structure called a sign. A sign gives a affects the rule set quality due to parsing errors. factored representation of the syntactic features of Therefore, we realign target function words to a a word/phrase, as well as a representation of their packed forest that compactly encodes exponentially semantic content. Phrases and w</context>
</contexts>
<marker>Sag, Wasow, Bender, 2003</marker>
<rawString>Ivan A. Sag, Thomas Wasow, and Emily M. Bender. 2003. Syntactic Theory: A Formal Introduction. Number 152 in CSLI Lecture Notes. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<contexts>
<context citStr="Stolcke, 2002" endWordPosition="3944" position="22835" startWordPosition="3943">18 # rules in test 0.77 1.23 1.37 2.15 DT(sec./sent.) 2.8 15.7 22.4 35.4 BLEU (%) 26.15 27.07 27.93 28.89 Table 3: Statistics and translation results for four types of tree-to-string rules. With the exception of ‘# nodes/tree’, the numbers in the table are in millions and the time is in hours. Here, fw denotes function word, and DT denotes the decoding time, and the BLEU scores were computed on the test set. We performed GIZA++ (Och and Ney, 2003) and the grow-diag-final-and symmetrizing strategy (Koehn et al., 2007) on the training set to obtain alignments. The SRI Language Modeling Toolkit (Stolcke, 2002) was employed to train a five-gram Japanese LM on the training set. We evaluated the translation quality using the BLEU-4 metric (Papineni et al., 2002). Joshua v1.3 (Li et al., 2009), which is a freely available decoder for hierarchical phrasebased SMT (Chiang, 2005), is used as an external baseline system for comparison. We extracted 4.5M translation rules from the training set for the 4K English sentences in the development and test sets. We used the default configuration of Joshua, with the exception of the maximum number of items/rules, and the value of k (of the k-best outputs) is set to</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm-an extensible language modeling toolkit. In Proceedings of International Conference on Spoken Language Processing, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A japaneseenglish patent parallel corpus.</title>
<date>2007</date>
<booktitle>In Proceedings of MT Summit XI,</booktitle>
<pages>475--482</pages>
<location>Copenhagen.</location>
<contexts>
<context citStr="Utiyama and Isahara, 2007" endWordPosition="3649" position="21181" startWordPosition="3646">Mi et al., 2008) that makes use of forestbased translation rules (Mi and Huang, 2008) as the baseline system for translating English HPSG forests into Japanese sentences. We analyzed the performance of the proposed translation rule sets by Train Dev. Test # sentence pairs 994K 2K 2K # En 1-best trees 987,401 1,982 1,984 # En forests 984,731 1,979 1,983 # En words 24.7M 50.3K 49.9K # Jp words 28.2M 57.4K 57.1K # Jp function words 8.0M 16.1K 16.1K Table 2: Statistics of the JST corpus. Here, En = English and Jp = Japanese. using the same decoder. The JST Japanese-English paper abstract corpus6 (Utiyama and Isahara, 2007), which consists of one million parallel sentences, was used for training, tuning, and testing. Table 2 shows the statistics of this corpus. Note that Japanese function words occupy more than a quarter of the Japanese words. Making use of Enju 2.3.1, we generated 987,401 1-best trees and 984,731 parse forests for the English sentences in the training set, with successful parse rates of 99.3% and 99.1%, respectively. Using the pruning criteria expressed in (Mi and Huang, 2008), we continue to prune a parse forest by setting pe to be 8, 5, and 2, until there are no more than eio = 22, 026 trees </context>
</contexts>
<marker>Utiyama, Isahara, 2007</marker>
<rawString>Masao Utiyama and Hitoshi Isahara. 2007. A japaneseenglish patent parallel corpus. In Proceedings of MT Summit XI, pages 475–482, Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Wang</author>
<author>Jonathan May</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Re-structuring, re-labeling, and realigning for syntax-based machine translation.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>2</issue>
<contexts>
<context citStr="Wang et al., 2010" endWordPosition="875" position="5703" startWordPosition="872">ords to source tree use head-drive phrase structure grammar (HPSG) fragments, so that the influence of incorrect align- forests generated by Enju2 (Miyao and Tsujii, 2008), ments is reduced and the function words can be gen- which is a state-of-the-art HPSG parser for English. erated by tree fragments on the fly. However, the HPSG (Pollard and Sag, 1994; Sag et al., 2003) is a current dominant research only uses 1-best trees for lexicalist grammar framework. In HPSG, linguistic syntactic realignment (Galley et al., 2006; May and entities such as words and phrases are represented Knight, 2007; Wang et al., 2010), which adversely by a data structure called a sign. A sign gives a affects the rule set quality due to parsing errors. factored representation of the syntactic features of Therefore, we realign target function words to a a word/phrase, as well as a representation of their packed forest that compactly encodes exponentially semantic content. Phrases and words represented by many parses. Given aligned forest-string pairs, we signs are collected into larger phrases by the appliextract composed tree-to-string translation rules that cations of schemata. The semantic representation of account for mu</context>
<context citStr="Wang et al., 2010" endWordPosition="4678" position="27037" startWordPosition="4675">coder achieved a significantly better (p &lt; 0.01) BLEU score by using either of the four types of translation rules. 5 Related Research Galley et al. (2006) first used derivation forests of aligned tree-string pairs to express multiple interpretations of unaligned target words. The EM algorithm was used to jointly estimate 1) the translation probabilities and fractional counts of rules and 2) the scores of derivations in the derivation forests. By dealing with the ambiguous word alignment instead of unaligned target words, syntaxbased re-alignment models were proposed by (May and Knight, 2007; Wang et al., 2010) for tree-based translations. Free attachment of the unaligned target word problem was ignored in (Mi and Huang, 2008), which was the first study on extracting tree-to-string rules from aligned forest-string pairs. This inspired the idea to re-align a packed forest and a target sentence. Specially, we observed that most incorrect or ambiguous word alignments are caused by function words rather than content words. Thus, we focus on the realignment of target function words to source tree fragments and use a dependency parser to limit the attachments of unaligned target words. 6 Conclusion We hav</context>
</contexts>
<marker>Wang, May, Knight, Marcu, 2010</marker>
<rawString>Wei Wang, Jonathan May, Kevin Knight, and Daniel Marcu. 2010. Re-structuring, re-labeling, and realigning for syntax-based machine translation. Computational Linguistics, 36(2):247–277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianchao Wu</author>
<author>Takuya Matsuzaki</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Fine-grained tree-to-string translation rule extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>325--334</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context citStr="Wu et al., 2010" endWordPosition="246" position="1729" startWordPosition="243">e-scale English-toJapanese translation revealed a significant improvement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system. 1 Introduction Rule generalization remains a key challenge for current syntax-based statistical machine translation (SMT) systems. On the one hand, there is a tendency to integrate richer syntactic information into a translation rule in order to better express the translation phenomena. Thus, flat phrases (Koehn et al., 2003), hierarchical phrases (Chiang, 2005), and syntactic tree fragments (Galley et al., 2006; Mi and Huang, 2008; Wu et al., 2010) are gradually used in SMT. On the other hand, the use of syntactic phrases continues due to the requirement for phrase coverage in most syntax-based systems. For example, 22 Mi et al. (2008) achieved a 3.1-point improvement in BLEU score (Papineni et al., 2002) by including bilingual syntactic phrases in their forest-based system. Compared with flat phrases, syntactic rules are good at capturing global reordering, which has been reported to be essential for translating between languages with substantial structural differences, such as English and Japanese, which is a subject-objectverb langua</context>
<context citStr="Wu et al., 2010" endWordPosition="770" position="5047" startWordPosition="767">such as ‘of’ extra meaning provided by a Japanese auxiliary verb and ‘the’, which may appear anywhere in an English alters the basic meaning of the main verb so that the sentence. Following these problematic alignments, main verb has one or more of the following funcwe are forced to make use of relatively large English tions: passive voice, progressive aspect, perfect astree fragments to construct translation rules that tend pect, modality, dummy, or emphasis. to be ill-formed and less generalized. 2.2 HPSG forests This is the motivation of the present approach of Following our precious work (Wu et al., 2010), we re-aligning the target function words to source tree use head-drive phrase structure grammar (HPSG) fragments, so that the influence of incorrect align- forests generated by Enju2 (Miyao and Tsujii, 2008), ments is reduced and the function words can be gen- which is a state-of-the-art HPSG parser for English. erated by tree fragments on the fly. However, the HPSG (Pollard and Sag, 1994; Sag et al., 2003) is a current dominant research only uses 1-best trees for lexicalist grammar framework. In HPSG, linguistic syntactic realignment (Galley et al., 2006; May and entities such as words and </context>
</contexts>
<marker>Wu, Matsuzaki, Tsujii, 2010</marker>
<rawString>Xianchao Wu, Takuya Matsuzaki, and Jun’ichi Tsujii. 2010. Fine-grained tree-to-string translation rule extraction. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 325–334, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
<author>Jaeho Kang</author>
<author>Michael Ringgaard</author>
<author>Franz Och</author>
</authors>
<title>Using a dependency parser to improve smt for subject-object-verb languages.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>245--253</pages>
<contexts>
<context citStr="Xu et al., 2009" endWordPosition="344" position="2349" startWordPosition="341"> gradually used in SMT. On the other hand, the use of syntactic phrases continues due to the requirement for phrase coverage in most syntax-based systems. For example, 22 Mi et al. (2008) achieved a 3.1-point improvement in BLEU score (Papineni et al., 2002) by including bilingual syntactic phrases in their forest-based system. Compared with flat phrases, syntactic rules are good at capturing global reordering, which has been reported to be essential for translating between languages with substantial structural differences, such as English and Japanese, which is a subject-objectverb language (Xu et al., 2009). Forest-based translation frameworks, which make use of packed parse forests on the source and/or target language side(s), are an increasingly promising approach to syntax-based SMT, being both algorithmically appealing (Mi et al., 2008) and empirically successful (Mi and Huang, 2008; Liu et al., 2009). However, forest-based translation systems, and, in general, most linguistically syntax-based SMT systems (Galley et al., 2004; Galley et al., 2006; Liu et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Chiang, 2010), are built upon word aligned parallel sentences and thus sh</context>
</contexts>
<marker>Xu, Kang, Ringgaard, Och, 2009</marker>
<rawString>Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz Och. 2009. Using a dependency parser to improve smt for subject-object-verb languages. In Proceedings of HLT-NAACL, pages 245–253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>Ai Ti Aw</author>
<author>Jun Sun</author>
<author>Sheng Li</author>
<author>Chew Lim Tan</author>
</authors>
<title>A tree-to-tree alignmentbased model for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of MT Summit XI,</booktitle>
<pages>535--542</pages>
<location>Copenhagen, Denmark,</location>
<contexts>
<context citStr="Zhang et al., 2007" endWordPosition="419" position="2839" startWordPosition="416"> with substantial structural differences, such as English and Japanese, which is a subject-objectverb language (Xu et al., 2009). Forest-based translation frameworks, which make use of packed parse forests on the source and/or target language side(s), are an increasingly promising approach to syntax-based SMT, being both algorithmically appealing (Mi et al., 2008) and empirically successful (Mi and Huang, 2008; Liu et al., 2009). However, forest-based translation systems, and, in general, most linguistically syntax-based SMT systems (Galley et al., 2004; Galley et al., 2006; Liu et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Chiang, 2010), are built upon word aligned parallel sentences and thus share a critical dependence on word alignments. For example, even a single spurious word alignment can invalidate a large number of otherwise extractable rules, and unaligned words can result in an exponentially large set of extractable rules for the interpretation of these unaligned words (Galley et al., 2006). What makes word alignment so fragile? In order to investigate this problem, we manually analyzed the alignments of the first 100 parallel sentences in our English-Japanese traini</context>
</contexts>
<marker>Zhang, Jiang, Aw, Sun, Li, Tan, 2007</marker>
<rawString>Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng Li, and Chew Lim Tan. 2007. A tree-to-tree alignmentbased model for statistical machine translation. In Proceedings of MT Summit XI, pages 535–542, Copenhagen, Denmark, September.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>