<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant confidence="0.001529" no="0">
<title confidence="0.758793">
Smoothed marginal distribution constraints for language modeling
</title>
<author confidence="0.924219">
Brian Roark†◦, Cyril Allauzen◦ and Michael Riley◦
</author>
<affiliation confidence="0.836817">
†Oregon Health &amp; Science University, Portland, Oregon ◦Google, Inc., New York
</affiliation>
<email confidence="0.98822">
roarkbr@gmail.com, {allauzen,riley}@google.com
</email>
<sectionHeader confidence="0.997307" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999846047619047">We present an algorithm for re-estimating parameters of backoff n-gram language models so as to preserve given marginal distributions, along the lines of wellknown Kneser-Ney (1995) smoothing. Unlike Kneser-Ney, our approach is designed to be applied to any given smoothed backoff model, including models that have already been heavily pruned. As a result, the algorithm avoids issues observed when pruning Kneser-Ney models (Siivola et al., 2007; Chelba et al., 2010), while retaining the benefits of such marginal distribution constraints. We present experimental results for heavily pruned backoff ngram models, and demonstrate perplexity and word error rate reductions when used with various baseline smoothing methods. An open-source version of the algorithm has been released as part of the OpenGrm ngram library.1</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999694133333333">Smoothed n-gram language models are the defacto standard statistical models of language for a wide range of natural language applications, including speech recognition and machine translation. Such models are trained on large text corpora, by counting the frequency of n-gram collocations, then normalizing and smoothing (regularizing) the resulting multinomial distributions. Standard techniques store the observed n-grams and derive probabilities of unobserved n-grams via their longest observed suffix and “backoff” costs associated with the prefix histories of the unobserved suffixes. Hence the size of the model grows with the number of observed n-grams, which is very large for typical training corpora.</bodyText>
<footnote confidence="0.94828">
1www.opengrm.org
</footnote>
<bodyText confidence="0.999881219512195">Natural language applications, however, are commonly used in scenarios requiring relatively small footprint models. For example, applications running on mobile devices or in low latency streaming scenarios may be required to limit the complexity of models and algorithms to achieve the desired operating profile. As a result, statistical language models – an important component of many such applications – are often trained on very large corpora, then modified to fit within some pre-specified size bound. One method to achieve significant space reduction is through randomized data structures, such as Bloom (Talbot and Osborne, 2007) or Bloomier (Talbot and Brants, 2008) filters. These data structures permit efficient querying for specific n-grams in a model that has been stored in a fraction of the space required to store the full, exact model, though with some probability of false positives. Another common approach – which we pursue in this paper – is model pruning, whereby some number of the n-grams are removed from explicit storage in the model, so that their probability must be assigned via backoff smoothing. One simple pruning method is count thresholding, i.e., discarding n-grams that occur less than k times in the corpus. Beyond count thresholding, the most widely used pruning methods (Seymore and Rosenfeld, 1996; Stolcke, 1998) employ greedy algorithms to reduce the number of stored n-grams by comparing the stored probabilities to those that would be assigned via the backoff smoothing mechanism, and removing those with the least impact according to some criterion. While these greedy pruning methods are highly effective for models estimated with most common smoothing approaches, they have been shown to be far less effective with Kneser-Ney trained language models (Siivola et al., 2007; Chelba et al., 2010), leading to severe degradation in model quality relative to other standard smoothing methods.</bodyText>
<page confidence="0.997803">
43
</page>
<note confidence="0.99157">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 43–52,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
4-gram models Backoff n-grams Interpolated n-grams
Perplexity Perplexity
Smoothing method full pruned (×1000) full pruned (×1000)
Absolute Discounting (Ney et al., 1994) 120.5 197.3 383.4 119.8 198.1 386.2
Witten-Bell (Witten and Bell, 1991) 118.8 196.3 380.4 121.6 202.3 396.4
Ristad (1995) 126.4 203.6 395.6 ——- N/A ——-
Katz (1987) 119.8 198.1 386.2 ——- N/A ——-
Kneser-Ney (Kneser and Ney, 1995) 114.5 285.1 388.2 115.8 274.3 398.7
Mod. Kneser-Ney (Chen and Goodman, 1998) 116.3 280.6 396.2 112.8 270.7 399.1
</note>
<tableCaption confidence="0.99531025">
Table 1: Reformatted version of Table 3 in Chelba et al. (2010), demonstrating perplexity degradation of Kneser-Ney
smoothed models in contrast to other common smoothing methods. Data: English Broadcast News, 128M words training;
692K words test; 143K word vocabulary. 4-gram language models, pruned using Stolcke (1998) relative entropy pruning to
approximately 1.3% of the original size of 31,095,260 n-grams.
</tableCaption>
<bodyText confidence="0.9998936625">Thus, while Kneser-Ney may be the preferred smoothing method for large, unpruned models – where it can achieve real improvements over other smoothing methods – when relatively sparse, pruned models are required, it has severely diminished utility. Table 1 presents a slightly reformatted version of Table 3 from Chelba et al.(2010). In their experiments (see Table 1 caption for specifics on training/test setup), they trained 4-gram Broadcast News language models using a variety of both backoff and interpolated smoothing methods and measured perplexity before and after Stolcke (1998) relative entropy based pruning. With this size training data, the perplexity of all of the smoothing methods other than Kneser-Ney degrades from around 120 with the full model to around 200 with the heavily pruned model. Kneser-Ney smoothed models have lower perplexity with the full model than the other methods by about 5 points, but degrade with pruning to far higher perplexity, between 270-285. The cause of this degradation is Kneser-Ney’s unique method for estimating smoothed language models, which will be presented in more detail in Section 3. Briefly, the smoothing method reestimates lower-order n-gram parameters in order to avoid over-estimating the likelihood of n-grams that already have ample probability mass allocated as part of higher-order n-grams. This is done via a marginal distribution constraint which requires the expected frequency of the lower-order n-grams to match their observed frequency in the training data, much as is commonly done for maximum entropy model training. Goodman (2001) proved that, under certain assumptions, such constraints can only improve language models. Lower-order n-gram parameters resulting from Kneser-Ney are not relative frequency estimates, as with other smoothing methods; rather they are parameters estimated specifically for use within the larger smoothed model. There are (at least) a couple of reasons why such parameters do not play well with model pruning. First, the pruning methods commonly use lower order n-gram probabilities to derive an estimate of state marginals, and, since these parameters are no longer smoothed relative frequency estimates, they do not serve that purpose well. For this reason, the widely-used SRILM toolkit recently provided switches to modify their pruning algorithm to use another model for state marginal estimates (Stolcke et al., 2011). Second, and perhaps more importantly, the marginal constraints that were applied prior to smoothing will not in general be consistent with the much smaller pruned model. For example, if a bigram parameter is modified due to the presence of some set of trigrams, and then some or all of those trigrams are pruned from the model, the bigram associated with the modified parameter will be unlikely to have an overall expected frequency equal to its observed frequency anymore. As a result, the resulting model degrades dramatically with pruning. In this paper, we present an algorithm that imposes marginal distribution constraints of the sort used in Kneser-Ney modeling on arbitrary smoothed backoff n-gram language models. Our approach makes use of the same sort of derivation as the original Kneser-Ney modeling, but, among other differences, relies on smoothed estimates of the empirical relative frequency rather than the unsmoothed observed frequency. The algorithm can be applied after the smoothed model has been pruned, hence avoiding the pitfalls associated with Kneser-Ney modeling. Furthermore, while Kneser-Ney is conventionally defined as a variant of absolute discounting, our method can be applied to models smoothed with any backoff smoothing, including mixtures of models, widely used for domain adaptation.</bodyText>
<page confidence="0.998009">
44
</page>
<bodyText confidence="0.9543795">We next establish formal preliminaries and our smoothed marginal distribution constraints method.</bodyText>
<sectionHeader confidence="0.987991" genericHeader="method">
2 Preliminaries
</sectionHeader>
<bodyText confidence="0.99770545">N-gram language models are typically presented mathematically in terms of words w, the strings (histories) h that precede them, and the suffixes of the histories (backoffs) h' that are used in the smoothing recursion. Let V be a vocabulary (alphabet), and V * a string of zero or more symbols drawn from V . Let V k denote the set of strings w E V * of length k, i.e., jwj = k. We will use variables u, v, w, x, y, z E V to denote single symbols from the vocabulary; h, g E V * to denote history sequences preceding the specific word; and h', g' E V * the respective backoff histories of h and g as typically defined (see below). For a string w = wi ... w* we can calculate the smoothed conditional probability of each word wi in the sequence given the k words that preceded it, depending on the order of the Markov model. Let hki = wi_k ... wi_1 be the previous k words in the sequence. Then the smoothed model is defined recursively as follows:</bodyText>
<equation confidence="0.996386">
P(wi  |hki ) _ P(wi  |hki ) 1 if c(hki wi) &gt; 0
{ α(hki ) P(wi  |hi ) otherwise
</equation>
<bodyText confidence="0.993538288135593">where c(hki wi) is the count of the n-gram sequence wi_k ... wi in the training corpus; P is a regularized probability estimate that provides some probability mass for unobserved n-grams; and α(hki ) is a factor that ensures normalization. Note that for h = hki , the typically defined backoff history h' = hk_1 i , i.e., the longest suffix of h that is not h itself. When we use h' and g' (for notational convenience) in future equations, it is this definition that we are using. There are many ways to estimate P, including absolute discounting (Ney et al., 1994), Katz (1987) and Witten and Bell (1991). Interpolated models are special cases of this form, where the P is determined using model mixing, and the α parameter is exactly the mixing factor value for the lower order model. N-gram language models allow for a sparse representation, so that only a subset of the possible ngrams must be explicitly stored. Probabilities for the rest of the n-grams are calculated through the “otherwise” semantics in the equation above. For an n-gram language model G, we will say that an n-gram hw E G if it is explicitly represented in the model; otherwise hw E� G. In the standard ngram formulation above, the assumption is that if c(hki wi) &gt; 0 then the n-gram has a parameter; yet with pruning, we remove many observed n-grams from the model, hence this is no longer the appropriate criterion. We reformulate the standard equation as follows: P(wi|hki ) _ J β(hki wi) if hki wi ∈ G 1 l α(hki , hi−1) P(wi|hi otherwise ( ) where O(hki wi) is the parameter associated with the n-gram hki wi and α(hki , hk_1 i ) is the backoff cost associated with going from state hki to state hk_1 i . We assume that, if hw E G then all prefixes and suffixes of hw are also in G. Figure 1 presents a schema of an automaton representation of an n-gram model, of the sort used in the OpenGrm library (Roark et al., 2012). States represent histories h, and the words w, whose probabilities are conditioned on h, label the arcs, leading to the history state for the subsequent word. State labels are provided in Figure 1 as a convenience, to show the (implicit) history encoded by the state, e.g., ‘xyz’ indicates that the state represents a history with the previous three symbols being x, y and z. Failure arcs, labeled with a φ in Figure 1, encode an “otherwise” semantics and have as destination the origin state’s backoff history. Many higher order states will back off to the same lower order state, specifically those that share the same suffix. Note that, in general, the recursive definition of backoff may require the traversal of several backoff arcs before emitting a word, e.g., the highest order states in Figure 1 needing to traverse a couple of φ arcs to reach state ‘z’.</bodyText>
<equation confidence="0.989813727272727">
yzu yzv
v/β(yyzv)
w/β(yyzw)
xyz
φ/α(yzw,zw)
zw
yz
w/β(zw)
z/β(z)
φ/α(yz,z)
φ/α(z,ε)
</equation>
<figureCaption confidence="0.785014666666667">
Figure 1: N-gram weighted automaton schema. State labels
are presented for convenience, to specify the history implic-
itly encoded by the state.
</figureCaption>
<figure confidence="0.721202875">
u/β(xyzu)
φ/α(xyz,yz)
yyz
φ/α(yyz,yz)
w/β(yzw)
yzw
z
ε
</figure>
<page confidence="0.99228">
45
</page>
<bodyText confidence="0.9998648">We can define the backoff cost between a state hki and any of its suffix states as follows. Let α(h, h) = 1 and for</bodyText>
<equation confidence="0.972801666666667">
m &gt; 1,
α hk−j+1 hk−j
(Z Z ).
</equation>
<bodyText confidence="0.994182714285714">If hki w E� G then the probability of that n-gram will be defined in terms of backoff to its longest suffix hk−m i w E G. Let hwG denote the longest suffix of h such that hwGw E G. Note that this is not necessarily a proper suffix, since hwG could be h itself or it could be E. Then which is equivalent to equation 1.</bodyText>
<equation confidence="0.963726">
P(w  |h) = α(h, hwG) ,6(hwGw) (2)
</equation>
<sectionHeader confidence="0.982547" genericHeader="method">
3 Marginal distribution constraints
</sectionHeader>
<bodyText confidence="0.992790235294118">Marginal distribution constraints attempt to match the expected frequency of an n-gram with its observed frequency. In other words, if we use the model to randomly generate a very large corpus, the n-grams should occur with the same relative frequency in both the generated and original (training) corpus. Standard smoothing methods overgenerate lower-order n-grams. Using standard n-gram notation (where g0 is the backoff history for g), this constraint is stated in Kneser and Ney (1995) as hence the longest history in the model is of length n. Assume the length of the particular backoff history |h0 |= k. Let V n−kh0 be the set of strings h E V n with h0 as a suffix. Then we can restate the marginal distribution constraint in equation 3 as</bodyText>
<equation confidence="0.995389">
P(w  |h0) = X P(h, w  |h0) (4)
h∈V n−kh'
</equation>
<bodyText confidence="0.999768">Next we solve for ,6(h0w) parameters used in equation 1. Note that h0 is a suffix of any h E V n−kh0, so conditioning probabilities on h and h0 is the same as conditioning on just h. Each of the following derivation steps simply relies on the chain rule or definition of conditional probability, as well as pulling terms out of the summation.</bodyText>
<equation confidence="0.999369545454546">
X
P(w  |h0) =
h∈V n−kh'
X=
h∈V n−kh'
X=
h∈V n−kh'
X
P(w  |h) P(h) (5)
P(g) h∈V n−kh'
g∈V n−kh'
</equation>
<bodyText confidence="0.8645985">Then, multiplying both sides by the normalizing denominator on the right-hand side and using equation 2 to substitute α(h, hwG) ,6(hwGw) for P(w  |h):</bodyText>
<equation confidence="0.998765047619048">
P(w  |h0) X P(g) = X P(w  |h) P(h)
g∈V n−kh' h∈V n−kh'
α(hki , hk−m
i ) =
m
H
j=1
P(h, w  |h0)
P(w  |h, h0) P(h  |h0)
P(w  |h) P(h)
X P(g)
g∈V n−kh'
1
=
X
�
P(w  |h0) =
P(g, w  |h0) (3)
X= α(h, hwG) O(hwGw) P(h) (6)
h∈V n−kh'
g:g0=h0
</equation>
<bodyText confidence="0.999953454545455">where P is the empirical relative frequency estimate. Taking this approach, certain base smoothing methods end up with very nice, easy to calculate solutions based on counts. Absolute discounting (Ney et al., 1994) in particular, using the above approach, leads to the well-known KneserNey smoothing approach (Kneser and Ney, 1995; Chen and Goodman, 1998). We will follow this same approach, with a couple of changes. First, we will make use of regularized estimates of relative frequency P rather than raw relative frequency P. Second, rather than just looking at observed histories h that back off to h0, we will look at all histories (observed or not) of the length of the longest history in the model. For notational simplicity, suppose we have an n+1-gram model, Note that we are only interested in h0w E G, hence there are two disjoint subsets of histories h E Vn−kh0 that are being summed over: those such that hwG = h0 and those such that |hwG |&gt; |h0|. We next separate these sums in the next step of the derivation:</bodyText>
<equation confidence="0.9977155">
P(w  |h0) X P(g) =
g∈V n−kh'
X α(h, hwG) O(hwGw) P(h) +
h∈V n−kh':|hwG|&gt;|h'|
X α(h, h0) O(h0w) P(h) (7)
h∈V n−kh':hwG=h'
</equation>
<bodyText confidence="0.99989975">Finally, we solve for ,6(h0w) in the second sum on the right-hand side of equation 7, yielding the formula in equation 8. Note that this equation is the correlate of equation (6) in Kneser and Ney</bodyText>
<page confidence="0.935855">
46
</page>
<equation confidence="0.996971142857143">
P(w  |h') � P(g) − � α(h, hwG) 0(hwGw) P(h)
0(h'w) =
E
hEV n−kh0:hwG=h0
(8)
α(h, h') P(h)
gEV n−kh0 hEV n−kh0:|hwG|&gt;|h0|
</equation>
<bodyText confidence="0.999964395833334">(1995), modulo the two differences noted earlier: use of smoothed probability P rather than raw relative frequency; and summing over all history substrings in Vn−kh' rather than just those with count greater than zero, which is also a change due to smoothing. Keep in mind, P is the target expected frequency from a given smoothed model. KneserNey models are not useful input models, since their P n-gram parameters are not relative frequency estimates. This means that we cannot simply ‘repair’ pruned Kneser-Ney models, but must use other smoothing methods where the smoothed values are based on relative frequency estimation. There are, in addition, two other important differences in our approach from that in Kneser and Ney (1995), which would remain as differences even if our target expected frequency were the unsmoothed relative frequency P instead of the smoothed estimate P. First, the sum in the numerator is over histories of length n, the highest order in the n-gram model, whereas in the KneserNey approach the sum is over histories that immediately back off to h', i.e., from the next highest order in the n-gram model. Thus the unigram distribution is with respect to the bigram model, the bigram model is with respect to the trigram model, and so forth. In our optimization, we sum instead over all possible history sequences of length n. Second, an early assumption made in Kneser and Ney (1995) is that the denominator term in their equation (6) (our Eq. 8) is constant across all words for a given history, which is clearly false. We do not make this assumption. Of course, the probabilities must be normalized, hence the final values of 0(h'w) will be proportional to the values in Eq. 8. We briefly note that, like Kneser-Ney, if the baseline smoothing method is consistent, then the amount of smoothing in the limit will go to zero and our resulting model will also be consistent. The smoothed relative frequency estimate P and higher order 0 values on the right-hand side of Eq. 8 are given values (from the input smoothed model and previous stages in the algorithm, respectively), implying an algorithm that estimates highest orders of the model first. In addition, steady state history probabilities P(h) must be calculated. We turn to the estimation algorithm next.</bodyText>
<sectionHeader confidence="0.983296" genericHeader="method">
4 Model constraint algorithm
</sectionHeader>
<bodyText confidence="0.9998770625">Our algorithm takes a smoothed backoff n-gram language model in an automaton format (see Figure 1) and returns a smoothed backoff n-gram language model with the same topology. For all ngrams in the model that are suffixes of other ngrams in the model – i.e., that are backed-off to – we calculate the weight provided by equation 8 and assign it (after normalization) to the appropriate n-gram arc in the automaton. There are several important considerations for this algorithm, which we address in this section. First, we must provide a probability for every state in the model. Second, we must memoize summed values that are used repeatedly. Finally, we must iterate the calculation of certain values that depend on the n-gram weights being re-estimated.</bodyText>
<subsectionHeader confidence="0.991209">
4.1 Steady state probability calculation
</subsectionHeader>
<bodyText confidence="0.9976516">The steady state probability P(h) is taken to be the probability of observing h after a long word sequence, i.e., the state’s relative frequency in a long sequence of randomly-generated sentences from the model:</bodyText>
<equation confidence="0.979618">
ˆP(w1 ... wmh) (9)
</equation>
<bodyText confidence="0.889286">where Pˆ is the corpus probability derived as follows: The smoothed n-gram probability model where A parameterizes the corpus length distribution.2 Assuming the n-gram language model automaton G has a single final state &lt;/s&gt; into</bodyText>
<equation confidence="0.973462375">
P(w  |h) is naturally extended to a sentence
s = w0 ... wl, where w0 = &lt;s&gt; and wl = &lt;/s&gt;
are the sentence initial and final words, by P(s) =
Hli=1 P(wi  |hni ). The corpus probability s1 ... sr
is taken as:
r
ˆP(s1 ... sr) = (1 − A)Ar−1 P(si) (10)
i=1
</equation>
<footnote confidence="0.7103895">
2Pˆ models words in a corpus rather than a single sen-
tence since Equation 9 tends to zero as m → oo otherwise.
In Markov chain terms, the corpus distribution is made irre-
ducible to allow a non-trivial stationary distribution.
</footnote>
<equation confidence="0.93066525">
�
P(h) = lim
m-+oo
w1...wm
</equation>
<page confidence="0.986396">
47
</page>
<bodyText confidence="0.999876032258064">which all &lt;/s&gt; arcs enter, adding a A weighted c arc from the &lt;/s&gt; state to the initial state and having a final weight 1 − A in order to leave the automaton at the &lt;/s&gt; state will model this corpus distribution. According to Eq. 9, P(h) is then the stationary distribution of the finite irreducible Markov Chain defined by this altered automaton. There are many methods for computing such a stationary distribution; we use the well-known power method (Stewart, 1999). One difficulty remains to be resolved. The backoff arcs have a special interpretation in the automaton: they are traversed only if a word fails to match at the higher order. These failure arcs must be properly handled before applying standard stationary distribution calculations. A simple approach would be for each word w' and state h such that hw' ∈/ G, but h'w' ∈ G, add a w' arc from state h to h'w' with weight α(h, h')β(h'w') and then remove all failure arcs (see Figure 2a). This however results in an automaton with |V  |arcs leaving every state, which is unwieldy with larger vocabularies and n-gram orders. Instead, for each word w and state h such that hw ∈ G, add a w arc from state h to h'w with weight −α(h, h')β(h'w) and then replace all failure labels with c labels (see Figure 2b). In this case, the added negativelyweighted arcs compensate for the excess probability mass allowed by the epsilon arcs3. The number of added arcs is no more than found in the original model.</bodyText>
<subsectionHeader confidence="0.999427">
4.2 Accumulation of higher order values
</subsectionHeader>
<bodyText confidence="0.999969933333334">We are summing over all possible histories of length n in equation 8, and the steady state probability calculation outlined in the previous section includes the probability mass for histories h ∈6 G. The probability mass of states not in G ends up being allocated to the state representing their longest suffix that is explicitly in G. That is the state that would be active when these histories are encountered. Hence, once we have calculated the steady state probabilities for each state in the smoothed model, we only need to sum over states explicitly in the model. As stated earlier, the use of β(hwGw) in the numerator of equation 8 for hwG that are larger than h' implies that the longer n-grams must be re-estimated first.</bodyText>
<footnote confidence="0.61325375">
3Since each negatively-weighted arc leaving a state
exactly cancels an epsilon arc followed by a matching
positively-weighted arc in each iteration of the power
method, convergence is assured.
</footnote>
<figureCaption confidence="0.993649">
Figure 2: Schemata showing failure arc handling: (a) 0
removal: add w' arc (red), delete 0 arc; (b) 0 replacement:
add w arc (red), replace 0 by e (red)
</figureCaption>
<bodyText confidence="0.997089025641026">Thus we process each history length in descending order, finishing with the unigram state. Since we assume that, for every ngram hw ∈ G, every prefix and suffix is also in G, we know that if h'w ∈6 G then there is no history h such that h' is a suffix of h and hw ∈ G. This allows us to recursively accumulate the α(h, h') P(h) in the denominator of Eq.8. For every n-gram, we can accumulate values required to calculate the three terms in equation 8, and pass them along to calculate lower order ngram values. Note, however, that a naive implementation of an algorithm to assign these values is O(|V |n). This is due to the fact that the denominator factor must be accumulated for all higher order states that do not have the given n-gram. Hence, for every state h directly backing off to h' (order |V |), and for every n-gram arc leaving state h' (order |V |), some value must be accumulated. This can be particularly clearly seen at the unigram state, which has an arc for every unigram (the size of the vocabulary): for every bigram state (also order of the vocabulary), in the naive algorithm we must look for every possible arc. Since there are O(|V |n−2) lower order histories in the model in the worst case, we have overall complexity O(|V |n). However, we know that the number of stored n-grams is very sparse relative to the possible number of n-grams, so the typical case complexity is far lower. Importantly, the denominator is calculated by first assuming that all higher order states back off to the current n-gram, then subtract out the mass associated with those that are already observed at the higher order. In such a way, we need only perform work for higher order n-grams hw that are explicitly in the model. This optimization achieves orders-of-magnitude speedups, so that models take seconds to process. Because smoothing is not necessarily constrained across n-gram orders, it is possible that higher-order n-grams could be smoothed less than lower order n-grams, so that the numerator of equation 8 can be less than zero, which is not valid.</bodyText>
<figure confidence="0.9981165625">
w/P(hw)
(a) hw (b)
w/P(hw)
hw
h
w'/α(h,h') P(h'w')
h'w'
h
w/-α(h,h') P(h'w)
h'w
w/P(h'w)
h'
h'
cp/α(h,h')
w'/P(h'w')
s/α(h,h')
</figure>
<page confidence="0.998336">
48
</page>
<bodyText confidence="0.999986166666667">A value less than zero means that the higher order n-grams will already produce the n-gram more frequently than its smoothed expected frequency. We set a minimum value c for the numerator, and any n-gram numerator value less than c is replaced with c (for the current study, c = 0.001). We find this to be relatively infrequent, about 1% of n-grams for most models.</bodyText>
<subsectionHeader confidence="0.998952">
4.3 Iteration
</subsectionHeader>
<bodyText confidence="0.999958615384615">Recall that P and β terms on the right-hand side of equation 8 are given and do not change. But there are two other terms in the equation that change as we update the n-gram parameters. The α(h, h') backoff weights in the denominator ensure normalization at the higher order states, and change as the n-gram parameters at the current state are modified. Further, the steady state probabilities will change as the model changes. Hence, at each state, we must iterate the calculation of the denominator term: first adjust n-gram weights and normalize; then recalculate backoff weights at higher order states and iterate. Since this only involves the denominator term, each n-gram weight can be updated by multiplying by the ratio of the old term and the new term. After the entire model has been re-estimated, the steady state probability calculation presented in Section 4.1 is run again and model estimation happens again. As we shall see in the experimental results, this typically converges after just a few iterations. At this time, we have no convergence proofs for either of these iterative components to the algorithm, but expect that something can be said about this, which will be a priority in future work.</bodyText>
<sectionHeader confidence="0.988926" genericHeader="result">
5 Experimental results
</sectionHeader>
<bodyText confidence="0.998002444444445">All results presented here are for English Broadcast News. We received scripts for replicating the Chelba et al. (2010) results from the authors, and we report statistics on our replication of their paper’s results in Table 2. The scripts are distributed in such a way that the user supplies the data from LDC98T31 (1996 CSR HUB4 Language Model corpus) and the script breaks the collection into training and testing sets, normalizes the text, and trains and prunes the language models using the SRILM toolkit (Stolcke et al., 2011).</bodyText>
<table confidence="0.999759571428571">
Smoothing full Perplexity n-grams (×1000)
method pruned model diff
Abs.Disc. 120.4 197.1 382.3 -1.1
Witten-Bell 118.7 196.1 379.3 -1.1
Ristad 126.2 203.4 394.6 -1.1
Katz 119.7 197.9 385.1 -1.1
Kneser-Ney† 114.4 234.1 375.4 -12.7
</table>
<tableCaption confidence="0.9675164">
Table 2: Replication of Chelba et al. (2010) using provided
script. Using the script, the size of the unpruned model is
31,091,219 ngrams, 4,041 fewer than Chelba et al. (2010).
† Kneser-Ney model pruned using -prune-history-lm
switch in SRILM.
</tableCaption>
<bodyText confidence="0.999967902439024">Presumably due to minor differences in text normalization, resulting in very slightly fewer n-grams in all conditions, we achieve negligibly lower perplexities (one or two tenths of a point) in all conditions, as can be seen when comparing with Table 1. All of the same trends result, thus that paper’s result is successfully replicated here. Note that we ran our Kneser-Ney pruning (noted with a † in the table), using the new -prune-history-lm switch in SRILM – created in response to the Chelba et al.(2010) paper – which allows the use of another model to calculate the state marginals for pruning. This fixes part of the problem – perplexity does not degrade as much as the Kneser-Ney pruned model in Table 1 – but, as argued earlier in this paper, this is not the sole reason for the degradation and the perplexity remains extremely inflated. We follow Chelba et al.(2010) in training and test set definition, vocabulary size, and parameters for reporting perplexity. Note that unigrams in the models are never pruned, hence all models assign probabilities over an identical vocabulary and perplexity is comparable across models. For all results reported here, we use the SRILM toolkit for baseline model training and pruning, then convert from the resulting ARPA format model to an OpenFst format (Allauzen et al., 2007), as used in the OpenGrm n-gram library (Roark et al., 2012). We then apply the marginal distribution constraints, and convert the result back to ARPA format for perplexity evaluation with the SRILM toolkit. All models are subjected to full normalization sanity checks, as with typical model functions in the OpenGrm library. Recall that our algorithm assumes that, for every n-gram in the model, all prefix and suffix ngrams are also in the model. For pruned models, the SRILM toolkit does not impose such a requirement, hence explicit arcs are added to the model during conversion, with probability equal to what they would receive in the the original model.</bodyText>
<page confidence="0.998917">
49
</page>
<table confidence="0.9933929">
Perplexity
Smoothing Pruned Perplexity Δ n-grams
Method Model Pruned (×1000)
+MDC in WFST
Abs.Disc. 197.1 187.4 9.7 389.2
Witten-Bell 196.1 185.7 10.4 385.0
Ristad 203.4 190.3 13.1 395.9
Katz 197.9 187.5 10.4 390.8
AD,WB,Katz 196.6 186.3 10.3 388.7
Mixture
</table>
<tableCaption confidence="0.9828488">
Table 3: Perplexity reductions achieved with marginal dis-
tribution constraints (MDC) on the heavily pruned models
from Chelba et al. (2010), and a mixture model. WFST n-
gram counts are slightly higher than ARPA format in Table 2
due to adding prefix and suffix n-grams.
</tableCaption>
<bodyText confidence="0.9988991">The resulting model is equivalent, but with a small number of additional arcs in the explicit representation (around 1% for the most heavily pruned models). Table 3 presents perplexity results for models that result from applying our marginal distribution constraints to the four heavily pruned models from Table 2. In all four cases, we get perplexity reductions of around 10 points. We present the number of n-grams represented explicitly in the WFST, which is a slight increase from those presented in Table 2 due to the reintroduction of prefix and suffix n-grams. In addition to the four models reported in Chelba et al.(2010), we produced a mixture model by interpolating (with equal weight) smoothed ngram probabilities from the full (unpruned) absolute discounting, Witten-Bell and Katz models, which share the same set of n-grams. After renormalizing and pruning to approximately the same size as the other models, we get commensurate gains using this model as with the other models. Figure 3 demonstrates the importance of iterating the steady state history calculation. All of the methods achieve perplexity reductions with subsequent iterations. Katz and absolute discounting achieve very little reduction in the first iteration, but catch back up in the second iteration. The other iterative part of the algorithm, discussed in Section 4.3, is the denominator of equation 8, which changes due to adjustments in the backoff weights required by the revised n-gram probabilities. If we do not iteratively update the backoff weights when reestimating the weights, the ‘Pruned+MDC’ perplexities in Table 3 increase by between 0.2–0.4 points. Hence, iterating the steady state probability calculation is quite important, as illustrated by Figure 3; iterating the denominator calculation much less so, at least for these models.</bodyText>
<subsubsectionHeader confidence="0.490431">
Iterations of estimation (recalculating steady state probs)
</subsubsectionHeader>
<figureCaption confidence="0.998990666666667">
Figure 3: Models resulting from different numbers of pa-
rameter re-estimation iterations. Iteration 0 is the baseline
pruned model.
</figureCaption>
<bodyText confidence="0.999948029411765">We noted in Section 3 that a key difference between our approach and Kneser and Ney (1995) is that their approach treated the denominator as a constant. If we do this, the ‘Pruned+MDC’ perplexities increase by between 4.5–5.6 points, i.e., about half of the perplexity reduction is lost for each method. Thus, while iteration of denominator calculation may not be critical, it should not be treated as a constant. We now look at the impacts on system performance we can achieve with these new models4, and whether the perplexity differences that we observe translate to real error rate reductions. For automatic speech recognition experiments, we used as test set the 1997 Hub4 evaluation set consisting of 32,689 words. The acoustic model is a tied-state triphone GMM-based HMM whose input features are 9-frame stacked 13-dimensional PLP-cepstral coefficients projected down to 39 dimensions using LDA. The model was trained on the 1996 and 1997 Hub4 acoustic model training sets (about 150 hours of data) using semi-tied covariance modeling and CMLLR-based speaker adaptive training and 4 iterations of boosted MMI. We used a multi-pass decoding strategy: two quick passes for adaptation supervision, CMLLR and MLLR estimation; then a slower full decoding pass running about 3 times slower than real time. Table 4 presents recognition results for the heavily pruned models that we have been considering, both for first pass decoding and rescoring of the resulting lattices using failure transitions rather than epsilon backoff approximations.</bodyText>
<footnote confidence="0.9916465">
4For space purposes, we exclude the Ristad method from
this point forward since it is not competitive with the others.
</footnote>
<page confidence="0.539255">
205
</page>
<figure confidence="0.994217090909091">
Witten−Bell
Ristad
Katz
Absolute Discounting
WB,AD,Katz mixture
195
190
185
180
0 1 2 3 4 5 6
200
</figure>
<page confidence="0.953839">
50
</page>
<table confidence="0.999903363636364">
Smoothing First Word rate (WER) Pruned
Method Pruned error Rescoring +MDC
Model pass Pruned
Pruned Model
+MDC
Abs.Disc. 20.5 19.7 20.2 19.6
Witten-Bell 20.5 19.9 20.1 19.6
Katz 20.5 19.7 20.2 19.7
Mixture 20.5 19.6 20.2 19.6
Kneser-Neya 22.1 22.2
Kneser-Neyb 20.5 20.6
</table>
<tableCaption confidence="0.932323">
Table 4: WER reductions achieved with marginal dis-
tribution constraints (MDC) on the heavily pruned models
from Chelba et al. (2010), and a mixture model. Kneser-
Ney results are shown for: a) original pruning; and b) with
-prune-history-lm switch.
</tableCaption>
<bodyText confidence="0.999946866666667">The perplexity reductions that were achieved for these models do translate to real word error rate reductions at both stages of between 0.5 and 0.9 percent absolute. All of these gains are statistically significant at p &lt; 0.0001 using the stratified shuffling test (Yeh, 2000). For pruned Kneser-Ney models, fixing the state marginals with the -prune-history-lm switch reduces the WER versus the original pruned model, but no reductions were achieved vs. baseline methods. Table 5 presents perplexity and WER results for less heavily pruned models, where the pruning thresholds were set to yield approximately 1.5 million n-grams (4 times more than the previous models); and another set at around 5 million n-grams, as well as the full, unpruned models. While the robust gains we’ve observed up to now persist with the 1.5M n-gram models (WER reductions significant, Witten-Bell at p &lt; 0.02, others at p &lt; 0.0001), the larger models yield diminishing gains, with no real WER improvements. Performance of Witten-Bell models with the marginal distribution constraints degrade badly for the larger models, indicating that this method of regularization, unmodified by aggressive pruning, does not provide a well suited distribution for this sort of optimization. We speculate that this is due to underregularization, having noted some floating point precision issues when allowing the backoff recalculation to run indefinitely.</bodyText>
<sectionHeader confidence="0.992389" genericHeader="conclusion">
6 Summary and Future Directions
</sectionHeader>
<bodyText confidence="0.999970260869565">The presented method reestimates lower order n-gram model parameters for a given smoothed backoff model, achieving perplexity and WER reductions for many smoothed models. There remain a number of open questions to investigate in the future. Recall that the numerator in Eq. 8 can be less than zero, meaning that no parameterization would lead to a model with the target frequency of the lower order n-gram, presumably due to overor under-regularization. We anticipate a pre-constraint on the baseline smoothing method, that would recognize this problem and adjust the smoothing to ensure that a solution does exist. Additionally, it is clear that different regularization methods yield different behaviors, notably that large, relatively lightly pruned WittenBell models yield poor results. We will look to identify the issues with such models and provide general guidelines for prepping models prior to processing. Finally, we would like to perform extensive controlled experimentation to examine the relative contribution of the various aspects of our approach.</bodyText>
<sectionHeader confidence="0.999196" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99953925">Thanks to Ciprian Chelba and colleagues for the scripts to replicate their results. This work was supported in part by a Google Faculty Research Award and NSF grant #IIS-0964102. Any opinions, findings, conclusions or recommendations expressed in this publication are those of the authors and do not necessarily reflect the views of the NSF.</bodyText>
<table confidence="0.917066461538461">
Smoothing M Less heavily PPL pruned model Moderately PPL pruned RS ngrams Full model FP WER
Method D ngrams WER RS ngrams model (×106) PPL RS
C (×106) FP (×106) WER
FP
Abs. N 1.53 146.6 18.1 17.9 5.19 129.1 17.0 16.6 31.1 120.4 16.2 16.1
Disc. Y 141.2 17.2 17.2 126.3 16.6 16.6 31.1 117.0 16.0 16.0
Witten- N 1.54 145.8 18.1 17.6 5.08 129.4 17.3 16.8 31.1 118.7 16.3 16.1
Bell Y 139.7 17.9 17.4 126.4 18.4 17.3 31.1 118.4 18.1 17.6
Katz N 1.57 146.6 17.8 17.7 5.10 128.9 16.8 16.6 31.1 119.7 16.2 16.1
Y 141.1 17.3 17.3 125.7 16.6 16.6 31.1 114.7 16.2 16.1
Mixture N 1.55 145.5 18.1 17.7 5.11 128.2 16.9 16.6 31.1 118.5 16.3 16.1
Y 139.2 17.3 17.2 123.6 16.6 16.4 31.1 114.6 17.3 16.4
Kneser-Ney backoff model, unpruned: 31.1 114.4 15.8 15.9
</table>
<tableCaption confidence="0.98461">
Table 5: Perplexity (PPL) and both first pass (FP) and rescoring (RS) WER reductions for less heavily pruned models using
marginal distribution constraints (MDC).
</tableCaption>
<page confidence="0.998906">
51
</page>
<sectionHeader confidence="0.99834" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99988512987013">
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of the Twelfth International
Conference on Implementation and Application of
Automata (CIAA 2007), Lecture Notes in Computer
Science, volume 4793, pages 11–23.
Ciprian Chelba, Thorsten Brants, Will Neveitt, and
Peng Xu. 2010. Study on interaction between en-
tropy pruning and Kneser-Ney smoothing. In Pro-
ceedings of Interspeech, page 24222425.
Stanley Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical Report, TR-10-98, Harvard
University.
Joshua Goodman. 2001. A bit of progress in lan-
guage modeling. Computer Speech and Language,
15(4):403–434.
Slava M. Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recogniser. IEEE Transactions on Acoustic,
Speech, and Signal Processing, 35(3):400–401.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP), pages
181–184.
Hermann Ney, Ute Essen, and Reinhard Kneser. 1994.
On structuring probabilistic dependences in stochas-
tic language modeling. Computer Speech and Lan-
guage, 8:1–38.
Eric S. Ristad. 1995. A natural law of succession.
Technical Report, CS-TR-495-95, Princeton Univer-
sity.
Brian Roark, Richard Sproat, Cyril Allauzen, Michael
Riley, Jeffrey Sorensen, and Terry Tai. 2012. The
OpenGrm open-source finite-state grammar soft-
ware libraries. In Proceedings of the ACL 2012 Sys-
tem Demonstrations, pages 61–66.
Kristie Seymore and Ronald Rosenfeld. 1996. Scal-
able backoff language models. In Proceedings of
the International Conference on Spoken Language
Processing (ICSLP).
Vesa Siivola, Teemu Hirsimaki, and Sami Virpioja.
2007. On growing and pruning kneserney smoothed
n-gram models. IEEE Transactions on Audio,
Speech, and Language Processing, 15(5):1617–
1624.
William J Stewart. 1999. Numerical methods for com-
puting stationary distributions of finite irreducible
markov chains. Computational Probability, pages
81–111.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. Srilm at sixteen: Update and out-
look. In Proceedings of the IEEE Automatic Speech
Recognition and Understanding Workshop (ASRU).
Andreas Stolcke. 1998. Entropy-based pruning of
backoff language models. In Proc. DARPA Broad-
cast News Transcription and Understanding Work-
shop, pages 270–274.
David Talbot and Thorsten Brants. 2008. Randomized
language models via perfect hash functions. In Pro-
ceedings of ACL-08: HLT, pages 505–513.
David Talbot and Miles Osborne. 2007. Smoothed
Bloom filter language models: Tera-scale LMs on
the cheap. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 468–476.
Ian H. Witten and Timothy C. Bell. 1991. The zero-
frequency problem: Estimating the probabilities of
novel events in adaptive text compression. IEEE
Transactions on Information Theory, 37(4):1085–
1094.
A. Yeh. 2000. More accurate tests for the statistical
significance of result differences. In Proceedings of
the 18th International COLING, pages 947–953.
</reference>
<page confidence="0.998825">
52
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant confidence="0.295008" no="0">
<title confidence="0.858114">Smoothed marginal distribution constraints for language modeling Cyril and Michael</title>
<author confidence="0.669242">Health</author>
<author confidence="0.669242">Science University</author>
<author confidence="0.669242">Oregon Inc Portland</author>
<author confidence="0.669242">New</author>
<abstract confidence="0.9876465">We present an algorithm for re-estimating parameters of backoff n-gram language models so as to preserve given marginal distributions, along the lines of wellknown Kneser-Ney (1995) smoothing. Unlike Kneser-Ney, our approach is designed to be applied to any given smoothed backoff model, including models that have already been heavily pruned. As a result, the algorithm avoids issues observed when pruning Kneser-Ney models (Siivola et al., 2007; Chelba et al., 2010), while retaining the benefits of such marginal distribution constraints. We present experimental results for heavily pruned backoff ngram models, and demonstrate perplexity and word error rate reductions when used with various baseline smoothing methods. An open-source version of the algorithm</abstract>
<intro confidence="0.674597">has been released as part of the OpenGrm</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Michael Riley</author>
<author>Johan Schalkwyk</author>
<author>Wojciech Skut</author>
<author>Mehryar Mohri</author>
</authors>
<title>OpenFst: A general and efficient weighted finite-state transducer library.</title>
<date>2007</date>
<booktitle>In Proceedings of the Twelfth International Conference on Implementation and Application of Automata (CIAA 2007), Lecture Notes in Computer Science,</booktitle>
<volume>4793</volume>
<pages>11--23</pages>
<contexts>
<context citStr="Allauzen et al., 2007" endWordPosition="5005" position="29216" startWordPosition="5002">Table 1 – but, as argued earlier in this paper, this is not the sole reason for the degradation and the perplexity remains extremely inflated. We follow Chelba et al. (2010) in training and test set definition, vocabulary size, and parameters for reporting perplexity. Note that unigrams in the models are never pruned, hence all models assign probabilities over an identical vocabulary and perplexity is comparable across models. For all results reported here, we use the SRILM toolkit for baseline model training and pruning, then convert from the resulting ARPA format model to an OpenFst format (Allauzen et al., 2007), as used in the OpenGrm n-gram library (Roark et al., 2012). We then apply the marginal distribution constraints, and convert the result back to ARPA format for perplexity evaluation with the SRILM toolkit. All models are subjected to full normalization sanity checks, as with typical model functions in the OpenGrm library. Recall that our algorithm assumes that, for every n-gram in the model, all prefix and suffix ngrams are also in the model. For pruned models, the SRILM toolkit does not impose such a requirement, hence explicit arcs are added to the 49 Perplexity Smoothing Pruned Perplexity</context>
</contexts>
<marker>Allauzen, Riley, Schalkwyk, Skut, Mohri, 2007</marker>
<rawString>Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri. 2007. OpenFst: A general and efficient weighted finite-state transducer library. In Proceedings of the Twelfth International Conference on Implementation and Application of Automata (CIAA 2007), Lecture Notes in Computer Science, volume 4793, pages 11–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Thorsten Brants</author>
<author>Will Neveitt</author>
<author>Peng Xu</author>
</authors>
<title>Study on interaction between entropy pruning and Kneser-Ney smoothing.</title>
<date>2010</date>
<booktitle>In Proceedings of Interspeech,</booktitle>
<pages>24222425</pages>
<contexts>
<context citStr="Chelba et al., 2010" endWordPosition="99" position="717" startWordPosition="96">nd Michael Riley◦ †Oregon Health &amp; Science University, Portland, Oregon ◦Google, Inc., New York roarkbr@gmail.com, {allauzen,riley}@google.com Abstract We present an algorithm for re-estimating parameters of backoff n-gram language models so as to preserve given marginal distributions, along the lines of wellknown Kneser-Ney (1995) smoothing. Unlike Kneser-Ney, our approach is designed to be applied to any given smoothed backoff model, including models that have already been heavily pruned. As a result, the algorithm avoids issues observed when pruning Kneser-Ney models (Siivola et al., 2007; Chelba et al., 2010), while retaining the benefits of such marginal distribution constraints. We present experimental results for heavily pruned backoff ngram models, and demonstrate perplexity and word error rate reductions when used with various baseline smoothing methods. An open-source version of the algorithm has been released as part of the OpenGrm ngram library.1 1 Introduction Smoothed n-gram language models are the defacto standard statistical models of language for a wide range of natural language applications, including speech recognition and machine translation. Such models are trained on large text c</context>
<context citStr="Chelba et al., 2010" endWordPosition="557" position="3653" startWordPosition="554">n k times in the corpus. Beyond count thresholding, the most widely used pruning methods (Seymore and Rosenfeld, 1996; Stolcke, 1998) employ greedy algorithms to reduce the number of stored n-grams by comparing the stored probabilities to those that would be assigned via the backoff smoothing mechanism, and removing those with the least impact according to some criterion. While these greedy pruning methods are highly effective for models estimated with most common smoothing approaches, they have been shown to be far less effective with Kneser-Ney trained language models (Siivola et al., 2007; Chelba et al., 2010), leading to severe degradation in model quality relative to other standard smoothing meth43 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 43–52, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 4-gram models Backoff n-grams Interpolated n-grams Perplexity Perplexity Smoothing method full pruned (×1000) full pruned (×1000) Absolute Discounting (Ney et al., 1994) 120.5 197.3 383.4 119.8 198.1 386.2 Witten-Bell (Witten and Bell, 1991) 118.8 196.3 380.4 121.6 202.3 396.4 Ristad (1995) 126.4 203.6 395.6 ——- N/A ——- </context>
<context citStr="Chelba et al. (2010)" endWordPosition="786" position="5191" startWordPosition="783">ls in contrast to other common smoothing methods. Data: English Broadcast News, 128M words training; 692K words test; 143K word vocabulary. 4-gram language models, pruned using Stolcke (1998) relative entropy pruning to approximately 1.3% of the original size of 31,095,260 n-grams. ods. Thus, while Kneser-Ney may be the preferred smoothing method for large, unpruned models – where it can achieve real improvements over other smoothing methods – when relatively sparse, pruned models are required, it has severely diminished utility. Table 1 presents a slightly reformatted version of Table 3 from Chelba et al. (2010). In their experiments (see Table 1 caption for specifics on training/test setup), they trained 4-gram Broadcast News language models using a variety of both backoff and interpolated smoothing methods and measured perplexity before and after Stolcke (1998) relative entropy based pruning. With this size training data, the perplexity of all of the smoothing methods other than Kneser-Ney degrades from around 120 with the full model to around 200 with the heavily pruned model. Kneser-Ney smoothed models have lower perplexity with the full model than the other methods by about 5 points, but degrade</context>
<context citStr="Chelba et al. (2010)" endWordPosition="4636" position="26999" startWordPosition="4633"> term and the new term. After the entire model has been re-estimated, the steady state probability calculation presented in Section 4.1 is run again and model estimation happens again. As we shall see in the experimental results, this typically converges after just a few iterations. At this time, we have no convergence proofs for either of these iterative components to the algorithm, but expect that something can be said about this, which will be a priority in future work. 5 Experimental results All results presented here are for English Broadcast News. We received scripts for replicating the Chelba et al. (2010) results from the authors, and we report statistics on our replication of their paper’s results in Table 2. The scripts are distributed in such a way that the user supplies the data from LDC98T31 (1996 CSR HUB4 Language Model corpus) and the script breaks the collection into training and testing sets, normalizes the text, and Smoothing full Perplexity n-grams (×1000) method pruned model diff Abs.Disc. 120.4 197.1 382.3 -1.1 Witten-Bell 118.7 196.1 379.3 -1.1 Ristad 126.2 203.4 394.6 -1.1 Katz 119.7 197.9 385.1 -1.1 Kneser-Ney† 114.4 234.1 375.4 -12.7 Table 2: Replication of Chelba et al. (2010</context>
<context citStr="Chelba et al. (2010)" endWordPosition="4868" position="28398" startWordPosition="4865">ry-lm switch in SRILM. trains and prunes the language models using the SRILM toolkit (Stolcke et al., 2011). Presumably due to minor differences in text normalization, resulting in very slightly fewer n-grams in all conditions, we achieve negligibly lower perplexities (one or two tenths of a point) in all conditions, as can be seen when comparing with Table 1. All of the same trends result, thus that paper’s result is successfully replicated here. Note that we ran our Kneser-Ney pruning (noted with a † in the table), using the new -prune-history-lm switch in SRILM – created in response to the Chelba et al. (2010) paper – which allows the use of another model to calculate the state marginals for pruning. This fixes part of the problem – perplexity does not degrade as much as the Kneser-Ney pruned model in Table 1 – but, as argued earlier in this paper, this is not the sole reason for the degradation and the perplexity remains extremely inflated. We follow Chelba et al. (2010) in training and test set definition, vocabulary size, and parameters for reporting perplexity. Note that unigrams in the models are never pruned, hence all models assign probabilities over an identical vocabulary and perplexity is</context>
<context citStr="Chelba et al. (2010)" endWordPosition="5162" position="30174" startWordPosition="5159">our algorithm assumes that, for every n-gram in the model, all prefix and suffix ngrams are also in the model. For pruned models, the SRILM toolkit does not impose such a requirement, hence explicit arcs are added to the 49 Perplexity Smoothing Pruned Perplexity Δ n-grams Method Model Pruned (×1000) +MDC in WFST Abs.Disc. 197.1 187.4 9.7 389.2 Witten-Bell 196.1 185.7 10.4 385.0 Ristad 203.4 190.3 13.1 395.9 Katz 197.9 187.5 10.4 390.8 AD,WB,Katz 196.6 186.3 10.3 388.7 Mixture Table 3: Perplexity reductions achieved with marginal distribution constraints (MDC) on the heavily pruned models from Chelba et al. (2010), and a mixture model. WFST ngram counts are slightly higher than ARPA format in Table 2 due to adding prefix and suffix n-grams. model during conversion, with probability equal to what they would receive in the the original model. The resulting model is equivalent, but with a small number of additional arcs in the explicit representation (around 1% for the most heavily pruned models). Table 3 presents perplexity results for models that result from applying our marginal distribution constraints to the four heavily pruned models from Table 2. In all four cases, we get perplexity reductions of a</context>
<context citStr="Chelba et al. (2010)" endWordPosition="5881" position="34603" startWordPosition="5878">space purposes, we exclude the Ristad method from this point forward since it is not competitive with the others. 205 Witten−Bell Ristad Katz Absolute Discounting WB,AD,Katz mixture 195 190 185 180 0 1 2 3 4 5 6 200 50 Smoothing First Word rate (WER) Pruned Method Pruned error Rescoring +MDC Model pass Pruned Pruned Model +MDC Abs.Disc. 20.5 19.7 20.2 19.6 Witten-Bell 20.5 19.9 20.1 19.6 Katz 20.5 19.7 20.2 19.7 Mixture 20.5 19.6 20.2 19.6 Kneser-Neya 22.1 22.2 Kneser-Neyb 20.5 20.6 Table 4: WER reductions achieved with marginal distribution constraints (MDC) on the heavily pruned models from Chelba et al. (2010), and a mixture model. KneserNey results are shown for: a) original pruning; and b) with -prune-history-lm switch. The perplexity reductions that were achieved for these models do translate to real word error rate reductions at both stages of between 0.5 and 0.9 percent absolute. All of these gains are statistically significant at p &lt; 0.0001 using the stratified shuffling test (Yeh, 2000). For pruned Kneser-Ney models, fixing the state marginals with the -prune-history-lm switch reduces the WER versus the original pruned model, but no reductions were achieved vs. baseline methods. Table 5 pres</context>
</contexts>
<marker>Chelba, Brants, Neveitt, Xu, 2010</marker>
<rawString>Ciprian Chelba, Thorsten Brants, Will Neveitt, and Peng Xu. 2010. Study on interaction between entropy pruning and Kneser-Ney smoothing. In Proceedings of Interspeech, page 24222425.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report, TR-10-98,</tech>
<institution>Harvard University.</institution>
<contexts>
<context citStr="Chen and Goodman, 1998" endWordPosition="667" position="4405" startWordPosition="664">ng of the Association for Computational Linguistics, pages 43–52, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 4-gram models Backoff n-grams Interpolated n-grams Perplexity Perplexity Smoothing method full pruned (×1000) full pruned (×1000) Absolute Discounting (Ney et al., 1994) 120.5 197.3 383.4 119.8 198.1 386.2 Witten-Bell (Witten and Bell, 1991) 118.8 196.3 380.4 121.6 202.3 396.4 Ristad (1995) 126.4 203.6 395.6 ——- N/A ——- Katz (1987) 119.8 198.1 386.2 ——- N/A ——- Kneser-Ney (Kneser and Ney, 1995) 114.5 285.1 388.2 115.8 274.3 398.7 Mod. Kneser-Ney (Chen and Goodman, 1998) 116.3 280.6 396.2 112.8 270.7 399.1 Table 1: Reformatted version of Table 3 in Chelba et al. (2010), demonstrating perplexity degradation of Kneser-Ney smoothed models in contrast to other common smoothing methods. Data: English Broadcast News, 128M words training; 692K words test; 143K word vocabulary. 4-gram language models, pruned using Stolcke (1998) relative entropy pruning to approximately 1.3% of the original size of 31,095,260 n-grams. ods. Thus, while Kneser-Ney may be the preferred smoothing method for large, unpruned models – where it can achieve real improvements over other smooth</context>
<context citStr="Chen and Goodman, 1998" endWordPosition="2567" position="15291" startWordPosition="2564">ubstitute α(h, hwG) ,6(hwGw) for P(w |h): P(w |h0) X P(g) = X P(w |h) P(h) g∈V n−kh' h∈V n−kh' α(hki , hk−m i ) = m H j=1 P(h, w |h0) P(w |h, h0) P(h |h0) P(w |h) P(h) X P(g) g∈V n−kh' 1 = X � P(w |h0) = P(g, w |h0) (3) X= α(h, hwG) O(hwGw) P(h) (6) h∈V n−kh' g:g0=h0 where P is the empirical relative frequency estimate. Taking this approach, certain base smoothing methods end up with very nice, easy to calculate solutions based on counts. Absolute discounting (Ney et al., 1994) in particular, using the above approach, leads to the well-known KneserNey smoothing approach (Kneser and Ney, 1995; Chen and Goodman, 1998). We will follow this same approach, with a couple of changes. First, we will make use of regularized estimates of relative frequency P rather than raw relative frequency P. Second, rather than just looking at observed histories h that back off to h0, we will look at all histories (observed or not) of the length of the longest history in the model. For notational simplicity, suppose we have an n+1-gram model, Note that we are only interested in h0w E G, hence there are two disjoint subsets of histories h E Vn−kh0 that are being summed over: those such that hwG = h0 and those such that |hwG |&gt; </context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report, TR-10-98, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>A bit of progress in language modeling.</title>
<date>2001</date>
<journal>Computer Speech and Language,</journal>
<volume>15</volume>
<issue>4</issue>
<contexts>
<context citStr="Goodman (2001)" endWordPosition="982" position="6466" startWordPosition="981">cause of this degradation is Kneser-Ney’s unique method for estimating smoothed language models, which will be presented in more detail in Section 3. Briefly, the smoothing method reestimates lower-order n-gram parameters in order to avoid over-estimating the likelihood of n-grams that already have ample probability mass allocated as part of higher-order n-grams. This is done via a marginal distribution constraint which requires the expected frequency of the lower-order n-grams to match their observed frequency in the training data, much as is commonly done for maximum entropy model training. Goodman (2001) proved that, under certain assumptions, such constraints can only improve language models. Lower-order n-gram parameters resulting from Kneser-Ney are not relative frequency estimates, as with other smoothing methods; rather they are parameters estimated specifically for use within the larger smoothed model. There are (at least) a couple of reasons why such parameters do not play well with model pruning. First, the pruning methods commonly use lower order n-gram probabilities to derive an estimate of state marginals, and, since these parameters are no longer smoothed relative frequency estima</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>Joshua Goodman. 2001. A bit of progress in language modeling. Computer Speech and Language, 15(4):403–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recogniser.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustic, Speech, and Signal Processing,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context citStr="Katz (1987)" endWordPosition="644" position="4264" startWordPosition="643">, leading to severe degradation in model quality relative to other standard smoothing meth43 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 43–52, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 4-gram models Backoff n-grams Interpolated n-grams Perplexity Perplexity Smoothing method full pruned (×1000) full pruned (×1000) Absolute Discounting (Ney et al., 1994) 120.5 197.3 383.4 119.8 198.1 386.2 Witten-Bell (Witten and Bell, 1991) 118.8 196.3 380.4 121.6 202.3 396.4 Ristad (1995) 126.4 203.6 395.6 ——- N/A ——- Katz (1987) 119.8 198.1 386.2 ——- N/A ——- Kneser-Ney (Kneser and Ney, 1995) 114.5 285.1 388.2 115.8 274.3 398.7 Mod. Kneser-Ney (Chen and Goodman, 1998) 116.3 280.6 396.2 112.8 270.7 399.1 Table 1: Reformatted version of Table 3 in Chelba et al. (2010), demonstrating perplexity degradation of Kneser-Ney smoothed models in contrast to other common smoothing methods. Data: English Broadcast News, 128M words training; 692K words test; 143K word vocabulary. 4-gram language models, pruned using Stolcke (1998) relative entropy pruning to approximately 1.3% of the original size of 31,095,260 n-grams. ods. Thus,</context>
<context citStr="Katz (1987)" endWordPosition="1645" position="10332" startWordPosition="1644">0 { α(hki ) P(wi |hi ) otherwise where c(hki wi) is the count of the n-gram sequence wi_k ... wi in the training corpus; P is a regularized probability estimate that provides some probability mass for unobserved n-grams; and α(hki ) is a factor that ensures normalization. Note that for h = hki , the typically defined backoff history h' = hk_1 i , i.e., the longest suffix of h that is not h itself. When we use h' and g' (for notational convenience) in future equations, it is this definition that we are using. There are many ways to estimate P, including absolute discounting (Ney et al., 1994), Katz (1987) and Witten and Bell (1991). Interpolated models are special cases of this form, where the P is determined using model mixing, and the α parameter is exactly the mixing factor value for the lower order model. N-gram language models allow for a sparse representation, so that only a subset of the possible ngrams must be explicitly stored. Probabilities for the rest of the n-grams are calculated through the “otherwise” semantics in the equation above. For an n-gram language model G, we will say that an n-gram hw E G if it is explicitly represented in the model; otherwise hw E� G. In the standard </context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Slava M. Katz. 1987. Estimation of probabilities from sparse data for the language model component of a speech recogniser. IEEE Transactions on Acoustic, Speech, and Signal Processing, 35(3):400–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP),</booktitle>
<pages>181--184</pages>
<contexts>
<context citStr="Kneser and Ney, 1995" endWordPosition="655" position="4328" startWordPosition="652">ive to other standard smoothing meth43 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 43–52, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 4-gram models Backoff n-grams Interpolated n-grams Perplexity Perplexity Smoothing method full pruned (×1000) full pruned (×1000) Absolute Discounting (Ney et al., 1994) 120.5 197.3 383.4 119.8 198.1 386.2 Witten-Bell (Witten and Bell, 1991) 118.8 196.3 380.4 121.6 202.3 396.4 Ristad (1995) 126.4 203.6 395.6 ——- N/A ——- Katz (1987) 119.8 198.1 386.2 ——- N/A ——- Kneser-Ney (Kneser and Ney, 1995) 114.5 285.1 388.2 115.8 274.3 398.7 Mod. Kneser-Ney (Chen and Goodman, 1998) 116.3 280.6 396.2 112.8 270.7 399.1 Table 1: Reformatted version of Table 3 in Chelba et al. (2010), demonstrating perplexity degradation of Kneser-Ney smoothed models in contrast to other common smoothing methods. Data: English Broadcast News, 128M words training; 692K words test; 143K word vocabulary. 4-gram language models, pruned using Stolcke (1998) relative entropy pruning to approximately 1.3% of the original size of 31,095,260 n-grams. ods. Thus, while Kneser-Ney may be the preferred smoothing method for larg</context>
<context citStr="Kneser and Ney (1995)" endWordPosition="2278" position="13827" startWordPosition="2275">self or it could be E. Then P(w |h) = α(h, hwG) ,6(hwGw) (2) which is equivalent to equation 1. 3 Marginal distribution constraints Marginal distribution constraints attempt to match the expected frequency of an n-gram with its observed frequency. In other words, if we use the model to randomly generate a very large corpus, the n-grams should occur with the same relative frequency in both the generated and original (training) corpus. Standard smoothing methods overgenerate lower-order n-grams. Using standard n-gram notation (where g0 is the backoff history for g), this constraint is stated in Kneser and Ney (1995) as hence the longest history in the model is of length n. Assume the length of the particular backoff history |h0 |= k. Let V n−kh0 be the set of strings h E V n with h0 as a suffix. Then we can restate the marginal distribution constraint in equation 3 as P(w |h0) = X P(h, w |h0) (4) h∈V n−kh' Next we solve for ,6(h0w) parameters used in equation 1. Note that h0 is a suffix of any h E V n−kh0, so conditioning probabilities on h and h0 is the same as conditioning on just h. Each of the following derivation steps simply relies on the chain rule or definition of conditional probability, as well</context>
<context citStr="Kneser and Ney, 1995" endWordPosition="2563" position="15266" startWordPosition="2560"> using equation 2 to substitute α(h, hwG) ,6(hwGw) for P(w |h): P(w |h0) X P(g) = X P(w |h) P(h) g∈V n−kh' h∈V n−kh' α(hki , hk−m i ) = m H j=1 P(h, w |h0) P(w |h, h0) P(h |h0) P(w |h) P(h) X P(g) g∈V n−kh' 1 = X � P(w |h0) = P(g, w |h0) (3) X= α(h, hwG) O(hwGw) P(h) (6) h∈V n−kh' g:g0=h0 where P is the empirical relative frequency estimate. Taking this approach, certain base smoothing methods end up with very nice, easy to calculate solutions based on counts. Absolute discounting (Ney et al., 1994) in particular, using the above approach, leads to the well-known KneserNey smoothing approach (Kneser and Ney, 1995; Chen and Goodman, 1998). We will follow this same approach, with a couple of changes. First, we will make use of regularized estimates of relative frequency P rather than raw relative frequency P. Second, rather than just looking at observed histories h that back off to h0, we will look at all histories (observed or not) of the length of the longest history in the model. For notational simplicity, suppose we have an n+1-gram model, Note that we are only interested in h0w E G, hence there are two disjoint subsets of histories h E Vn−kh0 that are being summed over: those such that hwG = h0 and</context>
<context citStr="Kneser and Ney (1995)" endWordPosition="2896" position="17136" startWordPosition="2893">; and summing over all history substrings in Vn−kh' rather than just those with count greater than zero, which is also a change due to smoothing. Keep in mind, P is the target expected frequency from a given smoothed model. KneserNey models are not useful input models, since their P n-gram parameters are not relative frequency estimates. This means that we cannot simply ‘repair’ pruned Kneser-Ney models, but must use other smoothing methods where the smoothed values are based on relative frequency estimation. There are, in addition, two other important differences in our approach from that in Kneser and Ney (1995), which would remain as differences even if our target expected frequency were the unsmoothed relative frequency P instead of the smoothed estimate P. First, the sum in the numerator is over histories of length n, the highest order in the n-gram model, whereas in the KneserNey approach the sum is over histories that immediately back off to h', i.e., from the next highest order in the n-gram model. Thus the unigram distribution is with respect to the bigram model, the bigram model is with respect to the trigram model, and so forth. In our optimization, we sum instead over all possible history s</context>
<context citStr="Kneser and Ney (1995)" endWordPosition="5543" position="32523" startWordPosition="5540"> do not iteratively update the backoff weights when reestimating the weights, the ‘Pruned+MDC’ perplexities in Table 3 increase by between 0.2–0.4 points. Hence, iterating the steady state probability calculation is quite important, as illustrated by Figure 3; iterating the Iterations of estimation (recalculating steady state probs) Figure 3: Models resulting from different numbers of parameter re-estimation iterations. Iteration 0 is the baseline pruned model. denominator calculation much less so, at least for these models. We noted in Section 3 that a key difference between our approach and Kneser and Ney (1995) is that their approach treated the denominator as a constant. If we do this, the ‘Pruned+MDC’ perplexities increase by between 4.5–5.6 points, i.e., about half of the perplexity reduction is lost for each method. Thus, while iteration of denominator calculation may not be critical, it should not be treated as a constant. We now look at the impacts on system performance we can achieve with these new models4, and whether the perplexity differences that we observe translate to real error rate reductions. For automatic speech recognition experiments, we used as test set the 1997 Hub4 evaluation s</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pages 181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermann Ney</author>
<author>Ute Essen</author>
<author>Reinhard Kneser</author>
</authors>
<title>On structuring probabilistic dependences in stochastic language modeling. Computer Speech and Language,</title>
<date>1994</date>
<pages>8--1</pages>
<contexts>
<context citStr="Ney et al., 1994" endWordPosition="617" position="4100" startWordPosition="614">d with most common smoothing approaches, they have been shown to be far less effective with Kneser-Ney trained language models (Siivola et al., 2007; Chelba et al., 2010), leading to severe degradation in model quality relative to other standard smoothing meth43 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 43–52, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 4-gram models Backoff n-grams Interpolated n-grams Perplexity Perplexity Smoothing method full pruned (×1000) full pruned (×1000) Absolute Discounting (Ney et al., 1994) 120.5 197.3 383.4 119.8 198.1 386.2 Witten-Bell (Witten and Bell, 1991) 118.8 196.3 380.4 121.6 202.3 396.4 Ristad (1995) 126.4 203.6 395.6 ——- N/A ——- Katz (1987) 119.8 198.1 386.2 ——- N/A ——- Kneser-Ney (Kneser and Ney, 1995) 114.5 285.1 388.2 115.8 274.3 398.7 Mod. Kneser-Ney (Chen and Goodman, 1998) 116.3 280.6 396.2 112.8 270.7 399.1 Table 1: Reformatted version of Table 3 in Chelba et al. (2010), demonstrating perplexity degradation of Kneser-Ney smoothed models in contrast to other common smoothing methods. Data: English Broadcast News, 128M words training; 692K words test; 143K word v</context>
<context citStr="Ney et al., 1994" endWordPosition="1643" position="10319" startWordPosition="1640">) 1 if c(hki wi) &gt; 0 { α(hki ) P(wi |hi ) otherwise where c(hki wi) is the count of the n-gram sequence wi_k ... wi in the training corpus; P is a regularized probability estimate that provides some probability mass for unobserved n-grams; and α(hki ) is a factor that ensures normalization. Note that for h = hki , the typically defined backoff history h' = hk_1 i , i.e., the longest suffix of h that is not h itself. When we use h' and g' (for notational convenience) in future equations, it is this definition that we are using. There are many ways to estimate P, including absolute discounting (Ney et al., 1994), Katz (1987) and Witten and Bell (1991). Interpolated models are special cases of this form, where the P is determined using model mixing, and the α parameter is exactly the mixing factor value for the lower order model. N-gram language models allow for a sparse representation, so that only a subset of the possible ngrams must be explicitly stored. Probabilities for the rest of the n-grams are calculated through the “otherwise” semantics in the equation above. For an n-gram language model G, we will say that an n-gram hw E G if it is explicitly represented in the model; otherwise hw E� G. In </context>
<context citStr="Ney et al., 1994" endWordPosition="2545" position="15150" startWordPosition="2542">) P(g) h∈V n−kh' g∈V n−kh' Then, multiplying both sides by the normalizing denominator on the right-hand side and using equation 2 to substitute α(h, hwG) ,6(hwGw) for P(w |h): P(w |h0) X P(g) = X P(w |h) P(h) g∈V n−kh' h∈V n−kh' α(hki , hk−m i ) = m H j=1 P(h, w |h0) P(w |h, h0) P(h |h0) P(w |h) P(h) X P(g) g∈V n−kh' 1 = X � P(w |h0) = P(g, w |h0) (3) X= α(h, hwG) O(hwGw) P(h) (6) h∈V n−kh' g:g0=h0 where P is the empirical relative frequency estimate. Taking this approach, certain base smoothing methods end up with very nice, easy to calculate solutions based on counts. Absolute discounting (Ney et al., 1994) in particular, using the above approach, leads to the well-known KneserNey smoothing approach (Kneser and Ney, 1995; Chen and Goodman, 1998). We will follow this same approach, with a couple of changes. First, we will make use of regularized estimates of relative frequency P rather than raw relative frequency P. Second, rather than just looking at observed histories h that back off to h0, we will look at all histories (observed or not) of the length of the longest history in the model. For notational simplicity, suppose we have an n+1-gram model, Note that we are only interested in h0w E G, h</context>
</contexts>
<marker>Ney, Essen, Kneser, 1994</marker>
<rawString>Hermann Ney, Ute Essen, and Reinhard Kneser. 1994. On structuring probabilistic dependences in stochastic language modeling. Computer Speech and Language, 8:1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric S Ristad</author>
</authors>
<title>A natural law of succession.</title>
<date>1995</date>
<tech>Technical Report, CS-TR-495-95,</tech>
<institution>Princeton University.</institution>
<contexts>
<context citStr="Ristad (1995)" endWordPosition="636" position="4222" startWordPosition="635"> (Siivola et al., 2007; Chelba et al., 2010), leading to severe degradation in model quality relative to other standard smoothing meth43 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 43–52, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 4-gram models Backoff n-grams Interpolated n-grams Perplexity Perplexity Smoothing method full pruned (×1000) full pruned (×1000) Absolute Discounting (Ney et al., 1994) 120.5 197.3 383.4 119.8 198.1 386.2 Witten-Bell (Witten and Bell, 1991) 118.8 196.3 380.4 121.6 202.3 396.4 Ristad (1995) 126.4 203.6 395.6 ——- N/A ——- Katz (1987) 119.8 198.1 386.2 ——- N/A ——- Kneser-Ney (Kneser and Ney, 1995) 114.5 285.1 388.2 115.8 274.3 398.7 Mod. Kneser-Ney (Chen and Goodman, 1998) 116.3 280.6 396.2 112.8 270.7 399.1 Table 1: Reformatted version of Table 3 in Chelba et al. (2010), demonstrating perplexity degradation of Kneser-Ney smoothed models in contrast to other common smoothing methods. Data: English Broadcast News, 128M words training; 692K words test; 143K word vocabulary. 4-gram language models, pruned using Stolcke (1998) relative entropy pruning to approximately 1.3% of the origi</context>
</contexts>
<marker>Ristad, 1995</marker>
<rawString>Eric S. Ristad. 1995. A natural law of succession. Technical Report, CS-TR-495-95, Princeton University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Richard Sproat</author>
<author>Cyril Allauzen</author>
<author>Michael Riley</author>
<author>Jeffrey Sorensen</author>
<author>Terry Tai</author>
</authors>
<title>The OpenGrm open-source finite-state grammar software libraries.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACL 2012 System Demonstrations,</booktitle>
<pages>61--66</pages>
<contexts>
<context citStr="Roark et al., 2012" endWordPosition="1895" position="11654" startWordPosition="1892">ith pruning, we remove many observed n-grams from the model, hence this is no longer the appropriate criterion. We reformulate the standard equation as follows: P(wi|hki ) _ J β(hki wi) if hki wi ∈ G 1 l α(hki , hi−1) P(wi|hi otherwise ( ) where O(hki wi) is the parameter associated with the n-gram hki wi and α(hki , hk_1 i ) is the backoff cost associated with going from state hki to state hk_1 i . We assume that, if hw E G then all prefixes and suffixes of hw are also in G. Figure 1 presents a schema of an automaton representation of an n-gram model, of the sort used in the OpenGrm library (Roark et al., 2012). States represent histories h, and the words w, whose probabilities are conditioned on h, label the arcs, leading to the history state for the subsequent word. State labels are provided in Figure 1 as a convenience, to show the (implicit) history encoded by the state, e.g., ‘xyz’ indicates that the state represents a history with the previous three symbols being x, y and z. Failure arcs, labeled with a φ in Figure 1, encode an “otherwise” semantics and have as destination the origin state’s backoff history. Many higher order states will back off to the same lower order state, specifically tho</context>
<context citStr="Roark et al., 2012" endWordPosition="5016" position="29276" startWordPosition="5013"> sole reason for the degradation and the perplexity remains extremely inflated. We follow Chelba et al. (2010) in training and test set definition, vocabulary size, and parameters for reporting perplexity. Note that unigrams in the models are never pruned, hence all models assign probabilities over an identical vocabulary and perplexity is comparable across models. For all results reported here, we use the SRILM toolkit for baseline model training and pruning, then convert from the resulting ARPA format model to an OpenFst format (Allauzen et al., 2007), as used in the OpenGrm n-gram library (Roark et al., 2012). We then apply the marginal distribution constraints, and convert the result back to ARPA format for perplexity evaluation with the SRILM toolkit. All models are subjected to full normalization sanity checks, as with typical model functions in the OpenGrm library. Recall that our algorithm assumes that, for every n-gram in the model, all prefix and suffix ngrams are also in the model. For pruned models, the SRILM toolkit does not impose such a requirement, hence explicit arcs are added to the 49 Perplexity Smoothing Pruned Perplexity Δ n-grams Method Model Pruned (×1000) +MDC in WFST Abs.Disc</context>
</contexts>
<marker>Roark, Sproat, Allauzen, Riley, Sorensen, Tai, 2012</marker>
<rawString>Brian Roark, Richard Sproat, Cyril Allauzen, Michael Riley, Jeffrey Sorensen, and Terry Tai. 2012. The OpenGrm open-source finite-state grammar software libraries. In Proceedings of the ACL 2012 System Demonstrations, pages 61–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristie Seymore</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>Scalable backoff language models.</title>
<date>1996</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing (ICSLP).</booktitle>
<contexts>
<context citStr="Seymore and Rosenfeld, 1996" endWordPosition="477" position="3150" startWordPosition="474">it efficient querying for specific n-grams in a model that has been stored in a fraction of the space required to store the full, exact model, though with some probability of false positives. Another common approach – which we pursue in this paper – is model pruning, whereby some number of the n-grams are removed from explicit storage in the model, so that their probability must be assigned via backoff smoothing. One simple pruning method is count thresholding, i.e., discarding n-grams that occur less than k times in the corpus. Beyond count thresholding, the most widely used pruning methods (Seymore and Rosenfeld, 1996; Stolcke, 1998) employ greedy algorithms to reduce the number of stored n-grams by comparing the stored probabilities to those that would be assigned via the backoff smoothing mechanism, and removing those with the least impact according to some criterion. While these greedy pruning methods are highly effective for models estimated with most common smoothing approaches, they have been shown to be far less effective with Kneser-Ney trained language models (Siivola et al., 2007; Chelba et al., 2010), leading to severe degradation in model quality relative to other standard smoothing meth43 Proc</context>
</contexts>
<marker>Seymore, Rosenfeld, 1996</marker>
<rawString>Kristie Seymore and Ronald Rosenfeld. 1996. Scalable backoff language models. In Proceedings of the International Conference on Spoken Language Processing (ICSLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vesa Siivola</author>
<author>Teemu Hirsimaki</author>
<author>Sami Virpioja</author>
</authors>
<title>On growing and pruning kneserney smoothed n-gram models.</title>
<date>2007</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>15</volume>
<issue>5</issue>
<pages>1624</pages>
<contexts>
<context citStr="Siivola et al., 2007" endWordPosition="95" position="695" startWordPosition="92">k†◦, Cyril Allauzen◦ and Michael Riley◦ †Oregon Health &amp; Science University, Portland, Oregon ◦Google, Inc., New York roarkbr@gmail.com, {allauzen,riley}@google.com Abstract We present an algorithm for re-estimating parameters of backoff n-gram language models so as to preserve given marginal distributions, along the lines of wellknown Kneser-Ney (1995) smoothing. Unlike Kneser-Ney, our approach is designed to be applied to any given smoothed backoff model, including models that have already been heavily pruned. As a result, the algorithm avoids issues observed when pruning Kneser-Ney models (Siivola et al., 2007; Chelba et al., 2010), while retaining the benefits of such marginal distribution constraints. We present experimental results for heavily pruned backoff ngram models, and demonstrate perplexity and word error rate reductions when used with various baseline smoothing methods. An open-source version of the algorithm has been released as part of the OpenGrm ngram library.1 1 Introduction Smoothed n-gram language models are the defacto standard statistical models of language for a wide range of natural language applications, including speech recognition and machine translation. Such models are t</context>
<context citStr="Siivola et al., 2007" endWordPosition="553" position="3631" startWordPosition="550">ms that occur less than k times in the corpus. Beyond count thresholding, the most widely used pruning methods (Seymore and Rosenfeld, 1996; Stolcke, 1998) employ greedy algorithms to reduce the number of stored n-grams by comparing the stored probabilities to those that would be assigned via the backoff smoothing mechanism, and removing those with the least impact according to some criterion. While these greedy pruning methods are highly effective for models estimated with most common smoothing approaches, they have been shown to be far less effective with Kneser-Ney trained language models (Siivola et al., 2007; Chelba et al., 2010), leading to severe degradation in model quality relative to other standard smoothing meth43 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 43–52, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 4-gram models Backoff n-grams Interpolated n-grams Perplexity Perplexity Smoothing method full pruned (×1000) full pruned (×1000) Absolute Discounting (Ney et al., 1994) 120.5 197.3 383.4 119.8 198.1 386.2 Witten-Bell (Witten and Bell, 1991) 118.8 196.3 380.4 121.6 202.3 396.4 Ristad (1995) 126.4 20</context>
</contexts>
<marker>Siivola, Hirsimaki, Virpioja, 2007</marker>
<rawString>Vesa Siivola, Teemu Hirsimaki, and Sami Virpioja. 2007. On growing and pruning kneserney smoothed n-gram models. IEEE Transactions on Audio, Speech, and Language Processing, 15(5):1617– 1624.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William J Stewart</author>
</authors>
<title>Numerical methods for computing stationary distributions of finite irreducible markov chains. Computational Probability,</title>
<date>1999</date>
<pages>81--111</pages>
<contexts>
<context citStr="Stewart, 1999" endWordPosition="3566" position="20955" startWordPosition="3565">. In Markov chain terms, the corpus distribution is made irreducible to allow a non-trivial stationary distribution. � P(h) = lim m-+oo w1...wm 47 which all &lt;/s&gt; arcs enter, adding a A weighted c arc from the &lt;/s&gt; state to the initial state and having a final weight 1 − A in order to leave the automaton at the &lt;/s&gt; state will model this corpus distribution. According to Eq. 9, P(h) is then the stationary distribution of the finite irreducible Markov Chain defined by this altered automaton. There are many methods for computing such a stationary distribution; we use the well-known power method (Stewart, 1999). One difficulty remains to be resolved. The backoff arcs have a special interpretation in the automaton: they are traversed only if a word fails to match at the higher order. These failure arcs must be properly handled before applying standard stationary distribution calculations. A simple approach would be for each word w' and state h such that hw' ∈/ G, but h'w' ∈ G, add a w' arc from state h to h'w' with weight α(h, h')β(h'w') and then remove all failure arcs (see Figure 2a). This however results in an automaton with |V |arcs leaving every state, which is unwieldy with larger vocabularies </context>
</contexts>
<marker>Stewart, 1999</marker>
<rawString>William J Stewart. 1999. Numerical methods for computing stationary distributions of finite irreducible markov chains. Computational Probability, pages 81–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Jing Zheng</author>
<author>Wen Wang</author>
<author>Victor Abrash</author>
</authors>
<title>Srilm at sixteen: Update and outlook.</title>
<date>2011</date>
<booktitle>In Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU).</booktitle>
<contexts>
<context citStr="Stolcke et al., 2011" endWordPosition="1105" position="7288" startWordPosition="1102">moothing methods; rather they are parameters estimated specifically for use within the larger smoothed model. There are (at least) a couple of reasons why such parameters do not play well with model pruning. First, the pruning methods commonly use lower order n-gram probabilities to derive an estimate of state marginals, and, since these parameters are no longer smoothed relative frequency estimates, they do not serve that purpose well. For this reason, the widely-used SRILM toolkit recently provided switches to modify their pruning algorithm to use another model for state marginal estimates (Stolcke et al., 2011). Second, and perhaps more importantly, the marginal constraints that were applied prior to smoothing will not in general be consistent with the much smaller pruned model. For example, if a bigram parameter is modified due to the presence of some set of trigrams, and then some or all of those trigrams are pruned from the model, the bigram associated with the modified parameter will be unlikely to have an overall expected frequency equal to its observed frequency anymore. As a result, the resulting model degrades dramatically with pruning. In this paper, we present an algorithm that imposes mar</context>
<context citStr="Stolcke et al., 2011" endWordPosition="4779" position="27885" startWordPosition="4776">ction into training and testing sets, normalizes the text, and Smoothing full Perplexity n-grams (×1000) method pruned model diff Abs.Disc. 120.4 197.1 382.3 -1.1 Witten-Bell 118.7 196.1 379.3 -1.1 Ristad 126.2 203.4 394.6 -1.1 Katz 119.7 197.9 385.1 -1.1 Kneser-Ney† 114.4 234.1 375.4 -12.7 Table 2: Replication of Chelba et al. (2010) using provided script. Using the script, the size of the unpruned model is 31,091,219 ngrams, 4,041 fewer than Chelba et al. (2010). † Kneser-Ney model pruned using -prune-history-lm switch in SRILM. trains and prunes the language models using the SRILM toolkit (Stolcke et al., 2011). Presumably due to minor differences in text normalization, resulting in very slightly fewer n-grams in all conditions, we achieve negligibly lower perplexities (one or two tenths of a point) in all conditions, as can be seen when comparing with Table 1. All of the same trends result, thus that paper’s result is successfully replicated here. Note that we ran our Kneser-Ney pruning (noted with a † in the table), using the new -prune-history-lm switch in SRILM – created in response to the Chelba et al. (2010) paper – which allows the use of another model to calculate the state marginals for pru</context>
</contexts>
<marker>Stolcke, Zheng, Wang, Abrash, 2011</marker>
<rawString>Andreas Stolcke, Jing Zheng, Wen Wang, and Victor Abrash. 2011. Srilm at sixteen: Update and outlook. In Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Entropy-based pruning of backoff language models.</title>
<date>1998</date>
<booktitle>In Proc. DARPA Broadcast News Transcription and Understanding Workshop,</booktitle>
<pages>270--274</pages>
<contexts>
<context citStr="Stolcke, 1998" endWordPosition="479" position="3166" startWordPosition="478">cific n-grams in a model that has been stored in a fraction of the space required to store the full, exact model, though with some probability of false positives. Another common approach – which we pursue in this paper – is model pruning, whereby some number of the n-grams are removed from explicit storage in the model, so that their probability must be assigned via backoff smoothing. One simple pruning method is count thresholding, i.e., discarding n-grams that occur less than k times in the corpus. Beyond count thresholding, the most widely used pruning methods (Seymore and Rosenfeld, 1996; Stolcke, 1998) employ greedy algorithms to reduce the number of stored n-grams by comparing the stored probabilities to those that would be assigned via the backoff smoothing mechanism, and removing those with the least impact according to some criterion. While these greedy pruning methods are highly effective for models estimated with most common smoothing approaches, they have been shown to be far less effective with Kneser-Ney trained language models (Siivola et al., 2007; Chelba et al., 2010), leading to severe degradation in model quality relative to other standard smoothing meth43 Proceedings of the 5</context>
<context citStr="Stolcke (1998)" endWordPosition="719" position="4762" startWordPosition="718"> (Witten and Bell, 1991) 118.8 196.3 380.4 121.6 202.3 396.4 Ristad (1995) 126.4 203.6 395.6 ——- N/A ——- Katz (1987) 119.8 198.1 386.2 ——- N/A ——- Kneser-Ney (Kneser and Ney, 1995) 114.5 285.1 388.2 115.8 274.3 398.7 Mod. Kneser-Ney (Chen and Goodman, 1998) 116.3 280.6 396.2 112.8 270.7 399.1 Table 1: Reformatted version of Table 3 in Chelba et al. (2010), demonstrating perplexity degradation of Kneser-Ney smoothed models in contrast to other common smoothing methods. Data: English Broadcast News, 128M words training; 692K words test; 143K word vocabulary. 4-gram language models, pruned using Stolcke (1998) relative entropy pruning to approximately 1.3% of the original size of 31,095,260 n-grams. ods. Thus, while Kneser-Ney may be the preferred smoothing method for large, unpruned models – where it can achieve real improvements over other smoothing methods – when relatively sparse, pruned models are required, it has severely diminished utility. Table 1 presents a slightly reformatted version of Table 3 from Chelba et al. (2010). In their experiments (see Table 1 caption for specifics on training/test setup), they trained 4-gram Broadcast News language models using a variety of both backoff and i</context>
</contexts>
<marker>Stolcke, 1998</marker>
<rawString>Andreas Stolcke. 1998. Entropy-based pruning of backoff language models. In Proc. DARPA Broadcast News Transcription and Understanding Workshop, pages 270–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Thorsten Brants</author>
</authors>
<title>Randomized language models via perfect hash functions.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>505--513</pages>
<contexts>
<context citStr="Talbot and Brants, 2008" endWordPosition="366" position="2487" startWordPosition="363">ly used in scenarios requiring relatively small footprint models. For example, applications running on mobile devices or in low latency streaming scenarios may be required to limit the complexity of models and algorithms to achieve the desired operating profile. As a result, statistical language models – an important component of many such applications – are often trained on very large corpora, then modified to fit within some pre-specified size bound. One method to achieve significant space reduction is through randomized data structures, such as Bloom (Talbot and Osborne, 2007) or Bloomier (Talbot and Brants, 2008) filters. These data structures permit efficient querying for specific n-grams in a model that has been stored in a fraction of the space required to store the full, exact model, though with some probability of false positives. Another common approach – which we pursue in this paper – is model pruning, whereby some number of the n-grams are removed from explicit storage in the model, so that their probability must be assigned via backoff smoothing. One simple pruning method is count thresholding, i.e., discarding n-grams that occur less than k times in the corpus. Beyond count thresholding, th</context>
</contexts>
<marker>Talbot, Brants, 2008</marker>
<rawString>David Talbot and Thorsten Brants. 2008. Randomized language models via perfect hash functions. In Proceedings of ACL-08: HLT, pages 505–513.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Miles Osborne</author>
</authors>
<title>Smoothed Bloom filter language models: Tera-scale LMs on the cheap.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>468--476</pages>
<contexts>
<context citStr="Talbot and Osborne, 2007" endWordPosition="360" position="2449" startWordPosition="357">guage applications, however, are commonly used in scenarios requiring relatively small footprint models. For example, applications running on mobile devices or in low latency streaming scenarios may be required to limit the complexity of models and algorithms to achieve the desired operating profile. As a result, statistical language models – an important component of many such applications – are often trained on very large corpora, then modified to fit within some pre-specified size bound. One method to achieve significant space reduction is through randomized data structures, such as Bloom (Talbot and Osborne, 2007) or Bloomier (Talbot and Brants, 2008) filters. These data structures permit efficient querying for specific n-grams in a model that has been stored in a fraction of the space required to store the full, exact model, though with some probability of false positives. Another common approach – which we pursue in this paper – is model pruning, whereby some number of the n-grams are removed from explicit storage in the model, so that their probability must be assigned via backoff smoothing. One simple pruning method is count thresholding, i.e., discarding n-grams that occur less than k times in the</context>
</contexts>
<marker>Talbot, Osborne, 2007</marker>
<rawString>David Talbot and Miles Osborne. 2007. Smoothed Bloom filter language models: Tera-scale LMs on the cheap. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 468–476.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Timothy C Bell</author>
</authors>
<title>The zerofrequency problem: Estimating the probabilities of novel events in adaptive text compression.</title>
<date>1991</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>37</volume>
<issue>4</issue>
<pages>1094</pages>
<contexts>
<context citStr="Witten and Bell, 1991" endWordPosition="628" position="4172" startWordPosition="625"> far less effective with Kneser-Ney trained language models (Siivola et al., 2007; Chelba et al., 2010), leading to severe degradation in model quality relative to other standard smoothing meth43 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 43–52, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 4-gram models Backoff n-grams Interpolated n-grams Perplexity Perplexity Smoothing method full pruned (×1000) full pruned (×1000) Absolute Discounting (Ney et al., 1994) 120.5 197.3 383.4 119.8 198.1 386.2 Witten-Bell (Witten and Bell, 1991) 118.8 196.3 380.4 121.6 202.3 396.4 Ristad (1995) 126.4 203.6 395.6 ——- N/A ——- Katz (1987) 119.8 198.1 386.2 ——- N/A ——- Kneser-Ney (Kneser and Ney, 1995) 114.5 285.1 388.2 115.8 274.3 398.7 Mod. Kneser-Ney (Chen and Goodman, 1998) 116.3 280.6 396.2 112.8 270.7 399.1 Table 1: Reformatted version of Table 3 in Chelba et al. (2010), demonstrating perplexity degradation of Kneser-Ney smoothed models in contrast to other common smoothing methods. Data: English Broadcast News, 128M words training; 692K words test; 143K word vocabulary. 4-gram language models, pruned using Stolcke (1998) relative </context>
<context citStr="Witten and Bell (1991)" endWordPosition="1650" position="10359" startWordPosition="1647"> |hi ) otherwise where c(hki wi) is the count of the n-gram sequence wi_k ... wi in the training corpus; P is a regularized probability estimate that provides some probability mass for unobserved n-grams; and α(hki ) is a factor that ensures normalization. Note that for h = hki , the typically defined backoff history h' = hk_1 i , i.e., the longest suffix of h that is not h itself. When we use h' and g' (for notational convenience) in future equations, it is this definition that we are using. There are many ways to estimate P, including absolute discounting (Ney et al., 1994), Katz (1987) and Witten and Bell (1991). Interpolated models are special cases of this form, where the P is determined using model mixing, and the α parameter is exactly the mixing factor value for the lower order model. N-gram language models allow for a sparse representation, so that only a subset of the possible ngrams must be explicitly stored. Probabilities for the rest of the n-grams are calculated through the “otherwise” semantics in the equation above. For an n-gram language model G, we will say that an n-gram hw E G if it is explicitly represented in the model; otherwise hw E� G. In the standard ngram formulation above, th</context>
</contexts>
<marker>Witten, Bell, 1991</marker>
<rawString>Ian H. Witten and Timothy C. Bell. 1991. The zerofrequency problem: Estimating the probabilities of novel events in adaptive text compression. IEEE Transactions on Information Theory, 37(4):1085– 1094.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International COLING,</booktitle>
<pages>947--953</pages>
<contexts>
<context citStr="Yeh, 2000" endWordPosition="5945" position="34994" startWordPosition="5944">19.7 20.2 19.7 Mixture 20.5 19.6 20.2 19.6 Kneser-Neya 22.1 22.2 Kneser-Neyb 20.5 20.6 Table 4: WER reductions achieved with marginal distribution constraints (MDC) on the heavily pruned models from Chelba et al. (2010), and a mixture model. KneserNey results are shown for: a) original pruning; and b) with -prune-history-lm switch. The perplexity reductions that were achieved for these models do translate to real word error rate reductions at both stages of between 0.5 and 0.9 percent absolute. All of these gains are statistically significant at p &lt; 0.0001 using the stratified shuffling test (Yeh, 2000). For pruned Kneser-Ney models, fixing the state marginals with the -prune-history-lm switch reduces the WER versus the original pruned model, but no reductions were achieved vs. baseline methods. Table 5 presents perplexity and WER results for less heavily pruned models, where the pruning thresholds were set to yield approximately 1.5 million n-grams (4 times more than the previous models); and another set at around 5 million n-grams, as well as the full, unpruned models. While the robust gains we’ve observed up to now persist with the 1.5M n-gram models (WER reductions significant, Witten-Be</context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>A. Yeh. 2000. More accurate tests for the statistical significance of result differences. In Proceedings of the 18th International COLING, pages 947–953.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>