<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant confidence="0.000236" no="0">
<title confidence="0.929822">
MAXSIM: A Maximum Similarity Metric
for Machine Translation Evaluation
</title>
<author confidence="0.989534">
Yee Seng Chan and Hwee Tou Ng
</author>
<affiliation confidence="0.9998905">
Department of Computer Science
National University of Singapore
</affiliation>
<address confidence="0.918477">
Law Link, Singapore 117590
</address>
<email confidence="0.994931">
{chanys, nght}@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.99856" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999755526315789">We propose an automatic machine translation (MT) evaluation metric that calculates a similarity score (based on precision and recall) of a pair of sentences. Unlike most metrics, we compute a similarity score between items across the two sentences. We then find a maximum weight matching between the items such that each item in one sentence is mapped to at most one item in the other sentence. This general framework allows us to use arbitrary similarity functions between items, and to incorporate different information in our comparison, such as n-grams, dependency relations, etc. When evaluated on data from the ACL-07 MT workshop, our proposed metric achieves higher correlation with human judgements than all 11 automatic MT evaluation metrics that were evaluated during the workshop.</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999906916666667">In recent years, machine translation (MT) research has made much progress, which includes the introduction of automatic metrics for MT evaluation. Since human evaluation of MT output is time consuming and expensive, having a robust and accurate automatic MT evaluation metric that correlates well with human judgement is invaluable. Among all the automatic MT evaluation metrics, BLEU (Papineni et al., 2002) is the most widely used. Although BLEU has played a crucial role in the progress of MT research, it is becoming evident that BLEU does not correlate with human judgement well enough, and suffers from several other deficiencies such as the lack of an intuitive interpretation of its scores.</bodyText>
<page confidence="0.987155">
55
</page>
<bodyText confidence="0.999409470588235">During the recent ACL-07 workshop on statistical MT (Callison-Burch et al., 2007), a total of 11 automatic MT evaluation metrics were evaluated for correlation with human judgement. The results show that, as compared to BLEU, several recently proposed metrics such as Semantic-role overlap (Gimenez and Marquez, 2007), ParaEval-recall (Zhou et al., 2006), and METEOR (Banerjee and Lavie, 2005) achieve higher correlation. In this paper, we propose a new automatic MT evaluation metric, MAXSTM, that compares a pair of system-reference sentences by extracting n-grams and dependency relations. Recognizing that different concepts can be expressed in a variety of ways, we allow matching across synonyms and also compute a score between two matching items (such as between two n-grams or between two dependency relations), which indicates their degree of similarity with each other. Having weighted matches between items means that there could be many possible ways to match, or link items from a system translation sentence to a reference translation sentence. To match each system item to at most one reference item, we model the items in the sentence pair as nodes in a bipartite graph and use the Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) to find a maximum weight matching (or alignment) between the items in polynomial time. The weights (from the edges) of the resulting graph will then be added to determine the final similarity score between the pair of sentences.</bodyText>
<note confidence="0.818598">
Proceedings of ACL-08: HLT, pages 55–62,
</note>
<page confidence="0.547578">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999954166666666">Although a maximum weight bipartite graph was also used in the recent work of (Taskar et al., 2005), their focus was on learning supervised models for single word alignment between sentences from a source and target language. The contributions of this paper are as follows. Current metrics (such as BLEU, METEOR, Semantic-role overlap, ParaEval-recall, etc.) do not assign different weights to their matches: either two items match, or they don’t. Also, metrics such as METEOR determine an alignment between the items of a sentence pair by using heuristics such as the least number of matching crosses. In contrast, we propose weighting different matches differently, and then obtain an optimal set of matches, or alignments, by using a maximum weight matching framework. We note that this framework is not used by any of the 11 automatic MT metrics in the ACL-07 MT workshop. Also, this framework allows for defining arbitrary similarity functions between two matching items, and we could match arbitrary concepts (such as dependency relations) gathered from a sentence pair. In contrast, most other metrics (notably BLEU) limit themselves to matching based only on the surface form of words. Finally, when evaluated on the datasets of the recent ACL07 MT workshop (Callison-Burch et al., 2007), our proposed metric achieves higher correlation with human judgements than all of the 11 automatic MT evaluation metrics evaluated during the workshop. In the next section, we describe several existing metrics. In Section 3, we discuss issues to consider when designing a metric. In Section 4, we describe our proposed metric. In Section 5, we present our experimental results. Finally, we outline future work in Section 6, before concluding in Section 7.</bodyText>
<sectionHeader confidence="0.956782" genericHeader="method">
2 Automatic Evaluation Metrics
</sectionHeader>
<bodyText confidence="0.999264333333333">In this section, we describe BLEU, and the three metrics which achieved higher correlation results than BLEU in the recent ACL-07 MT workshop.</bodyText>
<subsectionHeader confidence="0.888891">
2.1 BLEU
</subsectionHeader>
<bodyText confidence="0.999967545454545">BLEU (Papineni et al., 2002) is essentially a precision-based metric and is currently the standard metric for automatic evaluation of MT performance. To score a system translation, BLEU tabulates the number of n-gram matches of the system translation against one or more reference translations. Generally, more n-gram matches result in a higher BLEU score. When determining the matches to calculate precision, BLEU uses a modified, or clipped n-gram precision. With this, an n-gram (from both the system and reference translation) is considered to be exhausted or used after participating in a match. Hence, each system n-gram is “clipped” by the maximum number of times it appears in any reference translation. To prevent short system translations from receiving too high a score and to compensate for its lack of a recall component, BLEU incorporates a brevity penalty. This penalizes the score of a system if the length of its entire translation output is shorter than the length of the reference text.</bodyText>
<subsectionHeader confidence="0.998177">
2.2 Semantic Roles
</subsectionHeader>
<bodyText confidence="0.999981842105263">(Gimenez and Marquez, 2007) proposed using deeper linguistic information to evaluate MT performance. For evaluation in the ACL-07 MT workshop, the authors used the metric which they termed as SR-Or-*1. This metric first counts the number of lexical overlaps SR-Or-t for all the different semantic roles t that are found in the system and reference translation sentence. A uniform average of the counts is then taken as the score for the sentence pair. In their work, the different semantic roles t they considered include the various core and adjunct arguments as defined in the PropBank project (Palmer et al., 2005). For instance, SR-Or-AO refers to the number of lexical overlaps between the AO arguments. To extract semantic roles from a sentence, several processes such as lemmatization, partof-speech tagging, base phrase chunking, named entity tagging, and finally semantic role tagging need to be performed.</bodyText>
<subsectionHeader confidence="0.994005">
2.3 ParaEval
</subsectionHeader>
<bodyText confidence="0.9998142">The ParaEval metric (Zhou et al., 2006) uses a large collection of paraphrases, automatically extracted from parallel corpora, to evaluate MT performance. To compare a pair of sentences, ParaEval first locates paraphrase matches between the two sentences.</bodyText>
<footnote confidence="0.560429">
'Verified through personal communication as this is not ev-
ident in their paper.
</footnote>
<page confidence="0.996889">
56
</page>
<bodyText confidence="0.99982975">Then, unigram matching is performed on the remaining words that are not matched using paraphrases. Based on the matches, ParaEval will then elect to use either unigram precision or unigram recall as its score for the sentence pair. In the ACL-07 MT workshop, ParaEval based on recall (ParaEval-recall) achieves good correlation with human judgements.</bodyText>
<subsectionHeader confidence="0.977679">
2.4 METEOR
</subsectionHeader>
<bodyText confidence="0.999675571428572">Given a pair of strings to compare (a system translation and a reference translation), METEOR (Banerjee and Lavie, 2005) first creates a word alignment between the two strings. Based on the number of word or unigram matches and the amount of string fragmentation represented by the alignment, METEOR calculates a score for the pair of strings. In aligning the unigrams, each unigram in one string is mapped, or linked, to at most one unigram in the other string. These word alignments are created incrementally through a series of stages, where each stage only adds alignments between unigrams which have not been matched in previous stages. At each stage, if there are multiple different alignments, then the alignment with the most number of mappings is selected. If there is a tie, then the alignment with the least number of unigram mapping crosses is selected. The three stages of “exact”, “porter stem”, and “WN synonymy” are usually applied in sequence to create alignments. The “exact” stage maps unigrams if they have the same surface form. The “porter stem” stage then considers the remaining unmapped unigrams and maps them if they are the same after applying the Porter stemmer. Finally, the “WN synonymy” stage considers all remaining unigrams and maps two unigrams if they are synonyms in the WordNet sense inventory (Miller, 1990). Once the final alignment has been produced, unigram precision P (number of unigram matches m divided by the total number of system unigrams) and unigram recall R (m divided by the total number of reference unigrams) are calculated and combined into a single parameterized harmonic mean (Rijsbergen, 1979):</bodyText>
<equation confidence="0.9997125">
P · R (1)
αP + (1 − α)R
</equation>
<bodyText confidence="0.999978333333333">To account for longer matches and the amount of fragmentation represented by the alignment, METEOR groups the matched unigrams into as few chunks as possible and imposes a penalty based on the number of chunks. The METEOR score for a pair of sentences is:</bodyText>
<equation confidence="0.876471666666667">
� score` = 1 − (no. of chunks)'a F
m
Y m ean
</equation>
<bodyText confidence="0.866698">where -y (no. of chunks) represents the fragmentam tion penalty of the alignment. Note that METEOR consists of three parameters that need to be optimized based on experimentation: α, Q, and 'y.</bodyText>
<sectionHeader confidence="0.99763" genericHeader="method">
3 Metric Design Considerations
</sectionHeader>
<bodyText confidence="0.99802">We first review some aspects of existing metrics and highlight issues that should be considered when designing an MT evaluation metric.</bodyText>
<bodyText confidence="0.8971313">• Intuitive interpretation: To compensate for the lack of recall, BLEU incorporates a brevity penalty. This, however, prevents an intuitive interpretation of its scores. To address this, standard measures like precision and recall could be used, as in some previous research (Banerjee and Lavie, 2005; Melamed et al., 2003). • Allowing for variation: BLEU only counts exact word matches. Languages, however, often allow a great deal of variety in vocabulary and in the ways concepts are expressed. Hence, using information such as synonyms or dependency relations could potentially address the issue better. • Matches should be weighted: Current metrics either match, or don’t match a pair of items. We note, however, that matches between items (such as words, n-grams, etc.) should be weighted according to their degree of similarity.</bodyText>
<sectionHeader confidence="0.990161" genericHeader="evaluation">
4 The Maximum Similarity Metric
</sectionHeader>
<bodyText confidence="0.9997605">We now describe our proposed metric, Maximum Similarity (MAXSIM), which is based on precision and recall, allows for synonyms, and weights the matches found.</bodyText>
<equation confidence="0.580712">
Fmean =
</equation>
<page confidence="0.976073">
57
</page>
<bodyText confidence="0.999872">Given a pair of English sentences to be compared (a system translation against a reference translation), we perform tokenization2, lemmatization using WordNet3, and part-of-speech (POS) tagging with the MXPOST tagger (Ratnaparkhi, 1996). Next, we remove all non-alphanumeric tokens. Then, we match the unigrams in the system translation to the unigrams in the reference translation. Based on the matches, we calculate the recall and precision, which we then combine into a single Fmean unigram score using Equation 1. Similarly, we also match the bigrams and trigrams of the sentence pair and calculate their corresponding Fmean scores. To obtain a single similarity score scores for this sentence pair s, we simply average the three Fmean scores. Then, to obtain a single similarity score sim-score for the entire system corpus, we repeat this process of calculating a scores for each system-reference sentence pair s, and compute the average over all |S |sentence pairs:</bodyText>
<equation confidence="0.998528">
� A1 N
N L Fme �n
s=1 n=1
</equation>
<bodyText confidence="0.999992166666667">where in our experiments, we set N=3, representing calculation of unigram, bigram, and trigram scores. If we are given access to multiple references, we calculate an individual sim-score between the system corpus and each reference corpus, and then average the scores obtained.</bodyText>
<subsectionHeader confidence="0.999404">
4.1 Using N-gram Information
</subsectionHeader>
<bodyText confidence="0.999447857142857">In this subsection, we describe in detail how we match the n-grams of a system-reference sentence pair. Lemma and POS match Representing each ngram by its sequence of lemma and POS-tag pairs, we first try to perform an exact match in both lemma and POS-tag. In all our n-gram matching, each ngram in the system translation can only match at most one n-gram in the reference translation. Representing each unigram (lipi) at position i by its lemma li and POS-tag pi, we count the number matchuni of system-reference unigram pairs where both their lemma and POS-tag match. To find matching pairs, we proceed in a left-to-right fashion</bodyText>
<footnote confidence="0.9998765">
2http://www.cis.upenn.edu/ treebank/tokenizer.sed
3http://wordnet.princeton.edu/man/morph.3WN
</footnote>
<figureCaption confidence="0.999871">
Figure 1: Bipartite matching.
</figureCaption>
<bodyText confidence="0.99973309375">(in both strings). We first compare the first system unigram to the first reference unigram, then to the second reference unigram, and so on until we find a match. If there is a match, we increment matchuni by 1 and remove this pair of system-reference unigrams from further consideration (removed items will not be matched again subsequently). Then, we move on to the second system unigram and try to match it against the reference unigrams, once again proceeding in a left-to-right fashion. We continue this process until we reach the last system unigram. To determine the number matchbi of bigram matches, a system bigram (lsipsi,lsi+1psi+1) matches a reference bigram (lripri, lri+1pri+1) if lsi = lri, psi = pri, lsi+1 = lri+1, and psi+1 = pri+1. For trigrams, we similarly determine matchtri by counting the number of trigram matches. Lemma match For the remaining set of n-grams that are not yet matched, we now relax our matching criteria by allowing a match if their corresponding lemmas match. That is, a system unigram (lsipsi) matches a reference unigram (lripri) if lsi = lri. In the case of bigrams, the matching conditions are lsi = lri and lsi+1 = lri+1. The conditions for trigrams are similar. Once again, we find matches in a left-to-right fashion. We add the number of unigram, bigram, and trigram matches found during this phase to matchuni, matchbi, and matchtri respectively. Bipartite graph matching For the remaining ngrams that are not matched so far, we try to match them by constructing bipartite graphs. During this phase, we will construct three bipartite graphs, one each for the remaining set of unigrams, bigrams, and trigrams.</bodyText>
<figure confidence="0.996437263157895">
r1 r2 r3
1
1
0.75
0.5
0.75
0.75
0.5 0
s1
s2
s3
r1 r2 r3
1
s1 s2 s3
1
0.75
1
1
sim-score = ISI
</figure>
<page confidence="0.995183">
58
</page>
<bodyText confidence="0.9950209375">Using bigrams to illustrate, we construct a weighted complete bipartite graph, where each edge e connecting a pair of system-reference bigrams has a weight w(e), indicating the degree of similarity between the bigrams connected. Note that, without loss of generality, if the number of system nodes and reference nodes (bigrams) are not the same, we can simply add dummy nodes with connecting edges of weight 0 to obtain a complete bipartite graph with equal number of nodes on both sides. In an n-gram bipartite graph, the similarity score, or the weight w(e) of the edge e connecting a system n-gram (ls1ps1, ... ,lsnpsn) and a reference n-gram (lr1pr1, ... , lrnprn) is calculated as follows:</bodyText>
<equation confidence="0.9996134">
I(psi, pri) + Syn(lsi, lri)
Si =
2
1
w(e) = n
</equation>
<bodyText confidence="0.999982">where I(psi,pri) evaluates to 1 if psi = pri, and 0 otherwise. The function Syn(lsi, lri) checks whether lsi is a synonym of lri. To determine this, we first obtain the set WNsyn(lsi) of WordNet synonyms for lsi and the set W Nsyn(lri) of WordNet synonyms for lri. Then,</bodyText>
<equation confidence="0.811837333333333">
{ 1, WNsyn(lsi) n WNsyn(lri)
=�0
0, otherwise
</equation>
<bodyText confidence="0.999950024390243">In gathering the set WNsyn for a word, we gather all the synonyms for all its senses and do not restrict to a particular POS category. Further, if we are comparing bigrams or trigrams, we impose an additional condition: Si =� 0, for 1 G i G n, else we will set w(e) = 0. This captures the intuition that in matching a system n-gram against a reference ngram, where n &gt; 1, we require each system token to have at least some degree of similarity with the corresponding reference token. In the top half of Figure 1, we show an example of a complete bipartite graph, constructed for a set of three system bigrams (s1, s2, s3) and three reference bigrams (r1, r2, r3), and the weight of the connecting edge between two bigrams represents their degree of similarity. Next, we aim to find a maximum weight matching (or alignment) between the bigrams such that each system (reference) bigram is connected to exactly one reference (system) bigram. This maximum weighted bipartite matching problem can be solved in O(n3) time (where n refers to the number of nodes, or vertices in the graph) using the KuhnMunkres algorithm (Kuhn, 1955; Munkres, 1957). The bottom half of Figure 1 shows the resulting maximum weighted bipartite graph, where the alignment represents the maximum weight matching, out of all possible alignments. Once we have solved and obtained a maximum weight matching M for the bigram bipartite graph, we sum up the weights of the edges to obtain the weight of the matching M: w(M) = EeEM w(e), and add w(M) to matchbi. From the unigram and trigram bipartite graphs, we similarly calculate their respective w(M) and add to the corresponding matchuni and matchtri. Based on matchuni, matchbi, and matchtri, we calculate their corresponding precision P and recall R, from which we obtain their respective Fmean scores via Equation 1. Using bigrams for illustration, we calculate its P and R as:</bodyText>
<equation confidence="0.8743444">
matchbi
P=
no. of bigrams in system translation
matchbi
R =
</equation>
<bodyText confidence="0.957844">no. of bigrams in reference translation</bodyText>
<subsectionHeader confidence="0.975953">
4.2 Dependency Relations
</subsectionHeader>
<bodyText confidence="0.999937733333333">Besides matching a pair of system-reference sentences based on the surface form of words, previous work such as (Gimenez and Marquez, 2007) and (Rajman and Hartley, 2002) had shown that deeper linguistic knowledge such as semantic roles and syntax can be usefully exploited. In the previous subsection, we describe our method of using bipartite graphs for matching of ngrams found in a sentence pair. This use of bipartite graphs, however, is a very general framework to obtain an optimal alignment of the corresponding “information items” contained within a sentence pair. Hence, besides matching based on n-gram strings, we can also match other “information items”, such as dependency relations.</bodyText>
<equation confidence="0.98745125">
n
Si
i=1
Syn(lsi,lri) =
</equation>
<page confidence="0.997473">
59
</page>
<table confidence="0.999233428571429">
Metric Adequacy Fluency Rank Constituent Average
MAXSIMn+d 0.780 0.827 0.875 0.760 0.811
MAXSIMn 0.804 0.845 0.893 0.766 0.827
Semantic-role 0.774 0.839 0.804 0.742 0.790
ParaEval-recall 0.712 0.742 0.769 0.798 0.755
METEOR 0.701 0.719 0.746 0.670 0.709
BLEU 0.690 0.722 0.672 0.603 0.672
</table>
<tableCaption confidence="0.974399">
Table 1: Overall correlations on the Europarl and News Commentary datasets. The “Semantic-role overlap” metric
is abbreviated as “Semantic-role”. Note that each figure above represents 6 translation tasks: the Europarl and News
Commentary datasets each with 3 language pairs (German-English, Spanish-English, French-English).
</tableCaption>
<bodyText confidence="0.9996315">In our work, we train the MSTParser4 (McDonald et al., 2005) on the Penn Treebank Wall Street Journal (WSJ) corpus, and use it to extract dependency relations from a sentence. Currently, we focus on extracting only two relations: subject and object. For each relation (ch, dp, pa) extracted, we note the child lemma ch of the relation (often a noun), the relation type dp (either subject or object), and the parent lemma pa of the relation (often a verb). Then, using the system relations and reference relations extracted from a system-reference sentence pair, we similarly construct a bipartite graph, where each node is a relation (ch, dp, pa). We define the weight w(e) of an edge e between a system relation (chs, dps, pas) and a reference relation (chr, dpr, par) as follows:</bodyText>
<equation confidence="0.8276775">
Syn(chs, chr) + I(dps, dpr) + Syn(pas, par)
3
</equation>
<bodyText confidence="0.999993083333333">where functions I and Syn are defined as in the previous subsection. Also, w(e) is non-zero only if dps = dpr. After solving for the maximum weight matching M, we divide w(M) by the number of system relations extracted to obtain a precision score P, and divide w(M) by the number of reference relations extracted to obtain a recall score R. P and R are then similarly combined into a Fmean score for the sentence pair. To compute the similarity score when incorporating dependency relations, we average the Fmean scores for unigrams, bigrams, trigrams, and dependency relations.</bodyText>
<sectionHeader confidence="0.999909" genericHeader="evaluation and result">
5 Results
</sectionHeader>
<bodyText confidence="0.9998445">To evaluate our metric, we conduct experiments on datasets from the ACL-07 MT workshop and NIST</bodyText>
<footnote confidence="0.993878">
4Available at: http://sourceforge.net/projects/Mstparser
</footnote>
<table confidence="0.999495375">
Europarl
Metric Adq Flu Rank Con Avg
MAXSIMn+d 0.749 0.786 0.857 0.651 0.761
MAXSIMn 0.749 0.786 0.857 0.651 0.761
Semantic-role 0.815 0.854 0.759 0.612 0.760
ParaEval-recall 0.701 0.708 0.737 0.772 0.730
METEOR 0.726 0.741 0.770 0.558 0.699
BLEU 0.803 0.822 0.699 0.512 0.709
</table>
<tableCaption confidence="0.953974">
Table 2: Correlations on the Europarl dataset.
</tableCaption>
<table confidence="0.9208187">
Adq=Adequacy, Flu=Fluency, Con=Constituent, and
Avg=Average.
News Commentary
Metric Adq Flu Rank Con Avg
MAXSIMn+d 0.812 0.869 0.893 0.869 0.861
MAXSIMn 0.860 0.905 0.929 0.881 0.894
Semantic-role 0.734 0.824 0.848 0.871 0.819
ParaEval-recall 0.722 0.777 0.800 0.824 0.781
METEOR 0.677 0.698 0.721 0.782 0.720
BLEU 0.577 0.622 0.646 0.693 0.635
</table>
<tableCaption confidence="0.999869">
Table 3: Correlations on the News Commentary dataset.
</tableCaption>
<note confidence="0.379578">
MT 2003 evaluation exercise.
</note>
<subsectionHeader confidence="0.958156">
5.1 ACL-07 MT Workshop
</subsectionHeader>
<bodyText confidence="0.999799818181818">The ACL-07 MT workshop evaluated the translation quality of MT systems on various translation tasks, and also measured the correlation (with human judgement) of 11 automatic MT evaluation metrics. The workshop used a Europarl dataset and a News Commentary dataset, where each dataset consisted of English sentences (2,000 English sentences for Europarl and 2,007 English sentences for News Commentary) and their translations in various languages. As part of the workshop, correlations of the automatic metrics were measured for the tasks of translating German, Spanish, and French into English.</bodyText>
<page confidence="0.996376">
60
</page>
<bodyText confidence="0.996482333333333">Hence, we will similarly measure the correlation of MAXSIM on these tasks.</bodyText>
<subsubsectionHeader confidence="0.850982">
5.1.1 Evaluation Criteria
</subsubsectionHeader>
<bodyText confidence="0.999987466666667">For human evaluation of the MT submissions, four different criteria were used in the workshop: Adequacy (how much of the original meaning is expressed in a system translation), Fluency (the translation’s fluency), Rank (different translations of a single source sentence are compared and ranked from best to worst), and Constituent (some constituents from the parse tree of the source sentence are translated, and human judges have to rank these translations). During the workshop, Kappa values measured for interand intra-annotator agreement for rank and constituent are substantially higher than those for adequacy and fluency, indicating that rank and constituent are more reliable criteria for MT evaluation.</bodyText>
<subsubsectionHeader confidence="0.672655">
5.1.2 Correlation Results
</subsubsectionHeader>
<bodyText confidence="0.999988692307692">We follow the ACL-07 MT workshop process of converting the raw scores assigned by an automatic metric to ranks and then using the Spearman’s rank correlation coefficient to measure correlation. During the workshop, only three automatic metrics (Semantic-role overlap, ParaEval-recall, and METEOR) achieve higher correlation than BLEU. We gather the correlation results of these metrics from the workshop paper (Callison-Burch et al., 2007), and show in Table 1 the overall correlations of these metrics over the Europarl and News Commentary datasets. In the table, MAXSIMn represents using only n-gram information (Section 4.1) for our metric, while MAXSIMn+d represents using both ngram and dependency information. We also show the breakdown of the correlation results into the Europarl dataset (Table 2) and the News Commentary dataset (Table 3). In all our results for MAXSIM in this paper, we follow METEOR and use α=0.9 (weighing recall more than precision) in our calculation of F,,an via Equation 1, unless otherwise stated. The results in Table 1 show that MAXSIMn and MAXSIMn+d achieve overall average (over the four criteria) correlations of 0.827 and 0.811 respectively. Note that these results are substantially higher than BLEU, and in particular higher than the best performing Semantic-role overlap metric in the ACL-07 MT workshop.</bodyText>
<table confidence="0.999496833333334">
Metric Adq Flu Avg
MAXSIM,+d 0.943 0.886 0.915
MAXSIM, 0.829 0.771 0.800
METEOR (optimized) 1.000 0.943 0.972
METEOR 0.943 0.886 0.915
BLEU 0.657 0.543 0.600
</table>
<tableCaption confidence="0.999885">
Table 4: Correlations on the NIST MT 2003 dataset.
</tableCaption>
<bodyText confidence="0.999870571428571">Also, Semantic-role overlap requires more processing steps (such as base phrase chunking, named entity tagging, etc.)than MAXSIM. For future work, we could experiment with incorporating semantic-role information into our current framework. We note that the ParaEvalrecall metric achieves higher correlation on the constituent criterion, which might be related to the fact that both ParaEval-recall and the constituent criterion are based on phrases: ParaEval-recall tries to match phrases, and the constituent criterion is based on judging translations of phrases.</bodyText>
<subsectionHeader confidence="0.804424">
5.2 NIST MT 2003 Dataset
</subsectionHeader>
<bodyText confidence="0.999902130434783">We also conduct experiments on the test data (LDC2006T04) of NIST MT 2003 Chinese-English translation task. For this dataset, human judgements are available on adequacy and fluency for six system submissions, and there are four English reference translation texts. Since implementations of the BLEU and METEOR metrics are publicly available, we score the system submissions using BLEU (version 11b with its default settings), METEOR, and MAXSIM, showing the resulting correlations in Table 4. For METEOR, when used with its originally proposed parameter values of (α=0.9, 0=3.0, 'y=0.5), which the METEOR researchers mentioned were based on some early experimental work (Banerjee and Lavie, 2005), we obtain an average correlation value of 0.915, as shown in the row “METEOR”. In the recent work of (Lavie and Agarwal, 2007), the values of these parameters were tuned to be (α=0.81, 0=0.83, 'y=0.28), based on experiments on the NIST 2003 and 2004 Arabic-English evaluation datasets. When METEOR was run with these new parameter values, it returned an average correlation value of</bodyText>
<page confidence="0.997544">
61
</page>
<bodyText confidence="0.999498454545455">0.972, as shown in the row “METEOR (optimized)”. MAXSIM using only n-gram information (MAXSIM,,,) gives an average correlation value of 0.800, while adding dependency information (MAXSIM�+d) improves the correlation value to 0.915. Note that so far, the parameters of MAXSIM are not optimized and we simply perform uniform averaging of the different n-grams and dependency scores. Under this setting, the correlation achieved by MAXSIM is comparable to that achieved by METEOR.</bodyText>
<sectionHeader confidence="0.999885" genericHeader="conclusion">
6 Future Work
</sectionHeader>
<bodyText confidence="0.999985866666667">In our current work, the parameters of MAXSIM are as yet un-optimized. We found that by setting α=0.7, MAXSIM�+d could achieve a correlation of 0.972 on the NIST MT 2003 dataset. Also, we have barely exploited the potential of weighted similarity matching. Possible future directions include adding semantic role information, using the distance between item pairs based on the token position within each sentence as additional weighting consideration, etc. Also, we have seen that dependency relations help to improve correlation on the NIST dataset, but not on the ACL-07 MT workshop datasets. Since the accuracy of dependency parsers is not perfect, a possible future work is to identify when best to incorporate such syntactic information.</bodyText>
<sectionHeader confidence="0.9995" genericHeader="conclusion">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999797454545454">In this paper, we present MAXSIM, a new automatic MT evaluation metric that computes a similarity score between corresponding items across a sentence pair, and uses a bipartite graph to obtain an optimal matching between item pairs. This general framework allows us to use arbitrary similarity functions between items, and to incorporate different information in our comparison. When evaluated for correlation with human judgements, MAXSIM achieves superior results when compared to current automatic MT evaluation metrics.</bodyText>
<sectionHeader confidence="0.99964" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999714">
S. Banerjee and A. Lavie. 2005. METEOR: An auto-
matic metric for MT evaluation with improved corre-
lation with human judgments. In Proceedings of the
Workshop on Intrinsic and Extrinsic Evaluation Mea-
sures for MT and/or Summarization, ACL05, pages
65–72.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and
J. Schroeder. 2007. (meta-) evaluation of machine
translation. In Proceedings of the Second Workshop on
Statistical Machine Translation, ACL07, pages 136–
158.
J. Gimenez and L. Marquez. 2007. Linguistic features
for automatic evaluation of heterogenous MT systems.
In Proceedings of the Second Workshop on Statistical
Machine Translation, ACL07, pages 256–264.
H. W. Kuhn. 1955. The hungarian method for the assign-
ment problem. Naval Research Logistic Quarterly,
2(1):83–97.
A. Lavie and A. Agarwal. 2007. METEOR: An auto-
matic metric for MT evaluation with high levels of cor-
relation with human judgments. In Proceedings of the
Second Workshop on Statistical Machine Translation,
ACL07, pages 228–231.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings ofACL05, pages 91–98.
I. D. Melamed, R. Green, and J. P. Turian. 2003. Preci-
sion and recall of machine translation. In Proceedings
ofHLT-NAACL03, pages 61–63.
G. A. Miller. 1990. WordNet: An on-line lexi-
cal database. International Journal of Lexicography,
3(4):235–312.
J. Munkres. 1957. Algorithms for the assignment and
transportation problems. Journal of the Society for In-
dustrial and Applied Mathematics, 5(1):32–38.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71–106.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: A method for automatic evaluation of machine
translation. In Proceedings ofACL02, pages 311–318.
M. Rajman and A. Hartley. 2002. Automatic ranking of
MT systems. In Proceedings ofLREC02, pages 1247–
1253.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of EMNLP96,
pages 133–142.
C. Rijsbergen. 1979. Information Retrieval. Butter-
worths, London, UK, 2nd edition.
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A dis-
criminative matching approach to word alignment. In
Proceedings ofHLT/EMNLP05, pages 73–80.
L. Zhou, C. Y. Lin, and E. Hovy. 2006. Re-evaluating
machine translation results with paraphrase support.
In Proceedings of EMNLP06, pages 77–84.
</reference>
<page confidence="0.999185">
62
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant confidence="0.737153" no="0">
<title confidence="0.999143">A Maximum Similarity Metric for Machine Translation Evaluation</title>
<author confidence="0.998182">Seng Chan Tou Ng</author>
<affiliation confidence="0.9998565">Department of Computer Science National University of Singapore</affiliation>
<address confidence="0.954598">Law Link, Singapore 117590</address>
<abstract confidence="0.9886967">We propose an automatic machine translation (MT) evaluation metric that calculates a similarity score (based on precision and recall) of a pair of sentences. Unlike most metrics, we compute a similarity score between items across the two sentences. We then find a maximum weight matching between the items such that each item in one sentence is mapped to at most one item in the other sentence. This general framework allows us to use arbitrary similarity functions between items, and to incorporate different information in our comparison, such as n-grams, dependency relations, etc. When evaluated on data from the ACL-07 MT workshop, our proposed metric achieves higher correlation with human judgements than all 11 automatic MT evaluation metrics that were evaluated during the workshop.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>A Lavie</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, ACL05,</booktitle>
<pages>65--72</pages>
<contexts>
<context citStr="Banerjee and Lavie, 2005" endWordPosition="339" position="2134" startWordPosition="336">ole in the progress of MT research, it is becoming evident that BLEU does not correlate with human judgement 55 well enough, and suffers from several other deficiencies such as the lack of an intuitive interpretation of its scores. During the recent ACL-07 workshop on statistical MT (Callison-Burch et al., 2007), a total of 11 automatic MT evaluation metrics were evaluated for correlation with human judgement. The results show that, as compared to BLEU, several recently proposed metrics such as Semantic-role overlap (Gimenez and Marquez, 2007), ParaEval-recall (Zhou et al., 2006), and METEOR (Banerjee and Lavie, 2005) achieve higher correlation. In this paper, we propose a new automatic MT evaluation metric, MAXSTM, that compares a pair of system-reference sentences by extracting n-grams and dependency relations. Recognizing that different concepts can be expressed in a variety of ways, we allow matching across synonyms and also compute a score between two matching items (such as between two n-grams or between two dependency relations), which indicates their degree of similarity with each other. Having weighted matches between items means that there could be many possible ways to match, or link items from </context>
<context citStr="Banerjee and Lavie, 2005" endWordPosition="1305" position="8053" startWordPosition="1301">first locates paraphrase matches between the two 'Verified through personal communication as this is not evident in their paper. 56 sentences. Then, unigram matching is performed on the remaining words that are not matched using paraphrases. Based on the matches, ParaEval will then elect to use either unigram precision or unigram recall as its score for the sentence pair. In the ACL-07 MT workshop, ParaEval based on recall (ParaEval-recall) achieves good correlation with human judgements. 2.4 METEOR Given a pair of strings to compare (a system translation and a reference translation), METEOR (Banerjee and Lavie, 2005) first creates a word alignment between the two strings. Based on the number of word or unigram matches and the amount of string fragmentation represented by the alignment, METEOR calculates a score for the pair of strings. In aligning the unigrams, each unigram in one string is mapped, or linked, to at most one unigram in the other string. These word alignments are created incrementally through a series of stages, where each stage only adds alignments between unigrams which have not been matched in previous stages. At each stage, if there are multiple different alignments, then the alignment </context>
<context citStr="Banerjee and Lavie, 2005" endWordPosition="1735" position="10572" startWordPosition="1731">he fragmentam tion penalty of the alignment. Note that METEOR consists of three parameters that need to be optimized based on experimentation: α, Q, and 'y. 3 Metric Design Considerations We first review some aspects of existing metrics and highlight issues that should be considered when designing an MT evaluation metric. • Intuitive interpretation: To compensate for the lack of recall, BLEU incorporates a brevity penalty. This, however, prevents an intuitive interpretation of its scores. To address this, standard measures like precision and recall could be used, as in some previous research (Banerjee and Lavie, 2005; Melamed et al., 2003). • Allowing for variation: BLEU only counts exact word matches. Languages, however, often allow a great deal of variety in vocabulary and in the ways concepts are expressed. Hence, using information such as synonyms or dependency relations could potentially address the issue better. • Matches should be weighted: Current metrics either match, or don’t match a pair of items. We note, however, that matches between items (such as words, n-grams, etc.) should be weighted according to their degree of similarity. 4 The Maximum Similarity Metric We now describe our proposed met</context>
<context citStr="Banerjee and Lavie, 2005" endWordPosition="4285" position="26183" startWordPosition="4282">03 Chinese-English translation task. For this dataset, human judgements are available on adequacy and fluency for six system submissions, and there are four English reference translation texts. Since implementations of the BLEU and METEOR metrics are publicly available, we score the system submissions using BLEU (version 11b with its default settings), METEOR, and MAXSIM, showing the resulting correlations in Table 4. For METEOR, when used with its originally proposed parameter values of (α=0.9, 0=3.0, 'y=0.5), which the METEOR researchers mentioned were based on some early experimental work (Banerjee and Lavie, 2005), we obtain an average correlation value of 0.915, as shown in the row “METEOR”. In the recent work of (Lavie and Agarwal, 2007), the values of these parameters were tuned to be (α=0.81, 0=0.83, 'y=0.28), based on experiments on the NIST 2003 and 2004 Arabic-English evaluation datasets. When METEOR was run with these new parameter values, it returned an average correlation value of 61 0.972, as shown in the row “METEOR (optimized)”. MAXSIM using only n-gram information (MAXSIM,,,) gives an average correlation value of 0.800, while adding dependency information (MAXSIM�+d) improves the correlat</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>S. Banerjee and A. Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, ACL05, pages 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>C Fordyce</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>J Schroeder</author>
</authors>
<title>(meta-) evaluation of machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation, ACL07,</booktitle>
<pages>136--158</pages>
<contexts>
<context citStr="Callison-Burch et al., 2007" endWordPosition="289" position="1822" startWordPosition="286">man evaluation of MT output is time consuming and expensive, having a robust and accurate automatic MT evaluation metric that correlates well with human judgement is invaluable. Among all the automatic MT evaluation metrics, BLEU (Papineni et al., 2002) is the most widely used. Although BLEU has played a crucial role in the progress of MT research, it is becoming evident that BLEU does not correlate with human judgement 55 well enough, and suffers from several other deficiencies such as the lack of an intuitive interpretation of its scores. During the recent ACL-07 workshop on statistical MT (Callison-Burch et al., 2007), a total of 11 automatic MT evaluation metrics were evaluated for correlation with human judgement. The results show that, as compared to BLEU, several recently proposed metrics such as Semantic-role overlap (Gimenez and Marquez, 2007), ParaEval-recall (Zhou et al., 2006), and METEOR (Banerjee and Lavie, 2005) achieve higher correlation. In this paper, we propose a new automatic MT evaluation metric, MAXSTM, that compares a pair of system-reference sentences by extracting n-grams and dependency relations. Recognizing that different concepts can be expressed in a variety of ways, we allow matc</context>
<context citStr="Callison-Burch et al., 2007" endWordPosition="747" position="4637" startWordPosition="744"> and then obtain an optimal set of matches, or alignments, by using a maximum weight matching framework. We note that this framework is not used by any of the 11 automatic MT metrics in the ACL-07 MT workshop. Also, this framework allows for defining arbitrary similarity functions between two matching items, and we could match arbitrary concepts (such as dependency relations) gathered from a sentence pair. In contrast, most other metrics (notably BLEU) limit themselves to matching based only on the surface form of words. Finally, when evaluated on the datasets of the recent ACL07 MT workshop (Callison-Burch et al., 2007), our proposed metric achieves higher correlation with human judgements than all of the 11 automatic MT evaluation metrics evaluated during the workshop. In the next section, we describe several existing metrics. In Section 3, we discuss issues to consider when designing a metric. In Section 4, we describe our proposed metric. In Section 5, we present our experimental results. Finally, we outline future work in Section 6, before concluding in Section 7. 2 Automatic Evaluation Metrics In this section, we describe BLEU, and the three metrics which achieved higher correlation results than BLEU in</context>
<context citStr="Callison-Burch et al., 2007" endWordPosition="3910" position="23778" startWordPosition="3907">k and constituent are substantially higher than those for adequacy and fluency, indicating that rank and constituent are more reliable criteria for MT evaluation. 5.1.2 Correlation Results We follow the ACL-07 MT workshop process of converting the raw scores assigned by an automatic metric to ranks and then using the Spearman’s rank correlation coefficient to measure correlation. During the workshop, only three automatic metrics (Semantic-role overlap, ParaEval-recall, and METEOR) achieve higher correlation than BLEU. We gather the correlation results of these metrics from the workshop paper (Callison-Burch et al., 2007), and show in Table 1 the overall correlations of these metrics over the Europarl and News Commentary datasets. In the table, MAXSIMn represents using only n-gram information (Section 4.1) for our metric, while MAXSIMn+d represents using both ngram and dependency information. We also show the breakdown of the correlation results into the Europarl dataset (Table 2) and the News Commentary dataset (Table 3). In all our results for MAXSIM in this paper, we follow METEOR and use α=0.9 (weighing recall more than precision) in our calculation of F,,an via Equation 1, unless otherwise stated. The res</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2007</marker>
<rawString>C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and J. Schroeder. 2007. (meta-) evaluation of machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, ACL07, pages 136– 158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gimenez</author>
<author>L Marquez</author>
</authors>
<title>Linguistic features for automatic evaluation of heterogenous MT systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation, ACL07,</booktitle>
<pages>256--264</pages>
<contexts>
<context citStr="Gimenez and Marquez, 2007" endWordPosition="328" position="2058" startWordPosition="325">i et al., 2002) is the most widely used. Although BLEU has played a crucial role in the progress of MT research, it is becoming evident that BLEU does not correlate with human judgement 55 well enough, and suffers from several other deficiencies such as the lack of an intuitive interpretation of its scores. During the recent ACL-07 workshop on statistical MT (Callison-Burch et al., 2007), a total of 11 automatic MT evaluation metrics were evaluated for correlation with human judgement. The results show that, as compared to BLEU, several recently proposed metrics such as Semantic-role overlap (Gimenez and Marquez, 2007), ParaEval-recall (Zhou et al., 2006), and METEOR (Banerjee and Lavie, 2005) achieve higher correlation. In this paper, we propose a new automatic MT evaluation metric, MAXSTM, that compares a pair of system-reference sentences by extracting n-grams and dependency relations. Recognizing that different concepts can be expressed in a variety of ways, we allow matching across synonyms and also compute a score between two matching items (such as between two n-grams or between two dependency relations), which indicates their degree of similarity with each other. Having weighted matches between item</context>
<context citStr="Gimenez and Marquez, 2007" endWordPosition="1021" position="6330" startWordPosition="1018">fied, or clipped n-gram precision. With this, an n-gram (from both the system and reference translation) is considered to be exhausted or used after participating in a match. Hence, each system n-gram is “clipped” by the maximum number of times it appears in any reference translation. To prevent short system translations from receiving too high a score and to compensate for its lack of a recall component, BLEU incorporates a brevity penalty. This penalizes the score of a system if the length of its entire translation output is shorter than the length of the reference text. 2.2 Semantic Roles (Gimenez and Marquez, 2007) proposed using deeper linguistic information to evaluate MT performance. For evaluation in the ACL-07 MT workshop, the authors used the metric which they termed as SR-Or-*1. This metric first counts the number of lexical overlaps SR-Or-t for all the different semantic roles t that are found in the system and reference translation sentence. A uniform average of the counts is then taken as the score for the sentence pair. In their work, the different semantic roles t they considered include the various core and adjunct arguments as defined in the PropBank project (Palmer et al., 2005). For inst</context>
<context citStr="Gimenez and Marquez, 2007" endWordPosition="3057" position="18353" startWordPosition="3054">igram and trigram bipartite graphs, we similarly calculate their respective w(M) and add to the corresponding matchuni and matchtri. Based on matchuni, matchbi, and matchtri, we calculate their corresponding precision P and recall R, from which we obtain their respective Fmean scores via Equation 1. Using bigrams for illustration, we calculate its P and R as: matchbi P= no. of bigrams in system translation matchbi R = no. of bigrams in reference translation 4.2 Dependency Relations Besides matching a pair of system-reference sentences based on the surface form of words, previous work such as (Gimenez and Marquez, 2007) and (Rajman and Hartley, 2002) had shown that deeper linguistic knowledge such as semantic roles and syntax can be usefully exploited. In the previous subsection, we describe our method of using bipartite graphs for matching of ngrams found in a sentence pair. This use of bipartite graphs, however, is a very general framework to obtain an optimal alignment of the corresponding “information items” contained within a sentence pair. Hence, besides matching based on n-gram strings, we can also match other “information items”, such as dependency relations. n Si i=1 Syn(lsi,lri) = 59 Metric Adequac</context>
</contexts>
<marker>Gimenez, Marquez, 2007</marker>
<rawString>J. Gimenez and L. Marquez. 2007. Linguistic features for automatic evaluation of heterogenous MT systems. In Proceedings of the Second Workshop on Statistical Machine Translation, ACL07, pages 256–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H W Kuhn</author>
</authors>
<title>The hungarian method for the assignment problem.</title>
<date>1955</date>
<journal>Naval Research Logistic Quarterly,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context citStr="Kuhn, 1955" endWordPosition="475" position="2974" startWordPosition="474">cepts can be expressed in a variety of ways, we allow matching across synonyms and also compute a score between two matching items (such as between two n-grams or between two dependency relations), which indicates their degree of similarity with each other. Having weighted matches between items means that there could be many possible ways to match, or link items from a system translation sentence to a reference translation sentence. To match each system item to at most one reference item, we model the items in the sentence pair as nodes in a bipartite graph and use the Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) to find a maximum weight matching (or alignment) between the items in polynomial time. The weights (from the edges) of the resulting graph will then be added to determine the final similarity score between the pair of sentences. Proceedings of ACL-08: HLT, pages 55–62, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics Although a maximum weight bipartite graph was also used in the recent work of (Taskar et al., 2005), their focus was on learning supervised models for single word alignment between sentences from a source and target language. The co</context>
<context citStr="Kuhn, 1955" endWordPosition="2883" position="17313" startWordPosition="2882"> show an example of a complete bipartite graph, constructed for a set of three system bigrams (s1, s2, s3) and three reference bigrams (r1, r2, r3), and the weight of the connecting edge between two bigrams represents their degree of similarity. Next, we aim to find a maximum weight matching (or alignment) between the bigrams such that each system (reference) bigram is connected to exactly one reference (system) bigram. This maximum weighted bipartite matching problem can be solved in O(n3) time (where n refers to the number of nodes, or vertices in the graph) using the KuhnMunkres algorithm (Kuhn, 1955; Munkres, 1957). The bottom half of Figure 1 shows the resulting maximum weighted bipartite graph, where the alignment represents the maximum weight matching, out of all possible alignments. Once we have solved and obtained a maximum weight matching M for the bigram bipartite graph, we sum up the weights of the edges to obtain the weight of the matching M: w(M) = EeEM w(e), and add w(M) to matchbi. From the unigram and trigram bipartite graphs, we similarly calculate their respective w(M) and add to the corresponding matchuni and matchtri. Based on matchuni, matchbi, and matchtri, we calculat</context>
</contexts>
<marker>Kuhn, 1955</marker>
<rawString>H. W. Kuhn. 1955. The hungarian method for the assignment problem. Naval Research Logistic Quarterly, 2(1):83–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavie</author>
<author>A Agarwal</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation, ACL07,</booktitle>
<pages>228--231</pages>
<contexts>
<context citStr="Lavie and Agarwal, 2007" endWordPosition="4309" position="26311" startWordPosition="4306">issions, and there are four English reference translation texts. Since implementations of the BLEU and METEOR metrics are publicly available, we score the system submissions using BLEU (version 11b with its default settings), METEOR, and MAXSIM, showing the resulting correlations in Table 4. For METEOR, when used with its originally proposed parameter values of (α=0.9, 0=3.0, 'y=0.5), which the METEOR researchers mentioned were based on some early experimental work (Banerjee and Lavie, 2005), we obtain an average correlation value of 0.915, as shown in the row “METEOR”. In the recent work of (Lavie and Agarwal, 2007), the values of these parameters were tuned to be (α=0.81, 0=0.83, 'y=0.28), based on experiments on the NIST 2003 and 2004 Arabic-English evaluation datasets. When METEOR was run with these new parameter values, it returned an average correlation value of 61 0.972, as shown in the row “METEOR (optimized)”. MAXSIM using only n-gram information (MAXSIM,,,) gives an average correlation value of 0.800, while adding dependency information (MAXSIM�+d) improves the correlation value to 0.915. Note that so far, the parameters of MAXSIM are not optimized and we simply perform uniform averaging of the </context>
</contexts>
<marker>Lavie, Agarwal, 2007</marker>
<rawString>A. Lavie and A. Agarwal. 2007. METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments. In Proceedings of the Second Workshop on Statistical Machine Translation, ACL07, pages 228–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL05,</booktitle>
<pages>91--98</pages>
<contexts>
<context citStr="McDonald et al., 2005" endWordPosition="3249" position="19614" startWordPosition="3245">IMn+d 0.780 0.827 0.875 0.760 0.811 MAXSIMn 0.804 0.845 0.893 0.766 0.827 Semantic-role 0.774 0.839 0.804 0.742 0.790 ParaEval-recall 0.712 0.742 0.769 0.798 0.755 METEOR 0.701 0.719 0.746 0.670 0.709 BLEU 0.690 0.722 0.672 0.603 0.672 Table 1: Overall correlations on the Europarl and News Commentary datasets. The “Semantic-role overlap” metric is abbreviated as “Semantic-role”. Note that each figure above represents 6 translation tasks: the Europarl and News Commentary datasets each with 3 language pairs (German-English, Spanish-English, French-English). In our work, we train the MSTParser4 (McDonald et al., 2005) on the Penn Treebank Wall Street Journal (WSJ) corpus, and use it to extract dependency relations from a sentence. Currently, we focus on extracting only two relations: subject and object. For each relation (ch, dp, pa) extracted, we note the child lemma ch of the relation (often a noun), the relation type dp (either subject or object), and the parent lemma pa of the relation (often a verb). Then, using the system relations and reference relations extracted from a system-reference sentence pair, we similarly construct a bipartite graph, where each node is a relation (ch, dp, pa). We define th</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings ofACL05, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
<author>R Green</author>
<author>J P Turian</author>
</authors>
<title>Precision and recall of machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT-NAACL03,</booktitle>
<pages>61--63</pages>
<contexts>
<context citStr="Melamed et al., 2003" endWordPosition="1739" position="10595" startWordPosition="1736"> of the alignment. Note that METEOR consists of three parameters that need to be optimized based on experimentation: α, Q, and 'y. 3 Metric Design Considerations We first review some aspects of existing metrics and highlight issues that should be considered when designing an MT evaluation metric. • Intuitive interpretation: To compensate for the lack of recall, BLEU incorporates a brevity penalty. This, however, prevents an intuitive interpretation of its scores. To address this, standard measures like precision and recall could be used, as in some previous research (Banerjee and Lavie, 2005; Melamed et al., 2003). • Allowing for variation: BLEU only counts exact word matches. Languages, however, often allow a great deal of variety in vocabulary and in the ways concepts are expressed. Hence, using information such as synonyms or dependency relations could potentially address the issue better. • Matches should be weighted: Current metrics either match, or don’t match a pair of items. We note, however, that matches between items (such as words, n-grams, etc.) should be weighted according to their degree of similarity. 4 The Maximum Similarity Metric We now describe our proposed metric, Maximum Similarity</context>
</contexts>
<marker>Melamed, Green, Turian, 2003</marker>
<rawString>I. D. Melamed, R. Green, and J. P. Turian. 2003. Precision and recall of machine translation. In Proceedings ofHLT-NAACL03, pages 61–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context citStr="Miller, 1990" endWordPosition="1510" position="9278" startWordPosition="1509">t number of mappings is selected. If there is a tie, then the alignment with the least number of unigram mapping crosses is selected. The three stages of “exact”, “porter stem”, and “WN synonymy” are usually applied in sequence to create alignments. The “exact” stage maps unigrams if they have the same surface form. The “porter stem” stage then considers the remaining unmapped unigrams and maps them if they are the same after applying the Porter stemmer. Finally, the “WN synonymy” stage considers all remaining unigrams and maps two unigrams if they are synonyms in the WordNet sense inventory (Miller, 1990). Once the final alignment has been produced, unigram precision P (number of unigram matches m divided by the total number of system unigrams) and unigram recall R (m divided by the total number of reference unigrams) are calculated and combined into a single parameterized harmonic mean (Rijsbergen, 1979): P · R (1) αP + (1 − α)R To account for longer matches and the amount of fragmentation represented by the alignment, METEOR groups the matched unigrams into as few chunks as possible and imposes a penalty based on the number of chunks. The METEOR score for a pair of sentences is: � score` = 1</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>G. A. Miller. 1990. WordNet: An on-line lexical database. International Journal of Lexicography, 3(4):235–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Munkres</author>
</authors>
<title>Algorithms for the assignment and transportation problems.</title>
<date>1957</date>
<journal>Journal of the Society for Industrial and Applied Mathematics,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context citStr="Munkres, 1957" endWordPosition="477" position="2990" startWordPosition="476"> expressed in a variety of ways, we allow matching across synonyms and also compute a score between two matching items (such as between two n-grams or between two dependency relations), which indicates their degree of similarity with each other. Having weighted matches between items means that there could be many possible ways to match, or link items from a system translation sentence to a reference translation sentence. To match each system item to at most one reference item, we model the items in the sentence pair as nodes in a bipartite graph and use the Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) to find a maximum weight matching (or alignment) between the items in polynomial time. The weights (from the edges) of the resulting graph will then be added to determine the final similarity score between the pair of sentences. Proceedings of ACL-08: HLT, pages 55–62, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics Although a maximum weight bipartite graph was also used in the recent work of (Taskar et al., 2005), their focus was on learning supervised models for single word alignment between sentences from a source and target language. The contributions of t</context>
<context citStr="Munkres, 1957" endWordPosition="2885" position="17329" startWordPosition="2884">mple of a complete bipartite graph, constructed for a set of three system bigrams (s1, s2, s3) and three reference bigrams (r1, r2, r3), and the weight of the connecting edge between two bigrams represents their degree of similarity. Next, we aim to find a maximum weight matching (or alignment) between the bigrams such that each system (reference) bigram is connected to exactly one reference (system) bigram. This maximum weighted bipartite matching problem can be solved in O(n3) time (where n refers to the number of nodes, or vertices in the graph) using the KuhnMunkres algorithm (Kuhn, 1955; Munkres, 1957). The bottom half of Figure 1 shows the resulting maximum weighted bipartite graph, where the alignment represents the maximum weight matching, out of all possible alignments. Once we have solved and obtained a maximum weight matching M for the bigram bipartite graph, we sum up the weights of the edges to obtain the weight of the matching M: w(M) = EeEM w(e), and add w(M) to matchbi. From the unigram and trigram bipartite graphs, we similarly calculate their respective w(M) and add to the corresponding matchuni and matchtri. Based on matchuni, matchbi, and matchtri, we calculate their correspo</context>
</contexts>
<marker>Munkres, 1957</marker>
<rawString>J. Munkres. 1957. Algorithms for the assignment and transportation problems. Journal of the Society for Industrial and Applied Mathematics, 5(1):32–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context citStr="Palmer et al., 2005" endWordPosition="1123" position="6920" startWordPosition="1120">es (Gimenez and Marquez, 2007) proposed using deeper linguistic information to evaluate MT performance. For evaluation in the ACL-07 MT workshop, the authors used the metric which they termed as SR-Or-*1. This metric first counts the number of lexical overlaps SR-Or-t for all the different semantic roles t that are found in the system and reference translation sentence. A uniform average of the counts is then taken as the score for the sentence pair. In their work, the different semantic roles t they considered include the various core and adjunct arguments as defined in the PropBank project (Palmer et al., 2005). For instance, SR-Or-AO refers to the number of lexical overlaps between the AO arguments. To extract semantic roles from a sentence, several processes such as lemmatization, partof-speech tagging, base phrase chunking, named entity tagging, and finally semantic role tagging need to be performed. 2.3 ParaEval The ParaEval metric (Zhou et al., 2006) uses a large collection of paraphrases, automatically extracted from parallel corpora, to evaluate MT performance. To compare a pair of sentences, ParaEval first locates paraphrase matches between the two 'Verified through personal communication as</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>M. Palmer, D. Gildea, and P. Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL02,</booktitle>
<pages>311--318</pages>
<contexts>
<context citStr="Papineni et al., 2002" endWordPosition="225" position="1447" startWordPosition="222">luated on data from the ACL-07 MT workshop, our proposed metric achieves higher correlation with human judgements than all 11 automatic MT evaluation metrics that were evaluated during the workshop. 1 Introduction In recent years, machine translation (MT) research has made much progress, which includes the introduction of automatic metrics for MT evaluation. Since human evaluation of MT output is time consuming and expensive, having a robust and accurate automatic MT evaluation metric that correlates well with human judgement is invaluable. Among all the automatic MT evaluation metrics, BLEU (Papineni et al., 2002) is the most widely used. Although BLEU has played a crucial role in the progress of MT research, it is becoming evident that BLEU does not correlate with human judgement 55 well enough, and suffers from several other deficiencies such as the lack of an intuitive interpretation of its scores. During the recent ACL-07 workshop on statistical MT (Callison-Burch et al., 2007), a total of 11 automatic MT evaluation metrics were evaluated for correlation with human judgement. The results show that, as compared to BLEU, several recently proposed metrics such as Semantic-role overlap (Gimenez and Mar</context>
<context citStr="Papineni et al., 2002" endWordPosition="854" position="5306" startWordPosition="851">on with human judgements than all of the 11 automatic MT evaluation metrics evaluated during the workshop. In the next section, we describe several existing metrics. In Section 3, we discuss issues to consider when designing a metric. In Section 4, we describe our proposed metric. In Section 5, we present our experimental results. Finally, we outline future work in Section 6, before concluding in Section 7. 2 Automatic Evaluation Metrics In this section, we describe BLEU, and the three metrics which achieved higher correlation results than BLEU in the recent ACL-07 MT workshop. 2.1 BLEU BLEU (Papineni et al., 2002) is essentially a precision-based metric and is currently the standard metric for automatic evaluation of MT performance. To score a system translation, BLEU tabulates the number of n-gram matches of the system translation against one or more reference translations. Generally, more n-gram matches result in a higher BLEU score. When determining the matches to calculate precision, BLEU uses a modified, or clipped n-gram precision. With this, an n-gram (from both the system and reference translation) is considered to be exhausted or used after participating in a match. Hence, each system n-gram i</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings ofACL02, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rajman</author>
<author>A Hartley</author>
</authors>
<title>Automatic ranking of MT systems.</title>
<date>2002</date>
<booktitle>In Proceedings ofLREC02,</booktitle>
<pages>1247--1253</pages>
<contexts>
<context citStr="Rajman and Hartley, 2002" endWordPosition="3062" position="18384" startWordPosition="3059">hs, we similarly calculate their respective w(M) and add to the corresponding matchuni and matchtri. Based on matchuni, matchbi, and matchtri, we calculate their corresponding precision P and recall R, from which we obtain their respective Fmean scores via Equation 1. Using bigrams for illustration, we calculate its P and R as: matchbi P= no. of bigrams in system translation matchbi R = no. of bigrams in reference translation 4.2 Dependency Relations Besides matching a pair of system-reference sentences based on the surface form of words, previous work such as (Gimenez and Marquez, 2007) and (Rajman and Hartley, 2002) had shown that deeper linguistic knowledge such as semantic roles and syntax can be usefully exploited. In the previous subsection, we describe our method of using bipartite graphs for matching of ngrams found in a sentence pair. This use of bipartite graphs, however, is a very general framework to obtain an optimal alignment of the corresponding “information items” contained within a sentence pair. Hence, besides matching based on n-gram strings, we can also match other “information items”, such as dependency relations. n Si i=1 Syn(lsi,lri) = 59 Metric Adequacy Fluency Rank Constituent Aver</context>
</contexts>
<marker>Rajman, Hartley, 2002</marker>
<rawString>M. Rajman and A. Hartley. 2002. Automatic ranking of MT systems. In Proceedings ofLREC02, pages 1247– 1253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of EMNLP96,</booktitle>
<pages>133--142</pages>
<contexts>
<context citStr="Ratnaparkhi, 1996" endWordPosition="1890" position="11545" startWordPosition="1889">ither match, or don’t match a pair of items. We note, however, that matches between items (such as words, n-grams, etc.) should be weighted according to their degree of similarity. 4 The Maximum Similarity Metric We now describe our proposed metric, Maximum Similarity (MAXSIM), which is based on precision and recall, allows for synonyms, and weights the matches found. Fmean = 57 Given a pair of English sentences to be compared (a system translation against a reference translation), we perform tokenization2, lemmatization using WordNet3, and part-of-speech (POS) tagging with the MXPOST tagger (Ratnaparkhi, 1996). Next, we remove all non-alphanumeric tokens. Then, we match the unigrams in the system translation to the unigrams in the reference translation. Based on the matches, we calculate the recall and precision, which we then combine into a single Fmean unigram score using Equation 1. Similarly, we also match the bigrams and trigrams of the sentence pair and calculate their corresponding Fmean scores. To obtain a single similarity score scores for this sentence pair s, we simply average the three Fmean scores. Then, to obtain a single similarity score sim-score for the entire system corpus, we rep</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proceedings of EMNLP96, pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Rijsbergen</author>
</authors>
<title>Information Retrieval.</title>
<date>1979</date>
<location>Butterworths, London, UK,</location>
<note>2nd edition.</note>
<contexts>
<context citStr="Rijsbergen, 1979" endWordPosition="1560" position="9584" startWordPosition="1558">e surface form. The “porter stem” stage then considers the remaining unmapped unigrams and maps them if they are the same after applying the Porter stemmer. Finally, the “WN synonymy” stage considers all remaining unigrams and maps two unigrams if they are synonyms in the WordNet sense inventory (Miller, 1990). Once the final alignment has been produced, unigram precision P (number of unigram matches m divided by the total number of system unigrams) and unigram recall R (m divided by the total number of reference unigrams) are calculated and combined into a single parameterized harmonic mean (Rijsbergen, 1979): P · R (1) αP + (1 − α)R To account for longer matches and the amount of fragmentation represented by the alignment, METEOR groups the matched unigrams into as few chunks as possible and imposes a penalty based on the number of chunks. The METEOR score for a pair of sentences is: � score` = 1 − (no. of chunks)'a F m Y m ean where -y (no. of chunks) represents the fragmentam tion penalty of the alignment. Note that METEOR consists of three parameters that need to be optimized based on experimentation: α, Q, and 'y. 3 Metric Design Considerations We first review some aspects of existing metrics</context>
</contexts>
<marker>Rijsbergen, 1979</marker>
<rawString>C. Rijsbergen. 1979. Information Retrieval. Butterworths, London, UK, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>S Lacoste-Julien</author>
<author>D Klein</author>
</authors>
<title>A discriminative matching approach to word alignment.</title>
<date>2005</date>
<booktitle>In Proceedings ofHLT/EMNLP05,</booktitle>
<pages>73--80</pages>
<contexts>
<context citStr="Taskar et al., 2005" endWordPosition="550" position="3441" startWordPosition="547"> item to at most one reference item, we model the items in the sentence pair as nodes in a bipartite graph and use the Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) to find a maximum weight matching (or alignment) between the items in polynomial time. The weights (from the edges) of the resulting graph will then be added to determine the final similarity score between the pair of sentences. Proceedings of ACL-08: HLT, pages 55–62, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics Although a maximum weight bipartite graph was also used in the recent work of (Taskar et al., 2005), their focus was on learning supervised models for single word alignment between sentences from a source and target language. The contributions of this paper are as follows. Current metrics (such as BLEU, METEOR, Semantic-role overlap, ParaEval-recall, etc.) do not assign different weights to their matches: either two items match, or they don’t. Also, metrics such as METEOR determine an alignment between the items of a sentence pair by using heuristics such as the least number of matching crosses. In contrast, we propose weighting different matches differently, and then obtain an optimal set </context>
</contexts>
<marker>Taskar, Lacoste-Julien, Klein, 2005</marker>
<rawString>B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A discriminative matching approach to word alignment. In Proceedings ofHLT/EMNLP05, pages 73–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zhou</author>
<author>C Y Lin</author>
<author>E Hovy</author>
</authors>
<title>Re-evaluating machine translation results with paraphrase support.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP06,</booktitle>
<pages>77--84</pages>
<contexts>
<context citStr="Zhou et al., 2006" endWordPosition="333" position="2095" startWordPosition="330">ough BLEU has played a crucial role in the progress of MT research, it is becoming evident that BLEU does not correlate with human judgement 55 well enough, and suffers from several other deficiencies such as the lack of an intuitive interpretation of its scores. During the recent ACL-07 workshop on statistical MT (Callison-Burch et al., 2007), a total of 11 automatic MT evaluation metrics were evaluated for correlation with human judgement. The results show that, as compared to BLEU, several recently proposed metrics such as Semantic-role overlap (Gimenez and Marquez, 2007), ParaEval-recall (Zhou et al., 2006), and METEOR (Banerjee and Lavie, 2005) achieve higher correlation. In this paper, we propose a new automatic MT evaluation metric, MAXSTM, that compares a pair of system-reference sentences by extracting n-grams and dependency relations. Recognizing that different concepts can be expressed in a variety of ways, we allow matching across synonyms and also compute a score between two matching items (such as between two n-grams or between two dependency relations), which indicates their degree of similarity with each other. Having weighted matches between items means that there could be many poss</context>
<context citStr="Zhou et al., 2006" endWordPosition="1178" position="7271" startWordPosition="1175">anslation sentence. A uniform average of the counts is then taken as the score for the sentence pair. In their work, the different semantic roles t they considered include the various core and adjunct arguments as defined in the PropBank project (Palmer et al., 2005). For instance, SR-Or-AO refers to the number of lexical overlaps between the AO arguments. To extract semantic roles from a sentence, several processes such as lemmatization, partof-speech tagging, base phrase chunking, named entity tagging, and finally semantic role tagging need to be performed. 2.3 ParaEval The ParaEval metric (Zhou et al., 2006) uses a large collection of paraphrases, automatically extracted from parallel corpora, to evaluate MT performance. To compare a pair of sentences, ParaEval first locates paraphrase matches between the two 'Verified through personal communication as this is not evident in their paper. 56 sentences. Then, unigram matching is performed on the remaining words that are not matched using paraphrases. Based on the matches, ParaEval will then elect to use either unigram precision or unigram recall as its score for the sentence pair. In the ACL-07 MT workshop, ParaEval based on recall (ParaEval-recall</context>
</contexts>
<marker>Zhou, Lin, Hovy, 2006</marker>
<rawString>L. Zhou, C. Y. Lin, and E. Hovy. 2006. Re-evaluating machine translation results with paraphrase support. In Proceedings of EMNLP06, pages 77–84.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>