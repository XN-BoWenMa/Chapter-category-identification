<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant confidence="0.000089" no="0">
<title confidence="0.996975">
Lattice Desegmentation for Statistical Machine Translation
</title>
<author confidence="0.999076">
Mohammad Salameh† Colin Cherry‡ Grzegorz Kondrak†
</author>
<affiliation confidence="0.9960985">
†Department of Computing Science ‡National Research Council Canada
University of Alberta 1200 Montreal Road
</affiliation>
<address confidence="0.777277">
Edmonton, AB, T6G 2E8, Canada Ottawa, ON, K1A 0R6, Canada
</address>
<email confidence="0.985871">
{msalameh,gkondrak}@ualberta.ca Colin.Cherry@nrc-cnrc.gc.ca
</email>
<sectionHeader confidence="0.997237" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999933695652174">Morphological segmentation is an effective sparsity reduction strategy for statistical machine translation (SMT) involving morphologically complex languages. When translating into a segmented language, an extra step is required to desegment the output; previous studies have desegmented the 1-best output from the decoder. In this paper, we expand our translation options by desegmenting n-best lists or lattices. Our novel lattice desegmentation algorithm effectively combines both segmented and desegmented views of the target language for a large subspace of possible translation outputs, which allows for inclusion of features related to the desegmentation process, as well as an unsegmented language model (LM). We investigate this technique in the context of English-to-Arabic and English-to-Finnish translation, showing significant improvements in translation quality over desegmentation of 1-best decoder outputs.</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999637348837209">Morphological segmentation is considered to be indispensable when translating between English and morphologically complex languages such as Arabic. Morphological complexity leads to much higher type to token ratios than English, which can create sparsity problems during translation model estimation. Morphological segmentation addresses this issue by splitting surface forms into meaningful morphemes, while also performing orthographic transformations to further reduce sparsity. For example, the Arabic noun J9.v-U lldwl “to the countries” is segmented as l+ “to” Aldwl “the countries”. When translating from Arabic, this segmentation process is performed as input preprocessing and is otherwise transparent to the translation system. However, when translating into Arabic, the decoder produces segmented output, which must be desegmented to produce readable text. For example, l+ Aldwl must be converted to lldwl. Desegmentation is typically performed as a post-processing step that is independent from the decoding process. While this division of labor is useful, the pipeline approach may prevent the desegmenter from recovering from errors made by the decoder. Despite the efforts of the decoder’s various component models, the system may produce mismatching segments, such as s+ hzymp, which pairs the future particle s+ “will” with a noun hzymp “defeat”, instead of a verb. In this scenario, there is no right desegmentation; the postprocessor has been dealt a losing hand. In this work, we show that it is possible to maintain the sparsity-reducing benefit of segmentation while translating directly into unsegmented text. We desegment a large set of possible decoder outputs by processing n-best lists or lattices, which allows us to consider both the segmented and desegmented output before locking in the decoder’s decision. We demonstrate that significant improvements in translation quality can be achieved by training a linear model to re-rank this transformed translation space.</bodyText>
<sectionHeader confidence="0.999947" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9995763">Translating into morphologically complex languages is a challenging and interesting task that has received much recent attention. Most techniques approach the problem by transforming the target language in some manner before training the translation model. They differ in what transformations are performed and at what stage they are reversed. The transformation might take the form of a morphological analysis or a morphological segmentation.</bodyText>
<page confidence="0.910078">
100
</page>
<note confidence="0.865054">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 100–110,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.978139">
2.1 Morphological Analysis
</subsectionHeader>
<bodyText confidence="0.999979761904762">Many languages have access to morphological analyzers, which annotate surface forms with their lemmas and morphological features. Bojar (2007) incorporates such analyses into a factored model, to either include a language model over target morphological tags, or model the generation of morphological features. Other approaches train an SMT system to predict lemmas instead of surface forms, and then inflect the SMT output as a postprocessing step (Minkov et al., 2007; Clifton and Sarkar, 2011; Fraser et al., 2012; El Kholy and Habash, 2012b). Alternatively, one can reparameterize existing phrase tables as exponential models, so that translation probabilities account for source context and morphological features (Jeong et al., 2010; Subotin, 2011). Of these approaches, ours is most similar to the translate-then-inflect approach, except we translate and then desegment. In particular, Toutanova et al. (2008) inflect and re-rank n-best lists in a similar manner to how we desegment and re-rank n-best lists or lattices.</bodyText>
<subsectionHeader confidence="0.997076">
2.2 Morphological Segmentation
</subsectionHeader>
<bodyText confidence="0.99980468918919">Instead of producing an abstract feature layer, morphological segmentation transforms the target sentence by segmenting relevant morphemes, which are then handled as regular tokens during alignment and translation. This is done to reduce sparsity and to improve correspondence with the source language (usually English). Such a segmentation can be produced as a byproduct of analysis (Oflazer and Durgar El-Kahlout, 2007; Badr et al., 2008; El Kholy and Habash, 2012a), or may be produced using an unsupervised morphological segmenter such as Morfessor (Luong et al., 2010; Clifton and Sarkar, 2011). Work on target language morphological segmentation for SMT can be divided into three subproblems: segmentation, desegmentation and integration. Our work is concerned primarily with the integration problem, but we will discuss each subproblem in turn. The usefulness of a target segmentation depends on its correspondence to the source language. If a morphological feature does not manifest itself as a separate token in the source, then it may be best to leave its corresponding segment attached to the stem. A number of studies have looked into what granularity of segmentation is best suited for a particular language pair (Oflazer and Durgar El-Kahlout, 2007; Badr et al., 2008; Clifton and Sarkar, 2011; El Kholy and Habash, 2012a). Since our focus here is on integrating segmentation into the decoding process, we simply adopt the segmentation strategies recommended by previous work: the Penn Arabic Treebank scheme for English-Arabic (El Kholy and Habash, 2012a), and an unsupervised scheme for EnglishFinnish (Clifton and Sarkar, 2011). Desegmentation is the process of converting segmented words into their original surface form. For many segmentations, especially unsupervised ones, this amounts to simple concatenation. However, more complex segmentations, such as the Arabic tokenization provided by MADA (Habash et al., 2009), require further orthographic adjustments to reverse normalizations performed during segmentation. Badr et al. (2008) present two Arabic desegmentation schemes: table-based and rule-based. El Kholy and Habash (2012a) provide an extensive study on the influence of segmentation and desegmentation on English-toArabic SMT. They introduce an additional desegmentation technique that augments the table-based approach with an unsegmented language model. Salameh et al. (2013) replace rule-based desegmentation with a discriminatively-trained character transducer. In this work, we adopt the Table+Rules approach of El Kholy and Habash (2012a) for English-Arabic, while concatenation is sufficient for English-Finnish. Work on integration attempts to improve SMT performance for morphologically complex target languages by going beyond simple preand postprocessing. Oflazer and Durgar El-Kahlout (2007) desegment 1000-best lists for English-to-Turkish translation to enable scoring with an unsegmented language model. Unlike our work, they replace the segmented language model with the unsegmented one, allowing them to tune the linear model parameters by hand. We use both segmented and unsegmented language models, and tune automatically to optimize BLEU. Like us, Luong et al. (2010) tune on unsegmented references,1 and translate with both segmented and unsegmented language models for English-to-Finnish translation. However, they adopt a scheme of word-boundary-aware morpheme-level phrase extraction, meaning that target phrases include only complete words, though those words are segmented into morphemes.</bodyText>
<footnote confidence="0.9987984">
1Tuning on unsegmented references does not require sub-
stantial modifications to the standard SMT pipeline. For ex-
ample, Badr et al. (2008) also tune on unsegmented refer-
ences by simply desegmenting SMT output before MERT
collects sufficient statistics for BLEU.
</footnote>
<page confidence="0.998583">
101
</page>
<bodyText confidence="0.999979961538462">This enables full decoder integration, where we do n-best and lattice re-ranking. But it also comes at a substantial cost: when target phrases include only complete words, the system can only generate word forms that were seen during training. In this setting, the sparsity reduction from segmentation helps word alignment and target language modeling, but it does not result in a more expressive translation model. Furthermore, it becomes substantially more difficult to have non-adjacent source tokens contribute morphemes to a single target word. For example, when translating “with his blue car” into the Arabic �hj �ll vjh.�bsyArth AlzrqA’, the target word bsyArth is composed of three tokens: b+ “with”, syArp “car” and +h “his”. With word-boundaryaware phrase extraction, a phrase pair containing all of “with his blue car” must have been seen in the parallel data to translate the phrase correctly at test time. With lattice desegmentation, we need only to have seen AlzrqA’ “blue” and the three morphological pieces of bsyArth for the decoder and desegmenter to assemble the phrase.</bodyText>
<sectionHeader confidence="0.998203" genericHeader="method">
3 Methods
</sectionHeader>
<bodyText confidence="0.999970454545455">Our goal in this work is to benefit from the sparsity-reducing properties of morphological segmentation while simultaneously allowing the system to reason about the final surface forms of the target language. We approach this problem by augmenting an SMT system built over target segments with features that reflect the desegmented target words. In this section, we describe our various strategies for desegmenting the SMT system’s output space, along with the features that we add to take advantage of this desegmented view.</bodyText>
<subsectionHeader confidence="0.999512">
3.1 Baselines
</subsectionHeader>
<bodyText confidence="0.998127428571429">The two obvious baseline approaches each decode using one view of the target language. The unsegmented approach translates without segmenting the target. This trivially allows for an unsegmented language model and never makes desegmentation errors. However, it suffers from data sparsity and poor token-to-token correspondence with the source language. The one-best desegmentation approach segments the target language at training time and then desegments the one-best output in postprocessing. This resolves the sparsity issue, but does not allow the decoder to take into account features of the desegmented target. To the best of our knowledge, we are the first group to go beyond one-best desegmentation for English-to-Arabic translation. In English-to-Finnish, although alternative integration strategies have seen some success (Luong et al., 2010), the current state-ofthe-art performs one-best-desegmentation (Clifton and Sarkar, 2011).</bodyText>
<subsectionHeader confidence="0.999163">
3.2 n-best Desegmentation
</subsectionHeader>
<bodyText confidence="0.999973590909091">The one-best approach can be extended easily by desegmenting n-best lists of segmented decoder output. Doing so enables the inclusion of an unsegmented target language model, and with a small amount of bookkeeping, it also allows the inclusion of features related to the operations performed during desegmentation (see Section 3.4). With new features reflecting the desegmented output, we can re-tune our enhanced linear model on a development set. Following previous work, we will desegment 1000-best lists (Oflazer and Durgar El-Kahlout, 2007). Once n-best lists have been desegmented, we can tune on unsegmented references as a sidebenefit. This could improve translation quality, as it brings our training scenario closer to our test scenario (test BLEU is always measured on unsegmented references). In particular, it could address issues with translation length mismatch. Previous work that has tuned on unsegmented references has reported mixed results (Badr et al., 2008; Luong et al., 2010).</bodyText>
<subsectionHeader confidence="0.999614">
3.3 Lattice Desegmentation
</subsectionHeader>
<bodyText confidence="0.999996428571429">An n-best list reflects a tiny portion of a decoder’s search space, typically fixed at 1000 hypotheses. Lattices2 can represent an exponential number of hypotheses in a compact structure. In this section, we discuss how a lattice from a multi-stack phrasebased decoder such as Moses (Koehn et al., 2007) can be desegmented to enable word-level features.</bodyText>
<subsectionHeader confidence="0.971998">
Finite State Analogy
</subsectionHeader>
<bodyText confidence="0.9998355">A phrase-based decoder produces its output from left to right, with each operation appending the translation of a source phrase to a growing target hypothesis. Translation continues until each source word has been covered exactly once (Koehn et al., 2003).</bodyText>
<footnote confidence="0.950614">
2Or forests for hierarchical and syntactic decoders.
</footnote>
<page confidence="0.995776">
102
</page>
<figureCaption confidence="0.901335">
Figure 1: The finite state pipeline for a lattice translating the English fragment “with the child’s game”.
The input morpheme lattice (a) is desegmented by composing it with the desegmenting transducer (b) to
produce the word lattice (c). The tokens in (a) are: b+ “with”, lEbp “game”, +hm “their”, +hA “her”,
and AlTfl “the child”.
</figureCaption>
<figure confidence="0.9993866">
(c)
(a)
5
5
+hm
blEbthm
+hA
blEbthA
Transduces
into
4
4
0
AlTß
blEbp
b+ lEbp
0 1 2
3
2
AlTß
3
(b)
AlTfl:AlTfl
1
b+:&lt;epsilon&gt;
&lt;epsilon&gt;:blEbp
+hA:blEbthA
+hm:blEbthm
0
lEbp:&lt;epsilon&gt;
</figure>
<page confidence="0.538007">
2
</page>
<bodyText confidence="0.994832863636364">The search graph of a phrase-based decoder can be interpreted as a lattice, which can be interpreted as a finite state acceptor over target strings. In its most natural form, such an acceptor emits target phrases on each edge, but it can easily be transformed into a form with one edge per token, as shown in Figure 1a. This is sometimes referred to as a word graph (Ueffing et al., 2002), although in our case the segmented phrase table also produces tokens that correspond to morphemes. Our goal is to desegment the decoder’s output lattice, and in doing so, gain access to a compact, desegmented view of a large portion of the translation search space. This can be accomplished by composing the lattice with a desegmenting transducer that consumes morphemes and outputs desegmented words. This transducer must be able to consume every word in our lattice’s output vocabulary. We define a word using the following regular expression:</bodyText>
<equation confidence="0.6866185">
[prefix]* [stem] [suffix]* I [prefix]+ [suffix]+
(1)
</equation>
<bodyText confidence="0.999940545454546">where [prefix], [stem] and [suffix] are nonoverlapping sets of morphemes, whose members are easily determined using the segmenter’s segment boundary markers.3 The second disjunct of Equation 1 covers words that have no clear stem, such as the Arabic al lh “for him”, segmented as l+ “for” +h “him”. Equation 1 may need to be modified for other languages or segmentation schemes, but our techniques generalize to any definition that can be written as a regular expression. A desegmenting transducer can be constructed by first encoding our desegmenter as a table that maps morpheme sequences to words. Regardless of whether the original desegmenter was based on concatenation, rules or table-lookup, it can be encoded as a lattice-specific table by applying it to an enumeration of all words found in the lattice. We can then transform that table into a finite state transducer with one path per table entry. Finally, we take the closure of this transducer, so that the resulting machine can transduce any sequence of words. The desegmenting transducer for our running example is shown in Figure 1b.</bodyText>
<footnote confidence="0.689396">
3Throughout this paper, we use “+” to mark morphemes
as prefixes or suffixes, as in w+ or +h. In Equation 1 only,
we overload “+” as the Kleene cross: X+ == XX∗.
</footnote>
<page confidence="0.998228">
103
</page>
<bodyText confidence="0.999991175">Note that tokens requiring no desegmentation simply emit themselves. The lattice (Figure 1a) can then be desegmented by composing it with the transducer (1b), producing a desegmented lattice (1c). This is a natural place to introduce features that describe the desegmentation process, such as scores provided by a desegmentation table, which can be incorporated into the desegmenting transducer’s edge weights. We now have a desegmented lattice, but it has not been annotated with an unsegmented (wordlevel) language model. In order to annotate lattice edges with an n-gram LM, every path coming into a node must end with the same sequence of (n−1) tokens. If this property does not hold, then nodes must be split until it does.4 This property is maintained by the decoder’s recombination rules for the segmented LM, but it is not guaranteed for the desegmented LM. Indeed, the expanded word-level context is one of the main benefits of incorporating a word-level LM. Fortunately, LM annotation as well as any necessary lattice modifications can be performed simultaneously by composing the desegmented lattice with a finite state acceptor encoding the LM (Roark et al., 2011). In summary, we are given a segmented lattice, which encodes the decoder’s translation space as an acceptor over morphemes. We compose this acceptor with a desegmenting transducer, and then with an unsegmented LM acceptor, producing a fully annotated, desegmented lattice. Instead of using a tool kit such as OpenFst (Allauzen et al., 2007), we implement both the desegmenting transducer and the LM acceptor programmatically. This eliminates the need to construct intermediate machines, such as the lattice-specific desegmenter in Figure 1b, and facilitates working with edges annotated with feature vectors as opposed to single weights.</bodyText>
<subsectionHeader confidence="0.9499">
Programmatic Desegmentation
</subsectionHeader>
<bodyText confidence="0.999888285714286">Lattice desegmentation is a non-local lattice transformation. That is, the morphemes forming a word might span several edges, making desegmentation non-trivial. Luong et al. (2010) address this problem by forcing the decoder’s phrase table to respect word boundaries, guaranteeing that each desegmentable token sequence is local to an edge.</bodyText>
<footnote confidence="0.920397333333333">
4Or the LM composition can be done dynamically, ef-
fectively decoding the lattice with a beam or cube-pruned
search (Huang and Chiang, 2007).
</footnote>
<bodyText confidence="0.986261807692308">Inspired by the use of non-local features in forest decoding (Huang, 2008), we present an algorithm to find chains of edges that correspond to desegmentable token sequences, allowing lattice desegmentation with no phrase-table restrictions. This algorithm can be seen as implicitly constructing a customized desegmenting transducer and composing it with the input lattice on the fly. Before describing the algorithm, we define some notation. An input morpheme lattice is a triple (ns, N, £), where N is a set of nodes, £ is a set of edges, and ns E N is the start node that begins each path through the lattice. Each edge e E £ is a 4-tuple (from, to, lex, w), where from, to E N are head and tail nodes, lex is a single token accepted by this edge, and w is the (potentially vector-valued) edge weight. Tokens are drawn from one of three non-overlapping morphosyntactic sets: lex E Prefix U Stem U Suffix, where tokens that do not require desegmentation, such as complete words, punctuation and numbers, are considered to be in Stem. It is also useful to consider the set of all outgoing edges for a node n.out = {e E £|e.from = n}. With this notation in place, we can define a chain c to be a sequence of edges [e1 ... el] such that for 1 &lt; i &lt; l : ei.to = ei+1.from. We denote singleton chains with [e], and when unambiguous, we abbreviate longer chains with their start and end node [e1.from —* el.to]. A chain is valid if it emits the beginning of a word as defined by the regular expression in Equation 1. A valid chain is complete if its edges form an entire word, and if it is part of a path through the lattice that consists only of words. In Figure 1a, the complete chains are [0 —* 2], [0 —* 4], [0 —* 5], and [2 —* 3]. The path restriction on complete chains forces words to be bounded by other words in order to be complete.5 For example, if we removed the edge 2 —* 3 (AlTfl) from Figure 1a, then [0 —* 2] ([b+ lEbp]) would cease to be a complete chain, but it would still be a valid chain. Note that in the finite-state analogy, the path restriction is implicit in the composition operation. Algorithm 1 desegments a lattice by finding all complete chains and replacing each one with a single edge. It maintains a work list of nodes that lie on the boundary between words, and for each node on this list, it launches a depth first search 5Sentence-initial suffix morphemes and sentence-final prefix morphemes represent a special case that we omit for the sake of brevity. Lacking stems, they are left segmented.</bodyText>
<page confidence="0.994715">
104
</page>
<bodyText confidence="0.6621155">Algorithm 1 Desegment a lattice hns, N, Ei {Initialize output lattice and work list WL} n0s = ns, N0 = ∅, E0 = ∅, WL = [ns] while n = WL.pop() do {Work on each node only once} if n ∈ N0 then continue</bodyText>
<equation confidence="0.8624474">
N0 = N0 ∪ {n}
{Initialize the chain stack C}
C = ∅
for e ∈ n.out do
if [e] is valid then C.push([e])
</equation>
<bodyText confidence="0.7504312">{Depth-first search for complete chains} while [e1, ... , el] = C.pop() do {Attempt to extend chain} for e ∈ el.to.out do if [e1 ... el, e] is valid then to find all complete chains extending from it.</bodyText>
<equation confidence="0.818272">
C.push([e1, ... , el, e])
else
Mark [e1, ... , el] as complete
{Desegment complete chains}
if [e1, ... , el] is complete then
WL.push(el.to)
E0 = E0 ∪ {deseg([e1, ... , el])}
return hn0s, N 0, E0i
</equation>
<bodyText confidence="0.9998840625">The search recognizes the valid chain c to be complete by finding an edge e such that c + e forms a chain, but not a valid one. By inspection of Equation 1, this can only happen when a prefix or stem follows a stem or suffix, which always marks a word boundary. The chains found by this search are desegmented and then added to the output lattice as edges. The nodes at end points of these chains are added to the work list, as they lie at word boundaries by definition. Note that although this algorithm creates completely new edges, the resulting node set N0 will be a subset of the input node set N. The complement N −N0 will consist of nodes that are word-internal in all paths through the input lattice, such as node 1 in Figure 1a.</bodyText>
<subsectionHeader confidence="0.453155">
Programmatic LM Integration
</subsectionHeader>
<bodyText confidence="0.999863625">Programmatic composition of a lattice with an n-gram LM acceptor is a well understood problem. We use a dynamic program to enumerate all (n − 1)-word contexts leading into a node, and then split the node into multiple copies, one for each context. With each node corresponding to a single LM context, annotation of outgoing edges with n-gram LM scores is straightforward.</bodyText>
<subsectionHeader confidence="0.982272">
3.4 Desegmentation Features
</subsectionHeader>
<bodyText confidence="0.999662358974359">Our re-ranker has access to all of the features used by the decoder, in addition to a number of features enabled by desegmentation. Desegmentation Score We use a table-based desegmentation method for Arabic, which is based on segmenting an Arabic training corpus and memorizing the observed transformations to reverse them later. Finnish does not require a table, as all words can be desegmented with simple concatenation. The Arabic table consists of X → Y entries, where X is a target morpheme sequence and Y is a desegmented surface form. Several entries may share the same X, resulting in multiple desegmentation options. For the sake of symmetry with the unambiguous Finnish case, we augment Arabic n-best lists or lattices with only the most frequent desegmentation Y .6 We provide the desegmentation score log p(Y |X)= log (co1 you t o x Y ) as a feature, to indicate the entry’s ambiguity in the training data.7 When an X is missing from the table, we fall back on a set of desegmentation rules (El Kholy and Habash, 2012a) and this feature is set to 0. This feature is always 0 for English-Finnish. Contiguity One advantage of our approach is that it allows discontiguous source words to translate into a single target word. In order to maintain some control over this powerful capability, we create three binary features that indicate the contiguity of a desegmentation. The first feature indicates that the desegmented morphemes were translated from contiguous source words. The second indicates that the source words contained a single discontiguity, as in a word-by-word translation of the “with his blue car” example from Section 2.2. The third indicates two or more discontiguities. Unsegmented LM A 5-gram LM trained on unsegmented target text is used to assess the fluency of the desegmented word sequence.</bodyText>
<sectionHeader confidence="0.999233" genericHeader="evaluation">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.9999305">We train our English-to-Arabic system using 1.49 million sentence pairs drawn from the NIST 2012 training set, excluding the UN data. This training set contains about 40 million Arabic tokens before segmentation, and 47 million after segmentation.</bodyText>
<footnote confidence="0.98921675">
6Allowing the re-ranker to choose between multiple Y s is
a natural avenue for future work.
7We also experimented on log p(X|Y ) as an additional
feature, but observed no improvement in translation quality.
</footnote>
<page confidence="0.99909">
105
</page>
<bodyText confidence="0.999754454545455">We tune on the NIST 2004 evaluation set (1353 sentences) and evaluate on NIST 2005 (1056 sentences). As these evaluation sets are intended for Arabic-to-English translation, we select the first English reference to use as our source text. Our English-to-Finnish system is trained on the same Europarl corpus as Luong et al.(2010) and Clifton and Sarkar (2011), which has roughly one million sentence pairs. We also use their development and test sets (2000 sentences each).</bodyText>
<subsectionHeader confidence="0.996396">
4.1 Segmentation
</subsectionHeader>
<bodyText confidence="0.999505">For Arabic, morphological segmentation is performed by MADA 3.2 (Habash et al., 2009), using the Penn Arabic Treebank (PATB) segmentation scheme as recommended by El Kholy and Habash (2012a). For both segmented and unsegmented Arabic, we further normalize the script by converting different forms of Alif sI I I I and Ya S S to bare Alif I and dotless Ya S. To generate the desegmentation table, we analyze the segmentations from the Arabic side of the parallel training data to collect mappings from morpheme sequences to surface forms.</bodyText>
<equation confidence="0.365566">
s
</equation>
<bodyText confidence="0.999247176470588">For Finnish, we adopt the Unsup L-match segmentation technique of Clifton and Sarkar (2011), which uses Morfessor (Creutz and Lagus, 2005) to analyze the 5,000 most frequent Finnish words. The analysis is then applied to the Finnish side of the parallel text, and a list of segmented suffixes is collected. To improve coverage, words are further segmented according to their longest matching suffix from the list. As Morfessor does not perform any orthographic normalizations, it can be desegmented with simple concatenation.</bodyText>
<subsectionHeader confidence="0.985189">
4.2 Systems
</subsectionHeader>
<bodyText confidence="0.999981316666667">We align the parallel data with GIZA++ (Och et al., 2003) and decode using Moses (Koehn et al., 2007). The decoder’s log-linear model includes a standard feature set. Four translation model features encode phrase translation probabilities and lexical scores in both directions. Seven distortion features encode a standard distortion penalty as well as a bidirectional lexicalized reordering model. A KN-smoothed 5-gram language model is trained on the target side of the parallel data with SRILM (Stolcke, 2002). Finally, we include word and phrase penalties. The decoder uses the default parameters for English-to-Arabic, except that the maximum phrase length is set to 8. For Englishto-Finnish, we follow Clifton and Sarkar (2011) in setting the hypothesis stack size to 100, distortion limit to 6, and maximum phrase length to 20. The decoder’s log-linear model is tuned with MERT (Och, 2003). Re-ranking models are tuned using a batch variant of hope-fear MIRA (Chiang et al., 2008; Cherry and Foster, 2012), using the n-best variant for n-best desegmentation, and the lattice variant for lattice desegmentation. MIRA was selected over MERT because we have an in-house implementation that can tune on lattices very quickly. During development, we confirmed that MERT and MIRA perform similarly, as is expected with fewer than 20 features. Both the decoder’s log-linear model and the re-ranking models are trained on the same development set. Historically, we have not seen improvements from using different tuning sets for decoding and reranking. Lattices are pruned to a density of 50 edges per word before re-ranking. We test four different systems. Our first baseline is Unsegmented, where we train on unsegmented target text, requiring no desegmentation step. Our second baseline is 1-best Deseg, where we train on segmented target text and desegment the decoder’s 1-best output. Starting from the system that produced 1-best Deseg, we then output either 1000-best lists or lattices to create our two experimental systems. The 1000-best Deseg system desegments, augments and re-ranks the decoder’s 1000-best list, while Lattice Deseg does the same in the lattice. We augment n-best lists and lattices using the features described in Section 3.4.8 We evaluate our system using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). Following Clark et al. (2011), we report average scores over five random tuning replications to account for optimizer instability. For the baselines, this means 5 runs of decoder tuning. For the desegmenting re-rankers, this means 5 runs of reranker tuning, each working on n-best lists or lattices produced by the same (representative) decoder weights. We measure statistical significance using MultEval (Clark et al., 2011), which implements a stratified approximate randomization test to account for multiple tuning replications.</bodyText>
<footnote confidence="0.9698745">
8Development experiments on a small-data English-to-
Arabic scenario indicated that the Desegmentation Score was
not particularly useful, so we exclude it from the main com-
parison, but include it in the ablation experiments.
</footnote>
<page confidence="0.997014">
106
</page>
<sectionHeader confidence="0.999877" genericHeader="result">
5 Results
</sectionHeader>
<bodyText confidence="0.999930040816327">Tables 1 and 2 report results averaged over 5 tuning replications on English-to-Arabic and Englishto-Finnish, respectively. In all scenarios, both 1000-best Deseg and Lattice Deseg significantly outperform the 1-best Deseg baseline (p &lt; 0.01). For English-to-Arabic, 1-best desegmentation results in a 0.7 BLEU point improvement over training on unsegmented Arabic. Moving to lattice desegmentation more than doubles that improvement, resulting in a BLEU score of 34.4 and an improvement of 1.0 BLEU point over 1-best desegmentation. 1000-best desegmentation also works well, resulting in a 0.6 BLEU point improvement over 1-best. Lattice desegmentation is significantly better (p &lt; 0.01) than 1000-best desegmentation. For English-to-Finnish, the Unsup L-match segmentation with 1-best desegmentation does not improve over the unsegmented baseline. The segmentation may be addressing issues with model sparsity, but it is also introducing errors that would have been impossible had words been left unsegmented. In fact, even with our lattice desegmenter providing a boost, we are unable to see a significant improvement over the unsegmented model. As we attempted to replicate the approach of Clifton and Sarkar (2011) exactly by working with their segmented data, this difference is likely due to changes in Moses since the publication of their result. Nonetheless, the 1000-best and lattice desegmenters both produce significant improvements over the 1-best desegmentation baseline, with Lattice Deseg achieving a 1-point improvement in TER. These results match the established state-of-the-art on this data set, but also indicate that there is still room for improvement in identifying the best segmentation strategy for Englishto-Finnish translation. We also tried a similar Morfessor-based segmentation for Arabic, which has an unsegmented test set BLEU of 32.7. As in Finnish, the 1-best desegmentation using Morfessor did not surpass the unsegmented baseline, producing a test BLEU of only 31.4 (not shown in Table 1). Lattice desegmentation was able to boost this to 32.9, slightly above 1-best desegmentation, but well below our best MADA desegmentation result of 34.4. There appears to be a large advantage to using MADA’s supervised segmentation in this scenario.</bodyText>
<table confidence="0.998706428571429">
Model Dev Test
BLEU
BLEU TER
Unsegmented 24.4 32.7 49.4
1-best Deseg 24.4 33.4 48.6
1000-best Deseg 25.0 34.0 48.0
Lattice Deseg 25.2 34.4 47.7
</table>
<tableCaption confidence="0.9649495">
Table 1: Results for English-to-Arabic translation
using MADA’s PATB segmentation.
</tableCaption>
<table confidence="0.999862857142857">
Model Dev Test
BLEU
BLEU TER
Unsegmented 15.4 15.1 70.8
1-best Deseg 15.3 14.8 71.9
1000-best Deseg 15.4 15.1 71.5
Lattice Deseg 15.5 15.1 70.9
</table>
<tableCaption confidence="0.9920755">
Table 2: Results for English-to-Finnish translation
using unsupervised segmentation.
</tableCaption>
<subsectionHeader confidence="0.988631">
5.1 Ablation
</subsectionHeader>
<bodyText confidence="0.9999995">We conducted an ablation experiment on Englishto-Arabic to measure the impact of the various features described in Section 3.4. Table 3 compares different combinations of features using lattice desegmentation. The unsegmented LM alone yields a 0.4 point improvement over the 1-best desegmentation score. Adding contiguity indicators on top of the unsegmented LM results in another 0.6 point improvement. As anticipated, the tuner assigns negative weights to discontiguous cases, encouraging the re-ranker to select a safer translation path when possible. Judging from the output on the NIST 2005 test set, the system uses these discontiguous desegmentations very rarely: only 5% of desegmented tokens align to discontiguous source phrases. Adding the desegmentation score to these two feature groups does not improve performance, confirming the results we observed during development. The desegmentation score would likely be useful in a scenario where we provide multiple desegmentation options to the re-ranker; for now, it indicates only the ambiguity of a fixed choice, and is likely redundant with information provided by the language model.</bodyText>
<subsectionHeader confidence="0.991433">
5.2 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999994">In order to better understand the source of our improvements in the English-to-Arabic scenario, we conducted an extensive manual analysis of the differences between 1-best and lattice desegmentation on our test set.</bodyText>
<page confidence="0.996465">
107
</page>
<table confidence="0.9993402">
Features dev test
1-best Deseg 24.5 33.4
+ Unsegmented LM 24.9 33.8
+ Contiguity 25.2 34.4
+ Desegmentation Score 25.2 34.3
</table>
<tableCaption confidence="0.81063">
Table 3: The effect of feature ablation on BLEU
score for English-to-Arabic translation with lattice
desegmentation.
</tableCaption>
<bodyText confidence="0.999453170731707">We compared the output of the two systems using the Unix tool wdiff, which transforms a solution to the longestcommon-subsequence problem into a sequence of multi-word insertions and deletions (Hunt and McIlroy, 1976). We considered adjacent insertiondeletion pairs to be (potentially phrasal) substitutions, and collected them into a file, omitting any unpaired insertions or deletions. We then sampled 650 cases where the two sides of the substitution were deemed to be related, and divided these cases into categories based on how the lattice desegmentation differs from the one-best desegmentation. We consider a phrase to be correct only if it can be found in the reference. Table 4 breaks down per-phrase accuracy according to four manually-assigned categories: (1) clitical – the two systems agree on a stem, but at least one clitic, often a prefix denoting a preposition or determiner, was dropped, added or replaced; (2) lexical – a word was changed to a morphologically unrelated word with a similar meaning; (3) inflectional – the words have the same stem, but different inflection due to a change in gender, number or verb tense; (4) part-of-speech – the two systems agree on the lemma, but have selected different parts of speech. For each case covering a single phrasal difference, we compare the phrases from each system to the reference. We report the number of instances where each system matched the reference, as well as cases where they were both incorrect. The majority of differences correspond to clitics, whose correction appears to be a major source of the improvements obtained by lattice desegmentation. This category is challenging for the decoder because English prepositions tend to correspond to multiple possible forms when translated into Arabic. It also includes the frequent cases involving the nominal determiner prefix Al “the” (left unsegmented by the PATB scheme), and the sentence-initial conjunction w+ “and”.</bodyText>
<table confidence="0.999264">
Lattice 1-best Both
Correct Correct Incorrect
Clitical 157 71 79
Lexical 61 39 80
Inflectional 37 32 47
Part-of-speech 19 17 11
</table>
<tableCaption confidence="0.9260275">
Table 4: Error analysis for English-to-Arabic
translation based on 650 sampled instances.
</tableCaption>
<bodyText confidence="0.9820346">The second most common category is lexical, where the unsegmented LM has drastically altered the choice of translation. The remaining categories show no major advantage for either system.</bodyText>
<sectionHeader confidence="0.99969" genericHeader="conclusion">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99996296">We have explored deeper integration of morphological desegmentation into the statistical machine translation pipeline. We have presented a novel, finite-state-inspired approach to lattice desegmentation, which allows the system to account for a desegmented view of many possible translations, without any modification to the decoder or any restrictions on phrase extraction. When applied to English-to-Arabic translation, lattice desegmentation results in a 1.0 BLEU point improvement over one-best desegmentation, and a 1.7 BLEU point improvement over unsegmented translation. We have also applied our approach to English-toFinnish translation, and although segmentation in general does not currently help, we are able to show significant improvements over a 1-best desegmentation baseline. In the future, we plan to explore introducing multiple segmentation options into the lattice, and the application of our method to a full morphological analysis (as opposed to segmentation) of the target language. Eventually, we would like to replace the functionality of factored translation models (Koehn and Hoang, 2007) with lattice transformation and augmentation.</bodyText>
<sectionHeader confidence="0.998811" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996958">Thanks to Ann Clifton for generously providing the data and segmentation for our English-toFinnish experiments, and to Marine Carpuat and Roland Kuhn for their helpful comments on an earlier draft. This research was supported by the Natural Sciences and Engineering Research Council of Canada.</bodyText>
<page confidence="0.998823">
108
</page>
<sectionHeader confidence="0.996057" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999782731481482">
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of the Ninth International
Conference on Implementation and Application of
Automata, (CIAA 2007), volume 4783 of Lecture
Notes in Computer Science, pages 11–23. Springer.
http://www.openfst.org.
Ibrahim Badr, Rabih Zbib, and James Glass. 2008.
Segmentation for English-to-Arabic statistical ma-
chine translation. In Proceedings of ACL, pages
153–156.
Ondˇrej Bojar. 2007. English-to-Czech factored ma-
chine translation. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
232–239, Prague, Czech Republic, June.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of HLT-NAACL, Montreal, Canada, June.
David Chiang, Yuval Marton, and Philip Resnik.
2008. Online large-margin training of syntactic and
structural translation features. In Proceedings of
EMNLP, pages 224–233.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing
for statistical machine translation: Controlling for
optimizer instability. In Proceedings of ACL, pages
176–181.
Ann Clifton and Anoop Sarkar. 2011. Combin-
ing morpheme-based machine translation with post-
processing morpheme prediction. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 32–42, Portland, Oregon, USA, June.
Mathias Creutz and Krista Lagus. 2005. Induc-
ing the morphological lexicon of a natural language
from unannotated text. In In Proceedings of the
International and Interdisciplinary Conference on
Adaptive Knowledge Representation and Reasoning
(AKRR05, pages 106–113.
Ahmed El Kholy and Nizar Habash. 2012a. Ortho-
graphic and morphological processing for English—
Arabic statistical machine translation. Machine
Translation, 26(1-2):25–45, March.
Ahmed El Kholy and Nizar Habash. 2012b. Trans-
late, predict or generate: Modeling rich morphology
in statistical machine translation. Proceeding of the
Meeting of the European Association for Machine
Translation.
Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling inflection and word-
formation in SMT. In Proceedings of the 13th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 664–674, Avi-
gnon, France, April. Association for Computational
Linguistics.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
Mada+tokan: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS
tagging, stemming and lemmatization. In Khalid
Choukri and Bente Maegaard, editors, Proceedings
of the Second International Conference on Arabic
Language Resources and Tools, Cairo, Egypt, April.
The MEDAR Consortium.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
144–151, Prague, Czech Republic, June.
Liang Huang. 2008. Forest reranking: Discrimina-
tive parsing with non-local features. In Proceedings
of ACL-08: HLT, pages 586–594, Columbus, Ohio,
June.
James W. Hunt and M. Douglas McIlroy. 1976. An
algorithm for differential file comparison. Technical
report, Bell Laboratories, June.
Minwoo Jeong, Kristina Toutanova, Hisami Suzuki,
and Chris Quirk. 2010. A discriminative lexicon
model for complex morphology. In The Ninth Con-
ference of the Association for Machine Translation
in the Americas.
Philipp Koehn and Hieu Hoang. 2007. Factored
translation models. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 868–
876, Prague, Czech Republic, June. Association for
Computational Linguistics.
Philipp Koehn, Franz Joesef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL, pages 127–133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177–180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Minh-Thang Luong, Preslav Nakov, and Min-Yen Kan.
2010. A hybrid morpheme-word representation for
machine translation of morphologically rich lan-
guages. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, pages 148–157, Cambridge, MA, October.
</reference>
<page confidence="0.983794">
109
</page>
<reference confidence="0.999775339285714">
Einat Minkov, Kristina Toutanova, and Hisami Suzuki.
2007. Generating complex morphology for machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 128–135, Prague, Czech Republic, June.
Franz Josef Och, Hermann Ney, Franz Josef, and
Och Hermann Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29.
Franz Joseph Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings of
ACL, pages 160–167.
Kemal Oflazer and Ilknur Durgar El-Kahlout. 2007.
Exploring different representational units in
English-to-Turkish statistical machine translation.
In Proceedings of the Second Workshop on Statis-
tical Machine Translation, pages 25–32, Prague,
Czech Republic, June.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318.
Brian Roark, Richard Sproat, and Izhak Shafran. 2011.
Lexicographic semirings for exact automata encod-
ing of sequence models. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1–5, Portland, Oregon, USA, June.
Mohammad Salameh, Colin Cherry, and Grzegorz
Kondrak. 2013. Reversing morphological tokeniza-
tion in English-to-Arabic SMT. In Proceedings of
the 2013 NAACL HLT Student Research Workshop,
pages 47–53, Atlanta, Georgia, June.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Intl. Conf. Spoken Language
Processing, pages 901–904.
Michael Subotin. 2011. An exponential translation
model for target language morphology. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 230–238, Portland, Ore-
gon, USA, June.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying morphology generation models to
machine translation. In Proceedings of ACL-08:
HLT, pages 514–522, Columbus, Ohio, June.
Nicola Ueffing, Franz J. Och, and Hermann Ney. 2002.
Generation of word graphs in statistical machine
translation. In Proceedings of EMNLP, pages 156–
163, Philadelphia, PA, July.
</reference>
<page confidence="0.998397">
110
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant confidence="0.670646" no="0">
<title confidence="0.999973">Lattice Desegmentation for Statistical Machine Translation</title>
<author confidence="0.999707">Colin Grzegorz</author>
<affiliation confidence="0.904923">of Computing Science Research Council Canada University of Alberta 1200 Montreal Road</affiliation>
<address confidence="0.958407">Edmonton, AB, T6G 2E8, Canada Ottawa, ON, K1A 0R6,</address>
<email confidence="0.885506">Colin.Cherry@nrc-cnrc.gc.ca</email>
<abstract confidence="0.99747125">Morphological segmentation is an effective sparsity reduction strategy for statistical machine translation (SMT) involving morphologically complex languages. When translating into a segmented language, an extra step is required to desegment the output; previous studies have desegmented the 1-best output from the decoder. In this paper, we expand our transoptions by desegmenting lists or lattices. Our novel lattice desegmentation algorithm effectively combines both segmented and desegmented views of the target language for a large subspace of possible translation outputs, which allows for inclusion of features related to the desegmentation process, as well as an unsegmented language model (LM). We investigate this technique in the context of English-to-Arabic and English-to-Finnish translation, showing significant improvements in translation quality over desegmentation of 1-best decoder outputs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Michael Riley</author>
<author>Johan Schalkwyk</author>
<author>Wojciech Skut</author>
<author>Mehryar Mohri</author>
</authors>
<title>OpenFst: A general and efficient weighted finite-state transducer library.</title>
<date>2007</date>
<booktitle>In Proceedings of the Ninth International Conference on Implementation and Application of Automata, (CIAA</booktitle>
<volume>4783</volume>
<pages>11--23</pages>
<publisher>Springer. http://www.openfst.org.</publisher>
<contexts>
<context citStr="Allauzen et al., 2007" endWordPosition="2707" position="17457" startWordPosition="2704">el context is one of the main benefits of incorporating a word-level LM. Fortunately, LM annotation as well as any necessary lattice modifications can be performed simultaneously by composing the desegmented lattice with a finite state acceptor encoding the LM (Roark et al., 2011). In summary, we are given a segmented lattice, which encodes the decoder’s translation space as an acceptor over morphemes. We compose this acceptor with a desegmenting transducer, and then with an unsegmented LM acceptor, producing a fully annotated, desegmented lattice. Instead of using a tool kit such as OpenFst (Allauzen et al., 2007), we implement both the desegmenting transducer and the LM acceptor programmatically. This eliminates the need to construct intermediate machines, such as the lattice-specific desegmenter in Figure 1b, and facilitates working with edges annotated with feature vectors as opposed to single weights. Programmatic Desegmentation Lattice desegmentation is a non-local lattice transformation. That is, the morphemes forming a word might span several edges, making desegmentation non-trivial. Luong et al. (2010) address this problem by forcing the decoder’s phrase table to respect word boundaries, guaran</context>
</contexts>
<marker>Allauzen, Riley, Schalkwyk, Skut, Mohri, 2007</marker>
<rawString>Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri. 2007. OpenFst: A general and efficient weighted finite-state transducer library. In Proceedings of the Ninth International Conference on Implementation and Application of Automata, (CIAA 2007), volume 4783 of Lecture Notes in Computer Science, pages 11–23. Springer. http://www.openfst.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ibrahim Badr</author>
<author>Rabih Zbib</author>
<author>James Glass</author>
</authors>
<title>Segmentation for English-to-Arabic statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>153--156</pages>
<contexts>
<context citStr="Badr et al., 2008" endWordPosition="809" position="5461" startWordPosition="806">particular, Toutanova et al. (2008) inflect and re-rank n-best lists in a similar manner to how we desegment and re-rank n-best lists or lattices. 2.2 Morphological Segmentation Instead of producing an abstract feature layer, morphological segmentation transforms the target sentence by segmenting relevant morphemes, which are then handled as regular tokens during alignment and translation. This is done to reduce sparsity and to improve correspondence with the source language (usually English). Such a segmentation can be produced as a byproduct of analysis (Oflazer and Durgar El-Kahlout, 2007; Badr et al., 2008; El Kholy and Habash, 2012a), or may be produced using an unsupervised morphological segmenter such as Morfessor (Luong et al., 2010; Clifton and Sarkar, 2011). Work on target language morphological segmentation for SMT can be divided into three subproblems: segmentation, desegmentation and integration. Our work is concerned primarily with the integration problem, but we will discuss each subproblem in turn. The usefulness of a target segmentation depends on its correspondence to the source language. If a morphological feature does not manifest itself as a separate token in the source, then i</context>
<context citStr="Badr et al. (2008)" endWordPosition="1057" position="7079" startWordPosition="1054">gmentation strategies recommended by previous work: the Penn Arabic Treebank scheme for English-Arabic (El Kholy and Habash, 2012a), and an unsupervised scheme for EnglishFinnish (Clifton and Sarkar, 2011). Desegmentation is the process of converting segmented words into their original surface form. For many segmentations, especially unsupervised ones, this amounts to simple concatenation. However, more complex segmentations, such as the Arabic tokenization provided by MADA (Habash et al., 2009), require further orthographic adjustments to reverse normalizations performed during segmentation. Badr et al. (2008) present two Arabic desegmentation schemes: table-based and rule-based. El Kholy and Habash (2012a) provide an extensive study on the influence of segmentation and desegmentation on English-toArabic SMT. They introduce an additional desegmentation technique that augments the table-based approach with an unsegmented language model. Salameh et al. (2013) replace rule-based desegmentation with a discriminatively-trained character transducer. In this work, we adopt the Table+Rules approach of El Kholy and Habash (2012a) for English-Arabic, while concatenation is sufficient for English-Finnish. Wor</context>
<context citStr="Badr et al. (2008)" endWordPosition="1269" position="8571" startWordPosition="1266">gmented language model. Unlike our work, they replace the segmented language model with the unsegmented one, allowing them to tune the linear model parameters by hand. We use both segmented and unsegmented language models, and tune automatically to optimize BLEU. Like us, Luong et al. (2010) tune on unsegmented references,1 and translate with both segmented and unsegmented language models for English-to-Finnish translation. However, they adopt a scheme of word-boundary-aware 1Tuning on unsegmented references does not require substantial modifications to the standard SMT pipeline. For example, Badr et al. (2008) also tune on unsegmented references by simply desegmenting SMT output before MERT collects sufficient statistics for BLEU. 101 morpheme-level phrase extraction, meaning that target phrases include only complete words, though those words are segmented into morphemes. This enables full decoder integration, where we do n-best and lattice re-ranking. But it also comes at a substantial cost: when target phrases include only complete words, the system can only generate word forms that were seen during training. In this setting, the sparsity reduction from segmentation helps word alignment and targe</context>
<context citStr="Badr et al., 2008" endWordPosition="1865" position="12429" startWordPosition="1862"> the desegmented output, we can re-tune our enhanced linear model on a development set. Following previous work, we will desegment 1000-best lists (Oflazer and Durgar El-Kahlout, 2007). Once n-best lists have been desegmented, we can tune on unsegmented references as a sidebenefit. This could improve translation quality, as it brings our training scenario closer to our test scenario (test BLEU is always measured on unsegmented references). In particular, it could address issues with translation length mismatch. Previous work that has tuned on unsegmented references has reported mixed results (Badr et al., 2008; Luong et al., 2010). 3.3 Lattice Desegmentation An n-best list reflects a tiny portion of a decoder’s search space, typically fixed at 1000 hypotheses. Lattices2 can represent an exponential number of hypotheses in a compact structure. In this section, we discuss how a lattice from a multi-stack phrasebased decoder such as Moses (Koehn et al., 2007) can be desegmented to enable word-level features. Finite State Analogy A phrase-based decoder produces its output from left to right, with each operation appending the translation of a source phrase to a growing target hypothesis. Translation con</context>
</contexts>
<marker>Badr, Zbib, Glass, 2008</marker>
<rawString>Ibrahim Badr, Rabih Zbib, and James Glass. 2008. Segmentation for English-to-Arabic statistical machine translation. In Proceedings of ACL, pages 153–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
</authors>
<title>English-to-Czech factored machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>232--239</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context citStr="Bojar (2007)" endWordPosition="600" position="4105" startWordPosition="599">e in some manner before training the translation model. They differ in what transformations are performed and at what stage they are reversed. The transformation might take the form of a morphological analysis or a morphological segmentation. 100 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 100–110, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics 2.1 Morphological Analysis Many languages have access to morphological analyzers, which annotate surface forms with their lemmas and morphological features. Bojar (2007) incorporates such analyses into a factored model, to either include a language model over target morphological tags, or model the generation of morphological features. Other approaches train an SMT system to predict lemmas instead of surface forms, and then inflect the SMT output as a postprocessing step (Minkov et al., 2007; Clifton and Sarkar, 2011; Fraser et al., 2012; El Kholy and Habash, 2012b). Alternatively, one can reparameterize existing phrase tables as exponential models, so that translation probabilities account for source context and morphological features (Jeong et al., 2010; Su</context>
</contexts>
<marker>Bojar, 2007</marker>
<rawString>Ondˇrej Bojar. 2007. English-to-Czech factored machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 232–239, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch tuning strategies for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<location>Montreal, Canada,</location>
<contexts>
<context citStr="Cherry and Foster, 2012" endWordPosition="4466" position="27546" startWordPosition="4463">ing model. A KN-smoothed 5-gram language model is trained on the target side of the parallel data with SRILM (Stolcke, 2002). Finally, we include word and phrase penalties. The decoder uses the default parameters for English-to-Arabic, except that the maximum phrase length is set to 8. For Englishto-Finnish, we follow Clifton and Sarkar (2011) in setting the hypothesis stack size to 100, distortion limit to 6, and maximum phrase length to 20. The decoder’s log-linear model is tuned with MERT (Och, 2003). Re-ranking models are tuned using a batch variant of hope-fear MIRA (Chiang et al., 2008; Cherry and Foster, 2012), using the n-best variant for n-best desegmentation, and the lattice variant for lattice desegmentation. MIRA was selected over MERT because we have an in-house implementation that can tune on lattices very quickly. During development, we confirmed that MERT and MIRA perform similarly, as is expected with fewer than 20 features. Both the decoder’s log-linear model and the re-ranking models are trained on the same development set. Historically, we have not seen improvements from using different tuning sets for decoding and reranking. Lattices are pruned to a density of 50 edges per word before</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In Proceedings of HLT-NAACL, Montreal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>224--233</pages>
<contexts>
<context citStr="Chiang et al., 2008" endWordPosition="4462" position="27520" startWordPosition="4458">l lexicalized reordering model. A KN-smoothed 5-gram language model is trained on the target side of the parallel data with SRILM (Stolcke, 2002). Finally, we include word and phrase penalties. The decoder uses the default parameters for English-to-Arabic, except that the maximum phrase length is set to 8. For Englishto-Finnish, we follow Clifton and Sarkar (2011) in setting the hypothesis stack size to 100, distortion limit to 6, and maximum phrase length to 20. The decoder’s log-linear model is tuned with MERT (Och, 2003). Re-ranking models are tuned using a batch variant of hope-fear MIRA (Chiang et al., 2008; Cherry and Foster, 2012), using the n-best variant for n-best desegmentation, and the lattice variant for lattice desegmentation. MIRA was selected over MERT because we have an in-house implementation that can tune on lattices very quickly. During development, we confirmed that MERT and MIRA perform similarly, as is expected with fewer than 20 features. Both the decoder’s log-linear model and the re-ranking models are trained on the same development set. Historically, we have not seen improvements from using different tuning sets for decoding and reranking. Lattices are pruned to a density o</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In Proceedings of EMNLP, pages 224–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>176--181</pages>
<contexts>
<context citStr="Clark et al. (2011)" endWordPosition="4687" position="28908" startWordPosition="4684">ntation step. Our second baseline is 1-best Deseg, where we train on segmented target text and desegment the decoder’s 1-best output. Starting from the system that produced 1-best Deseg, we then output either 1000-best lists or lattices to create our two experimental systems. The 1000-best Deseg system desegments, augments and re-ranks the decoder’s 1000-best list, while Lattice Deseg does the same in the lattice. We augment n-best lists and lattices using the features described in Section 3.4.8 We evaluate our system using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). Following Clark et al. (2011), we report average scores over five random tuning replications to account for optimizer instability. For the baselines, this means 5 runs of decoder tuning. For the desegmenting re-rankers, this means 5 runs of reranker tuning, each working on n-best lists or lattices produced by the same (representative) decoder weights. We measure statistical significance using MultEval (Clark et al., 2011), which implements a stratified approximate randomization test to account for multiple tuning replications. 8Development experiments on a small-data English-toArabic scenario indicated that the Desegmenta</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proceedings of ACL, pages 176–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Clifton</author>
<author>Anoop Sarkar</author>
</authors>
<title>Combining morpheme-based machine translation with postprocessing morpheme prediction.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>32--42</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context citStr="Clifton and Sarkar, 2011" endWordPosition="658" position="4458" startWordPosition="655"> 100–110, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics 2.1 Morphological Analysis Many languages have access to morphological analyzers, which annotate surface forms with their lemmas and morphological features. Bojar (2007) incorporates such analyses into a factored model, to either include a language model over target morphological tags, or model the generation of morphological features. Other approaches train an SMT system to predict lemmas instead of surface forms, and then inflect the SMT output as a postprocessing step (Minkov et al., 2007; Clifton and Sarkar, 2011; Fraser et al., 2012; El Kholy and Habash, 2012b). Alternatively, one can reparameterize existing phrase tables as exponential models, so that translation probabilities account for source context and morphological features (Jeong et al., 2010; Subotin, 2011). Of these approaches, ours is most similar to the translate-then-inflect approach, except we translate and then desegment. In particular, Toutanova et al. (2008) inflect and re-rank n-best lists in a similar manner to how we desegment and re-rank n-best lists or lattices. 2.2 Morphological Segmentation Instead of producing an abstract fea</context>
<context citStr="Clifton and Sarkar, 2011" endWordPosition="949" position="6329" startWordPosition="946">ee subproblems: segmentation, desegmentation and integration. Our work is concerned primarily with the integration problem, but we will discuss each subproblem in turn. The usefulness of a target segmentation depends on its correspondence to the source language. If a morphological feature does not manifest itself as a separate token in the source, then it may be best to leave its corresponding segment attached to the stem. A number of studies have looked into what granularity of segmentation is best suited for a particular language pair (Oflazer and Durgar El-Kahlout, 2007; Badr et al., 2008; Clifton and Sarkar, 2011; El Kholy and Habash, 2012a). Since our focus here is on integrating segmentation into the decoding process, we simply adopt the segmentation strategies recommended by previous work: the Penn Arabic Treebank scheme for English-Arabic (El Kholy and Habash, 2012a), and an unsupervised scheme for EnglishFinnish (Clifton and Sarkar, 2011). Desegmentation is the process of converting segmented words into their original surface form. For many segmentations, especially unsupervised ones, this amounts to simple concatenation. However, more complex segmentations, such as the Arabic tokenization provid</context>
<context citStr="Clifton and Sarkar, 2011" endWordPosition="1711" position="11423" startWordPosition="1708">espondence with the source language. The one-best desegmentation approach segments the target language at training time and then desegments the one-best output in postprocessing. This resolves the sparsity issue, but does not allow the decoder to take into account features of the desegmented target. To the best of our knowledge, we are the first group to go beyond one-best desegmentation for English-to-Arabic translation. In English-to-Finnish, although alternative integration strategies have seen some success (Luong et al., 2010), the current state-ofthe-art performs one-best-desegmentation (Clifton and Sarkar, 2011). 3.2 n-best Desegmentation The one-best approach can be extended easily by desegmenting n-best lists of segmented decoder output. Doing so enables the inclusion of an unsegmented target language model, and with a small amount of bookkeeping, it also allows the inclusion of features related to the operations performed during desegmentation (see Section 3.4). With new features reflecting the desegmented output, we can re-tune our enhanced linear model on a development set. Following previous work, we will desegment 1000-best lists (Oflazer and Durgar El-Kahlout, 2007). Once n-best lists have be</context>
<context citStr="Clifton and Sarkar (2011)" endWordPosition="4105" position="25325" startWordPosition="4102">fore 6Allowing the re-ranker to choose between multiple Y s is a natural avenue for future work. 7We also experimented on log p(X|Y ) as an additional feature, but observed no improvement in translation quality. 105 segmentation, and 47 million after segmentation. We tune on the NIST 2004 evaluation set (1353 sentences) and evaluate on NIST 2005 (1056 sentences). As these evaluation sets are intended for Arabic-to-English translation, we select the first English reference to use as our source text. Our English-to-Finnish system is trained on the same Europarl corpus as Luong et al. (2010) and Clifton and Sarkar (2011), which has roughly one million sentence pairs. We also use their development and test sets (2000 sentences each). 4.1 Segmentation For Arabic, morphological segmentation is performed by MADA 3.2 (Habash et al., 2009), using the Penn Arabic Treebank (PATB) segmentation scheme as recommended by El Kholy and Habash (2012a). For both segmented and unsegmented Arabic, we further normalize the script by converts ing different forms of Alif sI I I I and Ya S S to bare Alif I and dotless Ya S. To generate the desegmentation table, we analyze the segmentations from the Arabic side of the parallel trai</context>
<context citStr="Clifton and Sarkar (2011)" endWordPosition="4418" position="27267" startWordPosition="4415">decoder’s log-linear model includes a standard feature set. Four translation model features encode phrase translation probabilities and lexical scores in both directions. Seven distortion features encode a standard distortion penalty as well as a bidirectional lexicalized reordering model. A KN-smoothed 5-gram language model is trained on the target side of the parallel data with SRILM (Stolcke, 2002). Finally, we include word and phrase penalties. The decoder uses the default parameters for English-to-Arabic, except that the maximum phrase length is set to 8. For Englishto-Finnish, we follow Clifton and Sarkar (2011) in setting the hypothesis stack size to 100, distortion limit to 6, and maximum phrase length to 20. The decoder’s log-linear model is tuned with MERT (Och, 2003). Re-ranking models are tuned using a batch variant of hope-fear MIRA (Chiang et al., 2008; Cherry and Foster, 2012), using the n-best variant for n-best desegmentation, and the lattice variant for lattice desegmentation. MIRA was selected over MERT because we have an in-house implementation that can tune on lattices very quickly. During development, we confirmed that MERT and MIRA perform similarly, as is expected with fewer than 20</context>
<context citStr="Clifton and Sarkar (2011)" endWordPosition="4987" position="30868" startWordPosition="4984"> BLEU point improvement over 1-best. Lattice desegmentation is significantly better (p &lt; 0.01) than 1000-best desegmentation. For English-to-Finnish, the Unsup L-match segmentation with 1-best desegmentation does not improve over the unsegmented baseline. The segmentation may be addressing issues with model sparsity, but it is also introducing errors that would have been impossible had words been left unsegmented. In fact, even with our lattice desegmenter providing a boost, we are unable to see a significant improvement over the unsegmented model. As we attempted to replicate the approach of Clifton and Sarkar (2011) exactly by working with their segmented data, this difference is likely due to changes in Moses since the publication of their result. Nonetheless, the 1000-best and lattice desegmenters both produce significant improvements over the 1-best desegmentation baseline, with Lattice Deseg achieving a 1-point improvement in TER. These results match the established state-of-the-art on this data set, but also indicate that there is still room for improvement in identifying the best segmentation strategy for Englishto-Finnish translation. We also tried a similar Morfessor-based segmentation for Arabic</context>
</contexts>
<marker>Clifton, Sarkar, 2011</marker>
<rawString>Ann Clifton and Anoop Sarkar. 2011. Combining morpheme-based machine translation with postprocessing morpheme prediction. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 32–42, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Inducing the morphological lexicon of a natural language from unannotated text. In</title>
<date>2005</date>
<booktitle>In Proceedings of the International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning (AKRR05,</booktitle>
<pages>106--113</pages>
<contexts>
<context citStr="Creutz and Lagus, 2005" endWordPosition="4241" position="26135" startWordPosition="4238">MADA 3.2 (Habash et al., 2009), using the Penn Arabic Treebank (PATB) segmentation scheme as recommended by El Kholy and Habash (2012a). For both segmented and unsegmented Arabic, we further normalize the script by converts ing different forms of Alif sI I I I and Ya S S to bare Alif I and dotless Ya S. To generate the desegmentation table, we analyze the segmentations from the Arabic side of the parallel training data to collect mappings from morpheme sequences to surface forms. For Finnish, we adopt the Unsup L-match segmentation technique of Clifton and Sarkar (2011), which uses Morfessor (Creutz and Lagus, 2005) to analyze the 5,000 most frequent Finnish words. The analysis is then applied to the Finnish side of the parallel text, and a list of segmented suffixes is collected. To improve coverage, words are further segmented according to their longest matching suffix from the list. As Morfessor does not perform any orthographic normalizations, it can be desegmented with simple concatenation. 4.2 Systems We align the parallel data with GIZA++ (Och et al., 2003) and decode using Moses (Koehn et al., 2007). The decoder’s log-linear model includes a standard feature set. Four translation model features e</context>
</contexts>
<marker>Creutz, Lagus, 2005</marker>
<rawString>Mathias Creutz and Krista Lagus. 2005. Inducing the morphological lexicon of a natural language from unannotated text. In In Proceedings of the International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning (AKRR05, pages 106–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed El Kholy</author>
<author>Nizar Habash</author>
</authors>
<title>Orthographic and morphological processing for English— Arabic statistical machine translation.</title>
<date>2012</date>
<journal>Machine Translation,</journal>
<pages>26--1</pages>
<marker>El Kholy, Habash, 2012</marker>
<rawString>Ahmed El Kholy and Nizar Habash. 2012a. Orthographic and morphological processing for English— Arabic statistical machine translation. Machine Translation, 26(1-2):25–45, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed El Kholy</author>
<author>Nizar Habash</author>
</authors>
<title>Translate, predict or generate: Modeling rich morphology in statistical machine translation. Proceeding of the Meeting of the European Association for Machine Translation.</title>
<date>2012</date>
<marker>El Kholy, Habash, 2012</marker>
<rawString>Ahmed El Kholy and Nizar Habash. 2012b. Translate, predict or generate: Modeling rich morphology in statistical machine translation. Proceeding of the Meeting of the European Association for Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Marion Weller</author>
<author>Aoife Cahill</author>
<author>Fabienne Cap</author>
</authors>
<title>Modeling inflection and wordformation in SMT.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>664--674</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Avignon, France,</location>
<contexts>
<context citStr="Fraser et al., 2012" endWordPosition="662" position="4479" startWordPosition="659">and, USA, June 23-25 2014. c�2014 Association for Computational Linguistics 2.1 Morphological Analysis Many languages have access to morphological analyzers, which annotate surface forms with their lemmas and morphological features. Bojar (2007) incorporates such analyses into a factored model, to either include a language model over target morphological tags, or model the generation of morphological features. Other approaches train an SMT system to predict lemmas instead of surface forms, and then inflect the SMT output as a postprocessing step (Minkov et al., 2007; Clifton and Sarkar, 2011; Fraser et al., 2012; El Kholy and Habash, 2012b). Alternatively, one can reparameterize existing phrase tables as exponential models, so that translation probabilities account for source context and morphological features (Jeong et al., 2010; Subotin, 2011). Of these approaches, ours is most similar to the translate-then-inflect approach, except we translate and then desegment. In particular, Toutanova et al. (2008) inflect and re-rank n-best lists in a similar manner to how we desegment and re-rank n-best lists or lattices. 2.2 Morphological Segmentation Instead of producing an abstract feature layer, morpholog</context>
</contexts>
<marker>Fraser, Weller, Cahill, Cap, 2012</marker>
<rawString>Alexander Fraser, Marion Weller, Aoife Cahill, and Fabienne Cap. 2012. Modeling inflection and wordformation in SMT. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 664–674, Avignon, France, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
<author>Ryan Roth</author>
</authors>
<title>Mada+tokan: A toolkit for Arabic tokenization, diacritization, morphological disambiguation, POS tagging, stemming and lemmatization.</title>
<date>2009</date>
<booktitle>In Khalid Choukri and Bente Maegaard, editors, Proceedings of the Second International Conference on Arabic Language Resources and Tools,</booktitle>
<publisher>The MEDAR Consortium.</publisher>
<location>Cairo, Egypt,</location>
<contexts>
<context citStr="Habash et al., 2009" endWordPosition="1041" position="6961" startWordPosition="1038">nd Habash, 2012a). Since our focus here is on integrating segmentation into the decoding process, we simply adopt the segmentation strategies recommended by previous work: the Penn Arabic Treebank scheme for English-Arabic (El Kholy and Habash, 2012a), and an unsupervised scheme for EnglishFinnish (Clifton and Sarkar, 2011). Desegmentation is the process of converting segmented words into their original surface form. For many segmentations, especially unsupervised ones, this amounts to simple concatenation. However, more complex segmentations, such as the Arabic tokenization provided by MADA (Habash et al., 2009), require further orthographic adjustments to reverse normalizations performed during segmentation. Badr et al. (2008) present two Arabic desegmentation schemes: table-based and rule-based. El Kholy and Habash (2012a) provide an extensive study on the influence of segmentation and desegmentation on English-toArabic SMT. They introduce an additional desegmentation technique that augments the table-based approach with an unsegmented language model. Salameh et al. (2013) replace rule-based desegmentation with a discriminatively-trained character transducer. In this work, we adopt the Table+Rules </context>
<context citStr="Habash et al., 2009" endWordPosition="4140" position="25542" startWordPosition="4137">tation, and 47 million after segmentation. We tune on the NIST 2004 evaluation set (1353 sentences) and evaluate on NIST 2005 (1056 sentences). As these evaluation sets are intended for Arabic-to-English translation, we select the first English reference to use as our source text. Our English-to-Finnish system is trained on the same Europarl corpus as Luong et al. (2010) and Clifton and Sarkar (2011), which has roughly one million sentence pairs. We also use their development and test sets (2000 sentences each). 4.1 Segmentation For Arabic, morphological segmentation is performed by MADA 3.2 (Habash et al., 2009), using the Penn Arabic Treebank (PATB) segmentation scheme as recommended by El Kholy and Habash (2012a). For both segmented and unsegmented Arabic, we further normalize the script by converts ing different forms of Alif sI I I I and Ya S S to bare Alif I and dotless Ya S. To generate the desegmentation table, we analyze the segmentations from the Arabic side of the parallel training data to collect mappings from morpheme sequences to surface forms. For Finnish, we adopt the Unsup L-match segmentation technique of Clifton and Sarkar (2011), which uses Morfessor (Creutz and Lagus, 2005) to ana</context>
</contexts>
<marker>Habash, Rambow, Roth, 2009</marker>
<rawString>Nizar Habash, Owen Rambow, and Ryan Roth. 2009. Mada+tokan: A toolkit for Arabic tokenization, diacritization, morphological disambiguation, POS tagging, stemming and lemmatization. In Khalid Choukri and Bente Maegaard, editors, Proceedings of the Second International Conference on Arabic Language Resources and Tools, Cairo, Egypt, April. The MEDAR Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>144--151</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context citStr="Huang and Chiang, 2007" endWordPosition="2827" position="18263" startWordPosition="2824">nter in Figure 1b, and facilitates working with edges annotated with feature vectors as opposed to single weights. Programmatic Desegmentation Lattice desegmentation is a non-local lattice transformation. That is, the morphemes forming a word might span several edges, making desegmentation non-trivial. Luong et al. (2010) address this problem by forcing the decoder’s phrase table to respect word boundaries, guaranteeing that each desegmentable token sequence is local to an edge. 4Or the LM composition can be done dynamically, effectively decoding the lattice with a beam or cube-pruned search (Huang and Chiang, 2007). Inspired by the use of non-local features in forest decoding (Huang, 2008), we present an algorithm to find chains of edges that correspond to desegmentable token sequences, allowing lattice desegmentation with no phrase-table restrictions. This algorithm can be seen as implicitly constructing a customized desegmenting transducer and composing it with the input lattice on the fly. Before describing the algorithm, we define some notation. An input morpheme lattice is a triple (ns, N, £), where N is a set of nodes, £ is a set of edges, and ns E N is the start node that begins each path through</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144–151, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>586--594</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context citStr="Huang, 2008" endWordPosition="2839" position="18339" startWordPosition="2838"> opposed to single weights. Programmatic Desegmentation Lattice desegmentation is a non-local lattice transformation. That is, the morphemes forming a word might span several edges, making desegmentation non-trivial. Luong et al. (2010) address this problem by forcing the decoder’s phrase table to respect word boundaries, guaranteeing that each desegmentable token sequence is local to an edge. 4Or the LM composition can be done dynamically, effectively decoding the lattice with a beam or cube-pruned search (Huang and Chiang, 2007). Inspired by the use of non-local features in forest decoding (Huang, 2008), we present an algorithm to find chains of edges that correspond to desegmentable token sequences, allowing lattice desegmentation with no phrase-table restrictions. This algorithm can be seen as implicitly constructing a customized desegmenting transducer and composing it with the input lattice on the fly. Before describing the algorithm, we define some notation. An input morpheme lattice is a triple (ns, N, £), where N is a set of nodes, £ is a set of edges, and ns E N is the start node that begins each path through the lattice. Each edge e E £ is a 4-tuple (from, to, lex, w), where from, t</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of ACL-08: HLT, pages 586–594, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James W Hunt</author>
<author>M Douglas McIlroy</author>
</authors>
<title>An algorithm for differential file comparison.</title>
<date>1976</date>
<tech>Technical report,</tech>
<institution>Bell Laboratories,</institution>
<contexts>
<context citStr="Hunt and McIlroy, 1976" endWordPosition="5510" position="34238" startWordPosition="5507">ments in the English-to-Arabic scenario, we conducted an extensive manual analysis of the differences between 1-best and lattice deseg107 Features dev test 1-best Deseg 24.5 33.4 + Unsegmented LM 24.9 33.8 + Contiguity 25.2 34.4 + Desegmentation Score 25.2 34.3 Table 3: The effect of feature ablation on BLEU score for English-to-Arabic translation with lattice desegmentation. mentation on our test set. We compared the output of the two systems using the Unix tool wdiff, which transforms a solution to the longestcommon-subsequence problem into a sequence of multi-word insertions and deletions (Hunt and McIlroy, 1976). We considered adjacent insertiondeletion pairs to be (potentially phrasal) substitutions, and collected them into a file, omitting any unpaired insertions or deletions. We then sampled 650 cases where the two sides of the substitution were deemed to be related, and divided these cases into categories based on how the lattice desegmentation differs from the one-best desegmentation. We consider a phrase to be correct only if it can be found in the reference. Table 4 breaks down per-phrase accuracy according to four manually-assigned categories: (1) clitical – the two systems agree on a stem, b</context>
</contexts>
<marker>Hunt, McIlroy, 1976</marker>
<rawString>James W. Hunt and M. Douglas McIlroy. 1976. An algorithm for differential file comparison. Technical report, Bell Laboratories, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minwoo Jeong</author>
<author>Kristina Toutanova</author>
<author>Hisami Suzuki</author>
<author>Chris Quirk</author>
</authors>
<title>A discriminative lexicon model for complex morphology.</title>
<date>2010</date>
<booktitle>In The Ninth Conference of the Association for Machine Translation in the Americas.</booktitle>
<contexts>
<context citStr="Jeong et al., 2010" endWordPosition="694" position="4701" startWordPosition="691">features. Bojar (2007) incorporates such analyses into a factored model, to either include a language model over target morphological tags, or model the generation of morphological features. Other approaches train an SMT system to predict lemmas instead of surface forms, and then inflect the SMT output as a postprocessing step (Minkov et al., 2007; Clifton and Sarkar, 2011; Fraser et al., 2012; El Kholy and Habash, 2012b). Alternatively, one can reparameterize existing phrase tables as exponential models, so that translation probabilities account for source context and morphological features (Jeong et al., 2010; Subotin, 2011). Of these approaches, ours is most similar to the translate-then-inflect approach, except we translate and then desegment. In particular, Toutanova et al. (2008) inflect and re-rank n-best lists in a similar manner to how we desegment and re-rank n-best lists or lattices. 2.2 Morphological Segmentation Instead of producing an abstract feature layer, morphological segmentation transforms the target sentence by segmenting relevant morphemes, which are then handled as regular tokens during alignment and translation. This is done to reduce sparsity and to improve correspondence wi</context>
</contexts>
<marker>Jeong, Toutanova, Suzuki, Quirk, 2010</marker>
<rawString>Minwoo Jeong, Kristina Toutanova, Hisami Suzuki, and Chris Quirk. 2010. A discriminative lexicon model for complex morphology. In The Ninth Conference of the Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
</authors>
<title>Factored translation models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>868--876</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<marker>Koehn, Hoang, 2007</marker>
<rawString>Philipp Koehn and Hieu Hoang. 2007. Factored translation models. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 868– 876, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Joesef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>127--133</pages>
<contexts>
<context citStr="Koehn et al., 2003" endWordPosition="2072" position="13684" startWordPosition="2069">l and syntactic decoders. 102 Figure 1: The finite state pipeline for a lattice translating the English fragment “with the child’s game”. The input morpheme lattice (a) is desegmented by composing it with the desegmenting transducer (b) to produce the word lattice (c). The tokens in (a) are: b+ “with”, lEbp “game”, +hm “their”, +hA “her”, and AlTfl “the child”. (c) (a) 5 5 +hm blEbthm +hA blEbthA Transduces into 4 4 0 AlTß blEbp b+ lEbp 0 1 2 3 2 AlTß 3 (b) AlTfl:AlTfl 1 b+:&lt;epsilon&gt; &lt;epsilon&gt;:blEbp +hA:blEbthA +hm:blEbthm 0 lEbp:&lt;epsilon&gt; 2 til each source word has been covered exactly once (Koehn et al., 2003). The search graph of a phrase-based decoder can be interpreted as a lattice, which can be interpreted as a finite state acceptor over target strings. In its most natural form, such an acceptor emits target phrases on each edge, but it can easily be transformed into a form with one edge per token, as shown in Figure 1a. This is sometimes referred to as a word graph (Ueffing et al., 2002), although in our case the segmented phrase table also produces tokens that correspond to morphemes. Our goal is to desegment the decoder’s output lattice, and in doing so, gain access to a compact, desegmented</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Joesef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT-NAACL, pages 127–133.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context citStr="Koehn et al., 2007" endWordPosition="1923" position="12782" startWordPosition="1920">enario closer to our test scenario (test BLEU is always measured on unsegmented references). In particular, it could address issues with translation length mismatch. Previous work that has tuned on unsegmented references has reported mixed results (Badr et al., 2008; Luong et al., 2010). 3.3 Lattice Desegmentation An n-best list reflects a tiny portion of a decoder’s search space, typically fixed at 1000 hypotheses. Lattices2 can represent an exponential number of hypotheses in a compact structure. In this section, we discuss how a lattice from a multi-stack phrasebased decoder such as Moses (Koehn et al., 2007) can be desegmented to enable word-level features. Finite State Analogy A phrase-based decoder produces its output from left to right, with each operation appending the translation of a source phrase to a growing target hypothesis. Translation continues un2Or forests for hierarchical and syntactic decoders. 102 Figure 1: The finite state pipeline for a lattice translating the English fragment “with the child’s game”. The input morpheme lattice (a) is desegmented by composing it with the desegmenting transducer (b) to produce the word lattice (c). The tokens in (a) are: b+ “with”, lEbp “game”, </context>
<context citStr="Koehn et al., 2007" endWordPosition="4324" position="26636" startWordPosition="4321">t the Unsup L-match segmentation technique of Clifton and Sarkar (2011), which uses Morfessor (Creutz and Lagus, 2005) to analyze the 5,000 most frequent Finnish words. The analysis is then applied to the Finnish side of the parallel text, and a list of segmented suffixes is collected. To improve coverage, words are further segmented according to their longest matching suffix from the list. As Morfessor does not perform any orthographic normalizations, it can be desegmented with simple concatenation. 4.2 Systems We align the parallel data with GIZA++ (Och et al., 2003) and decode using Moses (Koehn et al., 2007). The decoder’s log-linear model includes a standard feature set. Four translation model features encode phrase translation probabilities and lexical scores in both directions. Seven distortion features encode a standard distortion penalty as well as a bidirectional lexicalized reordering model. A KN-smoothed 5-gram language model is trained on the target side of the parallel data with SRILM (Stolcke, 2002). Finally, we include word and phrase penalties. The decoder uses the default parameters for English-to-Arabic, except that the maximum phrase length is set to 8. For Englishto-Finnish, we f</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minh-Thang Luong</author>
<author>Preslav Nakov</author>
<author>Min-Yen Kan</author>
</authors>
<title>A hybrid morpheme-word representation for machine translation of morphologically rich languages.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>148--157</pages>
<location>Cambridge, MA,</location>
<contexts>
<context citStr="Luong et al., 2010" endWordPosition="830" position="5594" startWordPosition="827">s or lattices. 2.2 Morphological Segmentation Instead of producing an abstract feature layer, morphological segmentation transforms the target sentence by segmenting relevant morphemes, which are then handled as regular tokens during alignment and translation. This is done to reduce sparsity and to improve correspondence with the source language (usually English). Such a segmentation can be produced as a byproduct of analysis (Oflazer and Durgar El-Kahlout, 2007; Badr et al., 2008; El Kholy and Habash, 2012a), or may be produced using an unsupervised morphological segmenter such as Morfessor (Luong et al., 2010; Clifton and Sarkar, 2011). Work on target language morphological segmentation for SMT can be divided into three subproblems: segmentation, desegmentation and integration. Our work is concerned primarily with the integration problem, but we will discuss each subproblem in turn. The usefulness of a target segmentation depends on its correspondence to the source language. If a morphological feature does not manifest itself as a separate token in the source, then it may be best to leave its corresponding segment attached to the stem. A number of studies have looked into what granularity of segme</context>
<context citStr="Luong et al. (2010)" endWordPosition="1223" position="8245" startWordPosition="1220"> concatenation is sufficient for English-Finnish. Work on integration attempts to improve SMT performance for morphologically complex target languages by going beyond simple pre- and postprocessing. Oflazer and Durgar El-Kahlout (2007) desegment 1000-best lists for English-to-Turkish translation to enable scoring with an unsegmented language model. Unlike our work, they replace the segmented language model with the unsegmented one, allowing them to tune the linear model parameters by hand. We use both segmented and unsegmented language models, and tune automatically to optimize BLEU. Like us, Luong et al. (2010) tune on unsegmented references,1 and translate with both segmented and unsegmented language models for English-to-Finnish translation. However, they adopt a scheme of word-boundary-aware 1Tuning on unsegmented references does not require substantial modifications to the standard SMT pipeline. For example, Badr et al. (2008) also tune on unsegmented references by simply desegmenting SMT output before MERT collects sufficient statistics for BLEU. 101 morpheme-level phrase extraction, meaning that target phrases include only complete words, though those words are segmented into morphemes. This e</context>
<context citStr="Luong et al., 2010" endWordPosition="1701" position="11334" startWordPosition="1698">ntation errors. However, it suffers from data sparsity and poor token-to-token correspondence with the source language. The one-best desegmentation approach segments the target language at training time and then desegments the one-best output in postprocessing. This resolves the sparsity issue, but does not allow the decoder to take into account features of the desegmented target. To the best of our knowledge, we are the first group to go beyond one-best desegmentation for English-to-Arabic translation. In English-to-Finnish, although alternative integration strategies have seen some success (Luong et al., 2010), the current state-ofthe-art performs one-best-desegmentation (Clifton and Sarkar, 2011). 3.2 n-best Desegmentation The one-best approach can be extended easily by desegmenting n-best lists of segmented decoder output. Doing so enables the inclusion of an unsegmented target language model, and with a small amount of bookkeeping, it also allows the inclusion of features related to the operations performed during desegmentation (see Section 3.4). With new features reflecting the desegmented output, we can re-tune our enhanced linear model on a development set. Following previous work, we will d</context>
<context citStr="Luong et al. (2010)" endWordPosition="2777" position="17963" startWordPosition="2774">ucing a fully annotated, desegmented lattice. Instead of using a tool kit such as OpenFst (Allauzen et al., 2007), we implement both the desegmenting transducer and the LM acceptor programmatically. This eliminates the need to construct intermediate machines, such as the lattice-specific desegmenter in Figure 1b, and facilitates working with edges annotated with feature vectors as opposed to single weights. Programmatic Desegmentation Lattice desegmentation is a non-local lattice transformation. That is, the morphemes forming a word might span several edges, making desegmentation non-trivial. Luong et al. (2010) address this problem by forcing the decoder’s phrase table to respect word boundaries, guaranteeing that each desegmentable token sequence is local to an edge. 4Or the LM composition can be done dynamically, effectively decoding the lattice with a beam or cube-pruned search (Huang and Chiang, 2007). Inspired by the use of non-local features in forest decoding (Huang, 2008), we present an algorithm to find chains of edges that correspond to desegmentable token sequences, allowing lattice desegmentation with no phrase-table restrictions. This algorithm can be seen as implicitly constructing a c</context>
<context citStr="Luong et al. (2010)" endWordPosition="4100" position="25295" startWordPosition="4097">million Arabic tokens before 6Allowing the re-ranker to choose between multiple Y s is a natural avenue for future work. 7We also experimented on log p(X|Y ) as an additional feature, but observed no improvement in translation quality. 105 segmentation, and 47 million after segmentation. We tune on the NIST 2004 evaluation set (1353 sentences) and evaluate on NIST 2005 (1056 sentences). As these evaluation sets are intended for Arabic-to-English translation, we select the first English reference to use as our source text. Our English-to-Finnish system is trained on the same Europarl corpus as Luong et al. (2010) and Clifton and Sarkar (2011), which has roughly one million sentence pairs. We also use their development and test sets (2000 sentences each). 4.1 Segmentation For Arabic, morphological segmentation is performed by MADA 3.2 (Habash et al., 2009), using the Penn Arabic Treebank (PATB) segmentation scheme as recommended by El Kholy and Habash (2012a). For both segmented and unsegmented Arabic, we further normalize the script by converts ing different forms of Alif sI I I I and Ya S S to bare Alif I and dotless Ya S. To generate the desegmentation table, we analyze the segmentations from the Ar</context>
</contexts>
<marker>Luong, Nakov, Kan, 2010</marker>
<rawString>Minh-Thang Luong, Preslav Nakov, and Min-Yen Kan. 2010. A hybrid morpheme-word representation for machine translation of morphologically rich languages. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 148–157, Cambridge, MA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Einat Minkov</author>
<author>Kristina Toutanova</author>
<author>Hisami Suzuki</author>
</authors>
<title>Generating complex morphology for machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>128--135</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context citStr="Minkov et al., 2007" endWordPosition="654" position="4432" startWordPosition="651">al Linguistics, pages 100–110, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics 2.1 Morphological Analysis Many languages have access to morphological analyzers, which annotate surface forms with their lemmas and morphological features. Bojar (2007) incorporates such analyses into a factored model, to either include a language model over target morphological tags, or model the generation of morphological features. Other approaches train an SMT system to predict lemmas instead of surface forms, and then inflect the SMT output as a postprocessing step (Minkov et al., 2007; Clifton and Sarkar, 2011; Fraser et al., 2012; El Kholy and Habash, 2012b). Alternatively, one can reparameterize existing phrase tables as exponential models, so that translation probabilities account for source context and morphological features (Jeong et al., 2010; Subotin, 2011). Of these approaches, ours is most similar to the translate-then-inflect approach, except we translate and then desegment. In particular, Toutanova et al. (2008) inflect and re-rank n-best lists in a similar manner to how we desegment and re-rank n-best lists or lattices. 2.2 Morphological Segmentation Instead of</context>
</contexts>
<marker>Minkov, Toutanova, Suzuki, 2007</marker>
<rawString>Einat Minkov, Kristina Toutanova, and Hisami Suzuki. 2007. Generating complex morphology for machine translation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 128–135, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
<author>Franz Josef</author>
<author>Och Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<contexts>
<context citStr="Och et al., 2003" endWordPosition="4316" position="26592" startWordPosition="4313">ces to surface forms. For Finnish, we adopt the Unsup L-match segmentation technique of Clifton and Sarkar (2011), which uses Morfessor (Creutz and Lagus, 2005) to analyze the 5,000 most frequent Finnish words. The analysis is then applied to the Finnish side of the parallel text, and a list of segmented suffixes is collected. To improve coverage, words are further segmented according to their longest matching suffix from the list. As Morfessor does not perform any orthographic normalizations, it can be desegmented with simple concatenation. 4.2 Systems We align the parallel data with GIZA++ (Och et al., 2003) and decode using Moses (Koehn et al., 2007). The decoder’s log-linear model includes a standard feature set. Four translation model features encode phrase translation probabilities and lexical scores in both directions. Seven distortion features encode a standard distortion penalty as well as a bidirectional lexicalized reordering model. A KN-smoothed 5-gram language model is trained on the target side of the parallel data with SRILM (Stolcke, 2002). Finally, we include word and phrase penalties. The decoder uses the default parameters for English-to-Arabic, except that the maximum phrase len</context>
</contexts>
<marker>Och, Ney, Josef, Ney, 2003</marker>
<rawString>Franz Josef Och, Hermann Ney, Franz Josef, and Och Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Joseph Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context citStr="Och, 2003" endWordPosition="4446" position="27430" startWordPosition="4445">istortion features encode a standard distortion penalty as well as a bidirectional lexicalized reordering model. A KN-smoothed 5-gram language model is trained on the target side of the parallel data with SRILM (Stolcke, 2002). Finally, we include word and phrase penalties. The decoder uses the default parameters for English-to-Arabic, except that the maximum phrase length is set to 8. For Englishto-Finnish, we follow Clifton and Sarkar (2011) in setting the hypothesis stack size to 100, distortion limit to 6, and maximum phrase length to 20. The decoder’s log-linear model is tuned with MERT (Och, 2003). Re-ranking models are tuned using a batch variant of hope-fear MIRA (Chiang et al., 2008; Cherry and Foster, 2012), using the n-best variant for n-best desegmentation, and the lattice variant for lattice desegmentation. MIRA was selected over MERT because we have an in-house implementation that can tune on lattices very quickly. During development, we confirmed that MERT and MIRA perform similarly, as is expected with fewer than 20 features. Both the decoder’s log-linear model and the re-ranking models are trained on the same development set. Historically, we have not seen improvements from </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Joseph Och. 2003. Minimum error rate training for statistical machine translation. In Proceedings of ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kemal Oflazer</author>
<author>Ilknur Durgar El-Kahlout</author>
</authors>
<title>Exploring different representational units in English-to-Turkish statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>25--32</pages>
<location>Prague, Czech Republic,</location>
<marker>Oflazer, El-Kahlout, 2007</marker>
<rawString>Kemal Oflazer and Ilknur Durgar El-Kahlout. 2007. Exploring different representational units in English-to-Turkish statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 25–32, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei jing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context citStr="Papineni et al., 2002" endWordPosition="4675" position="28847" startWordPosition="4672"> where we train on unsegmented target text, requiring no desegmentation step. Our second baseline is 1-best Deseg, where we train on segmented target text and desegment the decoder’s 1-best output. Starting from the system that produced 1-best Deseg, we then output either 1000-best lists or lattices to create our two experimental systems. The 1000-best Deseg system desegments, augments and re-ranks the decoder’s 1000-best list, while Lattice Deseg does the same in the lattice. We augment n-best lists and lattices using the features described in Section 3.4.8 We evaluate our system using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). Following Clark et al. (2011), we report average scores over five random tuning replications to account for optimizer instability. For the baselines, this means 5 runs of decoder tuning. For the desegmenting re-rankers, this means 5 runs of reranker tuning, each working on n-best lists or lattices produced by the same (representative) decoder weights. We measure statistical significance using MultEval (Clark et al., 2011), which implements a stratified approximate randomization test to account for multiple tuning replications. 8Development experiments on a small</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Richard Sproat</author>
<author>Izhak Shafran</author>
</authors>
<title>Lexicographic semirings for exact automata encoding of sequence models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1--5</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context citStr="Roark et al., 2011" endWordPosition="2654" position="17116" startWordPosition="2651">s with an n-gram LM, every path coming into a node must end with the same sequence of (n−1) tokens. If this property does not hold, then nodes must be split until it does.4 This property is maintained by the decoder’s recombination rules for the segmented LM, but it is not guaranteed for the desegmented LM. Indeed, the expanded word-level context is one of the main benefits of incorporating a word-level LM. Fortunately, LM annotation as well as any necessary lattice modifications can be performed simultaneously by composing the desegmented lattice with a finite state acceptor encoding the LM (Roark et al., 2011). In summary, we are given a segmented lattice, which encodes the decoder’s translation space as an acceptor over morphemes. We compose this acceptor with a desegmenting transducer, and then with an unsegmented LM acceptor, producing a fully annotated, desegmented lattice. Instead of using a tool kit such as OpenFst (Allauzen et al., 2007), we implement both the desegmenting transducer and the LM acceptor programmatically. This eliminates the need to construct intermediate machines, such as the lattice-specific desegmenter in Figure 1b, and facilitates working with edges annotated with feature</context>
</contexts>
<marker>Roark, Sproat, Shafran, 2011</marker>
<rawString>Brian Roark, Richard Sproat, and Izhak Shafran. 2011. Lexicographic semirings for exact automata encoding of sequence models. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1–5, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad Salameh</author>
<author>Colin Cherry</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Reversing morphological tokenization in English-to-Arabic SMT.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 NAACL HLT Student Research Workshop,</booktitle>
<pages>47--53</pages>
<location>Atlanta, Georgia,</location>
<contexts>
<context citStr="Salameh et al. (2013)" endWordPosition="1106" position="7433" startWordPosition="1103">es, this amounts to simple concatenation. However, more complex segmentations, such as the Arabic tokenization provided by MADA (Habash et al., 2009), require further orthographic adjustments to reverse normalizations performed during segmentation. Badr et al. (2008) present two Arabic desegmentation schemes: table-based and rule-based. El Kholy and Habash (2012a) provide an extensive study on the influence of segmentation and desegmentation on English-toArabic SMT. They introduce an additional desegmentation technique that augments the table-based approach with an unsegmented language model. Salameh et al. (2013) replace rule-based desegmentation with a discriminatively-trained character transducer. In this work, we adopt the Table+Rules approach of El Kholy and Habash (2012a) for English-Arabic, while concatenation is sufficient for English-Finnish. Work on integration attempts to improve SMT performance for morphologically complex target languages by going beyond simple pre- and postprocessing. Oflazer and Durgar El-Kahlout (2007) desegment 1000-best lists for English-to-Turkish translation to enable scoring with an unsegmented language model. Unlike our work, they replace the segmented language mod</context>
</contexts>
<marker>Salameh, Cherry, Kondrak, 2013</marker>
<rawString>Mohammad Salameh, Colin Cherry, and Grzegorz Kondrak. 2013. Reversing morphological tokenization in English-to-Arabic SMT. In Proceedings of the 2013 NAACL HLT Student Research Workshop, pages 47–53, Atlanta, Georgia, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of Association for Machine Translation in the Americas.</booktitle>
<contexts>
<context citStr="Snover et al., 2006" endWordPosition="4681" position="28877" startWordPosition="4678">arget text, requiring no desegmentation step. Our second baseline is 1-best Deseg, where we train on segmented target text and desegment the decoder’s 1-best output. Starting from the system that produced 1-best Deseg, we then output either 1000-best lists or lattices to create our two experimental systems. The 1000-best Deseg system desegments, augments and re-ranks the decoder’s 1000-best list, while Lattice Deseg does the same in the lattice. We augment n-best lists and lattices using the features described in Section 3.4.8 We evaluate our system using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). Following Clark et al. (2011), we report average scores over five random tuning replications to account for optimizer instability. For the baselines, this means 5 runs of decoder tuning. For the desegmenting re-rankers, this means 5 runs of reranker tuning, each working on n-best lists or lattices produced by the same (representative) decoder weights. We measure statistical significance using MultEval (Clark et al., 2011), which implements a stratified approximate randomization test to account for multiple tuning replications. 8Development experiments on a small-data English-toArabic scenari</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Intl. Conf. Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<contexts>
<context citStr="Stolcke, 2002" endWordPosition="4384" position="27046" startWordPosition="4383"> perform any orthographic normalizations, it can be desegmented with simple concatenation. 4.2 Systems We align the parallel data with GIZA++ (Och et al., 2003) and decode using Moses (Koehn et al., 2007). The decoder’s log-linear model includes a standard feature set. Four translation model features encode phrase translation probabilities and lexical scores in both directions. Seven distortion features encode a standard distortion penalty as well as a bidirectional lexicalized reordering model. A KN-smoothed 5-gram language model is trained on the target side of the parallel data with SRILM (Stolcke, 2002). Finally, we include word and phrase penalties. The decoder uses the default parameters for English-to-Arabic, except that the maximum phrase length is set to 8. For Englishto-Finnish, we follow Clifton and Sarkar (2011) in setting the hypothesis stack size to 100, distortion limit to 6, and maximum phrase length to 20. The decoder’s log-linear model is tuned with MERT (Och, 2003). Re-ranking models are tuned using a batch variant of hope-fear MIRA (Chiang et al., 2008; Cherry and Foster, 2012), using the n-best variant for n-best desegmentation, and the lattice variant for lattice desegmenta</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In Intl. Conf. Spoken Language Processing, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Subotin</author>
</authors>
<title>An exponential translation model for target language morphology.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>230--238</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context citStr="Subotin, 2011" endWordPosition="696" position="4717" startWordPosition="695">7) incorporates such analyses into a factored model, to either include a language model over target morphological tags, or model the generation of morphological features. Other approaches train an SMT system to predict lemmas instead of surface forms, and then inflect the SMT output as a postprocessing step (Minkov et al., 2007; Clifton and Sarkar, 2011; Fraser et al., 2012; El Kholy and Habash, 2012b). Alternatively, one can reparameterize existing phrase tables as exponential models, so that translation probabilities account for source context and morphological features (Jeong et al., 2010; Subotin, 2011). Of these approaches, ours is most similar to the translate-then-inflect approach, except we translate and then desegment. In particular, Toutanova et al. (2008) inflect and re-rank n-best lists in a similar manner to how we desegment and re-rank n-best lists or lattices. 2.2 Morphological Segmentation Instead of producing an abstract feature layer, morphological segmentation transforms the target sentence by segmenting relevant morphemes, which are then handled as regular tokens during alignment and translation. This is done to reduce sparsity and to improve correspondence with the source la</context>
</contexts>
<marker>Subotin, 2011</marker>
<rawString>Michael Subotin. 2011. An exponential translation model for target language morphology. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 230–238, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Hisami Suzuki</author>
<author>Achim Ruopp</author>
</authors>
<title>Applying morphology generation models to machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>514--522</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context citStr="Toutanova et al. (2008)" endWordPosition="720" position="4879" startWordPosition="717">logical features. Other approaches train an SMT system to predict lemmas instead of surface forms, and then inflect the SMT output as a postprocessing step (Minkov et al., 2007; Clifton and Sarkar, 2011; Fraser et al., 2012; El Kholy and Habash, 2012b). Alternatively, one can reparameterize existing phrase tables as exponential models, so that translation probabilities account for source context and morphological features (Jeong et al., 2010; Subotin, 2011). Of these approaches, ours is most similar to the translate-then-inflect approach, except we translate and then desegment. In particular, Toutanova et al. (2008) inflect and re-rank n-best lists in a similar manner to how we desegment and re-rank n-best lists or lattices. 2.2 Morphological Segmentation Instead of producing an abstract feature layer, morphological segmentation transforms the target sentence by segmenting relevant morphemes, which are then handled as regular tokens during alignment and translation. This is done to reduce sparsity and to improve correspondence with the source language (usually English). Such a segmentation can be produced as a byproduct of analysis (Oflazer and Durgar El-Kahlout, 2007; Badr et al., 2008; El Kholy and Hab</context>
</contexts>
<marker>Toutanova, Suzuki, Ruopp, 2008</marker>
<rawString>Kristina Toutanova, Hisami Suzuki, and Achim Ruopp. 2008. Applying morphology generation models to machine translation. In Proceedings of ACL-08: HLT, pages 514–522, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>Generation of word graphs in statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>156--163</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context citStr="Ueffing et al., 2002" endWordPosition="2144" position="14074" startWordPosition="2141">+hA blEbthA Transduces into 4 4 0 AlTß blEbp b+ lEbp 0 1 2 3 2 AlTß 3 (b) AlTfl:AlTfl 1 b+:&lt;epsilon&gt; &lt;epsilon&gt;:blEbp +hA:blEbthA +hm:blEbthm 0 lEbp:&lt;epsilon&gt; 2 til each source word has been covered exactly once (Koehn et al., 2003). The search graph of a phrase-based decoder can be interpreted as a lattice, which can be interpreted as a finite state acceptor over target strings. In its most natural form, such an acceptor emits target phrases on each edge, but it can easily be transformed into a form with one edge per token, as shown in Figure 1a. This is sometimes referred to as a word graph (Ueffing et al., 2002), although in our case the segmented phrase table also produces tokens that correspond to morphemes. Our goal is to desegment the decoder’s output lattice, and in doing so, gain access to a compact, desegmented view of a large portion of the translation search space. This can be accomplished by composing the lattice with a desegmenting transducer that consumes morphemes and outputs desegmented words. This transducer must be able to consume every word in our lattice’s output vocabulary. We define a word using the following regular expression: [prefix]* [stem] [suffix]* I [prefix]+ [suffix]+ (1)</context>
</contexts>
<marker>Ueffing, Och, Ney, 2002</marker>
<rawString>Nicola Ueffing, Franz J. Och, and Hermann Ney. 2002. Generation of word graphs in statistical machine translation. In Proceedings of EMNLP, pages 156– 163, Philadelphia, PA, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>