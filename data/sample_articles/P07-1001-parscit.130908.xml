<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant confidence="0.000510" no="0">
<title confidence="0.958273">
Guiding Statistical Word Alignment Models With Prior Knowledge
</title>
<author confidence="0.661932">
Yonggang Deng and Yuqing Gao
</author>
<note confidence="0.41903">
IBM T. J. Watson Research Center
</note>
<address confidence="0.480097">
Yorktown Heights, NY 10598
</address>
<email confidence="0.996296">
{ydeng,yuqing}@us.ibm.com
</email>
<sectionHeader confidence="0.998587" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999800333333333">We present a general framework to incorporate prior knowledge such as heuristics or linguistic features in statistical generative word alignment models. Prior knowledge plays a role of probabilistic soft constraints between bilingual word pairs that shall be used to guide word alignment model training. We investigate knowledge that can be derived automatically from entropy principle and bilingual latent semantic analysis and show how they can be applied to improve translation performance.</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997006192307693">Statistical word alignment models learn word associations between parallel sentences from statistics. Most models are trained from corpora in an unsupervised manner whose success is heavily dependent on the quality and quantity of the training data. It has been shown that human knowledge, in the form of a small amount of manually annotated parallel data to be used to seed or guide model training, can significantly improve word alignment F-measure and translation performance (Ittycheriah and Roukos, 2005; Fraser and Marcu, 2006). As formulated in the competitive linking algorithm (Melamed, 2000), the problem of word alignment can be regarded as a process of word linkage disambiguation, that is, choosing correct associations among all competing hypothesis. The more reasonable constraints are imposed on this process, the easier the task would become. For instance, the 1 most relaxed IBM Model-1, which assumes that any source word can be generated by any target word equally regardless of distance, can be improved by demanding a Markov process of alignments as in HMM-based models (Vogel et al., 1996), or implementing a distribution of number of target words linked to a source word as in IBM fertility-based models (Brown et al., 1993). Following the path, we shall put more constraints on word alignment models and investigate ways of implementing them in a statistical framework. We have seen examples showing that names tend to align to names and function words are likely to be linked to function words. These observations are independent of language and can be understood by common sense. Moreover, there are other linguistically motivated constraints. For instance, words aligned to each other presumably are semantically consistent; and likely to be, they are syntactically agreeable. In these paper, we shall exploit some of these constraints in building better word alignments in the application of statistical machine translation. We propose a simple framework that can integrate prior knowledge into statistical word alignment model training. In the framework, prior knowledge serves as probabilistic soft constraints that will guide word alignment model training. We present two types of constraints that are derived in an unsupervised way: one is based on the entropy principle, the other comes from bilingual latent semantic analysis. We investigate their impact on word alignments and show their effectiveness in improving translation performance.</bodyText>
<note confidence="0.9135555">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1–8,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.977498" genericHeader="method">
2 Constrained Word Alignment Models
</sectionHeader>
<bodyText confidence="0.999828142857143">The framework that we propose to incorporate statistical constraints into word alignment models is generic. It can be applied to complicated models such IBM Model-4 (Brown et al., 1993). We shall take HMM-based word alignment model (Vogel et al., 1996) as an example and follow the notation of (Brown et al., 1993). Let e = e1 represent a source string and f = fm a target string. The random variable a = am specifies the indices of source words that target words are aligned to. In an HMM-based word alignment model, source words are treated as Markov states while target words are observations that are generated when jumping to states:</bodyText>
<equation confidence="0.996557333333333">
M
P(a,f|e) = H P(aj|aj−1, e)t(fj|eaj)
j=1
</equation>
<bodyText confidence="0.999290285714286">Notice that a target word f is generated from a source state e by a simple lookup of the translation table, a.k.a., t-table t(f|e), as depicted in (A) of Figure 1. To incorporate prior knowledge or impose constraints, we introduce two nodes E and F representing the hidden tags of the source word e and the target word f respectively, and organize the dependency structure as in (B) of Figure 1. Given this generative procedure, f will also depend on its tag F, which is determined probabilistically by the source tag E. The dependency from E to F functions as a soft constraint showing how the two hidden tags are agreeable to each other. Mathematically, the conditional distribution follows:</bodyText>
<equation confidence="0.98689975">
P(f|e) = E P(f, E, F|e)
E,F
E= P(E|e)P(F|E)P(f|e, F)
E,F
</equation>
<bodyText confidence="0.998639">is the soft weight attached to the t-table entry. It considers all possible hidden tags of e and f and serves as constraint between the link.</bodyText>
<figure confidence="0.864737">
A B
</figure>
<figureCaption confidence="0.727599333333333">
Figure 1: A simple table lookup (A) vs. a con-
strained procedure (B) of generating a target word
f from a source word e.
</figureCaption>
<bodyText confidence="0.999896054054054">We do not change the value of Con(f, e) during iterative model training but rather keep it constant as an indicator of how strong the word pair should be considered as a candidate. This information is derived before word alignment model training and will act as soft constraints that need to be respected during training and alignments. For a given word pair, the soft constraint can have different assignment in different sentence pairs since the word tags can be context dependent. To understand why we take the “detour” of generating a target word rather than directly from a ttable, consider the hidden tag as binary value indicating being a name or not. Without these constraints, t-table entries for names with low frequency tend to be flat and word alignments can be chosen randomly without sufficient statistics or strong lexical preference under maximum likelihood criterion. If we assume that a name is produced by a name with a high probability but by a non-name with a low probability, i.e. P(F = E) &gt;&gt; P(F =6 E), proper names with low counts then are encouraged to link to proper names during training; and consequently, conditional probability mass would be more focused on correct name translations. On the other hand, names are discouraged to produce non-names. This will potentially avoid incorrect word associations. We are able to apply this type of constraint since usually there are many monolingual resources available to build a high performance probabilistic name tagger. The example suggests that putting reasonable constraints learned from monolingual analysis can alleviate data spareness problem in bilingual applications. The weights Con(f, e) are the prior knowledge that shall be assigned with care but respected during training. The baseline is to set all these weights to 1, which is equivalent to placing no prior knowledge on model training.</bodyText>
<equation confidence="0.7357135">
f
e
F
E
f
e
= t(f|e) · Con(f, e), (1)
where
Con(f, e) = E P(E|e)P(F|E)P(F|f)/P(F) (2)
E,F
</equation>
<page confidence="0.96473">
2
</page>
<bodyText confidence="0.99988468">The introduction of these weights does not complicate parameter estimation procedure. Whenever a source word e is hypothesized to generate a target word f, the translation probability t(f|e) should be weighted by Con(f, e). We point out that the constraints between f and e through their hidden tags are in probabilities. There are no hard decisions made before training. A strong preference between two words can be expressed by assigning corresponding weights close to 1. This will affect the final alignment model. Depending on the hidden tags, there are many realizations of reasonable constraints that can be put beforehand. They can be semantic classes, syntactic annotations, or as simple as whether being a function word or content word. Moreover, the source side and the target side do not have to share the same set of tags. The framework is also flexible to support multiple types of constraints that can be implemented in parallel or cascaded sequence. Moreover, the constraints between words can be dependent on context within parallel sentences. Next, we will describe two types of constraints that we proposed. Both of them are derived from data in an unsupervised way.</bodyText>
<subsectionHeader confidence="0.964751">
2.1 Entropy Principle
</subsectionHeader>
<bodyText confidence="0.999993888888889">It is assumed that generally speaking, a source function word generates a target function word with a higher probability than generating a target content word; similar assumption applies to a source content word as well. We capture this type of constraint by defining the hidden tag E and F as binary labels indicating being a content word or not. Based on the assumption, we design probabilistic relationship between the two hidden tags as:</bodyText>
<equation confidence="0.975193">
P(E = F) = 1 − P(E =� F) = α,
</equation>
<bodyText confidence="0.995231115384615">where α is a scalar whose value is close to 1, say 0.9. The bigger α is, the tighter constraint we put on word pairs to be connected requiring the same type of label. To determine the probability of a word being a function word, we apply the entropy principle. A function word, say “of”,“in” or “have”, appears more frequently than a content word, say “journal” or “chemistry”, in a document or sentence. We will approximate the probability of a word as a function word with the relative uncertainty of its being observed in a sentence. More specifically, suppose we have N parallel sentences in the training corpus. For each word wi1, let ci9 be the number of word wi observed in the j-th sentence pair, and let ci be the total number of occurrences of wi in the corpus. We define the relative entropy of word wi as cilo cii . ci g ci With the entropy of a word, the likelihood of word w being tagged as a function word is approximated with w(1) = ew and being tagged as a content word with w(0) = 1 − ew. We ignore the denominator in Equ. (2) and find the constraint under the entropy principle:</bodyText>
<equation confidence="0.998602">
Con(f, e) = α(e(0)f(0) + e(1)f(1)) +
(1 − α)(e(1)f(0) + e(0)f(1)).
</equation>
<bodyText confidence="0.999833545454545">As can be seen, the connection between two words is simulated with a binary symmetric channel. An example distribution of the constraint function is illustrated in Figure 2. A high value of α encourages connecting word pairs with comparable entropy; When α = 0.5, Con(f, e) is constant which corresponds to applying no prior constraint; When α is close to 0, the function plays opposite role on word alignment training where a high frequency word is pushed to associate with a low frequency word.</bodyText>
<subsectionHeader confidence="0.999421">
2.2 Bilingual Latent Semantic Analysis
</subsectionHeader>
<bodyText confidence="0.999024444444444">Latent Semantic Analysis (LSA) is a theory and method for extracting and representing the meaning of words by statistically analyzing word contextual usages in a collection of text. It provides a method by which to calculate the similarity of meaning of given words and documents. LSA has been successfully applied to information retrieval (Deerwester et al., 1990), statistical langauge modeling (Bellegarda, 2000) and etc.</bodyText>
<footnote confidence="0.826379666666667">
1We prefix ‘E ’ to source words and ‘F ’ to target words
to distinguish words that have the same spelling but are from
different languages.
</footnote>
<figure confidence="0.7771205">
1
N
E
9==1
log N
Ewi = −
</figure>
<page confidence="0.688217">
3
</page>
<figureCaption confidence="0.994">
Figure 2: Distribution of the constraint function
based on entropy principle when α = 0.9 on the left and α = 0.1 on the right.</figureCaption>
<bodyText confidence="0.9709278">We explore LSA techniques in bilingual environment to derive semantic constraints as prior knowledge for guiding a word alignment model training. The idea is to find semantic representation of source words and target words in the so-called lowdimensional LSA-space, and then to use their similarities to quantitatively establish semantic consistencies. We propose two different approaches.</bodyText>
<subsectionHeader confidence="0.955168">
2.2.1 A Simple Bag-of-word Model
</subsectionHeader>
<bodyText confidence="0.999627666666667">One method we investigate is a simple bag-ofword model as in monolingual LSA. We treat each sentence pair as a document and do not distinguish source words and target words as if they are terms generated from the same vocabulary. A sparse matrix W characterizing word-document cooccurrence is constructed. Following the notation in section 2.1, the ij-th entry of the matrix W is defined as in (Bellegarda, 2000)</bodyText>
<equation confidence="0.9789275">
WZj = (1 − C-i)cZj
cj
</equation>
<bodyText confidence="0.995544208333333">where cj is the total number of words in the j-th sentence pair. This construction considers the importance of words globally (corpus wide) and locally (within sentence pairs). Alternative constructions of the matrix are possible using raw counts or TF-IDF (Deerwester et al., 1990). W is a M x N sparse matrix, where M is the size of vocabulary including both source and target words. To obtain a compact representation, singular value decomposition (SVD) is employed (cf. Berry et al (1993)) to yield W S W�= U x S x V T as Figure 3 shows, where, for some order R � min(M, N) of the decomposition, U is a MxR left singular matrix with rows uZ, i = 1, · · · , M, S is a RxR diagonal matrix of singular values s1 &gt; s2 &gt; ... &gt; sR » 0, and V is NxR a right singular matrix with rows vj, j = 1, · · · , N. For each i, the scaled R-vector uZS may be viewed as representing wZ, the i-th word in the vocabulary, and similarly the scaled R-vector vjS as representing dj, j-th document in the corpus. Note that the uZS’s and vjS’s both belong to IRR, the so-called LSA-space. All target and source words are projected into the same LSA-space too.</bodyText>
<figure confidence="0.913392">
W U S VT
</figure>
<figureCaption confidence="0.990506">
Figure 3: SVD of the Sparse Matrix W.
</figureCaption>
<equation confidence="0.9698765">
W
M ×N M ×R
</equation>
<bodyText confidence="0.999455666666667">As Equ. (2) suggested, to induce semantic constraints in a straightforward way, one would proceed as follows: firstly, perform word semantic clustering with, say, their compact representations in the LSA-space; secondly, construct cluster generating dependencies by specifying the conditional distribution of P(F|E); andvfinally, for each word pair, induce the semantic constraint by considering all possible semantic labeling schemes. We approximate this long process with simply finding word similarities defined by their cosine distance in the low dimension space:</bodyText>
<equation confidence="0.961807666666667">
o
1
Con(f, e) = �(cos(ufS, u,S) + 1) (3)
</equation>
<bodyText confidence="0.999975666666667">The linear mapping above is introduced to avoid negative constraints and to set the maximum constraint value as 1. In building word alignment models, a special “NULL” word is usually introduced to address target words that align to no source words. Since this physically non-existing word is not in the vocabulary of the bilingual LSA, we use the centroid of all source words as its vector representation in the LSAspace. The semantic constraints between “NULL” and any target words can be derived in the same way. However, this is chosen for mostly computational convenience, and is not the only way to address the empty word issue.</bodyText>
<figure confidence="0.886943375">
Documents
d1 dN
R orthonormal vectors
wM
w1
R×R R×N
,
R
</figure>
<page confidence="0.952684">
4
</page>
<subsectionHeader confidence="0.774437">
2.2.2 Utilizing Word Alignment Statistics
</subsectionHeader>
<bodyText confidence="0.999993206896552">While the simple bag-of-word model puts all source words and target words as rows in the matrix, another method of deriving semantic constraint constructs the sparse matrix by taking source words as rows and target words as columns and uses statistics from word alignment training to form word pair co-occurrence association. More specifically, we regard each target word f as a “document” and each source word e as a “term”. The number of occurrences of the source word e in the document f is defined as the expected number of times that f generates e in the parallel corpus under the word alignment model. This method requires training the baseline word alignment model in another direction by taking fs as source words and es as target words, which is often done for symmetric alignments, and then dumping out the soft counts when model converges. We threshold the minimum word-to-word translation probability to remove word pairs that have low co-occurrence counts. Following the similarity induced semantic constraints in section 2.2.1, we need to find the distance between a term and a document. Let vf be the projection of the document representing the target word f and ue the projection of the term representing the source word e after performing SVD on the sparse matrix, we calculate the similarity between (f, e) and then find their semantic constraint to be</bodyText>
<equation confidence="0.997812">
Con(f, e) = �(cos(vfS1/2, ueS1/2) + �) (4)
1
</equation>
<bodyText confidence="0.9996648">Unlike the method in section 2.2.1, there is no empty word issue here since we do have statistics of the “NULL” word as a source word generating e words and therefore there is a “document” assigned to it.</bodyText>
<sectionHeader confidence="0.998107" genericHeader="result">
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.999985777777778">We test our framework on the task of large vocabulary translation from dialectical (Iraqi) Arabic utterances into English. The task covers multiple domains including travel, emergency medical diagnosis, defense-oriented force protection, security and etc. To avoid impacts of speech recognition errors, we only report experiments from text to text translation. The training corpus consists of 390K sentence pairs, with total 2.43M Arabic words and 3.38M English words. These sentences are in typical spoken transcription form, i.e., spelling errors, disfluencies, such as word or phrase repetition, and ungrammatical utterances are commonly observed. Arabic utterance length ranges from 3 to 70 words with the average of 6 words. There are 25K entries in the English vocabulary and 90K in Arabic side. Data sparseness severely challenges word alignment model and consequently automatic phrase translation induction. There are 42K singletons in Arabic vocabulary, and 14K Arabic words with occurrence of twice each in the corpus. Since Arabic is a morphologically rich language where affixes are attached to stem words to indicate gender, tense, case and etc, in order to reduce vocabulary size and address out-of-vocabulary words, we split Arabic words into affix and root according to a rule-based segmentation scheme (Xiang et al., 2006) with the help from the Buckwalter analyzer (LDC, 2002) output. This reduces the size of Arabic vocabulary to 52K. Our test data consists of 1294 sentence pairs. They are split into two parts: half of them is used as the development set, on which training parameters and decoding feature weights are tuned, the other half is for test.</bodyText>
<subsectionHeader confidence="0.998719">
3.1 Training and Translation Setup
</subsectionHeader>
<bodyText confidence="0.9995856">Starting from the collection of parallel training sentences, we train word alignment models in two translation directions, from English to Iraqi Arabic and from Iraqi Arabic to English, and derive two sets of Viterbi alignments. By combining word alignments in two directions using heuristics (Och and Ney, 2003), a single set of static word alignments is then formed. All phrase pairs which respect to the word alignment boundary constraint are identified and pooled to build phrase translation tables with the Maximum Likelihood criterion. We prune phrase translation entries by their probabilities. The maximum number of tokens in Arabic phrases is set to 5 for all conditions. Our decoder is a phrase-based multi-stack implementation of the log-linear model similar to Pharaoh (Koehn et al., 2003).</bodyText>
<page confidence="0.98488">
5
</page>
<bodyText confidence="0.9999663">Like other log-linear model based decoders, active features in our translation engine include translation models in two directions, lexicon weights in two directions, language model, distortion model, and sentence length penalty. These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method (Och and Ney, 2002). The language model is a statistical trigram model estimated with Modified Kneser-Ney smoothing (Chen and Goodman, 1996) using all English sentences in the parallel training data. We measure translation performance by the BLEU score (Papineni et al., 2002) and Translation Error Rate (TER) (Snover et al., 2006) with one reference for each hypothesis. Word alignment models trained with different constraints are compared to show their effects on the resulting phrase translation tables and the final translation performance.</bodyText>
<subsectionHeader confidence="0.999832">
3.2 Translation Results
</subsectionHeader>
<bodyText confidence="0.999986176470589">Our baseline word alignment model is the word-toword Hidden Markov Model (Vogel et al., 1996). Basic models in two translation directions are trained simultaneously where statistics of two directions are shared to learn symmetric translation lexicon and word alignments with high precision motivated by (Zens et al., 2004) and (Liang et al., 2006). The baseline translation results (BLEU and TER) on the dev and test set are presented in the line “HMM” of Table 1. We also compare with results of IBM Model-4 word alignments implemented in GIZA++ toolkit (Och and Ney, 2003). We study and compare two types of constraint and see how they affect word alignments and translation output. One is based on the entropy principle as described in Section 2.1, where α is set to 0.9; The other is based on bilingual latent semantic analysis. For the simple bag-of-word bilingual LSA as described in Section 2.2.1, after SVD on the sparse matrix using the toolkit SVDPACK (Berry et al., 1993), all source and target words are projected into a lowdimensional (R = 88) LSA-space. Word pair semantic constrains are calculated based on their similarity as in Equ. 3 before word alignment training. Like the baseline, we perform 6 iterations of IBM Model-1 training and then 4 iteration of HMM training. The semantic constraints are used to guide word alignment model training for each iteration. The BLEU score and TER with this constraint are shown in the line “BiLSA-1” of Table 1. To exploit word alignment statistics in bilingual LSA as described in Section 2.2.2, we dump out the statistics of the baseline word alignment model and use them to construct the sparse matrix. We find low-dimensional representation (R = 67) of English words and Arabic words and use their similarity to establish semantic constraints as in Equ. 4. The training procedure is the same as the baseline and “BiLSA-1”. The translation results with these word alignments are shown as “BiLSA-2” in Table 1. As Table 1 shows, when the entropy based constraints are applied, BLEU score improves 0.5 point on the test set. Clearly, when bilingual LSA constraints are applied, translation performance can be improved up to 1.6 BLEU points. We also observe that TER can drop 2.1 points with the “BiLSA-1” constraint. While “BiLSA-1” constraint performs better on the test set, “BiLSA-2” constraint achieves slightly higher BLEU score on the dev set. We then try a simple combination of these two types of constraints, that is the geometric mean of ConBZLSA_1(f, e) and ConBZLSA_2(f, e), and find out that BLEU score can be improved a little bit further on both sets as the line “Mix” shows. We notice that the relatively simpler HMM model can perform comparable or better than the sophisticated Model-4 when proper constraints are active in guiding word alignment model training. We also try to put constraints in Model-4. As the Equation 1 implies, when a word-to-word generative probability is needed, one should multiply corresponding lexicon entry in the t-table with the word pair constraint. We simply modify the GIZA++ toolkit (Och and Ney, 2003) by always weighting lexicon probabilities with soft constraints during iterative model training, and obtain 0.7% TER reduction on both sets and 0.4% BLEU improvement on the test set.</bodyText>
<subsectionHeader confidence="0.999386">
3.3 Analysis
</subsectionHeader>
<bodyText confidence="0.998796">To understand how prior knowledge encoded as soft constraints plays a role in guiding word alignment training, we compare statistics of different word alignment models. We find that our baseline HMM generates 2.6% less number of total word links than that of Model-4.</bodyText>
<page confidence="0.999693">
6
</page>
<tableCaption confidence="0.9890265">
Table 1: Translation Results with different word
alignments.
</tableCaption>
<table confidence="0.999350888888889">
Alignments BLEU TER
dev test dev test
Model-4 0.310 0.296 0.528 0.530
+Mix 0.306 0.300 0.521 0.523
HMM 0.289 0.288 0.543 0.542
+Entropy 0.289 0.293 0.534 0.536
+BiLSA-1 0.294 0.300 0.531 0.521
+BiLSA-2 0.298 0.292 0.530 0.528
+Mix 0.302 0.304 0.532 0.524
</table>
<bodyText confidence="0.990291384615385">Part of the reason is that models of two directions in the baseline are trained simultaneously. The requirement of bi-directional evidence places a certain constraint on word alignments. When “BiLSA-1” constraints are applied in the baseline model, 2.7% less number of total word links are hypothesized, and consequently, less number of Arabic n-gram translations in the final phrase translation table are induced. The observation suggests that the constraints improve word alignment precision and accuracy of phrase translation tables as well.</bodyText>
<figure confidence="0.8432095">
+BiLSA-1
Model-4
</figure>
<figureCaption confidence="0.912114416666667">
Figure 4: An example of word alignments under dif-
ferent models
Figure 4 shows example word alignments of a par-
tial sentence pair. The complete English sentence is
“have you ever had like any reflux diseases in your
esophagus”. We notice that the Arabic word “mrM”
(means esophagus) appears only once in the corpus.
Some of the word pair constraints are listed in Ta-
ble 2. The example demos that due to reasonable
constraints placed in word alignment training, the
link to “ tK” is corrected and consequently we have
accurate word translation for the Arabic singleton
</figureCaption>
<figure confidence="0.9401105">
English e Arabic f ConBiLSa_1(f,e)
esophagus mrM 0.6424
mAl 0.1819
tk 0.2897
your mrM 0.6319
mAl 0.4930
tk 0.9672
“mrM”.
</figure>
<sectionHeader confidence="0.999455" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999957942857143">Heuristics based on co-occurrence analysis, such as point-wise mutual information or Dice coefficients , have been shown to be indicative for word alignments (Zhang and Vogel, 2005; Melamed, 2000). The framework presented in this paper demonstrates the possibility of taking heuristics as constraints guiding statistical generative word alignment model training. Their effectiveness can be expected especially when data sparseness is severe. Discriminative word alignment models, such as Ittycheriah and Roukos (2005); Moore (2005); Blunsom and Cohn (2006), have received great amount of study recently. They have proven that linguistic knowledge is useful in modeling word alignments under log-linear distributions as morphological, semantic or syntactic features. Our framework proposes to exploit these features differently by taking them as soft constraints of translation lexicon under a generative model. While word alignments can help identifying semantic relations (van der Plas and Tiedemann, 2006), we proceed in the reverse direction. We investigate the impact of semantic constraints on statistical word alignment models as prior knowledge. In (Ma et al., 2004), bilingual semantic maps are constructed to guide word alignment. The framework we proposed seamlessly integrates derived semantic similarities into a statistical word alignment model. And we extended monolingual latent semantic analysis in bilingual applications. Toutanova et al. (2002) augmented bilingual sentence pairs with part-of-speech tags as linguistic constraints for HMM-based word alignments. The constraints between tags are automatically learned in a parallel generative procedure along with lexicon.</bodyText>
<figure confidence="0.768568875">
gloss (in) (esophagus) (ownership) (yours)
bAl_ mrM mAl _tk
HMM
in your esophagus
bAl_ mrM mAl _tk
in your esophagus
bAl_ mrM mAl _tk
in your esophagus
</figure>
<tableCaption confidence="0.907043">
Table 2: Word pair constraint values
</tableCaption>
<page confidence="0.998914">
7
</page>
<bodyText confidence="0.999938333333333">We have introduced hidden tags between a word pair to specialize their soft constraints, which serve as prior knowledge that will be used in guiding word alignment model training. Constraint between tags are embedded into the word to word generative process.</bodyText>
<sectionHeader confidence="0.998736" genericHeader="conclusion">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999975631578947">We have presented a simple and effective framework to incorporate prior knowledge such as heuristics or linguistic features into statistical generative word alignment models. Prior knowledge serves as soft constraints that shall be placed on translation lexicon to guide word alignment model training and disambiguation during Viterbi alignment process. We studied two types of constraints that can be obtained automatically from data and showed improved performance (up to 1.6% absolute BLEU increase or 2.1% absolute TER reduction) in translating dialectical Arabic into English. Future work includes implementing the idea in alternative alignment models and also exploiting prior knowledge derived from such as manually-aligned data and pre-existing linguistic resources. Acknowledgement We thank Mohamed Afify for discussions and the anonymous reviewers for suggestions.</bodyText>
<sectionHeader confidence="0.999541" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999790614285714">
J. R. Bellegarda. 2000. Exploiting latent semantic informa-
tion in statistical language modeling. Proc. of the IEEE,
88(8):1279–1296, August.
M. Berry, T. Do, and S. Varadhan. 1993. Svdpackc (version
1.0) user’s guide. Tech. report cs-93-194, University of Ten-
nessee, Knoxville, TN.
P. Blunsom and T. Cohn. 2006. Discriminative word alignment
with conditional random fields. In Proc. of COLING/ACL,
pages 65–72.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer. 1993.
The mathematics of machine translation: Parameter estima-
tion. Computational Linguistics, 19:263–312.
S. F. Chen and J. Goodman. 1996. An empirical study of
smoothing techniques for language modeling. In Proc. of
ACL, pages 310–318.
S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas,
and R. A. Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society of Information
Science, 41(6):391–407.
A. Fraser and D. Marcu. 2006. Semi-supervised training for
statistical word alignment. In Proc. of COLING/ACL, pages
769–776.
A. Ittycheriah and S. Roukos. 2005. A maximum entropy word
aligner for arabic-english machine translation. In Proc. of
HLT/EMNLP, pages 89–96.
P. Koehn, F. Och, and D. Marcu. 2003. Statistical phrase-based
translation. In Proc. of HLT-NAACL.
LDC, 2002. Buckwalter Arabic Morphological Analyzer Ver-
sion 1.0. LDC Catalog Number LDC2002L49.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by agree-
ment. In Proc. of HLT/NAACL, pages 104–111.
Q. Ma, K. Kanzaki, Y. Zhang, M. Murata, and H. Isahara.
2004. Self-organizing semantic maps and its application to
word alignment in japanese-chinese parallel corpora. Neural
Netw., 17(8-9):1241–1253.
I. Dan. Melamed. 2000. Models of translational equivalence
among words. Computational Linguistics, 26(2):221–249.
R. C. Moore. 2005. A discriminative framework for bilingual
word alignment. In Proc. of HLT/EMNLP, pages 81–88.
F. J. Och and H. Ney. 2002. Discriminative training and max-
imum entropy models for statistical machine translation. In
Proc. of ACL, pages 295–302.
F. J. Och and H. Ney. 2003. A systematic comparison of vari-
ous statistical alignment models. Computational Linguistics,
29(1):19–51.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. Bleu: a
method for automatic evaluation of machine translation. In
Proc. ofACL, pages 311–318.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul.
2006. A study of translation edit rate with targeted human
annotation. In Proc. of AMTA.
K. Toutanova, H. T. Ilhan, and C. Manning. 2002. Extentions
to HMM-based statistical word alignment models. In Proc.
of EMNLP.
Lonneke van der Plas and J¨org Tiedemann. 2006. Finding syn-
onyms using automatic word alignment and measures of dis-
tributional similarity. In Proc. of the COLING/ACL 2006
Main Conference Poster Sessions, pages 866–873.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM based word
alignment in statistical translation. In Proc. of COLING.
B. Xiang, K. Nguyen, L. Nguyen, R. Schwartz, and J. Makhoul.
2006. Morphological decomposition for arabic broadcast
news transcription. In Proc. of ICASSP, pages 1089–1092.
R. Zens, E. Matusov, and H. Ney. 2004. Improved word align-
ment using a symmetric lexicon model. In Proc. of COL-
ING, pages 36–42.
Y. Zhang and S. Vogel. 2005. Competitive grouping in inte-
grated phrase segmentation and alignment model. In Proc.
of the ACL Workshop on Building and Using Parallel Texts,
pages 159–162.
</reference>
<page confidence="0.998473">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant confidence="0.964111" no="0">
<title confidence="0.998803">Guiding Statistical Word Alignment Models With Prior Knowledge</title>
<author confidence="0.995751">Deng Gao</author>
<affiliation confidence="0.999932">IBM T. J. Watson Research Center</affiliation>
<address confidence="0.9869">Yorktown Heights, NY 10598</address>
<abstract confidence="0.998618230769231">We present a general framework to incorporate prior knowledge such as heuristics or linguistic features in statistical generative word alignment models. Prior knowledge plays a role of probabilistic soft constraints between bilingual word pairs that shall be used to guide word alignment model training. We investigate knowledge that can be derived automatically from entropy principle and bilingual latent semantic analysis and show how they can be applied to improve translation performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J R Bellegarda</author>
</authors>
<title>Exploiting latent semantic information in statistical language modeling.</title>
<date>2000</date>
<booktitle>Proc. of the IEEE,</booktitle>
<volume>88</volume>
<issue>8</issue>
<contexts>
<context citStr="Bellegarda, 2000" endWordPosition="1830" position="10853" startWordPosition="1828">t; When α is close to 0, the function plays opposite role on word alignment training where a high frequency word is pushed to associate with a low frequency word. 2.2 Bilingual Latent Semantic Analysis Latent Semantic Analysis (LSA) is a theory and method for extracting and representing the meaning of words by statistically analyzing word contextual usages in a collection of text. It provides a method by which to calculate the similarity of meaning of given words and documents. LSA has been successfully applied to information retrieval (Deerwester et al., 1990), statistical langauge modeling (Bellegarda, 2000) and etc. 1We prefix ‘E ’ to source words and ‘F ’ to target words to distinguish words that have the same spelling but are from different languages. 1 N E 9==1 log N Ewi = − 3 Figure 2: Distribution of the constraint function based on entropy principle when α = 0.9 on the left and α = 0.1 on the right. We explore LSA techniques in bilingual environment to derive semantic constraints as prior knowledge for guiding a word alignment model training. The idea is to find semantic representation of source words and target words in the so-called lowdimensional LSA-space, and then to use their similar</context>
</contexts>
<marker>Bellegarda, 2000</marker>
<rawString>J. R. Bellegarda. 2000. Exploiting latent semantic information in statistical language modeling. Proc. of the IEEE, 88(8):1279–1296, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Berry</author>
<author>T Do</author>
<author>S Varadhan</author>
</authors>
<title>Svdpackc (version 1.0) user’s guide.</title>
<date>1993</date>
<tech>Tech. report cs-93-194,</tech>
<institution>University of Tennessee,</institution>
<location>Knoxville, TN.</location>
<contexts>
<context citStr="Berry et al (1993)" endWordPosition="2119" position="12508" startWordPosition="2116">lowing the notation in section 2.1, the ij-th entry of the matrix W is defined as in (Bellegarda, 2000) WZj = (1 − C-i)cZj cj where cj is the total number of words in the j-th sentence pair. This construction considers the importance of words globally (corpus wide) and locally (within sentence pairs). Alternative constructions of the matrix are possible using raw counts or TF-IDF (Deerwester et al., 1990). W is a M x N sparse matrix, where M is the size of vocabulary including both source and target words. To obtain a compact representation, singular value decomposition (SVD) is employed (cf. Berry et al (1993)) to yield W S W�= U x S x V T as Figure 3 shows, where, for some order R � min(M, N) of the decomposition, U is a MxR left singular matrix with rows uZ, i = 1, · · · , M, S is a RxR diagonal matrix of singular values s1 &gt; s2 &gt; ... &gt; sR » 0, and V is NxR a right singular matrix with rows vj, j = 1, · · · , N. For each i, the scaled R-vector uZS may be viewed as representing wZ, the i-th word in the vocabulary, and similarly the scaled R-vector vjS as representing dj, j-th document in the corpus. Note that the uZS’s and vjS’s both belong to IRR, the so-called LSA-space. All target and source wo</context>
<context citStr="Berry et al., 1993" endWordPosition="3500" position="20617" startWordPosition="3497">ranslation results (BLEU and TER) on the dev and test set are presented in the line “HMM” of Table 1. We also compare with results of IBM Model-4 word alignments implemented in GIZA++ toolkit (Och and Ney, 2003). We study and compare two types of constraint and see how they affect word alignments and translation output. One is based on the entropy principle as described in Section 2.1, where α is set to 0.9; The other is based on bilingual latent semantic analysis. For the simple bag-of-word bilingual LSA as described in Section 2.2.1, after SVD on the sparse matrix using the toolkit SVDPACK (Berry et al., 1993), all source and target words are projected into a lowdimensional (R = 88) LSA-space. Word pair semantic constrains are calculated based on their similarity as in Equ. 3 before word alignment training. Like the baseline, we perform 6 iterations of IBM Model-1 training and then 4 iteration of HMM training. The semantic constraints are used to guide word alignment model training for each iteration. The BLEU score and TER with this constraint are shown in the line “BiLSA-1” of Table 1. To exploit word alignment statistics in bilingual LSA as described in Section 2.2.2, we dump out the statistics </context>
</contexts>
<marker>Berry, Do, Varadhan, 1993</marker>
<rawString>M. Berry, T. Do, and S. Varadhan. 1993. Svdpackc (version 1.0) user’s guide. Tech. report cs-93-194, University of Tennessee, Knoxville, TN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blunsom</author>
<author>T Cohn</author>
</authors>
<title>Discriminative word alignment with conditional random fields.</title>
<date>2006</date>
<booktitle>In Proc. of COLING/ACL,</booktitle>
<pages>65--72</pages>
<contexts>
<context citStr="Blunsom and Cohn (2006)" endWordPosition="4271" position="25351" startWordPosition="4268"> 0.2897 your mrM 0.6319 mAl 0.4930 tk 0.9672 “mrM”. 4 Related Work Heuristics based on co-occurrence analysis, such as point-wise mutual information or Dice coefficients , have been shown to be indicative for word alignments (Zhang and Vogel, 2005; Melamed, 2000). The framework presented in this paper demonstrates the possibility of taking heuristics as constraints guiding statistical generative word alignment model training. Their effectiveness can be expected especially when data sparseness is severe. Discriminative word alignment models, such as Ittycheriah and Roukos (2005); Moore (2005); Blunsom and Cohn (2006), have received great amount of study recently. They have proven that linguistic knowledge is useful in modeling word alignments under log-linear distributions as morphological, semantic or syntactic features. Our framework proposes to exploit these features differently by taking them as soft constraints of translation lexicon under a generative model. While word alignments can help identifying semantic relations (van der Plas and Tiedemann, 2006), we proceed in the reverse direction. We investigate the impact of semantic constraints on statistical word alignment models as prior knowledge. In </context>
</contexts>
<marker>Blunsom, Cohn, 2006</marker>
<rawString>P. Blunsom and T. Cohn. 2006. Discriminative word alignment with conditional random fields. In Proc. of COLING/ACL, pages 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>R Mercer</author>
</authors>
<title>The mathematics of machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--263</pages>
<contexts>
<context citStr="Brown et al., 1993" endWordPosition="305" position="1944" startWordPosition="302">ent can be regarded as a process of word linkage disambiguation, that is, choosing correct associations among all competing hypothesis. The more reasonable constraints are imposed on this process, the easier the task would become. For instance, the 1 most relaxed IBM Model-1, which assumes that any source word can be generated by any target word equally regardless of distance, can be improved by demanding a Markov process of alignments as in HMM-based models (Vogel et al., 1996), or implementing a distribution of number of target words linked to a source word as in IBM fertility-based models (Brown et al., 1993). Following the path, we shall put more constraints on word alignment models and investigate ways of implementing them in a statistical framework. We have seen examples showing that names tend to align to names and function words are likely to be linked to function words. These observations are independent of language and can be understood by common sense. Moreover, there are other linguistically motivated constraints. For instance, words aligned to each other presumably are semantically consistent; and likely to be, they are syntactically agreeable. In these paper, we shall exploit some of th</context>
<context citStr="Brown et al., 1993" endWordPosition="554" position="3576" startWordPosition="551">way: one is based on the entropy principle, the other comes from bilingual latent semantic analysis. We investigate their impact on word alignments and show their effectiveness in improving translation performance. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1–8, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics 2 Constrained Word Alignment Models The framework that we propose to incorporate statistical constraints into word alignment models is generic. It can be applied to complicated models such IBM Model-4 (Brown et al., 1993). We shall take HMM-based word alignment model (Vogel et al., 1996) as an example and follow the notation of (Brown et al., 1993). Let e = e1 represent a source string and f = fm a target string. The random variable a = am specifies the indices of source words that target words are aligned to. In an HMM-based word alignment model, source words are treated as Markov states while target words are observations that are generated when jumping to states: M P(a,f|e) = H P(aj|aj−1, e)t(fj|eaj) j=1 Notice that a target word f is generated from a source state e by a simple lookup of the translation tab</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer. 1993. The mathematics of machine translation: Parameter estimation. Computational Linguistics, 19:263–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>310--318</pages>
<contexts>
<context citStr="Chen and Goodman, 1996" endWordPosition="3263" position="19205" startWordPosition="3259">ditions. Our decoder is a phrase-based multi-stack imple5 mentation of the log-linear model similar to Pharaoh (Koehn et al., 2003). Like other log-linear model based decoders, active features in our translation engine include translation models in two directions, lexicon weights in two directions, language model, distortion model, and sentence length penalty. These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method (Och and Ney, 2002). The language model is a statistical trigram model estimated with Modified Kneser-Ney smoothing (Chen and Goodman, 1996) using all English sentences in the parallel training data. We measure translation performance by the BLEU score (Papineni et al., 2002) and Translation Error Rate (TER) (Snover et al., 2006) with one reference for each hypothesis. Word alignment models trained with different constraints are compared to show their effects on the resulting phrase translation tables and the final translation performance. 3.2 Translation Results Our baseline word alignment model is the word-toword Hidden Markov Model (Vogel et al., 1996). Basic models in two translation directions are trained simultaneously where</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>S. F. Chen and J. Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proc. of ACL, pages 310–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S C Deerwester</author>
<author>S T Dumais</author>
<author>T K Landauer</author>
<author>G W Furnas</author>
<author>R A Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society of Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context citStr="Deerwester et al., 1990" endWordPosition="1824" position="10803" startWordPosition="1821">constant which corresponds to applying no prior constraint; When α is close to 0, the function plays opposite role on word alignment training where a high frequency word is pushed to associate with a low frequency word. 2.2 Bilingual Latent Semantic Analysis Latent Semantic Analysis (LSA) is a theory and method for extracting and representing the meaning of words by statistically analyzing word contextual usages in a collection of text. It provides a method by which to calculate the similarity of meaning of given words and documents. LSA has been successfully applied to information retrieval (Deerwester et al., 1990), statistical langauge modeling (Bellegarda, 2000) and etc. 1We prefix ‘E ’ to source words and ‘F ’ to target words to distinguish words that have the same spelling but are from different languages. 1 N E 9==1 log N Ewi = − 3 Figure 2: Distribution of the constraint function based on entropy principle when α = 0.9 on the left and α = 0.1 on the right. We explore LSA techniques in bilingual environment to derive semantic constraints as prior knowledge for guiding a word alignment model training. The idea is to find semantic representation of source words and target words in the so-called lowdi</context>
<context citStr="Deerwester et al., 1990" endWordPosition="2082" position="12298" startWordPosition="2079">entence pair as a document and do not distinguish source words and target words as if they are terms generated from the same vocabulary. A sparse matrix W characterizing word-document cooccurrence is constructed. Following the notation in section 2.1, the ij-th entry of the matrix W is defined as in (Bellegarda, 2000) WZj = (1 − C-i)cZj cj where cj is the total number of words in the j-th sentence pair. This construction considers the importance of words globally (corpus wide) and locally (within sentence pairs). Alternative constructions of the matrix are possible using raw counts or TF-IDF (Deerwester et al., 1990). W is a M x N sparse matrix, where M is the size of vocabulary including both source and target words. To obtain a compact representation, singular value decomposition (SVD) is employed (cf. Berry et al (1993)) to yield W S W�= U x S x V T as Figure 3 shows, where, for some order R � min(M, N) of the decomposition, U is a MxR left singular matrix with rows uZ, i = 1, · · · , M, S is a RxR diagonal matrix of singular values s1 &gt; s2 &gt; ... &gt; sR » 0, and V is NxR a right singular matrix with rows vj, j = 1, · · · , N. For each i, the scaled R-vector uZS may be viewed as representing wZ, the i-th </context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society of Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fraser</author>
<author>D Marcu</author>
</authors>
<title>Semi-supervised training for statistical word alignment.</title>
<date>2006</date>
<booktitle>In Proc. of COLING/ACL,</booktitle>
<pages>769--776</pages>
<contexts>
<context citStr="Fraser and Marcu, 2006" endWordPosition="186" position="1229" startWordPosition="183">lysis and show how they can be applied to improve translation performance. 1 Introduction Statistical word alignment models learn word associations between parallel sentences from statistics. Most models are trained from corpora in an unsupervised manner whose success is heavily dependent on the quality and quantity of the training data. It has been shown that human knowledge, in the form of a small amount of manually annotated parallel data to be used to seed or guide model training, can significantly improve word alignment F-measure and translation performance (Ittycheriah and Roukos, 2005; Fraser and Marcu, 2006). As formulated in the competitive linking algorithm (Melamed, 2000), the problem of word alignment can be regarded as a process of word linkage disambiguation, that is, choosing correct associations among all competing hypothesis. The more reasonable constraints are imposed on this process, the easier the task would become. For instance, the 1 most relaxed IBM Model-1, which assumes that any source word can be generated by any target word equally regardless of distance, can be improved by demanding a Markov process of alignments as in HMM-based models (Vogel et al., 1996), or implementing a d</context>
</contexts>
<marker>Fraser, Marcu, 2006</marker>
<rawString>A. Fraser and D. Marcu. 2006. Semi-supervised training for statistical word alignment. In Proc. of COLING/ACL, pages 769–776.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ittycheriah</author>
<author>S Roukos</author>
</authors>
<title>A maximum entropy word aligner for arabic-english machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of HLT/EMNLP,</booktitle>
<pages>89--96</pages>
<contexts>
<context citStr="Ittycheriah and Roukos, 2005" endWordPosition="182" position="1204" startWordPosition="179"> bilingual latent semantic analysis and show how they can be applied to improve translation performance. 1 Introduction Statistical word alignment models learn word associations between parallel sentences from statistics. Most models are trained from corpora in an unsupervised manner whose success is heavily dependent on the quality and quantity of the training data. It has been shown that human knowledge, in the form of a small amount of manually annotated parallel data to be used to seed or guide model training, can significantly improve word alignment F-measure and translation performance (Ittycheriah and Roukos, 2005; Fraser and Marcu, 2006). As formulated in the competitive linking algorithm (Melamed, 2000), the problem of word alignment can be regarded as a process of word linkage disambiguation, that is, choosing correct associations among all competing hypothesis. The more reasonable constraints are imposed on this process, the easier the task would become. For instance, the 1 most relaxed IBM Model-1, which assumes that any source word can be generated by any target word equally regardless of distance, can be improved by demanding a Markov process of alignments as in HMM-based models (Vogel et al., 1</context>
<context citStr="Ittycheriah and Roukos (2005)" endWordPosition="4265" position="25312" startWordPosition="4262">LSa_1(f,e) esophagus mrM 0.6424 mAl 0.1819 tk 0.2897 your mrM 0.6319 mAl 0.4930 tk 0.9672 “mrM”. 4 Related Work Heuristics based on co-occurrence analysis, such as point-wise mutual information or Dice coefficients , have been shown to be indicative for word alignments (Zhang and Vogel, 2005; Melamed, 2000). The framework presented in this paper demonstrates the possibility of taking heuristics as constraints guiding statistical generative word alignment model training. Their effectiveness can be expected especially when data sparseness is severe. Discriminative word alignment models, such as Ittycheriah and Roukos (2005); Moore (2005); Blunsom and Cohn (2006), have received great amount of study recently. They have proven that linguistic knowledge is useful in modeling word alignments under log-linear distributions as morphological, semantic or syntactic features. Our framework proposes to exploit these features differently by taking them as soft constraints of translation lexicon under a generative model. While word alignments can help identifying semantic relations (van der Plas and Tiedemann, 2006), we proceed in the reverse direction. We investigate the impact of semantic constraints on statistical word a</context>
</contexts>
<marker>Ittycheriah, Roukos, 2005</marker>
<rawString>A. Ittycheriah and S. Roukos. 2005. A maximum entropy word aligner for arabic-english machine translation. In Proc. of HLT/EMNLP, pages 89–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<contexts>
<context citStr="Koehn et al., 2003" endWordPosition="3190" position="18713" startWordPosition="3187">, and derive two sets of Viterbi alignments. By combining word alignments in two directions using heuristics (Och and Ney, 2003), a single set of static word alignments is then formed. All phrase pairs which respect to the word alignment boundary constraint are identified and pooled to build phrase translation tables with the Maximum Likelihood criterion. We prune phrase translation entries by their probabilities. The maximum number of tokens in Arabic phrases is set to 5 for all conditions. Our decoder is a phrase-based multi-stack imple5 mentation of the log-linear model similar to Pharaoh (Koehn et al., 2003). Like other log-linear model based decoders, active features in our translation engine include translation models in two directions, lexicon weights in two directions, language model, distortion model, and sentence length penalty. These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method (Och and Ney, 2002). The language model is a statistical trigram model estimated with Modified Kneser-Ney smoothing (Chen and Goodman, 1996) using all English sentences in the parallel training data. We measure translation performance by the BLEU s</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. Och, and D. Marcu. 2003. Statistical phrase-based translation. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LDC</author>
</authors>
<title>Buckwalter Arabic Morphological Analyzer Version 1.0. LDC Catalog Number LDC2002L49.</title>
<date>2002</date>
<contexts>
<context citStr="LDC, 2002" endWordPosition="3006" position="17595" startWordPosition="3005">K in Arabic side. Data sparseness severely challenges word alignment model and consequently automatic phrase translation induction. There are 42K singletons in Arabic vocabulary, and 14K Arabic words with occurrence of twice each in the corpus. Since Arabic is a morphologically rich language where affixes are attached to stem words to indicate gender, tense, case and etc, in order to reduce vocabulary size and address out-of-vocabulary words, we split Arabic words into affix and root according to a rule-based segmentation scheme (Xiang et al., 2006) with the help from the Buckwalter analyzer (LDC, 2002) output. This reduces the size of Arabic vocabulary to 52K. Our test data consists of 1294 sentence pairs. They are split into two parts: half of them is used as the development set, on which training parameters and decoding feature weights are tuned, the other half is for test. 3.1 Training and Translation Setup Starting from the collection of parallel training sentences, we train word alignment models in two translation directions, from English to Iraqi Arabic and from Iraqi Arabic to English, and derive two sets of Viterbi alignments. By combining word alignments in two directions using heu</context>
</contexts>
<marker>LDC, 2002</marker>
<rawString>LDC, 2002. Buckwalter Arabic Morphological Analyzer Version 1.0. LDC Catalog Number LDC2002L49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>B Taskar</author>
<author>D Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proc. of HLT/NAACL,</booktitle>
<pages>104--111</pages>
<contexts>
<context citStr="Liang et al., 2006" endWordPosition="3387" position="19982" startWordPosition="3384"> Rate (TER) (Snover et al., 2006) with one reference for each hypothesis. Word alignment models trained with different constraints are compared to show their effects on the resulting phrase translation tables and the final translation performance. 3.2 Translation Results Our baseline word alignment model is the word-toword Hidden Markov Model (Vogel et al., 1996). Basic models in two translation directions are trained simultaneously where statistics of two directions are shared to learn symmetric translation lexicon and word alignments with high precision motivated by (Zens et al., 2004) and (Liang et al., 2006). The baseline translation results (BLEU and TER) on the dev and test set are presented in the line “HMM” of Table 1. We also compare with results of IBM Model-4 word alignments implemented in GIZA++ toolkit (Och and Ney, 2003). We study and compare two types of constraint and see how they affect word alignments and translation output. One is based on the entropy principle as described in Section 2.1, where α is set to 0.9; The other is based on bilingual latent semantic analysis. For the simple bag-of-word bilingual LSA as described in Section 2.2.1, after SVD on the sparse matrix using the t</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>P. Liang, B. Taskar, and D. Klein. 2006. Alignment by agreement. In Proc. of HLT/NAACL, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Ma</author>
<author>K Kanzaki</author>
<author>Y Zhang</author>
<author>M Murata</author>
<author>H Isahara</author>
</authors>
<title>Self-organizing semantic maps and its application to word alignment in japanese-chinese parallel corpora.</title>
<date>2004</date>
<journal>Neural Netw.,</journal>
<pages>17--8</pages>
<contexts>
<context citStr="Ma et al., 2004" endWordPosition="4368" position="25968" startWordPosition="4365"> have received great amount of study recently. They have proven that linguistic knowledge is useful in modeling word alignments under log-linear distributions as morphological, semantic or syntactic features. Our framework proposes to exploit these features differently by taking them as soft constraints of translation lexicon under a generative model. While word alignments can help identifying semantic relations (van der Plas and Tiedemann, 2006), we proceed in the reverse direction. We investigate the impact of semantic constraints on statistical word alignment models as prior knowledge. In (Ma et al., 2004), bilingual semantic maps are constructed to guide word alignment. The framework we proposed seamlessly integrates derived semantic similarities into a statistical word alignment model. And we extended monolingual latent semantic analysis in bilingual applications. Toutanova et al. (2002) augmented bilingual sentence pairs with part-of-speech tags as linguistic constraints for HMM-based word alignments. The constraints between tags are automatically learned in a parallel generative procedure along with lexgloss (in) (esophagus) (ownership) (yours) bAl_ mrM mAl _tk HMM in your esophagus bAl_ mr</context>
</contexts>
<marker>Ma, Kanzaki, Zhang, Murata, Isahara, 2004</marker>
<rawString>Q. Ma, K. Kanzaki, Y. Zhang, M. Murata, and H. Isahara. 2004. Self-organizing semantic maps and its application to word alignment in japanese-chinese parallel corpora. Neural Netw., 17(8-9):1241–1253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melamed</author>
</authors>
<title>Models of translational equivalence among words.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>2</issue>
<contexts>
<context citStr="Melamed, 2000" endWordPosition="196" position="1297" startWordPosition="195">Introduction Statistical word alignment models learn word associations between parallel sentences from statistics. Most models are trained from corpora in an unsupervised manner whose success is heavily dependent on the quality and quantity of the training data. It has been shown that human knowledge, in the form of a small amount of manually annotated parallel data to be used to seed or guide model training, can significantly improve word alignment F-measure and translation performance (Ittycheriah and Roukos, 2005; Fraser and Marcu, 2006). As formulated in the competitive linking algorithm (Melamed, 2000), the problem of word alignment can be regarded as a process of word linkage disambiguation, that is, choosing correct associations among all competing hypothesis. The more reasonable constraints are imposed on this process, the easier the task would become. For instance, the 1 most relaxed IBM Model-1, which assumes that any source word can be generated by any target word equally regardless of distance, can be improved by demanding a Markov process of alignments as in HMM-based models (Vogel et al., 1996), or implementing a distribution of number of target words linked to a source word as in </context>
<context citStr="Melamed, 2000" endWordPosition="4222" position="24991" startWordPosition="4221">ears only once in the corpus. Some of the word pair constraints are listed in Table 2. The example demos that due to reasonable constraints placed in word alignment training, the link to “ tK” is corrected and consequently we have accurate word translation for the Arabic singleton English e Arabic f ConBiLSa_1(f,e) esophagus mrM 0.6424 mAl 0.1819 tk 0.2897 your mrM 0.6319 mAl 0.4930 tk 0.9672 “mrM”. 4 Related Work Heuristics based on co-occurrence analysis, such as point-wise mutual information or Dice coefficients , have been shown to be indicative for word alignments (Zhang and Vogel, 2005; Melamed, 2000). The framework presented in this paper demonstrates the possibility of taking heuristics as constraints guiding statistical generative word alignment model training. Their effectiveness can be expected especially when data sparseness is severe. Discriminative word alignment models, such as Ittycheriah and Roukos (2005); Moore (2005); Blunsom and Cohn (2006), have received great amount of study recently. They have proven that linguistic knowledge is useful in modeling word alignments under log-linear distributions as morphological, semantic or syntactic features. Our framework proposes to expl</context>
</contexts>
<marker>Melamed, 2000</marker>
<rawString>I. Dan. Melamed. 2000. Models of translational equivalence among words. Computational Linguistics, 26(2):221–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Moore</author>
</authors>
<title>A discriminative framework for bilingual word alignment.</title>
<date>2005</date>
<booktitle>In Proc. of HLT/EMNLP,</booktitle>
<pages>81--88</pages>
<contexts>
<context citStr="Moore (2005)" endWordPosition="4267" position="25326" startWordPosition="4266"> mAl 0.1819 tk 0.2897 your mrM 0.6319 mAl 0.4930 tk 0.9672 “mrM”. 4 Related Work Heuristics based on co-occurrence analysis, such as point-wise mutual information or Dice coefficients , have been shown to be indicative for word alignments (Zhang and Vogel, 2005; Melamed, 2000). The framework presented in this paper demonstrates the possibility of taking heuristics as constraints guiding statistical generative word alignment model training. Their effectiveness can be expected especially when data sparseness is severe. Discriminative word alignment models, such as Ittycheriah and Roukos (2005); Moore (2005); Blunsom and Cohn (2006), have received great amount of study recently. They have proven that linguistic knowledge is useful in modeling word alignments under log-linear distributions as morphological, semantic or syntactic features. Our framework proposes to exploit these features differently by taking them as soft constraints of translation lexicon under a generative model. While word alignments can help identifying semantic relations (van der Plas and Tiedemann, 2006), we proceed in the reverse direction. We investigate the impact of semantic constraints on statistical word alignment model</context>
</contexts>
<marker>Moore, 2005</marker>
<rawString>R. C. Moore. 2005. A discriminative framework for bilingual word alignment. In Proc. of HLT/EMNLP, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>295--302</pages>
<contexts>
<context citStr="Och and Ney, 2002" endWordPosition="3245" position="19084" startWordPosition="3242">e translation entries by their probabilities. The maximum number of tokens in Arabic phrases is set to 5 for all conditions. Our decoder is a phrase-based multi-stack imple5 mentation of the log-linear model similar to Pharaoh (Koehn et al., 2003). Like other log-linear model based decoders, active features in our translation engine include translation models in two directions, lexicon weights in two directions, language model, distortion model, and sentence length penalty. These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method (Och and Ney, 2002). The language model is a statistical trigram model estimated with Modified Kneser-Ney smoothing (Chen and Goodman, 1996) using all English sentences in the parallel training data. We measure translation performance by the BLEU score (Papineni et al., 2002) and Translation Error Rate (TER) (Snover et al., 2006) with one reference for each hypothesis. Word alignment models trained with different constraints are compared to show their effects on the resulting phrase translation tables and the final translation performance. 3.2 Translation Results Our baseline word alignment model is the word-tow</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>F. J. Och and H. Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proc. of ACL, pages 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context citStr="Och and Ney, 2003" endWordPosition="3111" position="18222" startWordPosition="3108">This reduces the size of Arabic vocabulary to 52K. Our test data consists of 1294 sentence pairs. They are split into two parts: half of them is used as the development set, on which training parameters and decoding feature weights are tuned, the other half is for test. 3.1 Training and Translation Setup Starting from the collection of parallel training sentences, we train word alignment models in two translation directions, from English to Iraqi Arabic and from Iraqi Arabic to English, and derive two sets of Viterbi alignments. By combining word alignments in two directions using heuristics (Och and Ney, 2003), a single set of static word alignments is then formed. All phrase pairs which respect to the word alignment boundary constraint are identified and pooled to build phrase translation tables with the Maximum Likelihood criterion. We prune phrase translation entries by their probabilities. The maximum number of tokens in Arabic phrases is set to 5 for all conditions. Our decoder is a phrase-based multi-stack imple5 mentation of the log-linear model similar to Pharaoh (Koehn et al., 2003). Like other log-linear model based decoders, active features in our translation engine include translation m</context>
<context citStr="Och and Ney, 2003" endWordPosition="3427" position="20209" startWordPosition="3424">lation performance. 3.2 Translation Results Our baseline word alignment model is the word-toword Hidden Markov Model (Vogel et al., 1996). Basic models in two translation directions are trained simultaneously where statistics of two directions are shared to learn symmetric translation lexicon and word alignments with high precision motivated by (Zens et al., 2004) and (Liang et al., 2006). The baseline translation results (BLEU and TER) on the dev and test set are presented in the line “HMM” of Table 1. We also compare with results of IBM Model-4 word alignments implemented in GIZA++ toolkit (Och and Ney, 2003). We study and compare two types of constraint and see how they affect word alignments and translation output. One is based on the entropy principle as described in Section 2.1, where α is set to 0.9; The other is based on bilingual latent semantic analysis. For the simple bag-of-word bilingual LSA as described in Section 2.2.1, after SVD on the sparse matrix using the toolkit SVDPACK (Berry et al., 1993), all source and target words are projected into a lowdimensional (R = 88) LSA-space. Word pair semantic constrains are calculated based on their similarity as in Equ. 3 before word alignment </context>
<context citStr="Och and Ney, 2003" endWordPosition="3859" position="22747" startWordPosition="3856">tric mean of ConBZLSA_1(f, e) and ConBZLSA_2(f, e), and find out that BLEU score can be improved a little bit further on both sets as the line “Mix” shows. We notice that the relatively simpler HMM model can perform comparable or better than the sophisticated Model-4 when proper constraints are active in guiding word alignment model training. We also try to put constraints in Model-4. As the Equation 1 implies, when a word-to-word generative probability is needed, one should multiply corresponding lexicon entry in the t-table with the word pair constraint. We simply modify the GIZA++ toolkit (Och and Ney, 2003) by always weighting lexicon probabilities with soft constraints during iterative model training, and obtain 0.7% TER reduction on both sets and 0.4% BLEU improvement on the test set. 3.3 Analysis To understand how prior knowledge encoded as soft constraints plays a role in guiding word alignment training, we compare statistics of different word alignment models. We find that our baseline HMM 6 Table 1: Translation Results with different word alignments. Alignments BLEU TER dev test dev test Model-4 0.310 0.296 0.528 0.530 +Mix 0.306 0.300 0.521 0.523 HMM 0.289 0.288 0.543 0.542 +Entropy 0.289</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context citStr="Papineni et al., 2002" endWordPosition="3285" position="19341" startWordPosition="3282"> other log-linear model based decoders, active features in our translation engine include translation models in two directions, lexicon weights in two directions, language model, distortion model, and sentence length penalty. These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method (Och and Ney, 2002). The language model is a statistical trigram model estimated with Modified Kneser-Ney smoothing (Chen and Goodman, 1996) using all English sentences in the parallel training data. We measure translation performance by the BLEU score (Papineni et al., 2002) and Translation Error Rate (TER) (Snover et al., 2006) with one reference for each hypothesis. Word alignment models trained with different constraints are compared to show their effects on the resulting phrase translation tables and the final translation performance. 3.2 Translation Results Our baseline word alignment model is the word-toword Hidden Markov Model (Vogel et al., 1996). Basic models in two translation directions are trained simultaneously where statistics of two directions are shared to learn symmetric translation lexicon and word alignments with high precision motivated by (Ze</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proc. ofACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>L Micciulla</author>
<author>J Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proc. of AMTA.</booktitle>
<contexts>
<context citStr="Snover et al., 2006" endWordPosition="3294" position="19396" startWordPosition="3291">n our translation engine include translation models in two directions, lexicon weights in two directions, language model, distortion model, and sentence length penalty. These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method (Och and Ney, 2002). The language model is a statistical trigram model estimated with Modified Kneser-Ney smoothing (Chen and Goodman, 1996) using all English sentences in the parallel training data. We measure translation performance by the BLEU score (Papineni et al., 2002) and Translation Error Rate (TER) (Snover et al., 2006) with one reference for each hypothesis. Word alignment models trained with different constraints are compared to show their effects on the resulting phrase translation tables and the final translation performance. 3.2 Translation Results Our baseline word alignment model is the word-toword Hidden Markov Model (Vogel et al., 1996). Basic models in two translation directions are trained simultaneously where statistics of two directions are shared to learn symmetric translation lexicon and word alignments with high precision motivated by (Zens et al., 2004) and (Liang et al., 2006). The baseline</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proc. of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>H T Ilhan</author>
<author>C Manning</author>
</authors>
<title>Extentions to HMM-based statistical word alignment models.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context citStr="Toutanova et al. (2002)" endWordPosition="4409" position="26257" startWordPosition="4406">em as soft constraints of translation lexicon under a generative model. While word alignments can help identifying semantic relations (van der Plas and Tiedemann, 2006), we proceed in the reverse direction. We investigate the impact of semantic constraints on statistical word alignment models as prior knowledge. In (Ma et al., 2004), bilingual semantic maps are constructed to guide word alignment. The framework we proposed seamlessly integrates derived semantic similarities into a statistical word alignment model. And we extended monolingual latent semantic analysis in bilingual applications. Toutanova et al. (2002) augmented bilingual sentence pairs with part-of-speech tags as linguistic constraints for HMM-based word alignments. The constraints between tags are automatically learned in a parallel generative procedure along with lexgloss (in) (esophagus) (ownership) (yours) bAl_ mrM mAl _tk HMM in your esophagus bAl_ mrM mAl _tk in your esophagus bAl_ mrM mAl _tk in your esophagus Table 2: Word pair constraint values 7 icon. We have introduced hidden tags between a word pair to specialize their soft constraints, which serve as prior knowledge that will be used in guiding word alignment model training. C</context>
</contexts>
<marker>Toutanova, Ilhan, Manning, 2002</marker>
<rawString>K. Toutanova, H. T. Ilhan, and C. Manning. 2002. Extentions to HMM-based statistical word alignment models. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lonneke van der Plas</author>
<author>J¨org Tiedemann</author>
</authors>
<title>Finding synonyms using automatic word alignment and measures of distributional similarity.</title>
<date>2006</date>
<booktitle>In Proc. of the COLING/ACL 2006 Main Conference Poster Sessions,</booktitle>
<pages>866--873</pages>
<marker>van der Plas, Tiedemann, 2006</marker>
<rawString>Lonneke van der Plas and J¨org Tiedemann. 2006. Finding synonyms using automatic word alignment and measures of distributional similarity. In Proc. of the COLING/ACL 2006 Main Conference Poster Sessions, pages 866–873.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>H Ney</author>
<author>C Tillmann</author>
</authors>
<title>HMM based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context citStr="Vogel et al., 1996" endWordPosition="281" position="1808" startWordPosition="278">nd Roukos, 2005; Fraser and Marcu, 2006). As formulated in the competitive linking algorithm (Melamed, 2000), the problem of word alignment can be regarded as a process of word linkage disambiguation, that is, choosing correct associations among all competing hypothesis. The more reasonable constraints are imposed on this process, the easier the task would become. For instance, the 1 most relaxed IBM Model-1, which assumes that any source word can be generated by any target word equally regardless of distance, can be improved by demanding a Markov process of alignments as in HMM-based models (Vogel et al., 1996), or implementing a distribution of number of target words linked to a source word as in IBM fertility-based models (Brown et al., 1993). Following the path, we shall put more constraints on word alignment models and investigate ways of implementing them in a statistical framework. We have seen examples showing that names tend to align to names and function words are likely to be linked to function words. These observations are independent of language and can be understood by common sense. Moreover, there are other linguistically motivated constraints. For instance, words aligned to each other</context>
<context citStr="Vogel et al., 1996" endWordPosition="565" position="3643" startWordPosition="562">lingual latent semantic analysis. We investigate their impact on word alignments and show their effectiveness in improving translation performance. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1–8, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics 2 Constrained Word Alignment Models The framework that we propose to incorporate statistical constraints into word alignment models is generic. It can be applied to complicated models such IBM Model-4 (Brown et al., 1993). We shall take HMM-based word alignment model (Vogel et al., 1996) as an example and follow the notation of (Brown et al., 1993). Let e = e1 represent a source string and f = fm a target string. The random variable a = am specifies the indices of source words that target words are aligned to. In an HMM-based word alignment model, source words are treated as Markov states while target words are observations that are generated when jumping to states: M P(a,f|e) = H P(aj|aj−1, e)t(fj|eaj) j=1 Notice that a target word f is generated from a source state e by a simple lookup of the translation table, a.k.a., t-table t(f|e), as depicted in (A) of Figure 1. To inco</context>
<context citStr="Vogel et al., 1996" endWordPosition="3346" position="19728" startWordPosition="3343"> statistical trigram model estimated with Modified Kneser-Ney smoothing (Chen and Goodman, 1996) using all English sentences in the parallel training data. We measure translation performance by the BLEU score (Papineni et al., 2002) and Translation Error Rate (TER) (Snover et al., 2006) with one reference for each hypothesis. Word alignment models trained with different constraints are compared to show their effects on the resulting phrase translation tables and the final translation performance. 3.2 Translation Results Our baseline word alignment model is the word-toword Hidden Markov Model (Vogel et al., 1996). Basic models in two translation directions are trained simultaneously where statistics of two directions are shared to learn symmetric translation lexicon and word alignments with high precision motivated by (Zens et al., 2004) and (Liang et al., 2006). The baseline translation results (BLEU and TER) on the dev and test set are presented in the line “HMM” of Table 1. We also compare with results of IBM Model-4 word alignments implemented in GIZA++ toolkit (Och and Ney, 2003). We study and compare two types of constraint and see how they affect word alignments and translation output. One is b</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>S. Vogel, H. Ney, and C. Tillmann. 1996. HMM based word alignment in statistical translation. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Xiang</author>
<author>K Nguyen</author>
<author>L Nguyen</author>
<author>R Schwartz</author>
<author>J Makhoul</author>
</authors>
<title>Morphological decomposition for arabic broadcast news transcription.</title>
<date>2006</date>
<booktitle>In Proc. of ICASSP,</booktitle>
<pages>1089--1092</pages>
<contexts>
<context citStr="Xiang et al., 2006" endWordPosition="2996" position="17540" startWordPosition="2993"> 6 words. There are 25K entries in the English vocabulary and 90K in Arabic side. Data sparseness severely challenges word alignment model and consequently automatic phrase translation induction. There are 42K singletons in Arabic vocabulary, and 14K Arabic words with occurrence of twice each in the corpus. Since Arabic is a morphologically rich language where affixes are attached to stem words to indicate gender, tense, case and etc, in order to reduce vocabulary size and address out-of-vocabulary words, we split Arabic words into affix and root according to a rule-based segmentation scheme (Xiang et al., 2006) with the help from the Buckwalter analyzer (LDC, 2002) output. This reduces the size of Arabic vocabulary to 52K. Our test data consists of 1294 sentence pairs. They are split into two parts: half of them is used as the development set, on which training parameters and decoding feature weights are tuned, the other half is for test. 3.1 Training and Translation Setup Starting from the collection of parallel training sentences, we train word alignment models in two translation directions, from English to Iraqi Arabic and from Iraqi Arabic to English, and derive two sets of Viterbi alignments. B</context>
</contexts>
<marker>Xiang, Nguyen, Nguyen, Schwartz, Makhoul, 2006</marker>
<rawString>B. Xiang, K. Nguyen, L. Nguyen, R. Schwartz, and J. Makhoul. 2006. Morphological decomposition for arabic broadcast news transcription. In Proc. of ICASSP, pages 1089–1092.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>E Matusov</author>
<author>H Ney</author>
</authors>
<title>Improved word alignment using a symmetric lexicon model.</title>
<date>2004</date>
<booktitle>In Proc. of COLING,</booktitle>
<pages>36--42</pages>
<contexts>
<context citStr="Zens et al., 2004" endWordPosition="3382" position="19957" startWordPosition="3379">2) and Translation Error Rate (TER) (Snover et al., 2006) with one reference for each hypothesis. Word alignment models trained with different constraints are compared to show their effects on the resulting phrase translation tables and the final translation performance. 3.2 Translation Results Our baseline word alignment model is the word-toword Hidden Markov Model (Vogel et al., 1996). Basic models in two translation directions are trained simultaneously where statistics of two directions are shared to learn symmetric translation lexicon and word alignments with high precision motivated by (Zens et al., 2004) and (Liang et al., 2006). The baseline translation results (BLEU and TER) on the dev and test set are presented in the line “HMM” of Table 1. We also compare with results of IBM Model-4 word alignments implemented in GIZA++ toolkit (Och and Ney, 2003). We study and compare two types of constraint and see how they affect word alignments and translation output. One is based on the entropy principle as described in Section 2.1, where α is set to 0.9; The other is based on bilingual latent semantic analysis. For the simple bag-of-word bilingual LSA as described in Section 2.2.1, after SVD on the </context>
</contexts>
<marker>Zens, Matusov, Ney, 2004</marker>
<rawString>R. Zens, E. Matusov, and H. Ney. 2004. Improved word alignment using a symmetric lexicon model. In Proc. of COLING, pages 36–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Vogel</author>
</authors>
<title>Competitive grouping in integrated phrase segmentation and alignment model.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL Workshop on Building and Using Parallel Texts,</booktitle>
<pages>159--162</pages>
<contexts>
<context citStr="Zhang and Vogel, 2005" endWordPosition="4220" position="24975" startWordPosition="4217">” (means esophagus) appears only once in the corpus. Some of the word pair constraints are listed in Table 2. The example demos that due to reasonable constraints placed in word alignment training, the link to “ tK” is corrected and consequently we have accurate word translation for the Arabic singleton English e Arabic f ConBiLSa_1(f,e) esophagus mrM 0.6424 mAl 0.1819 tk 0.2897 your mrM 0.6319 mAl 0.4930 tk 0.9672 “mrM”. 4 Related Work Heuristics based on co-occurrence analysis, such as point-wise mutual information or Dice coefficients , have been shown to be indicative for word alignments (Zhang and Vogel, 2005; Melamed, 2000). The framework presented in this paper demonstrates the possibility of taking heuristics as constraints guiding statistical generative word alignment model training. Their effectiveness can be expected especially when data sparseness is severe. Discriminative word alignment models, such as Ittycheriah and Roukos (2005); Moore (2005); Blunsom and Cohn (2006), have received great amount of study recently. They have proven that linguistic knowledge is useful in modeling word alignments under log-linear distributions as morphological, semantic or syntactic features. Our framework </context>
</contexts>
<marker>Zhang, Vogel, 2005</marker>
<rawString>Y. Zhang and S. Vogel. 2005. Competitive grouping in integrated phrase segmentation and alignment model. In Proc. of the ACL Workshop on Building and Using Parallel Texts, pages 159–162.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>