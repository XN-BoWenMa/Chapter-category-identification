<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant confidence="0.000538" no="0">
<title confidence="0.998485">
An Improved Parser for Data-Oriented Lexical-Functional Analysis
</title>
<author confidence="0.995337">
Rens Bod
</author>
<affiliation confidence="0.989862">
Informatics Research Institute, University of Leeds, Leeds LS2 9JT, UK, &amp;
Institute for Logic, Language and Computation, University of Amsterdam
</affiliation>
<email confidence="0.990804">
rens@scs.leeds.ac.uk
</email>
<sectionHeader confidence="0.993721" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998029923076923">We present an LFG-DOP parser which uses fragments from LFG-annotated sentences to parse new sentences. Experiments with the Verbmobil and Homecentre corpora show that (1) Viterbi n best search performs about 100 times faster than Monte Carlo search while both achieve the same accuracy; (2) the DOP hypothesis which states that parse accuracy increases with increasing fragment size is confirmed for LFG-DOP; (3) LFGDOP's relative frequency estimator performs worse than a discounted frequency estimator; and (4) LFG-DOP significantly outperforms TreeDOP if evaluated on tree structures only.</bodyText>
<sectionHeader confidence="0.998747" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999142648148148">Data-Oriented Parsing (DOP) models learn how to provide linguistic representations for an unlimited set of utterances by generalizing from a given corpus of properly annotated exemplars. They operate by decomposing the given representations into (arbitrarily large) fragments and recomposing those pieces to analyze new utterances. A probability model is used to choose from the collection of different fragments of different sizes those that make up the most appropriate analysis of an utterance. DOP models have been shown to achieve state-of-the-art parsing performance on benchmarks such as the Wall Street Journal corpus (see Bod 2000a). The original DOP model in Bod (1993) was based on utterance analyses represented as surface trees, and is equivalent to a Stochastic Tree-Substitution Grammar. But the model has also been applied to several other grammatical frameworks, e.g. Tree-Insertion Grammar (Hoogweg 2000), Tree-Adjoining Grammar (Neumann 1998), Lexical-Functional Grammar (Bod &amp; Kaplan 1998; Cormons 1999), Head-driven Phrase Structure Grammar (Neumann &amp; Flickinger 1999), and Montague Grammar (Bonnema et al. 1997; Bod 1999). Most probability models for DOP use the relative frequency estimator to estimate fragment probabilities, although Bod (2000b) trains fragment probabilities by a maximum likelihood reestimation procedure belonging to the class of expectation-maximization algorithms. The DOP model has also been tested as a model for human sentence processing (Bod 2000d). This paper presents ongoing work on DOP models for Lexical-Functional Grammar representations, known as LFG-DOP (Bod &amp; Kaplan 1998). We develop a parser which uses fragments from LFG-annotated sentences to parse new sentences, and we derive some experimental properties of LFG-DOP on two LFG-annotated corpora: the Verbmobil and Homecentre corpus. The experiments show that the DOP hypothesis, which states that there is an increase in parse accuracy if larger fragments are taken into account (Bod 1998), is confirmed for LFG-DOP. We report on an improved search technique for estimating the most probable analysis. While a Monte Carlo search converges provably to the most probable parse, a Viterbi n best search performs as well as Monte Carlo while its processing time is two orders of magnitude faster. We also show that LFG-DOP outperforms TreeDOP if evaluated on tree structures only.</bodyText>
<sectionHeader confidence="0.827691" genericHeader="method">
2 Summary of LFG-DOP
</sectionHeader>
<bodyText confidence="0.998122">In accordance with Bod (1998), a particular DOP model is described by • a definition of a well-formed representation for utterance analyses, • a set of decomposition operations that divide a given utterance analysis into a set of fragments, • a set of composition operations by which such fragments may be recombined to derive an analysis of a new utterance, and • a definition of a probability model that indicates how the probability of a new utterance analysis is computed.</bodyText>
<bodyText confidence="0.999347333333333">In defining a DOP model for LFG representations, Bod &amp; Kaplan (1998) give the following settings for DOP's four parameters.</bodyText>
<subsectionHeader confidence="0.974036">
2.1 Representations
</subsectionHeader>
<bodyText confidence="0.999823">The representations used by LFG-DOP are directly taken from LFG: they consist of a cstructure, an f-structure and a mapping 0 between them. The following figure shows an example representation for Kim eats. (We leave out some features to keep the example simple.)</bodyText>
<figureCaption confidence="0.924083">
Figure 1
</figureCaption>
<bodyText confidence="0.999283928571429">Bod &amp; Kaplan also introduce the notion of accessibility which they later use for defining the decomposition operations of LFG-DOP: An f-structure unit f is 0-accessible from a node n iff either n is 0-linked to f (that is, f = 0(n) ) or f is contained within 0(n) (that is, there is a chain of attributes that leads from 0(n) to f). According to the LFG representation theory, cstructures and f-structures must satisfy certain formal well-formedness conditions. A cstructure/f-structure pair is a valid LFG representation only if it satisfies the Nonbranching Dominance, Uniqueness, Coherence and Completeness conditions (Kaplan &amp; Bresnan 1982).</bodyText>
<subsectionHeader confidence="0.996802">
2.2 Decomposition operations and Fragments
</subsectionHeader>
<bodyText confidence="0.973918375">The fragments for LFG-DOP consist of connected subtrees whose nodes are in 0correspondence with the correponding sub-units of f-structures. To give a precise definition of LFG-DOP fragments, it is convenient to recall the decomposition operations employed by the orginal DOP model which is also known as the &amp;quot;Tree-DOP&amp;quot; model (Bod 1993, 1998):</bodyText>
<listItem confidence="0.938024375">(1) Root: the Root operation selects any node of a tree to be the root of the new subtree and erases all nodes except the selected node and the nodes it dominates. (2) Frontier: the Frontier operation then chooses a set (possibly empty) of nodes in the new subtree different from its root and erases all subtrees dominated by the chosen nodes.</listItem>
<bodyText confidence="0.997893846153846">Bod &amp; Kaplan extend Tree-DOP's Root and Frontier operations so that they also apply to the nodes of the c-structure in LFG, while respecting the principles of c/f-structure correspondence. When a node is selected by the Root operation, all nodes outside of that node's subtree are erased, just as in Tree-DOP. Further, for LFG-DOP, all 0 links leaving the erased nodes are removed and all f-structure units that are not 0-accessible from the remaining nodes are erased. For example, if Root selects the NP in figure 1, then the f-structure corresponding to the S node is erased, giving figure 2 as a possible fragment:</bodyText>
<figureCaption confidence="0.818209">
Figure 2
</figureCaption>
<bodyText confidence="0.999875625">In addition the Root operation deletes from the remaining f-structure all semantic forms that are local to f-structures that correspond to erased cstructure nodes, and it thereby also maintains the fundamental two-way connection between words and meanings. Thus, if Root selects the VP node so that the NP is erased, the subject semantic form &amp;quot;Kim&amp;quot; is also deleted:</bodyText>
<figureCaption confidence="0.683171">
Figure 3
</figureCaption>
<figure confidence="0.84018275">
PRED 'Kim'
NUM❑ SG
SUBJ
TENSE PRES
PRED 'eat(SUBJ)'
Kim eats
S
NP VP
PRED 'Kim'
NUM❑ SG
NP
Kim
SUBJ NUM❑ SG
TENSE PRES
PRED 'eat(SUBJ)'
VP
</figure>
<page confidence="0.408352">
eats
</page>
<bodyText confidence="0.99973575">As with Tree-DOP, the Frontier operation then selects a set of frontier nodes and deletes all subtrees they dominate. Like Root, it also removes the 0 links of the deleted nodes and erases any semantic form that corresponds to any of those nodes. For instance, if the NP in figure 1 is selected as a frontier node, Frontier erases the predicate &amp;quot;Kim&amp;quot; from the fragment:</bodyText>
<figureCaption confidence="0.766639">
Figure 4
</figureCaption>
<bodyText confidence="0.999833083333333">Finally, Bod &amp; Kaplan present a third decomposition operation, Discard, defined to construct generalizations of the fragments supplied by Root and Frontier. Discard acts to delete combinations of attribute-value pairs subject to the following condition: Discard does not delete pairs whose values 0 -correspond to remaining c-structure nodes. According to Bod &amp; Kaplan (1998), Discard-generated fragments are needed to parse sentences that are &amp;quot;ungrammatical with respect to the corpus&amp;quot;, thus increasing the robustness of the model.</bodyText>
<subsectionHeader confidence="0.996758">
2.3 The composition operation
</subsectionHeader>
<bodyText confidence="0.999884583333333">In LFG-DOP the operation for combining fragments is carried out in two steps. First the cstructures are combined by leftmost substitution subject to the category-matching condition, as in Tree-DOP. This is followed by the recursive unification of the f-structures corresponding to the matching nodes. A derivation for an LFG-DOP representation R is a sequence of fragments the first of which is labeled with S and for which the iterative application of the composition operation produces R. For an illustration of the composition operation, see Bod &amp; Kaplan (1998).</bodyText>
<subsectionHeader confidence="0.99456">
2.4 Probability models
</subsectionHeader>
<bodyText confidence="0.9974548">As in Tree-DOP, an LFG-DOP representation R can typically be derived in many different ways. If each derivation D has a probability P(D), then the probability of deriving R is the sum of the individual derivation probabilities:</bodyText>
<listItem confidence="0.688697">(1) P(R) = ED derives R P(D)</listItem>
<bodyText confidence="0.9993416875">An LFG-DOP derivation is produced by a stochastic process which starts by randomly choosing a fragment whose c-structure is labeled with the initial category. At each subsequent step, a next fragment is chosen at random from among the fragments that can be composed with the current subanalysis. The chosen fragment is composed with the current subanalysis to produce a new one; the process stops when an analysis results with no non-terminal leaves. We will call the set of composable fragments at a certain step in the stochastic process the competition set at that step. Let CP(f  |CS) denote the probability of choosing a fragment f from a competition set CS containing f, then the probability of a derivation D = &lt;f1, f2 ... fk&gt; is</bodyText>
<listItem confidence="0.909014">(2) P(&lt;f1, f2 ... fk&gt;) = ni CP(fi  |CSi)</listItem>
<bodyText confidence="0.9335285">where the competition probability CP(f  |CS) is expressed in terms of fragment probabilities P(f):</bodyText>
<listItem confidence="0.994388">(3) CP(f  |CS) =</listItem>
<bodyText confidence="0.99982115">Bod &amp; Kaplan give three definitions of increasing complexity for the competition set: the first definition groups all fragments that only satisfy the Category-matching condition of the composition operation; the second definition groups all fragments which satisfy both Categorymatching and Uniqueness; and the third definition groups all fragments which satisfy Categorymatching, Uniqueness and Coherence. Bod &amp; Kaplan point out that the Completeness condition cannot be enforced at each step of the stochastic derivation process, and is a property of the final representation which can only be enforced by sampling valid representations from the output of the stochastic process. In this paper, we will only deal with the third definition of competition set, as it selects only those fragments at each derivation step that may finally result into a valid LFG representation, thus reducing the off-line validity checking to the Completeness condition.</bodyText>
<figure confidence="0.960809555555556">
S
SUBJ
NUMD SG
NP VP
eats
TENSE PRES
PRED 'eat(SUBJ)'
P(f)
Ef'tCS P(f')
</figure>
<bodyText confidence="0.999906777777778">Note that the computation of the competition probability in the above formulas still requires a definition for the fragment probability P(f). Bod and Kaplan define the probability of a fragment simply as its relative frequency in the bag of all fragments generated from the corpus, just as in most Tree-DOP models. We will refer to this fragment estimator as &amp;quot;simple relative frequency&amp;quot; or &amp;quot;simple RF&amp;quot;.</bodyText>
<listItem confidence="0.991118032258065">❑We will also use an alternative definition of fragment probability which is a refinement of simple RF. This alternative fragment probability definition distinguishes between fragments supplied by Root/Frontier and fragments supplied by Discard. We will treat the first type of fragments as seen events, and the second type of fragments as previously unseen events. We thus create two separate bags corresponding to two separate distributions: a bag with fragments generated by Root and Frontier, and a bag with fragments generated by Discard. We assign probability mass to the fragments of each bag by means of discounting: the relative frequencies of seen events are discounted and the gained probability mass is reserved for the bag of unseen events (cf. Ney et al. 1997). We accomplish this by a very simple estimator: the Turing-Good estimator (Good 1953) which computes the probability mass of unseen events as n1/N where n1 is the number of singleton events and N is the total number of seen events. This probability mass is assigned to the bag of Discard-generated fragments. The remaining mass (1 − n1/N) is assigned to the bag of Root/Frontier-generated fragments. The probability of each fragment is then computed as its relative frequency in its bag multiplied by the probability mass assigned to this bag. Let  |f  |denote the frequency of a fragment f, then its probability is given by: (4) P(f  |f is generated by Root/Frontier) =</listItem>
<equation confidence="0.992477">
(1 − n1/N)
Σf': f' is generated by Root/Frontier  |f'|
(5) P(f  |f is generated by Discard) =
 |f |
(n1/N)
Σf': f' is generated by Discard  |f'|
</equation>
<bodyText confidence="0.999408">We will refer to this fragment probability estimator as &amp;quot;discounted relative frequency&amp;quot; or &amp;quot;discounted RF&amp;quot;.</bodyText>
<sectionHeader confidence="0.796073" genericHeader="method">
4 Parsing with LFG-DOP
</sectionHeader>
<bodyText confidence="0.999868666666667">In his PhD-thesis, Cormons (1999) presents a parsing algorithm for LFG-DOP which is based on the Tree-DOP parsing technique described in Bod (1998). Cormons first converts LFGrepresentations into more compact indexed trees: each node in the c-structure is assigned an index which refers to the O-corresponding f-structure unit. For example, the representation in figure 1 is indexed as</bodyText>
<equation confidence="0.955303875">
(S.1 (NP.2 Kim.2)
(VP.1 eats.1))
where
1 --&gt; [ (SUBJ = 2)
(TENSE = PRES)
(PRED = eat(SUBJ)) ]
2 --&gt; [ (PRED = Kim)
(NUM = SG) ]
</equation>
<bodyText confidence="0.99411127027027">The indexed trees are then fragmented by applying the Tree-DOP decomposition operations described in section 2. Next, the LFG-DOP decomposition operations Root, Frontier and Discard are applied to the f-structure units that correspond to the indices in the c-structure subtrees. Having obtained the set of LFG-DOP fragments in this way, each test sentence is parsed by a bottom-up chart parser using initially the indexed subtrees only. Thus only the Category-matching condition is enforced during the chart-parsing process. The Uniqueness and Coherence conditions of the corresponding f-structure units are enforced during the disambiguation or chart decoding process. Disambiguation is accomplished by computing a large number of random derivations from the chart and by selecting the analysis which results most often from these derivations. This technique is known as &amp;quot;Monte Carlo disambiguation&amp;quot; and has been extensively described in the literature (e.g. Bod 1993, 1998; Chappelier &amp; Rajman 2000; Goodman 1998; Hoogweg 2000). Sampling a random derivation from the chart consists of choosing at random one of the fragments from the set of composable fragments at every labeled chart-entry (where the random choices at each chart-entry are based on the probabilities of the fragments). The derivations are sampled in a topdown, leftmost order so as to maintain the LFGDOP derivation order. Thus the competition sets of composable fragments are computed on the fly during the Monte Carlo sampling process by grouping the f-structure units that unify and that are coherent with the subderivation built so far. As mentioned in section 3, the Completeness condition can only be checked after the derivation process. Incomplete derivations are simply removed from the sampling distribution. After sampling a sufficiently large number of random derivations that satisfy the LFG validity requirements, the most probable analysis is estimated by the analysis which results most often from the sampled derivations. As a stop condition on the number of sampled derivations, we compute the probability of error, which is the probability that the analysis that is most frequently generated by the sampled derivations is not equal to the most probable analysis, and which is set to 0.05 (see Bod 1998). In order to rule out the possibility that the sampling process never stops, we use a maximum sample size of 10,000 derivations. While the Monte Carlo disambiguation technique converges provably to the most probable analysis, it is quite inefficient. It is possible to use an alternative, heuristic search based on Viterbi n best (we will not go into the PCFG-reduction technique presented in Goodman (1998) since that heuristic only works for TreeDOP and is beneficial only if all subtrees are taken into account and if the so-called &amp;quot;labeled recall parse&amp;quot; is computed). A Viterbi n best search for LFG-DOP estimates the most probable analysis by computing n most probable derivations, and by then summing up the probabilities of the valid derivations that produce the same analysis. The algorithm for computing n most probable derivations follows straightforwardly from the algorithm which computes the most probable derivation by means of Viterbi optimization (see e.g. Sima'an 1999).</bodyText>
<sectionHeader confidence="0.997364" genericHeader="evaluation and result">
5 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999944896551724">We derived some experimental properties of LFG-DOP by studying its behavior on the two LFG-annotated corpora that are currently available: the Verbmobil corpus and the Homecentre corpus. Both corpora were annotated at Xerox PARC. They contain packed LFGrepresentations (Maxwell &amp; Kaplan 1991) of the grammatical parses of each sentence together with an indication which of these parses is the correct one. For our experiments we only used the correct parses of each sentence resulting in 540 Verbmobil parses and 980 Homecentre parses. Each corpus was divided into a 90% training set and a 10% test set. This division was random except for one constraint: that all the words in the test set actually occurred in the training set. The sentences from the test set were parsed and disambiguated by means of the fragments from the training set. Due to memory limitations, we restricted the maximum depth of the indexed subtrees to 4. Because of the small size of the corpora we averaged our results on 10 different training/test set splits. Besides an exact match accuracy metric, we also used a more fine-grained score based on the well-known PARSEVAL metrics that evaluate phrase-structure trees (Black et al. 1991). The PARSEVAL metrics compare a proposed parse P with the corresponding correct treebank parse T as follows:</bodyText>
<equation confidence="0.9625925">
# correct constituents in P
Precision =
# constituents in P
# correct constituents in P
Recall =
# constituents in T
</equation>
<bodyText confidence="0.9991454">A constituent in P is correct if there exists a constituent in T of the same label that spans the same words and that f-corresponds to the same f-structure unit (see Bod 2000c for some illustrations of these metrics for LFG-DOP).</bodyText>
<subsectionHeader confidence="0.848536">
5.1 Comparing the two fragment estimators
</subsectionHeader>
<bodyText confidence="0.999333647058824">We were first interested in comparing the performance of the simple RF estimator against the discounted RF estimator. Furthermore, we want to study the contribution of generalized fragments to the parse accuracy. We therefore created for each training set two sets of fragments: one which contains all fragments (up to depth 4) and one which excludes the generalized fragments as generated by Discard. The exclusion of these Discard-generated fragments means that all probability mass goes to the fragments generated by Root and Frontier in which case the two estimators are equivalent. The following two tables present the results of our experiments where +Discard refers to the full set of fragments and −Discard refers to the fragment set without Discard-generated fragments.</bodyText>
<table confidence="0.9988325">
Estimator Exact Match Precision Recall
+Discard −Discard +Discard −Discard +Discard −Discard
Simple RF 1.1% 35.2% 13.8% 76.0% 11.5% 74.9%
Discounted RF 35.9% 35.2% 77.5% 76.0% 76.4% 74.9%
</table>
<tableCaption confidence="0.998138">
Table 1. Experimental results on the Verbmobil
</tableCaption>
<table confidence="0.993097">
Estimator Exact Match Precision Recall
+Discard −Discard +Discard −Discard +Discard −Discard
Simple RF 2.7% 37.9% 17.1% 77.8% 15.5% 77.2%
Discounted RF 38.4% 37.9% 80.0% 77.8% 78.6% 77.2%
</table>
<tableCaption confidence="0.999867">
Table 2. Experimental results on the Homecentre
</tableCaption>
<bodyText confidence="0.999960173913044">The tables show that the simple RF estimator scores extremely bad if all fragments are used: the exact match is only 1.1% on the Verbmobil corpus and 2.7% on the Homecentre corpus, whereas the discounted RF estimator scores respectively 35.9% and 38.4% on these corpora. Also the more fine-grained precision and recall scores obtained with the simple RF estimator are quite low: e.g. 13.8% and 11.5% on the Verbmobil corpus, where the discounted RF estimator obtains 77.5% and 76.4%. Interestingly, the accuracy of the simple RF estimator is much higher if Discard-generated fragments are excluded. This suggests that treating generalized fragments probabilistically in the same way as ungeneralized fragments is harmful. The tables also show that the inclusion of Discard-generated fragments leads only to a slight accuracy increase under the discounted RF estimator. Unfortunately, according to paired ttesting only the differences for the precision scores on the Homecentre corpus were statistically significant.</bodyText>
<subsectionHeader confidence="0.99941">
5.2 Comparing different fragment sizes
</subsectionHeader>
<bodyText confidence="0.998815363636363">We were also interested in the impact of fragment size on the parse accuracy. We therefore performed a series of experiments where the fragment set is restricted to fragments of a certain maximum depth (where the depth of a fragment is defined as the longest path from root to leaf of its c-structure unit). We used the same training/test set splits as in the previous experiments and used both ungeneralized and generalized fragments together with the discounted RF estimator.</bodyText>
<table confidence="0.9961664">
Fragment Depth Exact Match Precision Recall
1 30.6% 74.2% 72.2%
£2 34.1% 76.2% 74.5%
£3 35.6% 76.8% 75.9%
£4 35.9% 77.5% 76.4%
</table>
<tableCaption confidence="0.998721">
Table 3. Accuracies on the Verbmobil
</tableCaption>
<table confidence="0.9988692">
Fragment Depth Exact Match Precision Recall
1 31.3% 75.0% 71.5%
£2 36.3% 77.1% 74.7%
£3 37.8% 77.8% 76.1%
£4 38.4% 80.0% 78.6%
</table>
<tableCaption confidence="0.999839">
Table 4. Accuracies on the Homecentre
</tableCaption>
<bodyText confidence="0.999860875">Tables 3 and 4 show that there is a consistent increase in parse accuracy for all metrics if larger fragments are included, but that the increase itself decreases. This phenomenon is also known as the DOP hypothesis (Bod 1998), and has been confirmed for Tree-DOP on the ATIS, OVIS and Wall Street Journal treebanks (see Bod 1993, 1998, 1999, 2000a; Sima'an 1999; Bonnema et al. 1997; Hoogweg 2000). The current result thus extends the validity of the DOP hypothesis to LFG annotations. We do not yet know whether the accuracy continues to increase if even larger fragments are included (for Tree-DOP it has been shown that the accuracy decreases after a certain depth, probably due to overfitting -cf. Bonnema et al. 1997; Bod 2000a).</bodyText>
<subsectionHeader confidence="0.992652">
5.3 Comparing LFG-DOP to Tree-DOP
</subsectionHeader>
<bodyText confidence="0.9999806">In the following experiment, we are interested in the impact of functional structures on predicting the correct tree structures. We therefore removed all f-structure units from the fragments, thus yielding a Tree-DOP model, and compared the results against the full LFG-DOP model (using the discounted RF estimator and all fragments up to depth 4). We evaluated the parse accuracy on the tree structures only, using exact match together with the standard PARSEVAL measures. We used the same training/test set splits as in the previous experiments. The results indicate that LFG-DOP's functional structures help to improve the parse accuracy of tree structures. In other words, LFG-DOP outperforms Tree-DOP if evaluated on tree structures only. According to paired t-tests all differences in accuracy were statistically significant. This result is promising since TreeDOP has been shown to obtain state-of-the-art performance on the Wall Street Journal corpus (see Bod 2000a).</bodyText>
<subsectionHeader confidence="0.950252">
5.4 Comparing Viterbi n best to Monte Carlo
</subsectionHeader>
<bodyText confidence="0.998733666666667">Finally, we were interested in comparing an alternative, more efficient search method for estimating the most probable analysis. In the following set of experiments we use a Viterbi n best search heuristic (as explained in section 4), and let n range from 1 to 10,000 derivations. We also compute the results obtained by Monte Carlo for the same number of derivations. We used the same training/test set splits as in the previous experiments and used both ungeneralized and generalized fragments up to depth 4 together with the discounted RF estimator.</bodyText>
<table confidence="0.918653">
Nr. of derivations Viterbi n best Monte Carlo
1 74.8% 20.1%
10 75.3% 36.7%
100 77.5% 67.0%
1,000 77.5% 77.1%
10,000 77.5% 77.5%
</table>
<tableCaption confidence="0.9989">
Table 7. Precision on the Verbmobil
</tableCaption>
<table confidence="0.991884">
Model Exact Match Precision Recall
Tree-DOP 46.6% 88.9% 86.7%
LFG-DOP 50.8% 90.3% 88.4%
</table>
<tableCaption confidence="0.996278">
Table 5. Tree accuracy on the Verbmobil
</tableCaption>
<table confidence="0.990645333333333">
Nr. of derivations Viterbi n best Monte Carlo
1 75.6% 25.6%
10 76.2% 44.3%
100 79.1% 74.6%
1,000 79.8% 79.1%
10,000 79.8% 80.0%
Model Exact Match Precision Recall Table 8. Precision on the Homecentre
Tree-DOP 49.0% 93.4% 92.1%
LFG-DOP 53.2% 95.8% 94.7%
</table>
<tableCaption confidence="0.985813">
Table 6. Tree accuracy on the Homecentre
</tableCaption>
<bodyText confidence="0.99973775">The tables show that Viterbi n best already achieves a maximum accuracy at 100 derivations (at least on the Verbmobil corpus) while Monte Carlo needs a much larger number of derivations to obtain these results. On the Homecentre corpus, Monte Carlo slightly outperforms Viterbi n best at 10,000 derivations, but these differences are not statistically significant. Also remarkable are the relatively high results obtained with Viterbi n best if only one derivation is used. This score corresponds to the analysis generated by the most probable (valid) derivation. Thus Viterbi n best is a promising alternative to Monte Carlo resulting in a speed up of about two orders of magnitude.</bodyText>
<sectionHeader confidence="0.999174" genericHeader="conclusion">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999993833333333">We presented a parser which analyzes new input by probabilistically combining fragments from LFG-annotated corpora into new analyses. We have seen that the parse accuracy increased with increasing fragment size, and that LFG's functional structures contribute to significantly higher parse accuracy on tree structures. We tested two search techniques for the most probable analysis, Viterbi n best and Monte Carlo. While these two techniques achieved about the same accuracy, Viterbi n best was about 100 times faster than Monte Carlo.</bodyText>
<sectionHeader confidence="0.999102" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999887948051948">
E. Black et al., 1991. &amp;quot;A Procedure for
Quantitatively Comparing the Syntactic
Coverage of English&amp;quot;, Proceedings DARPA
Workshop, Pacific Grove, Morgan Kaufmann.
R. Bod, 1993. &amp;quot;Using an Annotated Language
Corpus as a Virtual Stochastic Grammar&amp;quot;,
Proceedings AAAI'93, Washington D.C.
R. Bod, 1998. Beyond Grammar: An Experience-
Based Theory of Language, CSLI Publications,
Cambridge University Press.
R. Bod 1999. &amp;quot;Context-Sensitive Dialogue
Processing with the DOP Model&amp;quot;, Natural
Language Engineering 5(4), 309-323.
R. Bod, 2000a. &amp;quot;Parsing with the Shortest
Derivation&amp;quot;, Proceedings COLING-2000,
Saarbrücken, Germany.
R. Bod 2000b. &amp;quot;Combining Semantic and Syntactic
Structure for Language Modeling&amp;quot;, Proceed-
ings ICSLP-2000, Beijing, China.
R. Bod 2000c. &amp;quot;An Empirical Evaluation of LFG-
DOP&amp;quot;, Proceedings COLING-2000, Saar-
brücken, Germany.
R. Bod, 2000d. &amp;quot;The Storage and Computation of
Frequent Sentences&amp;quot;, Proceedings AMLAP-
2000, Leiden, The Netherlands.
R. Bod and R. Kaplan, 1998. &amp;quot;A Probabilistic
Corpus-Driven Model for Lexical Functional
Analysis&amp;quot;, Proceedings COLING-ACL'98,
Montreal, Canada.
R. Bonnema, R. Bod and R. Scha, 1997. &amp;quot;A DOP
Model for Semantic Interpretation&amp;quot;,
Proceedings ACL/EACL-97, Madrid, Spain.
J. Chappelier and M. Rajman, 2000. &amp;quot;Monte Carlo
Sampling for NP-hard Maximization Problems
in the Framework of Weighted Parsing&amp;quot;, in
NLP 2000, Lecture Notes in Artificial
Intelligence 1835, 106-117.
B. Cormons, 1999. Analyse et désambiguisation:
Une approche à base de corpus (Data-Oriented
Parsing) pour les répresentations lexicales
fonctionnelles. PhD thesis, Université de
Rennes, France.
I. Good, 1953. &amp;quot;The Population Frequencies of
Species and the Estimation of Population
Parameters&amp;quot;, Biometrika 40, 237-264.
J. Goodman, 1998. Parsing Inside-Out, PhD thesis,
Harvard University, Mass.
L. Hoogweg, 2000. Enriching DOP1 with the
Insertion Operation, MSc Thesis, Dept. of
Computer Science, University of Amsterdam.
R. Kaplan, and J. Bresnan, 1982. &amp;quot;Lexical-
Functional Grammar: A Formal System for
Grammatical Representation&amp;quot;, in J. Bresnan
(ed.), The Mental Representation of
Grammatical Relations, The MIT Press,
Cambridge, Mass.
J. Maxwell and R. Kaplan, 1991. &amp;quot;A Method for
Disjunctive Constraint Satisfaction&amp;quot;, in M.
Tomita (ed.), Current Issues in Parsing
Technology, Kluwer Academic Publishers.
G. Neumann, 1998. &amp;quot;Automatic Extraction of
Stochastic Lexicalized Tree Grammars from
Treebanks&amp;quot;, Proceedings of the 4th Workshop
on Tree-Adjoining Grammars and Related
Frameworks, Philadelphia, PA.
G. Neumann and D. Flickinger, 1999. &amp;quot;Learning
Stochastic Lexicalized Tree Grammars from
HPSG&amp;quot;, DFKI Technical Report, Saarbrücken,
Germany.
H. Ney, S. Martin and F. Wessel, 1997. &amp;quot;Statistical
Language Modeling Using Leaving-One-Out&amp;quot;,
in S. Young &amp; G. Bloothooft (eds.), Corpus-
Based Methods in Language and Speech
Processing, Kluwer Academic Publishers.
K. Sima'an, 1999. Learning Efficient Disambigu-
ation. PhD thesis, ILLC dissertation series
number 1999-02. Utrecht / Amsterdam.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant confidence="0.606382" no="0">
<title confidence="0.998886">An Improved Parser for Data-Oriented Lexical-Functional Analysis</title>
<author confidence="0.822009">Rens Bod</author>
<affiliation confidence="0.9235505">Informatics Research Institute, University of Leeds, Leeds LS2 9JT, UK, &amp; Institute for Logic, Language and Computation, University of Amsterdam</affiliation>
<email confidence="0.998848">rens@scs.leeds.ac.uk</email>
<abstract confidence="0.979220571428572">We present an LFG-DOP parser which uses fragments from LFG-annotated sentences to parse new sentences. Experiments with the Verbmobil Homecentre corpora show that (1) Viterbi best search performs about 100 times faster than Monte Carlo search while both achieve the same accuracy; (2) the DOP hypothesis which states that parse accuracy increases with increasing fragment size is confirmed for LFG-DOP; (3) LFG- DOP's relative frequency estimator performs worse than a discounted frequency estimator; and (4) LFG-DOP significantly outperforms Tree- DOP if evaluated on tree structures only.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Black</author>
</authors>
<title>A Procedure for Quantitatively Comparing the Syntactic Coverage of English&amp;quot;,</title>
<date>1991</date>
<booktitle>Proceedings DARPA Workshop,</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<location>Pacific Grove,</location>
<marker>Black, 1991</marker>
<rawString>E. Black et al., 1991. &amp;quot;A Procedure for Quantitatively Comparing the Syntactic Coverage of English&amp;quot;, Proceedings DARPA Workshop, Pacific Grove, Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bod</author>
</authors>
<title>Using an Annotated Language Corpus as a Virtual Stochastic Grammar&amp;quot;,</title>
<date>1993</date>
<booktitle>Proceedings AAAI'93,</booktitle>
<location>Washington D.C.</location>
<contexts>
<context citStr="Bod (1993)" endWordPosition="222" position="1536" startWordPosition="221">presentations for an unlimited set of utterances by generalizing from a given corpus of properly annotated exemplars. They operate by decomposing the given representations into (arbitrarily large) fragments and recomposing those pieces to analyze new utterances. A probability model is used to choose from the collection of different fragments of different sizes those that make up the most appropriate analysis of an utterance. DOP models have been shown to achieve state-of-the-art parsing performance on benchmarks such as the Wall Street Journal corpus (see Bod 2000a). The original DOP model in Bod (1993) was based on utterance analyses represented as surface trees, and is equivalent to a Stochastic Tree-Substitution Grammar. But the model has also been applied to several other grammatical frameworks, e.g. Tree-Insertion Grammar (Hoogweg 2000), Tree-Adjoining Grammar (Neumann 1998), Lexical-Functional Grammar (Bod &amp; Kaplan 1998; Cormons 1999), Head-driven Phrase Structure Grammar (Neumann &amp; Flickinger 1999), and Montague Grammar (Bonnema et al. 1997; Bod 1999). Most probability models for DOP use the relative frequency estimator to estimate fragment probabilities, although Bod (2000b) trains f</context>
<context citStr="Bod 1993" endWordPosition="789" position="5189" startWordPosition="788">y certain formal well-formedness conditions. A cstructure/f-structure pair is a valid LFG representation only if it satisfies the Nonbranching Dominance, Uniqueness, Coherence and Completeness conditions (Kaplan &amp; Bresnan 1982). 2.2 Decomposition operations and Fragments The fragments for LFG-DOP consist of connected subtrees whose nodes are in 0- correspondence with the correponding sub-units of f-structures. To give a precise definition of LFG-DOP fragments, it is convenient to recall the decomposition operations employed by the orginal DOP model which is also known as the &amp;quot;Tree-DOP&amp;quot; model (Bod 1993, 1998): (1) Root: the Root operation selects any node of a tree to be the root of the new subtree and erases all nodes except the selected node and the nodes it dominates. (2) Frontier: the Frontier operation then chooses a set (possibly empty) of nodes in the new subtree different from its root and erases all subtrees dominated by the chosen nodes. Bod &amp; Kaplan extend Tree-DOP's Root and Frontier operations so that they also apply to the nodes of the c-structure in LFG, while respecting the principles of c/f-structure correspondence. When a node is selected by the Root operation, all nodes o</context>
<context citStr="Bod 1993" endWordPosition="2218" position="13999" startWordPosition="2217">y a bottom-up chart parser using initially the indexed subtrees only. Thus only the Category-matching condition is enforced during the chart-parsing process. The Uniqueness and Coherence conditions of the corresponding f-structure units are enforced during the disambiguation or chart - decoding process. Disambiguation is accomplished by computing a large number of |f | random derivations from the chart and by selecting the analysis which results most often from these derivations. This technique is known as &amp;quot;Monte Carlo disambiguation&amp;quot; and has been extensively described in the literature (e.g. Bod 1993, 1998; Chappelier &amp; Rajman 2000; Goodman 1998; Hoogweg 2000). Sampling a random derivation from the chart consists of choosing at random one of the fragments from the set of composable fragments at every labeled chart-entry (where the random choices at each chart-entry are based on the probabilities of the fragments). The derivations are sampled in a topdown, leftmost order so as to maintain the LFGDOP derivation order. Thus the competition sets of composable fragments are computed on the fly during the Monte Carlo sampling process by grouping the f-structure units that unify and that are coh</context>
<context citStr="Bod 1993" endWordPosition="3409" position="21490" startWordPosition="3408">% 74.2% 72.2% £2 34.1% 76.2% 74.5% £3 35.6% 76.8% 75.9% £4 35.9% 77.5% 76.4% Table 3. Accuracies on the Verbmobil Fragment Depth Exact Match Precision Recall 1 31.3% 75.0% 71.5% £2 36.3% 77.1% 74.7% £3 37.8% 77.8% 76.1% £4 38.4% 80.0% 78.6% Table 4. Accuracies on the Homecentre Tables 3 and 4 show that there is a consistent increase in parse accuracy for all metrics if larger fragments are included, but that the increase itself decreases. This phenomenon is also known as the DOP hypothesis (Bod 1998), and has been confirmed for Tree-DOP on the ATIS, OVIS and Wall Street Journal treebanks (see Bod 1993, 1998, 1999, 2000a; Sima'an 1999; Bonnema et al. 1997; Hoogweg 2000). The current result thus extends the validity of the DOP hypothesis to LFG annotations. We do not yet know whether the accuracy continues to increase if even larger fragments are included (for Tree-DOP it has been shown that the accuracy decreases after a certain depth, probably due to overfitting -- cf. Bonnema et al. 1997; Bod 2000a). 5.3 Comparing LFG-DOP to Tree-DOP In the following experiment, we are interested in the impact of functional structures on predicting the correct tree structures. We therefore removed all f-s</context>
</contexts>
<marker>Bod, 1993</marker>
<rawString>R. Bod, 1993. &amp;quot;Using an Annotated Language Corpus as a Virtual Stochastic Grammar&amp;quot;, Proceedings AAAI'93, Washington D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bod</author>
</authors>
<title>Beyond Grammar: An ExperienceBased Theory of Language,</title>
<date>1998</date>
<publisher>CSLI Publications, Cambridge University Press.</publisher>
<contexts>
<context citStr="Bod 1998" endWordPosition="410" position="2861" startWordPosition="409">ion algorithms. The DOP model has also been tested as a model for human sentence processing (Bod 2000d). This paper presents ongoing work on DOP models for Lexical-Functional Grammar representations, known as LFG-DOP (Bod &amp; Kaplan 1998). We develop a parser which uses fragments from LFG-annotated sentences to parse new sentences, and we derive some experimental properties of LFG-DOP on two LFG-annotated corpora: the Verbmobil and Homecentre corpus. The experiments show that the DOP hypothesis, which states that there is an increase in parse accuracy if larger fragments are taken into account (Bod 1998), is confirmed for LFG-DOP. We report on an improved search technique for estimating the most probable analysis. While a Monte Carlo search converges provably to the most probable parse, a Viterbi n best search performs as well as Monte Carlo while its processing time is two orders of magnitude faster. We also show that LFG-DOP outperforms TreeDOP if evaluated on tree structures only. 2 Summary of LFG-DOP In accordance with Bod (1998), a particular DOP model is described by • a definition of a well-formed representation for utterance analyses, • a set of decomposition operations that divide a </context>
<context citStr="Bod (1998)" endWordPosition="2009" position="12657" startWordPosition="2008">n its bag multiplied by the probability mass assigned to this bag. Let |f |denote the frequency of a fragment f, then its probability is given by: (4) P(f |f is generated by Root/Frontier) = (1 − n1/N) Σf': f' is generated by Root/Frontier |f'| (5) P(f |f is generated by Discard) = |f | (n1/N) Σf': f' is generated by Discard |f'| We will refer to this fragment probability estimator as &amp;quot;discounted relative frequency&amp;quot; or &amp;quot;discounted RF&amp;quot;. 4 Parsing with LFG-DOP In his PhD-thesis, Cormons (1999) presents a parsing algorithm for LFG-DOP which is based on the Tree-DOP parsing technique described in Bod (1998). Cormons first converts LFGrepresentations into more compact indexed trees: each node in the c-structure is assigned an index which refers to the O-corresponding f-structure unit. For example, the representation in figure 1 is indexed as (S.1 (NP.2 Kim.2) (VP.1 eats.1)) where 1 --&gt; [ (SUBJ = 2) (TENSE = PRES) (PRED = eat(SUBJ)) ] 2 --&gt; [ (PRED = Kim) (NUM = SG) ] The indexed trees are then fragmented by applying the Tree-DOP decomposition operations described in section 2. Next, the LFG-DOP decomposition operations Root, Frontier and Discard are applied to the f-structure units that correspon</context>
<context citStr="Bod 1998" endWordPosition="2428" position="15322" startWordPosition="2427">ed after the derivation process. Incomplete derivations are simply removed from the sampling distribution. After sampling a sufficiently large number of random derivations that satisfy the LFG validity requirements, the most probable analysis is estimated by the analysis which results most often from the sampled derivations. As a stop condition on the number of sampled derivations, we compute the probability of error, which is the probability that the analysis that is most frequently generated by the sampled derivations is not equal to the most probable analysis, and which is set to 0.05 (see Bod 1998). In order to rule out the possibility that the sampling process never stops, we use a maximum sample size of 10,000 derivations. While the Monte Carlo disambiguation technique converges provably to the most probable analysis, it is quite inefficient. It is possible to use an alternative, heuristic search based on Viterbi n best (we will not go into the PCFG-reduction technique presented in Goodman (1998) since that heuristic only works for TreeDOP and is beneficial only if all subtrees are taken into account and if the so-called &amp;quot;labeled recall parse&amp;quot; is computed). A Viterbi n best search for</context>
<context citStr="Bod 1998" endWordPosition="3391" position="21387" startWordPosition="3390"> fragments together with the discounted RF estimator. Fragment Depth Exact Match Precision Recall 1 30.6% 74.2% 72.2% £2 34.1% 76.2% 74.5% £3 35.6% 76.8% 75.9% £4 35.9% 77.5% 76.4% Table 3. Accuracies on the Verbmobil Fragment Depth Exact Match Precision Recall 1 31.3% 75.0% 71.5% £2 36.3% 77.1% 74.7% £3 37.8% 77.8% 76.1% £4 38.4% 80.0% 78.6% Table 4. Accuracies on the Homecentre Tables 3 and 4 show that there is a consistent increase in parse accuracy for all metrics if larger fragments are included, but that the increase itself decreases. This phenomenon is also known as the DOP hypothesis (Bod 1998), and has been confirmed for Tree-DOP on the ATIS, OVIS and Wall Street Journal treebanks (see Bod 1993, 1998, 1999, 2000a; Sima'an 1999; Bonnema et al. 1997; Hoogweg 2000). The current result thus extends the validity of the DOP hypothesis to LFG annotations. We do not yet know whether the accuracy continues to increase if even larger fragments are included (for Tree-DOP it has been shown that the accuracy decreases after a certain depth, probably due to overfitting -- cf. Bonnema et al. 1997; Bod 2000a). 5.3 Comparing LFG-DOP to Tree-DOP In the following experiment, we are interested in the </context>
</contexts>
<marker>Bod, 1998</marker>
<rawString>R. Bod, 1998. Beyond Grammar: An ExperienceBased Theory of Language, CSLI Publications, Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bod</author>
</authors>
<title>Context-Sensitive Dialogue Processing with the DOP Model&amp;quot;,</title>
<date>1999</date>
<journal>Natural Language Engineering</journal>
<volume>5</volume>
<issue>4</issue>
<pages>309--323</pages>
<contexts>
<context citStr="Bod 1999" endWordPosition="285" position="2000" startWordPosition="284">e state-of-the-art parsing performance on benchmarks such as the Wall Street Journal corpus (see Bod 2000a). The original DOP model in Bod (1993) was based on utterance analyses represented as surface trees, and is equivalent to a Stochastic Tree-Substitution Grammar. But the model has also been applied to several other grammatical frameworks, e.g. Tree-Insertion Grammar (Hoogweg 2000), Tree-Adjoining Grammar (Neumann 1998), Lexical-Functional Grammar (Bod &amp; Kaplan 1998; Cormons 1999), Head-driven Phrase Structure Grammar (Neumann &amp; Flickinger 1999), and Montague Grammar (Bonnema et al. 1997; Bod 1999). Most probability models for DOP use the relative frequency estimator to estimate fragment probabilities, although Bod (2000b) trains fragment probabilities by a maximum likelihood reestimation procedure belonging to the class of expectation-maximization algorithms. The DOP model has also been tested as a model for human sentence processing (Bod 2000d). This paper presents ongoing work on DOP models for Lexical-Functional Grammar representations, known as LFG-DOP (Bod &amp; Kaplan 1998). We develop a parser which uses fragments from LFG-annotated sentences to parse new sentences, and we derive so</context>
</contexts>
<marker>Bod, 1999</marker>
<rawString>R. Bod 1999. &amp;quot;Context-Sensitive Dialogue Processing with the DOP Model&amp;quot;, Natural Language Engineering 5(4), 309-323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bod</author>
</authors>
<title>Parsing with the Shortest Derivation&amp;quot;,</title>
<date>2000</date>
<booktitle>Proceedings COLING-2000,</booktitle>
<location>Saarbrücken, Germany.</location>
<contexts>
<context citStr="Bod 2000" endWordPosition="215" position="1496" startWordPosition="214">els learn how to provide linguistic representations for an unlimited set of utterances by generalizing from a given corpus of properly annotated exemplars. They operate by decomposing the given representations into (arbitrarily large) fragments and recomposing those pieces to analyze new utterances. A probability model is used to choose from the collection of different fragments of different sizes those that make up the most appropriate analysis of an utterance. DOP models have been shown to achieve state-of-the-art parsing performance on benchmarks such as the Wall Street Journal corpus (see Bod 2000a). The original DOP model in Bod (1993) was based on utterance analyses represented as surface trees, and is equivalent to a Stochastic Tree-Substitution Grammar. But the model has also been applied to several other grammatical frameworks, e.g. Tree-Insertion Grammar (Hoogweg 2000), Tree-Adjoining Grammar (Neumann 1998), Lexical-Functional Grammar (Bod &amp; Kaplan 1998; Cormons 1999), Head-driven Phrase Structure Grammar (Neumann &amp; Flickinger 1999), and Montague Grammar (Bonnema et al. 1997; Bod 1999). Most probability models for DOP use the relative frequency estimator to estimate fragment prob</context>
<context citStr="Bod 2000" endWordPosition="2857" position="17951" startWordPosition="2856">rent training/test set splits. Besides an exact match accuracy metric, we also used a more fine-grained score based on the well-known PARSEVAL metrics that evaluate phrase-structure trees (Black et al. 1991). The PARSEVAL metrics compare a proposed parse P with the corresponding correct treebank parse T as follows: # correct constituents in P Precision = # constituents in P # correct constituents in P Recall = # constituents in T A constituent in P is correct if there exists a constituent in T of the same label that spans the same words and that f-corresponds to the same f-structure unit (see Bod 2000c for some illustrations of these metrics for LFG-DOP). 5.1 Comparing the two fragment estimators We were first interested in comparing the performance of the simple RF estimator against the discounted RF estimator. Furthermore, we want to study the contribution of generalized fragments to the parse accuracy. We therefore created for each training set two sets of fragments: one which contains all fragments (up to depth 4) and one which excludes the generalized fragments as generated by Discard. The exclusion of these Discard-generated fragments means that all probability mass goes to the fragm</context>
<context citStr="Bod 2000" endWordPosition="3477" position="21895" startWordPosition="3476">t that the increase itself decreases. This phenomenon is also known as the DOP hypothesis (Bod 1998), and has been confirmed for Tree-DOP on the ATIS, OVIS and Wall Street Journal treebanks (see Bod 1993, 1998, 1999, 2000a; Sima'an 1999; Bonnema et al. 1997; Hoogweg 2000). The current result thus extends the validity of the DOP hypothesis to LFG annotations. We do not yet know whether the accuracy continues to increase if even larger fragments are included (for Tree-DOP it has been shown that the accuracy decreases after a certain depth, probably due to overfitting -- cf. Bonnema et al. 1997; Bod 2000a). 5.3 Comparing LFG-DOP to Tree-DOP In the following experiment, we are interested in the impact of functional structures on predicting the correct tree structures. We therefore removed all f-structure units from the fragments, thus yielding a Tree-DOP model, and compared the results against the full LFG-DOP model (using the discounted RF estimator and all fragments up to depth 4). We evaluated the parse accuracy on the tree structures only, using exact match together with the standard PARSEVAL measures. We used the same training/test set splits as in the previous experiments. tree structure</context>
</contexts>
<marker>Bod, 2000</marker>
<rawString>R. Bod, 2000a. &amp;quot;Parsing with the Shortest Derivation&amp;quot;, Proceedings COLING-2000, Saarbrücken, Germany.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Bod</author>
</authors>
<title>2000b. &amp;quot;Combining Semantic and Syntactic Structure for Language Modeling&amp;quot;,</title>
<booktitle>Proceedings ICSLP-2000,</booktitle>
<location>Beijing, China.</location>
<marker>Bod, </marker>
<rawString>R. Bod 2000b. &amp;quot;Combining Semantic and Syntactic Structure for Language Modeling&amp;quot;, Proceedings ICSLP-2000, Beijing, China.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Bod</author>
</authors>
<title>2000c. &amp;quot;An Empirical Evaluation of LFGDOP&amp;quot;,</title>
<booktitle>Proceedings COLING-2000,</booktitle>
<location>Saarbrücken, Germany.</location>
<marker>Bod, </marker>
<rawString>R. Bod 2000c. &amp;quot;An Empirical Evaluation of LFGDOP&amp;quot;, Proceedings COLING-2000, Saarbrücken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bod</author>
</authors>
<title>The Storage and Computation of Frequent Sentences&amp;quot;,</title>
<date>2000</date>
<booktitle>Proceedings AMLAP2000,</booktitle>
<location>Leiden, The Netherlands.</location>
<contexts>
<context citStr="Bod 2000" endWordPosition="215" position="1496" startWordPosition="214">els learn how to provide linguistic representations for an unlimited set of utterances by generalizing from a given corpus of properly annotated exemplars. They operate by decomposing the given representations into (arbitrarily large) fragments and recomposing those pieces to analyze new utterances. A probability model is used to choose from the collection of different fragments of different sizes those that make up the most appropriate analysis of an utterance. DOP models have been shown to achieve state-of-the-art parsing performance on benchmarks such as the Wall Street Journal corpus (see Bod 2000a). The original DOP model in Bod (1993) was based on utterance analyses represented as surface trees, and is equivalent to a Stochastic Tree-Substitution Grammar. But the model has also been applied to several other grammatical frameworks, e.g. Tree-Insertion Grammar (Hoogweg 2000), Tree-Adjoining Grammar (Neumann 1998), Lexical-Functional Grammar (Bod &amp; Kaplan 1998; Cormons 1999), Head-driven Phrase Structure Grammar (Neumann &amp; Flickinger 1999), and Montague Grammar (Bonnema et al. 1997; Bod 1999). Most probability models for DOP use the relative frequency estimator to estimate fragment prob</context>
<context citStr="Bod 2000" endWordPosition="2857" position="17951" startWordPosition="2856">rent training/test set splits. Besides an exact match accuracy metric, we also used a more fine-grained score based on the well-known PARSEVAL metrics that evaluate phrase-structure trees (Black et al. 1991). The PARSEVAL metrics compare a proposed parse P with the corresponding correct treebank parse T as follows: # correct constituents in P Precision = # constituents in P # correct constituents in P Recall = # constituents in T A constituent in P is correct if there exists a constituent in T of the same label that spans the same words and that f-corresponds to the same f-structure unit (see Bod 2000c for some illustrations of these metrics for LFG-DOP). 5.1 Comparing the two fragment estimators We were first interested in comparing the performance of the simple RF estimator against the discounted RF estimator. Furthermore, we want to study the contribution of generalized fragments to the parse accuracy. We therefore created for each training set two sets of fragments: one which contains all fragments (up to depth 4) and one which excludes the generalized fragments as generated by Discard. The exclusion of these Discard-generated fragments means that all probability mass goes to the fragm</context>
<context citStr="Bod 2000" endWordPosition="3477" position="21895" startWordPosition="3476">t that the increase itself decreases. This phenomenon is also known as the DOP hypothesis (Bod 1998), and has been confirmed for Tree-DOP on the ATIS, OVIS and Wall Street Journal treebanks (see Bod 1993, 1998, 1999, 2000a; Sima'an 1999; Bonnema et al. 1997; Hoogweg 2000). The current result thus extends the validity of the DOP hypothesis to LFG annotations. We do not yet know whether the accuracy continues to increase if even larger fragments are included (for Tree-DOP it has been shown that the accuracy decreases after a certain depth, probably due to overfitting -- cf. Bonnema et al. 1997; Bod 2000a). 5.3 Comparing LFG-DOP to Tree-DOP In the following experiment, we are interested in the impact of functional structures on predicting the correct tree structures. We therefore removed all f-structure units from the fragments, thus yielding a Tree-DOP model, and compared the results against the full LFG-DOP model (using the discounted RF estimator and all fragments up to depth 4). We evaluated the parse accuracy on the tree structures only, using exact match together with the standard PARSEVAL measures. We used the same training/test set splits as in the previous experiments. tree structure</context>
</contexts>
<marker>Bod, 2000</marker>
<rawString>R. Bod, 2000d. &amp;quot;The Storage and Computation of Frequent Sentences&amp;quot;, Proceedings AMLAP2000, Leiden, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bod</author>
<author>R Kaplan</author>
</authors>
<title>A Probabilistic Corpus-Driven Model for Lexical Functional Analysis&amp;quot;,</title>
<date>1998</date>
<booktitle>Proceedings COLING-ACL'98,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context citStr="Bod &amp; Kaplan 1998" endWordPosition="266" position="1865" startWordPosition="263">f different fragments of different sizes those that make up the most appropriate analysis of an utterance. DOP models have been shown to achieve state-of-the-art parsing performance on benchmarks such as the Wall Street Journal corpus (see Bod 2000a). The original DOP model in Bod (1993) was based on utterance analyses represented as surface trees, and is equivalent to a Stochastic Tree-Substitution Grammar. But the model has also been applied to several other grammatical frameworks, e.g. Tree-Insertion Grammar (Hoogweg 2000), Tree-Adjoining Grammar (Neumann 1998), Lexical-Functional Grammar (Bod &amp; Kaplan 1998; Cormons 1999), Head-driven Phrase Structure Grammar (Neumann &amp; Flickinger 1999), and Montague Grammar (Bonnema et al. 1997; Bod 1999). Most probability models for DOP use the relative frequency estimator to estimate fragment probabilities, although Bod (2000b) trains fragment probabilities by a maximum likelihood reestimation procedure belonging to the class of expectation-maximization algorithms. The DOP model has also been tested as a model for human sentence processing (Bod 2000d). This paper presents ongoing work on DOP models for Lexical-Functional Grammar representations, known as LFG-</context>
<context citStr="Bod &amp; Kaplan (1998)" endWordPosition="570" position="3815" startWordPosition="567">rforms TreeDOP if evaluated on tree structures only. 2 Summary of LFG-DOP In accordance with Bod (1998), a particular DOP model is described by • a definition of a well-formed representation for utterance analyses, • a set of decomposition operations that divide a given utterance analysis into a set of fragments, • a set of composition operations by which such fragments may be recombined to derive an analysis of a new utterance, and • a definition of a probability model that indicates how the probability of a new utterance analysis is computed. In defining a DOP model for LFG representations, Bod &amp; Kaplan (1998) give the following settings for DOP's four parameters. 2.1 Representations The representations used by LFG-DOP are directly taken from LFG: they consist of a cstructure, an f-structure and a mapping 0 between them. The following figure shows an example representation for Kim eats. (We leave out some features to keep the example simple.) Figure 1 Bod &amp; Kaplan also introduce the notion of accessibility which they later use for defining the decomposition operations of LFG-DOP: An f-structure unit f is 0-accessible from a node n iff either n is 0-linked to f (that is, f = 0(n) ) or f is contained</context>
<context citStr="Bod &amp; Kaplan (1998)" endWordPosition="1169" position="7443" startWordPosition="1166">o removes the 0 links of the deleted nodes and erases any semantic form that corresponds to any of those nodes. For instance, if the NP in figure 1 is selected as a frontier node, Frontier erases the predicate &amp;quot;Kim&amp;quot; from the fragment: Figure 4 Finally, Bod &amp; Kaplan present a third decomposition operation, Discard, defined to construct generalizations of the fragments supplied by Root and Frontier. Discard acts to delete combinations of attribute-value pairs subject to the following condition: Discard does not delete pairs whose values 0 -correspond to remaining c-structure nodes. According to Bod &amp; Kaplan (1998), Discard-generated fragments are needed to parse sentences that are &amp;quot;ungrammatical with respect to the corpus&amp;quot;, thus increasing the robustness of the model. 2.3 The composition operation In LFG-DOP the operation for combining fragments is carried out in two steps. First the cstructures are combined by leftmost substitution subject to the category-matching condition, as in Tree-DOP. This is followed by the recursive unification of the f-structures corresponding to the matching nodes. A derivation for an LFG-DOP representation R is a sequence of fragments the first of which is labeled with S an</context>
</contexts>
<marker>Bod, Kaplan, 1998</marker>
<rawString>R. Bod and R. Kaplan, 1998. &amp;quot;A Probabilistic Corpus-Driven Model for Lexical Functional Analysis&amp;quot;, Proceedings COLING-ACL'98, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bonnema</author>
<author>R Bod</author>
<author>R Scha</author>
</authors>
<title>A DOP Model for Semantic Interpretation&amp;quot;,</title>
<date>1997</date>
<booktitle>Proceedings ACL/EACL-97,</booktitle>
<location>Madrid,</location>
<contexts>
<context citStr="Bonnema et al. 1997" endWordPosition="283" position="1989" startWordPosition="280"> been shown to achieve state-of-the-art parsing performance on benchmarks such as the Wall Street Journal corpus (see Bod 2000a). The original DOP model in Bod (1993) was based on utterance analyses represented as surface trees, and is equivalent to a Stochastic Tree-Substitution Grammar. But the model has also been applied to several other grammatical frameworks, e.g. Tree-Insertion Grammar (Hoogweg 2000), Tree-Adjoining Grammar (Neumann 1998), Lexical-Functional Grammar (Bod &amp; Kaplan 1998; Cormons 1999), Head-driven Phrase Structure Grammar (Neumann &amp; Flickinger 1999), and Montague Grammar (Bonnema et al. 1997; Bod 1999). Most probability models for DOP use the relative frequency estimator to estimate fragment probabilities, although Bod (2000b) trains fragment probabilities by a maximum likelihood reestimation procedure belonging to the class of expectation-maximization algorithms. The DOP model has also been tested as a model for human sentence processing (Bod 2000d). This paper presents ongoing work on DOP models for Lexical-Functional Grammar representations, known as LFG-DOP (Bod &amp; Kaplan 1998). We develop a parser which uses fragments from LFG-annotated sentences to parse new sentences, and w</context>
<context citStr="Bonnema et al. 1997" endWordPosition="3418" position="21544" startWordPosition="3415"> 76.8% 75.9% £4 35.9% 77.5% 76.4% Table 3. Accuracies on the Verbmobil Fragment Depth Exact Match Precision Recall 1 31.3% 75.0% 71.5% £2 36.3% 77.1% 74.7% £3 37.8% 77.8% 76.1% £4 38.4% 80.0% 78.6% Table 4. Accuracies on the Homecentre Tables 3 and 4 show that there is a consistent increase in parse accuracy for all metrics if larger fragments are included, but that the increase itself decreases. This phenomenon is also known as the DOP hypothesis (Bod 1998), and has been confirmed for Tree-DOP on the ATIS, OVIS and Wall Street Journal treebanks (see Bod 1993, 1998, 1999, 2000a; Sima'an 1999; Bonnema et al. 1997; Hoogweg 2000). The current result thus extends the validity of the DOP hypothesis to LFG annotations. We do not yet know whether the accuracy continues to increase if even larger fragments are included (for Tree-DOP it has been shown that the accuracy decreases after a certain depth, probably due to overfitting -- cf. Bonnema et al. 1997; Bod 2000a). 5.3 Comparing LFG-DOP to Tree-DOP In the following experiment, we are interested in the impact of functional structures on predicting the correct tree structures. We therefore removed all f-structure units from the fragments, thus yielding a Tre</context>
</contexts>
<marker>Bonnema, Bod, Scha, 1997</marker>
<rawString>R. Bonnema, R. Bod and R. Scha, 1997. &amp;quot;A DOP Model for Semantic Interpretation&amp;quot;, Proceedings ACL/EACL-97, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chappelier</author>
<author>M Rajman</author>
</authors>
<title>Monte Carlo Sampling for NP-hard Maximization Problems in the Framework of Weighted Parsing&amp;quot;,</title>
<date>2000</date>
<booktitle>in NLP 2000, Lecture Notes in Artificial Intelligence 1835,</booktitle>
<pages>106--117</pages>
<contexts>
<context citStr="Chappelier &amp; Rajman 2000" endWordPosition="2223" position="14031" startWordPosition="2220">art parser using initially the indexed subtrees only. Thus only the Category-matching condition is enforced during the chart-parsing process. The Uniqueness and Coherence conditions of the corresponding f-structure units are enforced during the disambiguation or chart - decoding process. Disambiguation is accomplished by computing a large number of |f | random derivations from the chart and by selecting the analysis which results most often from these derivations. This technique is known as &amp;quot;Monte Carlo disambiguation&amp;quot; and has been extensively described in the literature (e.g. Bod 1993, 1998; Chappelier &amp; Rajman 2000; Goodman 1998; Hoogweg 2000). Sampling a random derivation from the chart consists of choosing at random one of the fragments from the set of composable fragments at every labeled chart-entry (where the random choices at each chart-entry are based on the probabilities of the fragments). The derivations are sampled in a topdown, leftmost order so as to maintain the LFGDOP derivation order. Thus the competition sets of composable fragments are computed on the fly during the Monte Carlo sampling process by grouping the f-structure units that unify and that are coherent with the subderivation bui</context>
</contexts>
<marker>Chappelier, Rajman, 2000</marker>
<rawString>J. Chappelier and M. Rajman, 2000. &amp;quot;Monte Carlo Sampling for NP-hard Maximization Problems in the Framework of Weighted Parsing&amp;quot;, in NLP 2000, Lecture Notes in Artificial Intelligence 1835, 106-117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Cormons</author>
</authors>
<title>Analyse et désambiguisation: Une approche à base de corpus (Data-Oriented Parsing) pour les répresentations lexicales fonctionnelles.</title>
<date>1999</date>
<tech>PhD thesis,</tech>
<institution>Université de Rennes,</institution>
<contexts>
<context citStr="Cormons 1999" endWordPosition="268" position="1880" startWordPosition="267">ts of different sizes those that make up the most appropriate analysis of an utterance. DOP models have been shown to achieve state-of-the-art parsing performance on benchmarks such as the Wall Street Journal corpus (see Bod 2000a). The original DOP model in Bod (1993) was based on utterance analyses represented as surface trees, and is equivalent to a Stochastic Tree-Substitution Grammar. But the model has also been applied to several other grammatical frameworks, e.g. Tree-Insertion Grammar (Hoogweg 2000), Tree-Adjoining Grammar (Neumann 1998), Lexical-Functional Grammar (Bod &amp; Kaplan 1998; Cormons 1999), Head-driven Phrase Structure Grammar (Neumann &amp; Flickinger 1999), and Montague Grammar (Bonnema et al. 1997; Bod 1999). Most probability models for DOP use the relative frequency estimator to estimate fragment probabilities, although Bod (2000b) trains fragment probabilities by a maximum likelihood reestimation procedure belonging to the class of expectation-maximization algorithms. The DOP model has also been tested as a model for human sentence processing (Bod 2000d). This paper presents ongoing work on DOP models for Lexical-Functional Grammar representations, known as LFG-DOP (Bod &amp; Kapl</context>
<context citStr="Cormons (1999)" endWordPosition="1991" position="12543" startWordPosition="1990">ag of Root/Frontier-generated fragments. The probability of each fragment is then computed as its relative frequency in its bag multiplied by the probability mass assigned to this bag. Let |f |denote the frequency of a fragment f, then its probability is given by: (4) P(f |f is generated by Root/Frontier) = (1 − n1/N) Σf': f' is generated by Root/Frontier |f'| (5) P(f |f is generated by Discard) = |f | (n1/N) Σf': f' is generated by Discard |f'| We will refer to this fragment probability estimator as &amp;quot;discounted relative frequency&amp;quot; or &amp;quot;discounted RF&amp;quot;. 4 Parsing with LFG-DOP In his PhD-thesis, Cormons (1999) presents a parsing algorithm for LFG-DOP which is based on the Tree-DOP parsing technique described in Bod (1998). Cormons first converts LFGrepresentations into more compact indexed trees: each node in the c-structure is assigned an index which refers to the O-corresponding f-structure unit. For example, the representation in figure 1 is indexed as (S.1 (NP.2 Kim.2) (VP.1 eats.1)) where 1 --&gt; [ (SUBJ = 2) (TENSE = PRES) (PRED = eat(SUBJ)) ] 2 --&gt; [ (PRED = Kim) (NUM = SG) ] The indexed trees are then fragmented by applying the Tree-DOP decomposition operations described in section 2. Next, t</context>
</contexts>
<marker>Cormons, 1999</marker>
<rawString>B. Cormons, 1999. Analyse et désambiguisation: Une approche à base de corpus (Data-Oriented Parsing) pour les répresentations lexicales fonctionnelles. PhD thesis, Université de Rennes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Good</author>
</authors>
<title>The Population Frequencies of Species and the Estimation of Population Parameters&amp;quot;,</title>
<date>1953</date>
<journal>Biometrika</journal>
<volume>40</volume>
<pages>237--264</pages>
<contexts>
<context citStr="Good 1953" endWordPosition="1840" position="11655" startWordPosition="1839">ill treat the first type of fragments as seen events, and the second type of fragments as previously unseen events. We thus create two separate bags corresponding to two separate distributions: a bag with fragments generated by Root and Frontier, and a bag with fragments generated by Discard. We assign probability mass to the fragments of each bag by means of discounting: the relative frequencies of seen events are discounted and the gained probability mass is reserved for the bag of unseen events (cf. Ney et al. 1997). We accomplish this by a very simple estimator: the Turing-Good estimator (Good 1953) which computes the probability mass of unseen events as n1/N where n1 is the number of singleton events and N is the total number of seen events. This probability mass is assigned to the bag of Discard-generated fragments. The remaining mass (1 − n1/N) is assigned to the bag of Root/Frontier-generated fragments. The probability of each fragment is then computed as its relative frequency in its bag multiplied by the probability mass assigned to this bag. Let |f |denote the frequency of a fragment f, then its probability is given by: (4) P(f |f is generated by Root/Frontier) = (1 − n1/N) Σf': f</context>
</contexts>
<marker>Good, 1953</marker>
<rawString>I. Good, 1953. &amp;quot;The Population Frequencies of Species and the Estimation of Population Parameters&amp;quot;, Biometrika 40, 237-264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>Parsing Inside-Out, PhD thesis,</title>
<date>1998</date>
<institution>Harvard University,</institution>
<location>Mass.</location>
<contexts>
<context citStr="Goodman 1998" endWordPosition="2225" position="14045" startWordPosition="2224"> the indexed subtrees only. Thus only the Category-matching condition is enforced during the chart-parsing process. The Uniqueness and Coherence conditions of the corresponding f-structure units are enforced during the disambiguation or chart - decoding process. Disambiguation is accomplished by computing a large number of |f | random derivations from the chart and by selecting the analysis which results most often from these derivations. This technique is known as &amp;quot;Monte Carlo disambiguation&amp;quot; and has been extensively described in the literature (e.g. Bod 1993, 1998; Chappelier &amp; Rajman 2000; Goodman 1998; Hoogweg 2000). Sampling a random derivation from the chart consists of choosing at random one of the fragments from the set of composable fragments at every labeled chart-entry (where the random choices at each chart-entry are based on the probabilities of the fragments). The derivations are sampled in a topdown, leftmost order so as to maintain the LFGDOP derivation order. Thus the competition sets of composable fragments are computed on the fly during the Monte Carlo sampling process by grouping the f-structure units that unify and that are coherent with the subderivation built so far. As </context>
<context citStr="Goodman (1998)" endWordPosition="2493" position="15730" startWordPosition="2492">bability of error, which is the probability that the analysis that is most frequently generated by the sampled derivations is not equal to the most probable analysis, and which is set to 0.05 (see Bod 1998). In order to rule out the possibility that the sampling process never stops, we use a maximum sample size of 10,000 derivations. While the Monte Carlo disambiguation technique converges provably to the most probable analysis, it is quite inefficient. It is possible to use an alternative, heuristic search based on Viterbi n best (we will not go into the PCFG-reduction technique presented in Goodman (1998) since that heuristic only works for TreeDOP and is beneficial only if all subtrees are taken into account and if the so-called &amp;quot;labeled recall parse&amp;quot; is computed). A Viterbi n best search for LFG-DOP estimates the most probable analysis by computing n most probable derivations, and by then summing up the probabilities of the valid derivations that produce the same analysis. The algorithm for computing n most probable derivations follows straightforwardly from the algorithm which computes the most probable derivation by means of Viterbi optimization (see e.g. Sima'an 1999). 5 Experimental Eval</context>
</contexts>
<marker>Goodman, 1998</marker>
<rawString>J. Goodman, 1998. Parsing Inside-Out, PhD thesis, Harvard University, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hoogweg</author>
</authors>
<title>Enriching DOP1 with the Insertion Operation, MSc Thesis,</title>
<date>2000</date>
<institution>Dept. of Computer Science, University of Amsterdam.</institution>
<contexts>
<context citStr="Hoogweg 2000" endWordPosition="256" position="1779" startWordPosition="255">nalyze new utterances. A probability model is used to choose from the collection of different fragments of different sizes those that make up the most appropriate analysis of an utterance. DOP models have been shown to achieve state-of-the-art parsing performance on benchmarks such as the Wall Street Journal corpus (see Bod 2000a). The original DOP model in Bod (1993) was based on utterance analyses represented as surface trees, and is equivalent to a Stochastic Tree-Substitution Grammar. But the model has also been applied to several other grammatical frameworks, e.g. Tree-Insertion Grammar (Hoogweg 2000), Tree-Adjoining Grammar (Neumann 1998), Lexical-Functional Grammar (Bod &amp; Kaplan 1998; Cormons 1999), Head-driven Phrase Structure Grammar (Neumann &amp; Flickinger 1999), and Montague Grammar (Bonnema et al. 1997; Bod 1999). Most probability models for DOP use the relative frequency estimator to estimate fragment probabilities, although Bod (2000b) trains fragment probabilities by a maximum likelihood reestimation procedure belonging to the class of expectation-maximization algorithms. The DOP model has also been tested as a model for human sentence processing (Bod 2000d). This paper presents on</context>
<context citStr="Hoogweg 2000" endWordPosition="2227" position="14060" startWordPosition="2226">ubtrees only. Thus only the Category-matching condition is enforced during the chart-parsing process. The Uniqueness and Coherence conditions of the corresponding f-structure units are enforced during the disambiguation or chart - decoding process. Disambiguation is accomplished by computing a large number of |f | random derivations from the chart and by selecting the analysis which results most often from these derivations. This technique is known as &amp;quot;Monte Carlo disambiguation&amp;quot; and has been extensively described in the literature (e.g. Bod 1993, 1998; Chappelier &amp; Rajman 2000; Goodman 1998; Hoogweg 2000). Sampling a random derivation from the chart consists of choosing at random one of the fragments from the set of composable fragments at every labeled chart-entry (where the random choices at each chart-entry are based on the probabilities of the fragments). The derivations are sampled in a topdown, leftmost order so as to maintain the LFGDOP derivation order. Thus the competition sets of composable fragments are computed on the fly during the Monte Carlo sampling process by grouping the f-structure units that unify and that are coherent with the subderivation built so far. As mentioned in se</context>
<context citStr="Hoogweg 2000" endWordPosition="3420" position="21559" startWordPosition="3419"> 77.5% 76.4% Table 3. Accuracies on the Verbmobil Fragment Depth Exact Match Precision Recall 1 31.3% 75.0% 71.5% £2 36.3% 77.1% 74.7% £3 37.8% 77.8% 76.1% £4 38.4% 80.0% 78.6% Table 4. Accuracies on the Homecentre Tables 3 and 4 show that there is a consistent increase in parse accuracy for all metrics if larger fragments are included, but that the increase itself decreases. This phenomenon is also known as the DOP hypothesis (Bod 1998), and has been confirmed for Tree-DOP on the ATIS, OVIS and Wall Street Journal treebanks (see Bod 1993, 1998, 1999, 2000a; Sima'an 1999; Bonnema et al. 1997; Hoogweg 2000). The current result thus extends the validity of the DOP hypothesis to LFG annotations. We do not yet know whether the accuracy continues to increase if even larger fragments are included (for Tree-DOP it has been shown that the accuracy decreases after a certain depth, probably due to overfitting -- cf. Bonnema et al. 1997; Bod 2000a). 5.3 Comparing LFG-DOP to Tree-DOP In the following experiment, we are interested in the impact of functional structures on predicting the correct tree structures. We therefore removed all f-structure units from the fragments, thus yielding a Tree-DOP model, an</context>
</contexts>
<marker>Hoogweg, 2000</marker>
<rawString>L. Hoogweg, 2000. Enriching DOP1 with the Insertion Operation, MSc Thesis, Dept. of Computer Science, University of Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kaplan</author>
<author>J Bresnan</author>
</authors>
<title>LexicalFunctional Grammar: A Formal System for Grammatical Representation&amp;quot;,</title>
<date>1982</date>
<booktitle>The Mental Representation of Grammatical Relations, The</booktitle>
<editor>in J. Bresnan (ed.),</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context citStr="Kaplan &amp; Bresnan 1982" endWordPosition="732" position="4808" startWordPosition="729">oduce the notion of accessibility which they later use for defining the decomposition operations of LFG-DOP: An f-structure unit f is 0-accessible from a node n iff either n is 0-linked to f (that is, f = 0(n) ) or f is contained within 0(n) (that is, there is a chain of attributes that leads from 0(n) to f). According to the LFG representation theory, cstructures and f-structures must satisfy certain formal well-formedness conditions. A cstructure/f-structure pair is a valid LFG representation only if it satisfies the Nonbranching Dominance, Uniqueness, Coherence and Completeness conditions (Kaplan &amp; Bresnan 1982). 2.2 Decomposition operations and Fragments The fragments for LFG-DOP consist of connected subtrees whose nodes are in 0- correspondence with the correponding sub-units of f-structures. To give a precise definition of LFG-DOP fragments, it is convenient to recall the decomposition operations employed by the orginal DOP model which is also known as the &amp;quot;Tree-DOP&amp;quot; model (Bod 1993, 1998): (1) Root: the Root operation selects any node of a tree to be the root of the new subtree and erases all nodes except the selected node and the nodes it dominates. (2) Frontier: the Frontier operation then choo</context>
</contexts>
<marker>Kaplan, Bresnan, 1982</marker>
<rawString>R. Kaplan, and J. Bresnan, 1982. &amp;quot;LexicalFunctional Grammar: A Formal System for Grammatical Representation&amp;quot;, in J. Bresnan (ed.), The Mental Representation of Grammatical Relations, The MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Maxwell</author>
<author>R Kaplan</author>
</authors>
<title>A Method for Disjunctive Constraint Satisfaction&amp;quot;,</title>
<date>1991</date>
<booktitle>Current Issues in Parsing Technology,</booktitle>
<editor>in M. Tomita (ed.),</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context citStr="Maxwell &amp; Kaplan 1991" endWordPosition="2630" position="16629" startWordPosition="2627">s, and by then summing up the probabilities of the valid derivations that produce the same analysis. The algorithm for computing n most probable derivations follows straightforwardly from the algorithm which computes the most probable derivation by means of Viterbi optimization (see e.g. Sima'an 1999). 5 Experimental Evaluation We derived some experimental properties of LFG-DOP by studying its behavior on the two LFG-annotated corpora that are currently available: the Verbmobil corpus and the Homecentre corpus. Both corpora were annotated at Xerox PARC. They contain packed LFGrepresentations (Maxwell &amp; Kaplan 1991) of the grammatical parses of each sentence together with an indication which of these parses is the correct one. For our experiments we only used the correct parses of each sentence resulting in 540 Verbmobil parses and 980 Homecentre parses. Each corpus was divided into a 90% training set and a 10% test set. This division was random except for one constraint: that all the words in the test set actually occurred in the training set. The sentences from the test set were parsed and disambiguated by means of the fragments from the training set. Due to memory limitations, we restricted the maximu</context>
</contexts>
<marker>Maxwell, Kaplan, 1991</marker>
<rawString>J. Maxwell and R. Kaplan, 1991. &amp;quot;A Method for Disjunctive Constraint Satisfaction&amp;quot;, in M. Tomita (ed.), Current Issues in Parsing Technology, Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Neumann</author>
</authors>
<title>Automatic Extraction of Stochastic Lexicalized Tree Grammars from Treebanks&amp;quot;,</title>
<date>1998</date>
<booktitle>Proceedings of the 4th Workshop on Tree-Adjoining Grammars and Related Frameworks,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context citStr="Neumann 1998" endWordPosition="260" position="1818" startWordPosition="259">del is used to choose from the collection of different fragments of different sizes those that make up the most appropriate analysis of an utterance. DOP models have been shown to achieve state-of-the-art parsing performance on benchmarks such as the Wall Street Journal corpus (see Bod 2000a). The original DOP model in Bod (1993) was based on utterance analyses represented as surface trees, and is equivalent to a Stochastic Tree-Substitution Grammar. But the model has also been applied to several other grammatical frameworks, e.g. Tree-Insertion Grammar (Hoogweg 2000), Tree-Adjoining Grammar (Neumann 1998), Lexical-Functional Grammar (Bod &amp; Kaplan 1998; Cormons 1999), Head-driven Phrase Structure Grammar (Neumann &amp; Flickinger 1999), and Montague Grammar (Bonnema et al. 1997; Bod 1999). Most probability models for DOP use the relative frequency estimator to estimate fragment probabilities, although Bod (2000b) trains fragment probabilities by a maximum likelihood reestimation procedure belonging to the class of expectation-maximization algorithms. The DOP model has also been tested as a model for human sentence processing (Bod 2000d). This paper presents ongoing work on DOP models for Lexical-Fu</context>
</contexts>
<marker>Neumann, 1998</marker>
<rawString>G. Neumann, 1998. &amp;quot;Automatic Extraction of Stochastic Lexicalized Tree Grammars from Treebanks&amp;quot;, Proceedings of the 4th Workshop on Tree-Adjoining Grammars and Related Frameworks, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Neumann</author>
<author>D Flickinger</author>
</authors>
<title>Learning Stochastic Lexicalized Tree Grammars from HPSG&amp;quot;,</title>
<date>1999</date>
<tech>DFKI Technical Report,</tech>
<location>Saarbrücken, Germany.</location>
<contexts>
<context citStr="Neumann &amp; Flickinger 1999" endWordPosition="276" position="1946" startWordPosition="273">ropriate analysis of an utterance. DOP models have been shown to achieve state-of-the-art parsing performance on benchmarks such as the Wall Street Journal corpus (see Bod 2000a). The original DOP model in Bod (1993) was based on utterance analyses represented as surface trees, and is equivalent to a Stochastic Tree-Substitution Grammar. But the model has also been applied to several other grammatical frameworks, e.g. Tree-Insertion Grammar (Hoogweg 2000), Tree-Adjoining Grammar (Neumann 1998), Lexical-Functional Grammar (Bod &amp; Kaplan 1998; Cormons 1999), Head-driven Phrase Structure Grammar (Neumann &amp; Flickinger 1999), and Montague Grammar (Bonnema et al. 1997; Bod 1999). Most probability models for DOP use the relative frequency estimator to estimate fragment probabilities, although Bod (2000b) trains fragment probabilities by a maximum likelihood reestimation procedure belonging to the class of expectation-maximization algorithms. The DOP model has also been tested as a model for human sentence processing (Bod 2000d). This paper presents ongoing work on DOP models for Lexical-Functional Grammar representations, known as LFG-DOP (Bod &amp; Kaplan 1998). We develop a parser which uses fragments from LFG-annota</context>
</contexts>
<marker>Neumann, Flickinger, 1999</marker>
<rawString>G. Neumann and D. Flickinger, 1999. &amp;quot;Learning Stochastic Lexicalized Tree Grammars from HPSG&amp;quot;, DFKI Technical Report, Saarbrücken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ney</author>
<author>S Martin</author>
<author>F Wessel</author>
</authors>
<title>Statistical Language Modeling Using Leaving-One-Out&amp;quot;,</title>
<date>1997</date>
<booktitle>CorpusBased Methods in Language and Speech Processing,</booktitle>
<editor>in S. Young &amp; G. Bloothooft (eds.),</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context citStr="Ney et al. 1997" endWordPosition="1827" position="11569" startWordPosition="1824">nguishes between fragments supplied by Root/Frontier and fragments supplied by Discard. We will treat the first type of fragments as seen events, and the second type of fragments as previously unseen events. We thus create two separate bags corresponding to two separate distributions: a bag with fragments generated by Root and Frontier, and a bag with fragments generated by Discard. We assign probability mass to the fragments of each bag by means of discounting: the relative frequencies of seen events are discounted and the gained probability mass is reserved for the bag of unseen events (cf. Ney et al. 1997). We accomplish this by a very simple estimator: the Turing-Good estimator (Good 1953) which computes the probability mass of unseen events as n1/N where n1 is the number of singleton events and N is the total number of seen events. This probability mass is assigned to the bag of Discard-generated fragments. The remaining mass (1 − n1/N) is assigned to the bag of Root/Frontier-generated fragments. The probability of each fragment is then computed as its relative frequency in its bag multiplied by the probability mass assigned to this bag. Let |f |denote the frequency of a fragment f, then its </context>
</contexts>
<marker>Ney, Martin, Wessel, 1997</marker>
<rawString>H. Ney, S. Martin and F. Wessel, 1997. &amp;quot;Statistical Language Modeling Using Leaving-One-Out&amp;quot;, in S. Young &amp; G. Bloothooft (eds.), CorpusBased Methods in Language and Speech Processing, Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sima'an</author>
</authors>
<title>Learning Efficient Disambiguation. PhD thesis, ILLC dissertation series number 1999-02. Utrecht /</title>
<date>1999</date>
<location>Amsterdam.</location>
<contexts>
<context citStr="Sima'an 1999" endWordPosition="2584" position="16309" startWordPosition="2583">hnique presented in Goodman (1998) since that heuristic only works for TreeDOP and is beneficial only if all subtrees are taken into account and if the so-called &amp;quot;labeled recall parse&amp;quot; is computed). A Viterbi n best search for LFG-DOP estimates the most probable analysis by computing n most probable derivations, and by then summing up the probabilities of the valid derivations that produce the same analysis. The algorithm for computing n most probable derivations follows straightforwardly from the algorithm which computes the most probable derivation by means of Viterbi optimization (see e.g. Sima'an 1999). 5 Experimental Evaluation We derived some experimental properties of LFG-DOP by studying its behavior on the two LFG-annotated corpora that are currently available: the Verbmobil corpus and the Homecentre corpus. Both corpora were annotated at Xerox PARC. They contain packed LFGrepresentations (Maxwell &amp; Kaplan 1991) of the grammatical parses of each sentence together with an indication which of these parses is the correct one. For our experiments we only used the correct parses of each sentence resulting in 540 Verbmobil parses and 980 Homecentre parses. Each corpus was divided into a 90% t</context>
<context citStr="Sima'an 1999" endWordPosition="3414" position="21523" startWordPosition="3413">74.5% £3 35.6% 76.8% 75.9% £4 35.9% 77.5% 76.4% Table 3. Accuracies on the Verbmobil Fragment Depth Exact Match Precision Recall 1 31.3% 75.0% 71.5% £2 36.3% 77.1% 74.7% £3 37.8% 77.8% 76.1% £4 38.4% 80.0% 78.6% Table 4. Accuracies on the Homecentre Tables 3 and 4 show that there is a consistent increase in parse accuracy for all metrics if larger fragments are included, but that the increase itself decreases. This phenomenon is also known as the DOP hypothesis (Bod 1998), and has been confirmed for Tree-DOP on the ATIS, OVIS and Wall Street Journal treebanks (see Bod 1993, 1998, 1999, 2000a; Sima'an 1999; Bonnema et al. 1997; Hoogweg 2000). The current result thus extends the validity of the DOP hypothesis to LFG annotations. We do not yet know whether the accuracy continues to increase if even larger fragments are included (for Tree-DOP it has been shown that the accuracy decreases after a certain depth, probably due to overfitting -- cf. Bonnema et al. 1997; Bod 2000a). 5.3 Comparing LFG-DOP to Tree-DOP In the following experiment, we are interested in the impact of functional structures on predicting the correct tree structures. We therefore removed all f-structure units from the fragments</context>
</contexts>
<marker>Sima'an, 1999</marker>
<rawString>K. Sima'an, 1999. Learning Efficient Disambiguation. PhD thesis, ILLC dissertation series number 1999-02. Utrecht / Amsterdam.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>