<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant confidence="0.000037" no="0">
<title confidence="0.862307">
Semi-Supervised SimHash for Efficient Document Similarity Search
</title>
<author confidence="0.812934">
Qixia Jiang and Maosong Sun
</author>
<affiliation confidence="0.900617">
State Key Laboratory on Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Sci. and Tech., Tsinghua University, Beijing 100084, China
</affiliation>
<email confidence="0.98885">
qixia.jiang@gmail.com, sms@tsinghua.edu.cn
</email>
<sectionHeader confidence="0.98418" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999556476190476">Searching documents that are similar to a query document is an important component in modern information retrieval. Some existing hashing methods can be used for efficient document similarity search. However, unsupervised hashing methods cannot incorporate prior knowledge for better hashing. Although some supervised hashing methods can derive effective hash functions from prior knowledge, they are either computationally expensive or poorly discriminative. This paper proposes a novel (semi-)supervised hashing method named Semi-Supervised SimHash (S3H) for high-dimensional data similarity search. The basic idea of S3H is to learn the optimal feature weights from prior knowledge to relocate the data such that similar data have similar hash codes. We evaluate our method with several state-of-the-art methods on two large datasets. All the results show that our method gets the best performance.</bodyText>
<sectionHeader confidence="0.991536" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998263636363636">Document Similarity Search (DSS) is to find similar documents to a query doc in a text corpus or on the web. It is an important component in modern information retrieval since DSS can improve the traditional search engines and user experience (Wan et al., 2008; Dean et al., 1999). Traditional search engines accept several terms submitted by a user as a query and return a set of docs that are relevant to the query. However, for those users who are not search experts, it is always difficult to accurately specify some query terms to express their search purposes.</bodyText>
<page confidence="0.640573">
93
</page>
<bodyText confidence="0.999813757575758">Unlike short-query based search, DSS queries by a full (long) document, which allows users to directly submit a page or a document to the search engines as the description of their information needs. Meanwhile, the explosion of information has brought great challenges to traditional methods. For example, Inverted List (IL) which is a primary key-term access method would return a very large set of docs for a query document, which leads to the time-consuming post-processing. Therefore, a new effective algorithm is required. Hashing methods can perform highly efficient but approximate similarity search, and have gained great success in many applications such as Content-Based Image Retrieval (CBIR) (Ke et al., 2004; Kulis et al., 2009b), near-duplicate data detection (Ke et al., 2004; Manku et al., 2007; Costa et al., 2010), etc. Hashing methods project high-dimensional objects to compact binary codes called fingerprints and make similar fingerprints for similar objects. The similarity search in the Hamming space' is much more efficient than in the original attribute space (Manku et al., 2007). Recently, several hashing methods have been proposed. Specifically, SimHash (SH) (Charikar M.S., 2002) uses random projections to hash data. Although it works well with long fingerprints, SH has poor discrimination power for short fingerprints. A kernelized variant of SH, called Kernelized Locality Sensitive Hashing (KLSH) (Kulis et al., 2009a), is proposed to handle non-linearly separable data. These methods are unsupervised thus cannot incorporate prior knowledge for better hashing.Motivated by this, some supervised methods are proposed to derive effective hash functions from prior knowledge, i.e., Spectral Hashing (Weiss et al., 2009) and Semi-Supervised Hashing (SSH) (Wang et al., 2010a).</bodyText>
<note confidence="0.836654666666667">
'Hamming space is a set of binary strings of length L.
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 93–101,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.998962885714286">Regardless of different objectives, both methods derive hash functions via Principle Component Analysis (PCA) (Jolliffe, 1986). However, PCA is computationally expensive, which limits their usage for high-dimensional data. This paper proposes a novel (semi-)supervised hashing method, Semi-Supervised SimHash (S3H), for high-dimensional data similarity search. Unlike SSH that tries to find a sequence of hash functions, S3H fixes the random projection directions and seeks the optimal feature weights from prior knowledge to relocate the objects such that similar objects have similar fingerprints. This is implemented by maximizing the empirical accuracy on the prior knowledge (labeled data) and the entropy of hash functions (estimated over labeled and unlabeled data). The proposed method avoids using PCA which is computationally expensive especially for high-dimensional data, and leads to an efficient Quasi-Newton based solution. To evaluate our method, we compare with several state-ofthe-art hashing methods on two large datasets, i.e., 20 Newsgroups (20K points) and Open Directory Project (ODP) (2.4 million points). All experiments show that S3H gets the best search performance. This paper is organized as follows: Section 2 briefly introduces the background and some related works. In Section 3, we describe our proposed SemiSupervised SimHash (S3H). Section 4 provides experimental validation on two datasets. The conclusions are given in Section 5.</bodyText>
<sectionHeader confidence="0.747875" genericHeader="related work">
2 Background and Related Works
</sectionHeader>
<bodyText confidence="0.9999823">Suppose we are given a set of N documents, X = {xi  |xi E R'}�i=1. For a given query doc q, DSS tries to find its nearest neighbors in X or a subset X′ C X in which distance from the documents to the query doc q is less than a give threshold. However, such two tasks are computationally infeasible for large-scale data. Thus, it turns to the approximate similarity search problem (Indyk et al., 1998). In this section, we briefly review some related approximate similarity search methods.</bodyText>
<subsectionHeader confidence="0.982993">
2.1 SimHash
</subsectionHeader>
<bodyText confidence="0.999863666666667">SimHash (SH) is first proposed by Charikar (Charikar M.S., 2002). SH uses random projections as hash functions, i.e., where w E R' is a vector randomly generated.</bodyText>
<equation confidence="0.994786333333333">
T r +1, if wT x ≥ 0
h(x) = sign(w x) = 5l (1)
−1, otherwise
</equation>
<bodyText confidence="0.995705666666667">SH specifies the distribution on a family of hash functions H = {h} such that for two objects xi and xj, where B(xi, xj) is the angle between xi and xj.</bodyText>
<equation confidence="0.991566333333333">
θ(xi,xj)
Pr {h(xi) = h(xj)} = 1 − (2)
h∈Hπ
</equation>
<bodyText confidence="0.9991005">Obviously, SH is an unsupervised hashing method.</bodyText>
<subsectionHeader confidence="0.998991">
2.2 Kernelized Locality Sensitive Hashing
</subsectionHeader>
<bodyText confidence="0.999972714285714">A kernelized variant of SH, named Kernelized Locality Sensitive Hashing (KLSH) (Kulis et al., 2009a), is proposed for non-linearly separable data. KLSH approximates the underling Gaussian distribution in the implicit embedding space of data based on central limit theory. To calculate the value of hashing fuction h(·), KLSH projects points onto the eigenvectors of the kernel matrix. In short, the complete procedure of KLSH can be summarized as follows: 1) randomly select P (a small value) points from X and form the kernel matrix, 2) for each hash function h(O(x)), calculate its weight w E RP just as Kernel PCA (Sch¨olkopf et al., 1997), and 3) the hash function is defined as:</bodyText>
<equation confidence="0.999455666666667">
P
h(O(x)) = sign( wi · K(x, xi)) (3)
i=1
</equation>
<bodyText confidence="0.99056">where K(·, ·) can be any kernel function. KLSH can improve hashing results via the kernel trick. However, KLSH is unsupervised, thus designing a data-specific kernel remains a big challenge.</bodyText>
<subsectionHeader confidence="0.99796">
2.3 Semi-Supervised Hashing
</subsectionHeader>
<bodyText confidence="0.999903">Semi-Supervised Hashing (SSH) (Wang et al., 2010a) is recently proposed to incorporate prior knowledge for better hashing. Besides X, prior knowledge in the form of similar and dissimilar object-pairs is also required in SSH. SSH tries to find L optimal hash functions which have maximum empirical accuracy on prior knowledge and maximum entropy by finding the top L eigenvectors of an extended covariance matrix2 via PCA or SVD.</bodyText>
<page confidence="0.928113">
94
</page>
<bodyText confidence="0.999972166666667">However, despite of the potential problems of numerical stability, SVD requires massive computational space and O(M3) computational time where M is feature dimension, which limits its usage for high-dimensional data (Trefethen et al., 1997). Furthermore, the variance of directions obtained by PCA decreases with the decrease of the rank (Jolliffe, 1986). Thus, lower hash functions tend to have smaller entropy and larger empirical errors.</bodyText>
<subsectionHeader confidence="0.9954">
2.4 Others
</subsectionHeader>
<bodyText confidence="0.99998125">Some other related works should be mentioned. A notable method is Locality Sensitive Hashing (LSH) (Indyk et al., 1998). LSH performs a random linear projection to map similar objects to similar hash codes. However, LSH suffers from the efficiency problem that it tends to generate long codes (Salakhutdinov et al., 2007). LAMP (Mu et al., 2009) considers each hash function as a binary partition problem as in SVMs (Burges, 1998). Spectral Hashing (Weiss et al., 2009) maintains similarity between objects in the reduced Hamming space by minimizing the averaged Hamming distance3 between similar neighbors in the original Euclidean space. However, spectral hashing takes the assumption that data should be distributed uniformly, which is always violated in real-world applications.</bodyText>
<sectionHeader confidence="0.988345" genericHeader="method">
3 Semi-Supervised SimHash
</sectionHeader>
<bodyText confidence="0.8696925">In this section, we present our hashing method, named Semi-Supervised SimHash (S3H). Let XL = {(x1, c1) ... (xu, cu)} be the labeled data, c ∈ {1... C}, x ∈ RM, and XU = {xu+1 ... xN} the unlabeled data. Let X = XL ∪ XU. Given the labeled data XL, we construct two sets, attraction set Oa and repulsion set Or. Specifically, any pair (xi, xj) ∈ Oa, i, j ≤ u, denotes that xi and xj are in the same class, i.e., ci = cj, while any pair (xi, xj) ∈ Or, i, j ≤ u, denotes that ci ≠ cj. Unlike 2The extended covariance matrix is composed of two components, one is an unsupervised covariance term and another is a constraint matrix involving labeled information. 3Hamming distance is defined as the number of bits that are different between two binary strings. previews works that attempt to find L optimal hyperplanes, the basic idea of S3H is to fix L random hyperplanes and to find an optimal feature-weight vector to relocate the objects such that similar objects have similar codes.</bodyText>
<subsectionHeader confidence="0.999604">
3.1 Data Representation
</subsectionHeader>
<bodyText confidence="0.997813333333333">Since L random hyperplanes are fixed, we can represent a object x ∈ X as its relative position to these random hyperplanes, i.e., where the element Vml ∈ {+1, −1, 0} of V indicates that the object x is above, below or just in the l-th hyperplane with respect to the m-th feature, and A = diag(|x1|,|x2|, ... , |xM|) is a diagonal matrix which, to some extent, reflects the distance from x to these hyperplanes.</bodyText>
<equation confidence="0.983367">
D = A · V (4)
</equation>
<subsectionHeader confidence="0.998702">
3.2 Formulation
</subsectionHeader>
<bodyText confidence="0.99997175">Hashing maps the data set X to an L-dimensional Hamming space for compact representations. If we represent each object as Equation (4), the l-th hash function is then defined as:</bodyText>
<equation confidence="0.999237">
hl(x) = hl(D) = sign(wTdl) (5)
</equation>
<bodyText confidence="0.9999725">where w ∈ RM is the feature weight to be determined and dl is the l-th column of the matrix D. Intuitively, the ”contribution” of a specific feature to different classes is different. Therefore, we hope to incorporate this side information in S3H for better hashing. Inspired by (Madani et al., 2009), we can measure this contribution over XL as in Algorithm 1. Clearly, if objects are represented as the occurrence numbers of features, the output of Algorithm 1 is just the conditional probability Pr(class|feature). Finally, each object (x, c) ∈ XL can be represented as an M × L matrix G:</bodyText>
<equation confidence="0.99435">
G = diag(v1,c, v2,c,..., vM,c) · D (6)
</equation>
<bodyText confidence="0.9995628">Note that, one pair (xi, xj) in Oa or Or corresponds to (Gi, Gj) while (Di, Dj) if we ignore features’ contribution to different classes. Furthermore, we also hope to maximize the empirical accuracy on the labeled data Oa and Or and maximize the entropy of hash functions.</bodyText>
<equation confidence="0.9459745">
95
Algorithm 1: Feature Contribution Calculation
for each (x, c) E XL do
for each f E x do
vf — vf + xf;
vf,c — vf,c + xf;
end
end
for each feature f and class c do
νf, C ;
νf
end
</equation>
<bodyText confidence="0.9990675">So, we define the following objective for h(·)s:</bodyText>
<equation confidence="0.999544555555556">
J(w) =
(Xi, Xj) ∈ ®r
N V ∑ hl (xi) hl (xj)
p 1= (Xi,Xj)∈Oa (7)
hl(xi)hl(xj)} + A1∑H(hl)
L
1
l=1
L
</equation>
<bodyText confidence="0.99995325">where Np = |Oa |+ |Or |is the number of attraction and repulsion pairs and A1 is a tradeoff between two terms. Wang et al. have proven that hash functions with maximum entropy must maximize the variance of the hash values, and vice-versa (Wang et al., 2010b). Thus, H(h(·)) can be estimated over the labeled and unlabeled data, XL and XU. Unfortunately, direct solution for above problem is non-trivial since Equation (7) is not differentiable. Thus, we relax the objective and add an additional regularization term which could effectively avoid overfitting. Finally, we obtain the total objective:</bodyText>
<equation confidence="0.994527285714286">
L(w) = { ψ(wTgi,l)ψ(wT gj,l)
Np l=1 (Gi,Gj)Eea
1 ∑ ∑
L
(8)
A2
− 2 ∥w∥2 2
</equation>
<bodyText confidence="0.999965333333333">where gi,l and di,l denote the l-th column of Gi and Di respectively, and 0(t) is a piece-wise linear function defined as:</bodyText>
<equation confidence="0.998294">
T9 t &gt; T9
t −T9 ≤ t ≤ T9 (9)
−T9 t &lt; −T9
</equation>
<bodyText confidence="0.999557411764706">This relaxation has a good intuitive explanation. That is, similar objects are desired to not only have the similar fingerprints but also have sufficient large projection magnitudes, while dissimilar objects are desired to not only differ in their fingerprints but also have large projection margin. However, we do not hope that a small fraction of object-pairs with very large projection magnitude or margin dominate the complete model. Thus, a piece-wise linear function 0(·) is applied in S3H. As a result, Equation (8) is a simply unconstrained optimization problem, which can be efficiently solved by a notable Quasi-Newton algorithm, i.e., L-BFGS (Liu et al., 1989). For description simplicity, only attraction set Oa is considered and the extension to repulsion set Or is straightforward. Thus, the gradient of L(w) is as follows:</bodyText>
<equation confidence="0.96725675">
∑{ ∑
l=1 (Gi, Gj) ∈ ®a,
L
|wT gi,l |≤ Ts ψ(wTgj,l) · gi,l
}ψ(wTgi,l) · gj,l (10)
∑u ψ(wTgi,l) · gi,l
i � �,
|wT gi,l |≤ Ts
+ i _∑ u � �,
|wT di,l |≤ Ts
N }ψ(wTdi,l) · di,l − A2w
Note that a0(t)/at = 0 when |t |&gt; Tg.
</equation>
<subsectionHeader confidence="0.993518">
3.3 Fingerprint Generation
</subsectionHeader>
<bodyText confidence="0.999968375">When we get the optimal weight w∗, we generate fingerprints for given objects through Equation (5). Then, it tunes to the problem how to efficiently obtain the representation as in Figure 4 for a object. After analysis, we find: 1) hyperplanes are randomly generated and we only need to determine which sides of these hyperplanes the given object lies on, and 2) in real-world applications, objects such as docs are always very sparse. Thus, we can avoid heavy computational demands and efficiently generate fingerprints for objects. In practice, given an object x, the procedure of generating an L-bit fingerprint is as follows: it maintains an L-dimensional vector initialized to zero. Each feature f E x is firstly mapped to an L-bit hash value by Jenkins Hashing Function4. Then, these L bits increment or decrement the L components of the vector by the value xf x w*f. After all features processed, the signs of components determine the corresponding bits of the final fingerprint.</bodyText>
<figure confidence="0.810143228571429">
4http://www.burtleburtle.net/bob/hash/doobs.html
vf,c —
∑− ψ(wTgi,l)ψ(wTgj,l)}
(Gi,Gj)Eer
N
∑ ψ2(wT di,l)}
i=u+1
ψ2(wTgi,l) +
+
A1
∑L
l=1
∑u
{
i=1
2N



ψ(t) =
aL(w) 1
=
aw Np
+
∑
(Gi, Gj) ∈ ®a,
|wT gj,l |≤ Ts
+
A1
N
∑{ L
l=1
96
Algorithm 2: Fast Fingerprint Generation
INPUT: x and w*;
</figure>
<equation confidence="0.827874909090909">
initialize  +— 0,  +— 0, ,  E RL;
for each f E x do
randomly project f to hf E {-1, +11L;
 +—  + xf · wf · hf;
end
for l = 1 to L do
if αl &gt; 0 then
,Ql +— 1;
end
end
RETURN ;
</equation>
<bodyText confidence="0.999063">The complete algorithm is presented in Algorithm 2.</bodyText>
<subsectionHeader confidence="0.98756">
3.4 Algorithmic Analysis
</subsectionHeader>
<bodyText confidence="0.9999688">This section briefly analyzes the relation between S3H and some existing methods. For analysis simplicity, we assume ψ(t) = t and ignore the regularization terms. So, Equation (8) can be rewritten as follows:</bodyText>
<equation confidence="0.996619333333333">
L
J(w)S3H =1wT[�
l=1
</equation>
<bodyText confidence="0.999329833333333">where Φ+ ij equals to 1 when (xi, xj) E Oa otherwise 0, Φ−ij equals to 1 when (xi, xj) E Or otherwise 0, and Fl = [g1,l ... gu,l, du+1,l ... dN,l]. We denote El Fl4;+FTl and El Fl4;−FT l asS+ and S− respectively. Therefore, maximizing above function is equivalent to maximizing the following:</bodyText>
<equation confidence="0.9985155">
J(w)S3H = |wT S+w |(12)
|wT S−w|
</equation>
<bodyText confidence="0.999817333333333">Clearly, Equation (12) is analogous to Linear Discriminant Analysis (LDA) (Duda et al., 2000) except for the difference: 1) measurement. S3H uses similarity while LDA uses distance. As a result, the objective function of S3H is just the reciprocal of LDA’s. 2) embedding space. LDA seeks the best separative direction in the original attribute space. In contrast, S3H firstly maps data from RM to RMxL through the following projection function where rl E RM, l = 1, ... , L, are L random hyperplanes.</bodyText>
<equation confidence="0.999946">
O(x) = x · [diag(sign(r1)), ... , diag(sign(rL))] (13)
</equation>
<bodyText confidence="0.9998965">Then, in that space (RMxL), S3H seeks a directions that can best separate the data. From this point of view, it is obvious that the basic SH is a special case of S3H when w is set to e = [1, 1, ... ,1]. That is, SH firstly maps the data via ϕ(·) just as S3H. But then, SH directly separates the data in that feature space at the direction e. Analogously, we ignore the regularization terms in SSH and rewrite the objective of SSH as:</bodyText>
<equation confidence="0.9983825">
J(W)SSH = 2tr[WTX(Φ+ − Φ−)XTW] (14)
1
</equation>
<bodyText confidence="0.996968">where W = [w1, ... , wL] E RMxL are L hyperplanes and X = [x1, ... , xN]. Maximizing this objective is equivalent to maximizing the following:</bodyText>
<equation confidence="0.992107">
J(W)SSH = |tr[WTS′+W] |(15)
 |tr[WT S′−W]|
</equation>
<bodyText confidence="0.995455857142857">where S′+ = X4;+XT and S′− = X4;−XT. Equation (15) shows that SSH is analogous to Multiple Discriminant Analysis (MDA) (Duda et al., 2000). In fact, SSH uses top L best-separative hyperplanes in the original attribute space found via PCA to hash the data. Furthermore, we rewrite the projection function ϕ(·) in S3H as:</bodyText>
<equation confidence="0.999912">
O(x) = x · [R1, ... , RL] (16)
</equation>
<bodyText confidence="0.99999">where Rl = diag(sign(rl)). Each Rl is a mapping from RM to RM and corresponds to one embedding space. From this perspective, unlike SSH, S3H globally seeks a direction that can best separate the data in L different embedding spaces simultaneously.</bodyText>
<sectionHeader confidence="0.999015" genericHeader="evaluation and result">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999461">We use two datasets 20 Newsgroups and Open Directory Project (ODP) in our experiments. Each document is represented as a vector of occurrence numbers of the terms within it. The class information of docs is considered as prior knowledge that two docs within a same class should have more similar fingerprints while two docs within different classes should have dissimilar fingerprints. We will demonstrate that our S3H can effectively incorporate this prior knowledge to improve the DSS performance.</bodyText>
<figure confidence="0.86414575">
5The direction is determined by concatenating w L times.
Γl(Φ+ − Φ−)ΓTl ]w (11)
97
Mean Averaged Precision (MAP)
Mean Averaged Precision (MAP)
Precision within Hamming radius 3
Precision within Hamming radius 3
(a) (b)
</figure>
<figureCaption confidence="0.923523666666667">
Figure 1: Mean Averaged Precision (MAP) for different
number of bits for hash ranking on 20 Newsgroups. (a)
10K features. (b) 30K features.
</figureCaption>
<figure confidence="0.998732">
(a) (b)
</figure>
<figureCaption confidence="0.989176333333333">
Figure 2: Precision within Hamming radius 3 for hash
lookup on 20 Newsgroups. (a) 10K features. (b) 30K
features.
</figureCaption>
<bodyText confidence="0.99921975">We use Inverted List (IL) (Manning et al., 2002) as the baseline. In fact, given a query doc, IL returns all the docs that contain any term within it. We also compare our method with three state-ofthe-art hashing methods, i.e., KLSH, SSH and SH. In KLSH, we adopt the RBF kernel rz(xZ7 xj) = exp(− ∥Xi 62j ∥2 ), where the scaling factor S2 takes 0.5 and the other two parameters p and t are set to be 500 and 50 respectively. The parameter A in SSH is set to 1. For S3H, we simply set the parameters A1 and A2 in Equation (8) to 4 and 0.5 respectively. To objectively reflect the performance of S3H, we evaluate our S3H with and without Feature Contribution Calculation algorithm (FCC) (Algorithm 1). Specifically, FCC-free S3H (denoted as S3H f) is just a simplification when Gs in S3H are simply set to Ds. For quantitative evaluation, as in literature (Wang et al., 2010b; Mu et al., 2009), we calculate the precision under two scenarios: hash lookup and hash ranking. For hash lookup, the proportion of good neighbors (have the same class label as the query) among the searched objects within a given Hamming radius is calculated as precision. Similarly to (Wang et al., 2010b; Weiss et al., 2009), for a query document, if no neighbors within the given Hamming radius can be found, it is considered as zero precision. Note that, the precision of IL is the proportion of good neighbors among the whole searched objects. For hash ranking, all the objects in X are ranked in terms of their Hamming distance from the query document, and the top K nearest neighbors are returned as the result. Then, Mean Averaged Precision (MAP) (Manning et al., 2002) is calculated. We also calculate the averaged intraand interclass Hamming distance for various hashing methods. Intuitively, a good hashing method should have small intra-class distance while large inter-class distance. We test all the methods on a PC with a 2.66 GHz processor and 12GB RAM. All experiments repeate 10 times and the averaged results are reported.</bodyText>
<subsectionHeader confidence="0.977775">
4.1 20 Newsgroups
</subsectionHeader>
<bodyText confidence="0.999907740740741">20 Newsgroups contains 20K messages, about 1K messages from each of 20 different newsgroups. The entire vocabulary includes 62,061 words. To evaluate the performance for different feature dimensions, we use Chi-squared feature selection algorithm (Forman, 2003) to select 10K and 30K features. The averaged message length is 54.1 for 10K features and 116.2 for 30K features. We randomly select 4K massages as the test set and the remain 16K as the training set. To train SSH and S3H, from the training set, we randomly generate 40K message-pairs as Oa and 80K message-pairs as Or. For hash ranking, Figure 1 shows MAP for various methods using different number of bits. It shows that performance of SSH decreases with the growing of hash bits. This is mainly because the variance of the directions obtained by PCA decreases with the decrease of their ranks. Thus, lower bits have larger empirical errors. For S3H, FCC (Algorithm 1) can significantly improve the MAP just as discussed in Section 3.2. Moreover, the MAP of FCC-free S3H (S3Hf) is affected by feature dimensions while FCC-based (S3H) is relatively stable. This implies FCC can also improve the satiability of S3H. As we see, S3Hf ignores the contribution of features to different classes. However, besides the local description of data locality in the form of object-pairs, such (a) (b)</bodyText>
<footnote confidence="0.752989">
6http://www.cs.cmu.edu/afs/cs/project/theo-3/www/
</footnote>
<note confidence="0.526005">
98
</note>
<figureCaption confidence="0.845595333333333">
Figure 3: Averaged searched sample numbers using 4K
query messages for hash lookup. (a) 10K features. (b)
30K features.
</figureCaption>
<bodyText confidence="0.999865742857143">(global) information also provides a proper guidance for hashing. So, for S3Hf, the reason why its results with 30K features are worse than the results with 10K features is probably because S3Hf learns to hash only according to the local description of data locality and many not too relevant features lead to relatively poor description. In contrast, S3H can utilize global information to better understand the similarity among objects. In short, S3H obtains the best MAP for all bits and feature dimensions. For hash lookup, Figure 2 presents the precision within Hamming radius 3 for different number of bits. It shows that IL even outperforms SH. This is because few objects can be hashed by SH into one hash bucket. Thus, for many queries, SH fails to return any neighbor even in a large Hamming radius of 3. Clearly, S3H outperforms all the other methods for different number of hash bits and features. The number of messages searched by different methods are reported in Figure 3. We find that the number of searched data of S3H (with/without FCC) decreases much more slowly than KLSH, SH and SSH with the growing of the number of hash bits. As discussed in Section 3.4, this mainly benefits from the design of S3H that S3H (globally) seeks a direction that can best separate the data in L embedding spaces simultaneously. We also find IL returns a large number of neighbors of each query message which leads to its poor efficiency. The averaged intraand interclass Hamming distance of different methods are reported in Table 1. As it shows, S3H has relatively larger margin (∆) between intraand inter-class Hamming distance. This indicates that S3H is more effective to make similar points have similar fingerprints while keep the dissimilar points away enough from each other.</bodyText>
<table confidence="0.997630833333333">
intra-class inter-class ∆
S3H 13.1264 15.6342 2.5078
S3Hf 12.5754 13.3479 0.7725
SSH 6.4134 6.5262 0.1128
SH 15.3908 15.6339 0.2431
KLSH 10.2876 10.8713 0.5841
</table>
<tableCaption confidence="0.8545005">
Table 1: Averaged intra- and inter- class Hamming dis-
tance of 20 Newsgroups for 32-bit fingerprint. ∆ is the
difference between the averaged inter- and intra- class
Hamming distance. Large ∆ implies good hashing.
</tableCaption>
<figure confidence="0.947502">
12340 �
�
�� (K)
</figure>
<figureCaption confidence="0.840068666666667">
Figure 4: Computational complexity of training for dif-
ferent feature dimensions for 32-bit fingerprint. (a) Train-
ing time (sec). (b) Training space cost (MB).
</figureCaption>
<bodyText confidence="0.9998732">Figure 4 shows the (training) computational complexity of different methods. We find that the time and space cost of SSH grows much faster than SH, KLSH and S3H with the growing of feature dimension. This is mainly because SSH requires SVD to find the optimal hashing functions which is computational expensive. Instead, S3H seeks the optimal feature weights via L-BFGS, which is still efficient even for very high-dimensional data.</bodyText>
<subsectionHeader confidence="0.985057">
4.2 Open Directory Project (ODP)
</subsectionHeader>
<bodyText confidence="0.999577111111111">Open Directory Project (ODP)7 is a multilingual open content directory of web links (docs) organized by a hierarchical ontology scheme. In our experiment, only English docs8 at level 3 of the category tree are utilized to evaluate the performance. In short, the dataset contains 2,483,388 docs within 6,008 classes. There are totally 862,050 distinct words and each doc contains 14.13 terms on average. Since docs are too short, we do not conduct intra-class inter-class ∆
S3H 14.0029 15.9508 1.9479
S3Hf 14.3801 15.5260 1.1459
SH 14.7725 15.6432 0.8707
KLSH 9.3382 10.5700 1.2328</bodyText>
<figure confidence="0.983487066666667">
7http://rdf.dmoz.org/
8The title together with the corresponding short description
of a page are considered as a document in our experiments.
Number of searched data
Number of searched data
(a) (b)
Time (see.)
Space (Me)
99
Precision within Hamming radius 2
(a) (b)
Mean Averaged Precision (MAP)
(a) (b)
Percentage
Percentage
</figure>
<figureCaption confidence="0.9861915">
Figure 5: Overview of ODP data set. (a) Class distribu-
tion at level 3. (b) Distribution of document length.
</figureCaption>
<tableCaption confidence="0.997466">
Table 2: Averaged intra- and inter- class Hamming dis-
tance of ODP for 32-bit fingerprint (860K features).</tableCaption>
<bodyText confidence="0.940872931034483">∆ is the difference between averaged intraand interclass Hamming distance.feature selection9. An overview of ODP is shown in Figure 5. We randomly sample 10% docs as the test set and the remain as the training set. Furthermore, from training set, we randomly generate 800K docpairs as Oa, and 1 million doc-pairs as Or. Note that, since there are totally over 800K features, it is extremely inefficient to train SSH. Therefore, we only compare our S3H with IL, KLSH and SH. The search performance is given in Figure 6. Figure 6(a) shows the MAP for various methods using different number of bits. It shows KLSH outperforms SH, which mainly contributes to the kernel trick. S3H and S3Hf have higher MAP than KLSH and SH. Clearly, FCC algorithm can improve the MAP of S3H for all bits. Figure 6(b) presents the precision within Hamming radius 2 for hash lookup. We find that IL outperforms SH since SH fails for many queries. It also shows that S3H (with FCC) can obtain the best precision for all bits. Table 2 reports the averaged intraand inter-class Hamming distance for various methods. It shows that S3H has the largest margin (∆). This demon9We have tested feature selection. However, if we select 40K features via Chi-squared feature selection method, documents are represented by 3.15 terms on average. About 44.9% documents are represented by no more than 2 terms.</bodyText>
<figureCaption confidence="0.8039765">
Figure 6: Retrieval performance of different methods on
ODP. (a) Mean Averaged Precision (MAP) for different
number of bits for hash ranking. (b) Precision within Hamming radius 2 for hash lookup.</figureCaption>
<bodyText confidence="0.979176714285714">strates S3H can measure the similarity among the data better than KLSH and SH. We should emphasize that KLSH needs 0.3ms to return the results for a query document for hash lookup, and S3H needs &lt;0.1ms. In contrast, IL requires about 75ms to finish searching. This is mainly because IL always returns a large number of objects (dozens or hundreds times more than S3H and KLSH) and requires much time for post-processing. All the experiments show S3H is more effective, efficient and stable than the baseline method and the state-of-the-art hashing methods.</bodyText>
<sectionHeader confidence="0.998924" genericHeader="conclusion">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999971615384615">We have proposed a novel supervised hashing method named Semi-Supervised Simhash (S3H) for high-dimensional data similarity search. S3H learns the optimal feature weights from prior knowledge to relocate the data such that similar objects have similar fingerprints. This is implemented by maximizing the empirical accuracy on labeled data together with the entropy of hash functions. The proposed method leads to a simple Quasi-Newton based solution which is efficient even for very highdimensional data. Experiments performed on two large datasets have shown that S3H has better search performance than several state-of-the-art methods.</bodyText>
<sectionHeader confidence="0.998808" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9988874">We thank Fangtao Li for his insightful suggestions. We would also like to thank the anonymous reviewers for their helpful comments. This work is supported by the National Natural Science Foundation of China under Grant No. 60873174.</bodyText>
<page confidence="0.8741285">
4
100
</page>
<sectionHeader confidence="0.977837" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999798197530864">
Christopher J.C. Burges. 1998. A tutorial on support
vector machines for pattern recognition. Data Mining
and Knowledge Discovery, 2(2):121-167.
Moses S. Charikar. 2002. Similarity estimation tech-
niques from rounding algorithms. In Proceedings
of the 34th annual ACM symposium on Theory of
computing, pages 380-388.
Gianni Costa, Giuseppe Manco and Riccardo Ortale.
2010. An incremental clustering scheme for data de-
duplication. Data Mining and Knowledge Discovery,
20(1):152-187.
Jeffrey Dean and Monika R. Henzinge. 1999. Finding
Related Pages in the World Wide Web. Computer
Networks, 31:1467-1479.
Richard O. Duda, Peter E. Hart and David G. Stork.
2000. Pattern classification, 2nd edition. Wiley-
Interscience.
George Forman 2003. An extensive empirical study of
feature selection metrics for text classification. The
Journal of Machine Learning Research, 3:1289-1305.
Piotr Indyk and Rajeev Motwani. 1998. Approximate
nearest neighbors: towards removing the curse of
dimensionality. In Proceedings of the 30th annual
ACM symposium on Theory of computing, pages
604-613.
Ian Jolliffe. 1986. Principal Component Analysis.
Springer-Verlag, New York.
Yan Ke, Rahul Sukthankar and Larry Huston. 2004.
Efficient near-duplicate detection and sub-image
retrieval. In Proceedings of the ACM International
Conference on Multimedia.
Brian Kulis and Kristen Grauman. 2009. Kernelized
locality-sensitive hashing for scalable image search.
In Proceedings of the 12th International Conference
on Computer Vision, pages 2130-2137.
Brian Kulis, Prateek Jain and Kristen Grauman. 2009.
Fast similarity search for learned metrics. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, pages 2143-2157.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical programming, 45(1): 503-528.
Omid Madani, Michael Connor and Wiley Greiner.
2009. Learning when concepts abound. The Journal
of Machine Learning Research, 10:2571-2613.
Gurmeet Singh Manku, Arvind Jain and Anish Das
Sarma. 2007. Detecting near-duplicates for web
crawling. In Proceedings of the 16th international
conference on World Wide Web, pages 141-150.
Christopher D. Manning, Prabhakar Raghavan and Hin-
rich Sch¨utze. 2002. An introduction to information
retrieval. Spring.
Yadong Mu, Jialie Shen and Shuicheng Yan. 2010.
Weakly-Supervised Hashing in Kernel Space. In Pro-
ceedings of International Conference on Computer
Vision and Pattern Recognition, pages 3344-3351.
Ruslan Salakhutdinov and Geoffrey Hintona. 2007.
Semantic hashing. In SIGIR workshop on Information
Retrieval and applications of Graphical Models.
Bernhard Sch¨olkopf, Alexander Smola and Klaus-Robert
M¨uller. 1997. Kernel principal component analysis.
Advances in Kernel Methods - Support Vector Learn-
ing, pages 583-588. MIT.
Lloyd N. Trefethen and David Bau. 1997. Numerical
linear algebra. Society for Industrial Mathematics.
Xiaojun Wan, Jianwu Yang and Jianguo Xiao. 2008.
Towards a unified approach to document similarity
search using manifold-ranking of blocks. Information
Processing &amp; Management, 44(3):1032-1048.
Jun Wang, Sanjiv Kumar and Shih-Fu Chang. 2010a.
Semi-Supervised Hashing for Scalable Image Re-
trieval. In Proceedings of International Conference
on Computer Vision and Pattern Recognition, pages
3424-3431.
Jun Wang, Sanjiv Kumar and Shih-Fu Chang. 2010b.
Sequential Projection Learning for Hashing with
Compact Codes. In Proceedings of International
Conference on Machine Learning.
Yair Weiss, Antonio Torralba and Rob Fergus. 2009.
Spectral hashing. In Proceedings of Advances in Neu-
ral Information Processing Systems.
</reference>
<page confidence="0.935256">
101
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant confidence="0.463633" no="0">
<title confidence="0.950672">Semi-Supervised SimHash for Efficient Document Similarity Search</title>
<author confidence="0.768744">Qixia Jiang</author>
<author confidence="0.768744">Maosong</author>
<affiliation confidence="0.830616">State Key Laboratory on Intelligent Technology and Tsinghua National Laboratory for Information Science and Department of Computer Sci. and Tech., Tsinghua University, Beijing 100084,</affiliation>
<email confidence="0.819308">qixia.jiang@gmail.com,sms@tsinghua.edu.cn</email>
<abstract confidence="0.999462272727273">Searching documents that are similar to a query document is an important component in modern information retrieval. Some existing hashing methods can be used for efficient document similarity search. However, unsupervised hashing methods cannot incorporate prior knowledge for better hashing. Although some supervised hashing methods can derive effective hash functions from prior knowledge, they are either computationally expensive or poorly discriminative. This paper proposes a novel (semi-)supervised hashing method named Semi-Supervised SimHash for high-dimensional data similarity The basic idea of is to learn the optimal feature weights from prior knowledge to relocate the data such that similar data have similar hash codes. We evaluate our method with several state-of-the-art methods on two large datasets. All the results show that our method gets the best performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Christopher J C Burges</author>
</authors>
<title>A tutorial on support vector machines for pattern recognition.</title>
<date>1998</date>
<journal>Data Mining and Knowledge Discovery,</journal>
<pages>2--2</pages>
<contexts>
<context citStr="Burges, 1998" endWordPosition="1380" position="8662" startWordPosition="1379">f directions obtained by PCA decreases with the decrease of the rank (Jolliffe, 1986). Thus, lower hash functions tend to have smaller entropy and larger empirical errors. 2.4 Others Some other related works should be mentioned. A notable method is Locality Sensitive Hashing (LSH) (Indyk et al., 1998). LSH performs a random linear projection to map similar objects to similar hash codes. However, LSH suffers from the efficiency problem that it tends to generate long codes (Salakhutdinov et al., 2007). LAMP (Mu et al., 2009) considers each hash function as a binary partition problem as in SVMs (Burges, 1998). Spectral Hashing (Weiss et al., 2009) maintains similarity between objects in the reduced Hamming space by minimizing the averaged Hamming distance3 between similar neighbors in the original Euclidean space. However, spectral hashing takes the assumption that data should be distributed uniformly, which is always violated in real-world applications. 3 Semi-Supervised SimHash In this section, we present our hashing method, named Semi-Supervised SimHash (S3H). Let XL = {(x1, c1) ... (xu, cu)} be the labeled data, c ∈ {1... C}, x ∈ RM, and XU = {xu+1 ... xN} the unlabeled data. Let X = XL ∪ XU. </context>
</contexts>
<marker>Burges, 1998</marker>
<rawString>Christopher J.C. Burges. 1998. A tutorial on support vector machines for pattern recognition. Data Mining and Knowledge Discovery, 2(2):121-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moses S Charikar</author>
</authors>
<title>Similarity estimation techniques from rounding algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 34th annual ACM symposium on Theory of computing,</booktitle>
<pages>380--388</pages>
<marker>Charikar, 2002</marker>
<rawString>Moses S. Charikar. 2002. Similarity estimation techniques from rounding algorithms. In Proceedings of the 34th annual ACM symposium on Theory of computing, pages 380-388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gianni Costa</author>
<author>Giuseppe Manco</author>
<author>Riccardo Ortale</author>
</authors>
<title>An incremental clustering scheme for data deduplication. Data Mining and Knowledge Discovery,</title>
<date>2010</date>
<pages>20--1</pages>
<contexts>
<context citStr="Costa et al., 2010" endWordPosition="409" position="2672" startWordPosition="406">he explosion of information has brought great challenges to traditional methods. For example, Inverted List (IL) which is a primary key-term access method would return a very large set of docs for a query document, which leads to the time-consuming post-processing. Therefore, a new effective algorithm is required. Hashing methods can perform highly efficient but approximate similarity search, and have gained great success in many applications such as Content-Based Image Retrieval (CBIR) (Ke et al., 2004; Kulis et al., 2009b), near-duplicate data detection (Ke et al., 2004; Manku et al., 2007; Costa et al., 2010), etc. Hashing methods project high-dimensional objects to compact binary codes called fingerprints and make similar fingerprints for similar objects. The similarity search in the Hamming space' is much more efficient than in the original attribute space (Manku et al., 2007). Recently, several hashing methods have been proposed. Specifically, SimHash (SH) (Charikar M.S., 2002) uses random projections to hash data. Although it works well with long fingerprints, SH has poor discrimination power for short fingerprints. A kernelized variant of SH, called Kernelized Locality Sensitive Hashing (KLSH</context>
</contexts>
<marker>Costa, Manco, Ortale, 2010</marker>
<rawString>Gianni Costa, Giuseppe Manco and Riccardo Ortale. 2010. An incremental clustering scheme for data deduplication. Data Mining and Knowledge Discovery, 20(1):152-187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Dean</author>
<author>Monika R Henzinge</author>
</authors>
<title>Finding Related Pages in the World Wide Web. Computer Networks,</title>
<date>1999</date>
<pages>31--1467</pages>
<marker>Dean, Henzinge, 1999</marker>
<rawString>Jeffrey Dean and Monika R. Henzinge. 1999. Finding Related Pages in the World Wide Web. Computer Networks, 31:1467-1479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard O Duda</author>
<author>Peter E Hart</author>
<author>David G Stork</author>
</authors>
<date>2000</date>
<note>Pattern classification, 2nd edition. WileyInterscience.</note>
<contexts>
<context citStr="Duda et al., 2000" endWordPosition="2737" position="16032" startWordPosition="2734">he relation between S3H and some existing methods. For analysis simplicity, we assume ψ(t) = t and ignore the regularization terms. So, Equation (8) can be rewritten as follows: L J(w)S3H =1wT[� l=1 where Φ+ ij equals to 1 when (xi, xj) E Oa otherwise 0, Φ−ij equals to 1 when (xi, xj) E Or otherwise 0, and Fl = [g1,l ... gu,l, du+1,l ... dN,l]. We denote El Fl4;+FTl and El Fl4;−FT l asS+ and S− respectively. Therefore, maximizing above function is equivalent to maximizing the following: J(w)S3H = |wT S+w |(12) |wT S−w| Clearly, Equation (12) is analogous to Linear Discriminant Analysis (LDA) (Duda et al., 2000) except for the difference: 1) measurement. S3H uses similarity while LDA uses distance. As a result, the objective function of S3H is just the reciprocal of LDA’s. 2) embedding space. LDA seeks the best separative direction in the original attribute space. In contrast, S3H firstly maps data from RM to RMxL through the following projection function O(x) = x · [diag(sign(r1)), ... , diag(sign(rL))] (13) where rl E RM, l = 1, ... , L, are L random hyperplanes. Then, in that space (RMxL), S3H seeks a directions that can best separate the data. From this point of view, it is obvious that the basic</context>
<context citStr="Duda et al., 2000" endWordPosition="2972" position="17290" startWordPosition="2969">et to e = [1, 1, ... ,1]. That is, SH firstly maps the data via ϕ(·) just as S3H. But then, SH directly separates the data in that feature space at the direction e. Analogously, we ignore the regularization terms in SSH and rewrite the objective of SSH as: J(W)SSH = 2tr[WTX(Φ+ − Φ−)XTW] (14) 1 where W = [w1, ... , wL] E RMxL are L hyperplanes and X = [x1, ... , xN]. Maximizing this objective is equivalent to maximizing the following: J(W)SSH = |tr[WTS′+W] |(15) |tr[WT S′−W]| where S′+ = X4;+XT and S′− = X4;−XT. Equation (15) shows that SSH is analogous to Multiple Discriminant Analysis (MDA) (Duda et al., 2000). In fact, SSH uses top L best-separative hyperplanes in the original attribute space found via PCA to hash the data. Furthermore, we rewrite the projection function ϕ(·) in S3H as: O(x) = x · [R1, ... , RL] (16) where Rl = diag(sign(rl)). Each Rl is a mapping from RM to RM and corresponds to one embedding space. From this perspective, unlike SSH, S3H globally seeks a direction that can best separate the data in L different embedding spaces simultaneously. 4 Experiments We use two datasets 20 Newsgroups and Open Directory Project (ODP) in our experiments. Each document is represented as a vect</context>
</contexts>
<marker>Duda, Hart, Stork, 2000</marker>
<rawString>Richard O. Duda, Peter E. Hart and David G. Stork. 2000. Pattern classification, 2nd edition. WileyInterscience.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Forman</author>
</authors>
<title>An extensive empirical study of feature selection metrics for text classification.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1289</pages>
<contexts>
<context citStr="Forman, 2003" endWordPosition="3618" position="21046" startWordPosition="3617">he averaged intra- and inter- class Hamming distance for various hashing methods. Intuitively, a good hashing method should have small intra-class distance while large inter-class distance. We test all the methods on a PC with a 2.66 GHz processor and 12GB RAM. All experiments repeate 10 times and the averaged results are reported. 4.1 20 Newsgroups 20 Newsgroups contains 20K messages, about 1K messages from each of 20 different newsgroups. The entire vocabulary includes 62,061 words. To evaluate the performance for different feature dimensions, we use Chi-squared feature selection algorithm (Forman, 2003) to select 10K and 30K features. The averaged message length is 54.1 for 10K features and 116.2 for 30K features. We randomly select 4K massages as the test set and the remain 16K as the training set. To train SSH and S3H, from the training set, we randomly generate 40K message-pairs as Oa and 80K message-pairs as Or. For hash ranking, Figure 1 shows MAP for various methods using different number of bits. It shows that performance of SSH decreases with the growing of hash bits. This is mainly because the variance of the directions obtained by PCA decreases with the decrease of their ranks. Thu</context>
</contexts>
<marker>Forman, 2003</marker>
<rawString>George Forman 2003. An extensive empirical study of feature selection metrics for text classification. The Journal of Machine Learning Research, 3:1289-1305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piotr Indyk</author>
<author>Rajeev Motwani</author>
</authors>
<title>Approximate nearest neighbors: towards removing the curse of dimensionality.</title>
<date>1998</date>
<booktitle>In Proceedings of the 30th annual ACM symposium on Theory of computing,</booktitle>
<pages>604--613</pages>
<marker>Indyk, Motwani, 1998</marker>
<rawString>Piotr Indyk and Rajeev Motwani. 1998. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the 30th annual ACM symposium on Theory of computing, pages 604-613.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Jolliffe</author>
</authors>
<title>Principal Component Analysis.</title>
<date>1986</date>
<publisher>Springer-Verlag,</publisher>
<location>New York.</location>
<contexts>
<context citStr="Jolliffe, 1986" endWordPosition="606" position="4021" startWordPosition="605">or knowledge for better hashing. Moti'Hamming space is a set of binary strings of length L. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 93–101, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics vated by this, some supervised methods are proposed to derive effective hash functions from prior knowledge, i.e., Spectral Hashing (Weiss et al., 2009) and Semi-Supervised Hashing (SSH) (Wang et al., 2010a). Regardless of different objectives, both methods derive hash functions via Principle Component Analysis (PCA) (Jolliffe, 1986). However, PCA is computationally expensive, which limits their usage for high-dimensional data. This paper proposes a novel (semi-)supervised hashing method, Semi-Supervised SimHash (S3H), for high-dimensional data similarity search. Unlike SSH that tries to find a sequence of hash functions, S3H fixes the random projection directions and seeks the optimal feature weights from prior knowledge to relocate the objects such that similar objects have similar fingerprints. This is implemented by maximizing the empirical accuracy on the prior knowledge (labeled data) and the entropy of hash functio</context>
<context citStr="Jolliffe, 1986" endWordPosition="1293" position="8134" startWordPosition="1291"> and dissimilar object-pairs is also required in SSH. SSH tries to find L optimal hash functions which have maximum 94 empirical accuracy on prior knowledge and maximum entropy by finding the top L eigenvectors of an extended covariance matrix2 via PCA or SVD. However, despite of the potential problems of numerical stability, SVD requires massive computational space and O(M3) computational time where M is feature dimension, which limits its usage for high-dimensional data (Trefethen et al., 1997). Furthermore, the variance of directions obtained by PCA decreases with the decrease of the rank (Jolliffe, 1986). Thus, lower hash functions tend to have smaller entropy and larger empirical errors. 2.4 Others Some other related works should be mentioned. A notable method is Locality Sensitive Hashing (LSH) (Indyk et al., 1998). LSH performs a random linear projection to map similar objects to similar hash codes. However, LSH suffers from the efficiency problem that it tends to generate long codes (Salakhutdinov et al., 2007). LAMP (Mu et al., 2009) considers each hash function as a binary partition problem as in SVMs (Burges, 1998). Spectral Hashing (Weiss et al., 2009) maintains similarity between obj</context>
</contexts>
<marker>Jolliffe, 1986</marker>
<rawString>Ian Jolliffe. 1986. Principal Component Analysis. Springer-Verlag, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yan Ke</author>
<author>Rahul Sukthankar</author>
<author>Larry Huston</author>
</authors>
<title>Efficient near-duplicate detection and sub-image retrieval.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACM International Conference on Multimedia.</booktitle>
<contexts>
<context citStr="Ke et al., 2004" endWordPosition="390" position="2561" startWordPosition="387">bmit a page or a document to the search engines as the description of their information needs. Meanwhile, the explosion of information has brought great challenges to traditional methods. For example, Inverted List (IL) which is a primary key-term access method would return a very large set of docs for a query document, which leads to the time-consuming post-processing. Therefore, a new effective algorithm is required. Hashing methods can perform highly efficient but approximate similarity search, and have gained great success in many applications such as Content-Based Image Retrieval (CBIR) (Ke et al., 2004; Kulis et al., 2009b), near-duplicate data detection (Ke et al., 2004; Manku et al., 2007; Costa et al., 2010), etc. Hashing methods project high-dimensional objects to compact binary codes called fingerprints and make similar fingerprints for similar objects. The similarity search in the Hamming space' is much more efficient than in the original attribute space (Manku et al., 2007). Recently, several hashing methods have been proposed. Specifically, SimHash (SH) (Charikar M.S., 2002) uses random projections to hash data. Although it works well with long fingerprints, SH has poor discriminati</context>
</contexts>
<marker>Ke, Sukthankar, Huston, 2004</marker>
<rawString>Yan Ke, Rahul Sukthankar and Larry Huston. 2004. Efficient near-duplicate detection and sub-image retrieval. In Proceedings of the ACM International Conference on Multimedia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Kulis</author>
<author>Kristen Grauman</author>
</authors>
<title>Kernelized locality-sensitive hashing for scalable image search.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th International Conference on Computer Vision,</booktitle>
<pages>2130--2137</pages>
<marker>Kulis, Grauman, 2009</marker>
<rawString>Brian Kulis and Kristen Grauman. 2009. Kernelized locality-sensitive hashing for scalable image search. In Proceedings of the 12th International Conference on Computer Vision, pages 2130-2137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Kulis</author>
<author>Prateek Jain</author>
<author>Kristen Grauman</author>
</authors>
<title>Fast similarity search for learned metrics.</title>
<date>2009</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>2143--2157</pages>
<contexts>
<context citStr="Kulis et al., 2009" endWordPosition="394" position="2581" startWordPosition="391">document to the search engines as the description of their information needs. Meanwhile, the explosion of information has brought great challenges to traditional methods. For example, Inverted List (IL) which is a primary key-term access method would return a very large set of docs for a query document, which leads to the time-consuming post-processing. Therefore, a new effective algorithm is required. Hashing methods can perform highly efficient but approximate similarity search, and have gained great success in many applications such as Content-Based Image Retrieval (CBIR) (Ke et al., 2004; Kulis et al., 2009b), near-duplicate data detection (Ke et al., 2004; Manku et al., 2007; Costa et al., 2010), etc. Hashing methods project high-dimensional objects to compact binary codes called fingerprints and make similar fingerprints for similar objects. The similarity search in the Hamming space' is much more efficient than in the original attribute space (Manku et al., 2007). Recently, several hashing methods have been proposed. Specifically, SimHash (SH) (Charikar M.S., 2002) uses random projections to hash data. Although it works well with long fingerprints, SH has poor discrimination power for short f</context>
<context citStr="Kulis et al., 2009" endWordPosition="1024" position="6501" startWordPosition="1021">1 SimHash SimHash (SH) is first proposed by Charikar (Charikar M.S., 2002). SH uses random projections as hash functions, i.e., T r +1, if wT x ≥ 0 h(x) = sign(w x) = 5l (1) −1, otherwise where w E R' is a vector randomly generated. SH specifies the distribution on a family of hash functions H = {h} such that for two objects xi and xj, θ(xi,xj) Pr {h(xi) = h(xj)} = 1 − (2) h∈Hπ where B(xi, xj) is the angle between xi and xj. Obviously, SH is an unsupervised hashing method. 2.2 Kernelized Locality Sensitive Hashing A kernelized variant of SH, named Kernelized Locality Sensitive Hashing (KLSH) (Kulis et al., 2009a), is proposed for non-linearly separable data. KLSH approximates the underling Gaussian distribution in the implicit embedding space of data based on central limit theory. To calculate the value of hashing fuction h(·), KLSH projects points onto the eigenvectors of the kernel matrix. In short, the complete procedure of KLSH can be summarized as follows: 1) randomly select P (a small value) points from X and form the kernel matrix, 2) for each hash function h(O(x)), calculate its weight w E RP just as Kernel PCA (Sch¨olkopf et al., 1997), and 3) the hash function is defined as: P h(O(x)) = si</context>
</contexts>
<marker>Kulis, Jain, Grauman, 2009</marker>
<rawString>Brian Kulis, Prateek Jain and Kristen Grauman. 2009. Fast similarity search for learned metrics. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 2143-2157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<journal>Mathematical programming,</journal>
<volume>45</volume>
<issue>1</issue>
<pages>503--528</pages>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C. Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical programming, 45(1): 503-528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omid Madani</author>
<author>Michael Connor</author>
<author>Wiley Greiner</author>
</authors>
<title>Learning when concepts abound.</title>
<date>2009</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>10--2571</pages>
<contexts>
<context citStr="Madani et al., 2009" endWordPosition="1799" position="10999" startWordPosition="1796">s a diagonal matrix which, to some extent, reflects the distance from x to these hyperplanes. 3.2 Formulation Hashing maps the data set X to an L-dimensional Hamming space for compact representations. If we represent each object as Equation (4), the l-th hash function is then defined as: hl(x) = hl(D) = sign(wTdl) (5) where w ∈ RM is the feature weight to be determined and dl is the l-th column of the matrix D. Intuitively, the ”contribution” of a specific feature to different classes is different. Therefore, we hope to incorporate this side information in S3H for better hashing. Inspired by (Madani et al., 2009), we can measure this contribution over XL as in Algorithm 1. Clearly, if objects are represented as the occurrence numbers of features, the output of Algorithm 1 is just the conditional probability Pr(class|feature). Finally, each object (x, c) ∈ XL can be represented as an M × L matrix G: G = diag(v1,c, v2,c,..., vM,c) · D (6) Note that, one pair (xi, xj) in Oa or Or corresponds to (Gi, Gj) while (Di, Dj) if we ignore features’ contribution to different classes. Furthermore, we also hope to maximize the empirical accuracy on the labeled data Oa and Or and 95 Algorithm 1: Feature Contribution</context>
</contexts>
<marker>Madani, Connor, Greiner, 2009</marker>
<rawString>Omid Madani, Michael Connor and Wiley Greiner. 2009. Learning when concepts abound. The Journal of Machine Learning Research, 10:2571-2613.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gurmeet Singh Manku</author>
</authors>
<title>Arvind Jain and Anish Das Sarma.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th international conference on World Wide Web,</booktitle>
<pages>141--150</pages>
<marker>Manku, 2007</marker>
<rawString>Gurmeet Singh Manku, Arvind Jain and Anish Das Sarma. 2007. Detecting near-duplicates for web crawling. In Proceedings of the 16th international conference on World Wide Web, pages 141-150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>An introduction to information retrieval.</title>
<date>2002</date>
<publisher>Spring.</publisher>
<marker>Manning, Raghavan, Sch¨utze, 2002</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan and Hinrich Sch¨utze. 2002. An introduction to information retrieval. Spring.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yadong Mu</author>
<author>Jialie Shen</author>
<author>Shuicheng Yan</author>
</authors>
<title>Weakly-Supervised Hashing in Kernel Space.</title>
<date>2010</date>
<booktitle>In Proceedings of International Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>3344--3351</pages>
<marker>Mu, Shen, Yan, 2010</marker>
<rawString>Yadong Mu, Jialie Shen and Shuicheng Yan. 2010. Weakly-Supervised Hashing in Kernel Space. In Proceedings of International Conference on Computer Vision and Pattern Recognition, pages 3344-3351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Salakhutdinov</author>
<author>Geoffrey Hintona</author>
</authors>
<title>Semantic hashing.</title>
<date>2007</date>
<booktitle>In SIGIR workshop on Information Retrieval and applications of Graphical Models.</booktitle>
<marker>Salakhutdinov, Hintona, 2007</marker>
<rawString>Ruslan Salakhutdinov and Geoffrey Hintona. 2007. Semantic hashing. In SIGIR workshop on Information Retrieval and applications of Graphical Models.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernhard Sch¨olkopf</author>
<author>Alexander Smola</author>
<author>Klaus-Robert M¨uller</author>
</authors>
<title>Kernel principal component analysis.</title>
<date>1997</date>
<booktitle>Advances in Kernel Methods - Support Vector Learning,</booktitle>
<pages>583--588</pages>
<publisher>MIT.</publisher>
<marker>Sch¨olkopf, Smola, M¨uller, 1997</marker>
<rawString>Bernhard Sch¨olkopf, Alexander Smola and Klaus-Robert M¨uller. 1997. Kernel principal component analysis. Advances in Kernel Methods - Support Vector Learning, pages 583-588. MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lloyd N Trefethen</author>
<author>David Bau</author>
</authors>
<title>Numerical linear algebra. Society for Industrial Mathematics.</title>
<date>1997</date>
<marker>Trefethen, Bau, 1997</marker>
<rawString>Lloyd N. Trefethen and David Bau. 1997. Numerical linear algebra. Society for Industrial Mathematics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianwu Yang</author>
<author>Jianguo Xiao</author>
</authors>
<title>Towards a unified approach to document similarity search using manifold-ranking of blocks.</title>
<date>2008</date>
<booktitle>Information Processing &amp; Management,</booktitle>
<pages>44--3</pages>
<contexts>
<context citStr="Wan et al., 2008" endWordPosition="223" position="1531" startWordPosition="220">high-dimensional data similarity search. The basic idea of S3H is to learn the optimal feature weights from prior knowledge to relocate the data such that similar data have similar hash codes. We evaluate our method with several state-of-the-art methods on two large datasets. All the results show that our method gets the best performance. 1 Introduction Document Similarity Search (DSS) is to find similar documents to a query doc in a text corpus or on the web. It is an important component in modern information retrieval since DSS can improve the traditional search engines and user experience (Wan et al., 2008; Dean et al., 1999). Traditional search engines accept several terms submitted by a user as a query and return a set of docs that are relevant to the query. However, for those users who are not search experts, it is always difficult to accurately specify some query terms to express their 93 search purposes. Unlike short-query based search, DSS queries by a full (long) document, which allows users to directly submit a page or a document to the search engines as the description of their information needs. Meanwhile, the explosion of information has brought great challenges to traditional method</context>
</contexts>
<marker>Wan, Yang, Xiao, 2008</marker>
<rawString>Xiaojun Wan, Jianwu Yang and Jianguo Xiao. 2008. Towards a unified approach to document similarity search using manifold-ranking of blocks. Information Processing &amp; Management, 44(3):1032-1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Wang</author>
<author>Sanjiv Kumar</author>
<author>Shih-Fu Chang</author>
</authors>
<title>Semi-Supervised Hashing for Scalable Image Retrieval.</title>
<date>2010</date>
<booktitle>In Proceedings of International Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>3424--3431</pages>
<contexts>
<context citStr="Wang et al., 2010" endWordPosition="590" position="3891" startWordPosition="587">(Kulis et al., 2009a), is proposed to handle non-linearly separable data. These methods are unsupervised thus cannot incorporate prior knowledge for better hashing. Moti'Hamming space is a set of binary strings of length L. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 93–101, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics vated by this, some supervised methods are proposed to derive effective hash functions from prior knowledge, i.e., Spectral Hashing (Weiss et al., 2009) and Semi-Supervised Hashing (SSH) (Wang et al., 2010a). Regardless of different objectives, both methods derive hash functions via Principle Component Analysis (PCA) (Jolliffe, 1986). However, PCA is computationally expensive, which limits their usage for high-dimensional data. This paper proposes a novel (semi-)supervised hashing method, Semi-Supervised SimHash (S3H), for high-dimensional data similarity search. Unlike SSH that tries to find a sequence of hash functions, S3H fixes the random projection directions and seeks the optimal feature weights from prior knowledge to relocate the objects such that similar objects have similar fingerprin</context>
<context citStr="Wang et al., 2010" endWordPosition="1175" position="7395" startWordPosition="1172">l matrix. In short, the complete procedure of KLSH can be summarized as follows: 1) randomly select P (a small value) points from X and form the kernel matrix, 2) for each hash function h(O(x)), calculate its weight w E RP just as Kernel PCA (Sch¨olkopf et al., 1997), and 3) the hash function is defined as: P h(O(x)) = sign( wi · K(x, xi)) (3) i=1 where K(·, ·) can be any kernel function. KLSH can improve hashing results via the kernel trick. However, KLSH is unsupervised, thus designing a data-specific kernel remains a big challenge. 2.3 Semi-Supervised Hashing Semi-Supervised Hashing (SSH) (Wang et al., 2010a) is recently proposed to incorporate prior knowledge for better hashing. Besides X, prior knowledge in the form of similar and dissimilar object-pairs is also required in SSH. SSH tries to find L optimal hash functions which have maximum 94 empirical accuracy on prior knowledge and maximum entropy by finding the top L eigenvectors of an extended covariance matrix2 via PCA or SVD. However, despite of the potential problems of numerical stability, SVD requires massive computational space and O(M3) computational time where M is feature dimension, which limits its usage for high-dimensional data</context>
<context citStr="Wang et al., 2010" endWordPosition="2032" position="12186" startWordPosition="2029">orithm 1: Feature Contribution Calculation for each (x, c) E XL do for each f E x do vf — vf + xf; vf,c — vf,c + xf; end end for each feature f and class c do νf, C ; νf end maximize the entropy of hash functions. So, we define the following objective for h(·)s: J(w) = (Xi, Xj) ∈ ®r N V ∑ hl (xi) hl (xj) p 1= (Xi,Xj)∈Oa (7) hl(xi)hl(xj)} + A1∑H(hl) L 1 l=1 L where Np = |Oa |+ |Or |is the number of attraction and repulsion pairs and A1 is a tradeoff between two terms. Wang et al. have proven that hash functions with maximum entropy must maximize the variance of the hash values, and vice-versa (Wang et al., 2010b). Thus, H(h(·)) can be estimated over the labeled and unlabeled data, XL and XU. Unfortunately, direct solution for above problem is non-trivial since Equation (7) is not differentiable. Thus, we relax the objective and add an additional regularization term which could effectively avoid overfitting. Finally, we obtain the total objective: L(w) = { ψ(wTgi,l)ψ(wT gj,l) Np l=1 (Gi,Gj)Eea 1 ∑ ∑ L (8) A2 − 2 ∥w∥2 2 where gi,l and di,l denote the l-th column of Gi and Di respectively, and 0(t) is a piece-wise linear function defined as: T9 t &gt; T9 t −T9 ≤ t ≤ T9 (9) −T9 t &lt; −T9 This relaxation has </context>
<context citStr="Wang et al., 2010" endWordPosition="3380" position="19619" startWordPosition="3377">e adopt the RBF kernel rz(xZ7 xj) = exp(− ∥Xi 62j ∥2 ), where the scaling factor S2 takes 0.5 and the other two parameters p and t are set to be 500 and 50 respectively. The parameter A in SSH is set to 1. For S3H, we simply set the parameters A1 and A2 in Equation (8) to 4 and 0.5 respectively. To objectively reflect the performance of S3H, we evaluate our S3H with and without Feature Contribution Calculation algorithm (FCC) (Algorithm 1). Specifically, FCC-free S3H (denoted as S3H f) is just a simplification when Gs in S3H are simply set to Ds. For quantitative evaluation, as in literature (Wang et al., 2010b; Mu et al., 2009), we calculate the precision under two scenarios: hash lookup and hash ranking. For hash lookup, the proportion of good neighbors (have the same class label as the query) among the searched objects within a given Hamming radius is calculated as precision. Similarly to (Wang et al., 2010b; Weiss et al., 2009), for a query document, if no neighbors within the given Hamming radius can be found, it is considered as zero precision. Note that, the precision of IL is the proportion of good neighbors among the whole searched objects. For hash ranking, all the objects in X are ranked</context>
</contexts>
<marker>Wang, Kumar, Chang, 2010</marker>
<rawString>Jun Wang, Sanjiv Kumar and Shih-Fu Chang. 2010a. Semi-Supervised Hashing for Scalable Image Retrieval. In Proceedings of International Conference on Computer Vision and Pattern Recognition, pages 3424-3431.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jun Wang</author>
</authors>
<title>Sanjiv Kumar and Shih-Fu Chang. 2010b. Sequential Projection Learning for Hashing with Compact Codes.</title>
<booktitle>In Proceedings of International Conference on Machine Learning.</booktitle>
<marker>Wang, </marker>
<rawString>Jun Wang, Sanjiv Kumar and Shih-Fu Chang. 2010b. Sequential Projection Learning for Hashing with Compact Codes. In Proceedings of International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yair Weiss</author>
<author>Antonio Torralba</author>
<author>Rob Fergus</author>
</authors>
<title>Spectral hashing.</title>
<date>2009</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context citStr="Weiss et al., 2009" endWordPosition="582" position="3838" startWordPosition="579">H, called Kernelized Locality Sensitive Hashing (KLSH) (Kulis et al., 2009a), is proposed to handle non-linearly separable data. These methods are unsupervised thus cannot incorporate prior knowledge for better hashing. Moti'Hamming space is a set of binary strings of length L. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 93–101, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics vated by this, some supervised methods are proposed to derive effective hash functions from prior knowledge, i.e., Spectral Hashing (Weiss et al., 2009) and Semi-Supervised Hashing (SSH) (Wang et al., 2010a). Regardless of different objectives, both methods derive hash functions via Principle Component Analysis (PCA) (Jolliffe, 1986). However, PCA is computationally expensive, which limits their usage for high-dimensional data. This paper proposes a novel (semi-)supervised hashing method, Semi-Supervised SimHash (S3H), for high-dimensional data similarity search. Unlike SSH that tries to find a sequence of hash functions, S3H fixes the random projection directions and seeks the optimal feature weights from prior knowledge to relocate the obje</context>
<context citStr="Weiss et al., 2009" endWordPosition="1387" position="8701" startWordPosition="1384">eases with the decrease of the rank (Jolliffe, 1986). Thus, lower hash functions tend to have smaller entropy and larger empirical errors. 2.4 Others Some other related works should be mentioned. A notable method is Locality Sensitive Hashing (LSH) (Indyk et al., 1998). LSH performs a random linear projection to map similar objects to similar hash codes. However, LSH suffers from the efficiency problem that it tends to generate long codes (Salakhutdinov et al., 2007). LAMP (Mu et al., 2009) considers each hash function as a binary partition problem as in SVMs (Burges, 1998). Spectral Hashing (Weiss et al., 2009) maintains similarity between objects in the reduced Hamming space by minimizing the averaged Hamming distance3 between similar neighbors in the original Euclidean space. However, spectral hashing takes the assumption that data should be distributed uniformly, which is always violated in real-world applications. 3 Semi-Supervised SimHash In this section, we present our hashing method, named Semi-Supervised SimHash (S3H). Let XL = {(x1, c1) ... (xu, cu)} be the labeled data, c ∈ {1... C}, x ∈ RM, and XU = {xu+1 ... xN} the unlabeled data. Let X = XL ∪ XU. Given the labeled data XL, we construct</context>
<context citStr="Weiss et al., 2009" endWordPosition="3436" position="19947" startWordPosition="3433">rformance of S3H, we evaluate our S3H with and without Feature Contribution Calculation algorithm (FCC) (Algorithm 1). Specifically, FCC-free S3H (denoted as S3H f) is just a simplification when Gs in S3H are simply set to Ds. For quantitative evaluation, as in literature (Wang et al., 2010b; Mu et al., 2009), we calculate the precision under two scenarios: hash lookup and hash ranking. For hash lookup, the proportion of good neighbors (have the same class label as the query) among the searched objects within a given Hamming radius is calculated as precision. Similarly to (Wang et al., 2010b; Weiss et al., 2009), for a query document, if no neighbors within the given Hamming radius can be found, it is considered as zero precision. Note that, the precision of IL is the proportion of good neighbors among the whole searched objects. For hash ranking, all the objects in X are ranked in terms of their Hamming distance from the query document, and the top K nearest neighbors are returned as the result. Then, Mean Averaged Precision (MAP) (Manning et al., 2002) is calculated. We also calculate the averaged intra- and inter- class Hamming distance for various hashing methods. Intuitively, a good hashing meth</context>
</contexts>
<marker>Weiss, Torralba, Fergus, 2009</marker>
<rawString>Yair Weiss, Antonio Torralba and Rob Fergus. 2009. Spectral hashing. In Proceedings of Advances in Neural Information Processing Systems.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>