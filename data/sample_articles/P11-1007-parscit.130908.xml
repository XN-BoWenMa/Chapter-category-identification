<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant confidence="0.005692" no="0">
<title confidence="0.9989975">
Domain Adaptation by Constraining Inter-Domain Variability
of Latent Feature Representation
</title>
<author confidence="0.997168">
Ivan Titov
</author>
<affiliation confidence="0.772304">
Saarland University
Saarbruecken, Germany
</affiliation>
<email confidence="0.989443">
titov@mmci.uni-saarland.de
</email>
<sectionHeader confidence="0.998554" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999924304347826">We consider a semi-supervised setting for domain adaptation where only unlabeled data is available for the target domain. One way to tackle this problem is to train a generative model with latent variables on the mixture of data from the source and target domains. Such a model would cluster features in both domains and ensure that at least some of the latent variables are predictive of the label on the source domain. The danger is that these predictive clusters will consist of features specific to the source domain only and, consequently, a classifier relying on such clusters would perform badly on the target domain. We introduce a constraint enforcing that marginal distributions of each cluster (i.e., each latent variable) do not vary significantly across domains. We show that this constraint is effective on the sentiment classification task (Pang et al., 2002), resulting in scores similar to the ones obtained by the structural correspondence methods (Blitzer et al., 2007) without the need to engineer auxiliary tasks.</bodyText>
<sectionHeader confidence="0.999469" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999746418604651">Supervised learning methods have become a standard tool in natural language processing, and large training sets have been annotated for a wide variety of tasks. However, most learning algorithms operate under assumption that the learning data originates from the same distribution as the test data, though in practice this assumption is often violated. This difference in the data distributions normally results in a significant drop in accuracy. To address this problem a number of domain-adaptation methods has recently been proposed (see e.g., (Daum´e and Marcu, 2006; Blitzer et al., 2006; Bickel et al., 2007)). In addition to the labeled data from the source domain, they also exploit small amounts of labeled data and/or unlabeled data from the target domain to estimate a more predictive model for the target domain. In this paper we focus on a more challenging and arguably more realistic version of the domainadaptation problem where only unlabeled data is available for the target domain. One of the most promising research directions on domain adaptation for this setting is based on the idea of inducing a shared feature representation (Blitzer et al., 2006), that is mapping from the initial feature representation to a new representation such that (1) examples from both domains ‘look similar’ and (2) an accurate classifier can be trained in this new representation. Blitzer et al. (2006) use auxiliary tasks based on unlabeled data for both domains (called pivot features) and a dimensionality reduction technique to induce such shared representation. The success of their domain-adaptation method (Structural Correspondence Learning, SCL) crucially depends on the choice of the auxiliary tasks, and defining them can be a non-trivial engineering problem for many NLP tasks (Plank, 2009). In this paper, we investigate methods which do not use auxiliary tasks to induce a shared feature representation. We use generative latent variable models (LVMs) learned on all the available data: unlabeled data for both domains and on the labeled data for the source domain. Our LVMs use vectors of latent features to represent examples.</bodyText>
<page confidence="0.990876">
62
</page>
<note confidence="0.979503">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 62–71,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999601625">The latent variables encode regularities observed on unlabeled data from both domains, and they are learned to be predictive of the labels on the source domain. Such LVMs can be regarded as composed of two parts: a mapping from initial (normally, word-based) representation to a new shared distributed representation, and also a classifier in this representation. The danger of this semi-supervised approach in the domain-adaptation setting is that some of the latent variables will correspond to clusters of features specific only to the source domain, and consequently, the classifier relying on this latent variable will be badly affected when tested on the target domain. Intuitively, one would want the model to induce only those features which generalize between domains. We encode this intuition by introducing a term in the learning objective which regularizes inter-domain difference in marginal distributions of each latent variable. Another, though conceptually similar, argument for our method is coming from theoretical results which postulate that the drop in accuracy of an adapted classifier is dependent on the discrepancy distance between the source and target domains (Blitzer et al., 2008; Mansour et al., 2009; Ben-David et al., 2010). Roughly, the discrepancy distance is small when linear classifiers cannot distinguish between examples from different domains. A necessary condition for this is that the feature expectations do not vary significantly across domains. Therefore, our approach can be regarded as minimizing a coarse approximation of the discrepancy distance. The introduced term regularizes model expectations and it can be viewed as a form of a generalized expectation (GE) criterion (Mann and McCallum, 2010). Unlike the standard GE criterion, where a model designer defines the prior for a model expectation, our criterion postulates that the model expectations should be similar across domains. In our experiments, we use a form of Harmonium Model (Smolensky, 1986) with a single layer of binary latent variables. Though exact inference with this class of models is infeasible we use an efficient approximation (Bengio and Delalleau, 2007), which can be regarded either as a mean-field approximation to the reconstruction error or a deterministic version of the Contrastive Divergence sampling method (Hinton, 2002). Though such an estimator is biased, in practice, it yields accurate models. We explain how the introduced regularizer can be integrated into the stochastic gradient descent learning algorithm for our model. We evaluate our approach on adapting sentiment classifiers on 4 domains: books, DVDs, electronics and kitchen appliances (Blitzer et al., 2007). The loss due to transfer to a new domain is very significant for this task: in our experiments it was approaching 9%, in average, for the non-adapted model. Our regularized model achieves 35% average relative error reduction with respect to the nonadapted classifier, whereas the non-regularized version demonstrates a considerably smaller reduction of 26%. Both the achieved error reduction and the absolute score match the results reported in (Blitzer et al., 2007) for the best version1 of the SCL method (SCL-MI, 36%), suggesting that our approach is a viable alternative to SCL. The rest of the paper is structured as follows. In Section 2 we introduce a model which uses vectors of latent variables to model statistical dependencies between the elementary features. In Section 3 we discuss its applicability in the domain-adaptation setting, and introduce constraints on inter-domain variability as a way to address the discovered limitations. Section 4 describes approximate learning and inference algorithms used in our experiments. In Section 5 we provide an empirical evaluation of the proposed method. We conclude in Section 6 with further examination of the related work.</bodyText>
<sectionHeader confidence="0.986354" genericHeader="method">
2 The Latent Variable Model
</sectionHeader>
<bodyText confidence="0.999955454545455">The adaptation method advocated in this paper is applicable to any joint probabilistic model which uses distributed representations, i.e. vectors of latent variables, to abstract away from hand-crafted features. These models, for example, include Restricted Boltzmann Machines (Smolensky, 1986; Hinton, 2002) and Sigmoid Belief Networks (SBNs) (Saul et al., 1996) for classification and regression tasks, Factorial HMMs (Ghahramani and Jordan, 1997) for sequence labeling problems, Incremental SBNs for parsing problems (Titov and Henderson, 2007a), as well as different types of Deep Belief Networks (Hinton and Salakhutdinov, 2006).</bodyText>
<footnote confidence="0.9774885">
1Among the versions which do not exploit labeled data from
the target domain.
</footnote>
<page confidence="0.999602">
63
</page>
<bodyText confidence="0.99944515625">The power of these methods is in their ability to automatically construct new features from elementary ones provided by the model designer. This feature induction capability is especially desirable for problems where engineering features is a labor-intensive process (e.g., multilingual syntactic parsing (Titov and Henderson, 2007b)), or for multitask learning problems where the nature of interactions between the tasks is not fully understood (Collobert and Weston, 2008; Gesmundo et al., 2009). In this paper we consider classification tasks, namely prediction of sentiment polarity of a user review (Pang et al., 2002), and model the joint distribution of the binary sentiment label y ∈ {0, 1} and the multiset of text features x, xi ∈ X. The hidden variable vector z (zi ∈ {0, 1}, i = 1, ... , m) encodes statistical dependencies between components of x and also dependencies between the label y and the features x. Intuitively, the model can be regarded as a logistic regression classifier with latent features. The model assumes that the features and the latent variable vector are generated jointly from a globallynormalized model and then the label y is generated from a conditional distribution dependent on z. Both of these distributions, P(x, z) and P(y|z), are parameterized as log-linear models and, consequently, our model can be seen as a combination of an undirected Harmonium model (Smolensky, 1986) and a directed SBN model (Saul et al., 1996). The formal definition is as follows:</bodyText>
<listItem confidence="0.9992925">(1) Draw (x, z) ∼ P(x, z|v), (2) Draw label y ∼ Q(w0 + P'i�1 wizi), where v and w are parameters, Q is the logistic sigmoid function, Q(t) = 1/(1 + e−t), and the joint distribution of (x, z) is given by the Gibbs distribution:</listItem>
<equation confidence="0.83306">
P(x, z|v) ∝ exp(
</equation>
<bodyText confidence="0.960042666666667">Figure 1 presents the corresponding graphical model. Note that the arcs between x and z are undirected, whereas arcs between y and z are directed. The parameters of this model 0 = (v, w) can be estimated by maximizing joint likelihood L(0) of labeled data for the source domain {x(l), y(l)}lESL fnd unlabeled data for the source and target domain lx(l)}lESUUTU, where SU and TU stand for the unlabeled datasets for the source and target domains, respectively.</bodyText>
<figureCaption confidence="0.99433375">
Figure 1: The latent variable model: x, z, y are random
variables, dependencies between x and z are parameter-
ized by matrix v, and dependencies between z and y - by
vector w.
</figureCaption>
<bodyText confidence="0.9970144375">However, given that, first, amount of unlabeled data |SU ∪ TU |normally vastly exceeds the amount of labeled data |SL |and, second, the number of features for each example |x(l) |is usually large, the label y will have only a minor effect on the mapping from the initial features x to the latent representation z (i.e.on the parameters v). Consequently, the latent representation induced in this way is likely to be inappropriate for the classification task in question. Therefore, we follow (McCallum et al., 2006) and use a multi-conditional objective, a specific form of hybrid learning, to emphasize the importance of labels y:</bodyText>
<equation confidence="0.944457">
L(0, α)=α X log P(y(l)|x(l), 0)+ X log P(x(l)|0),
lESL lESUUTUUSL
</equation>
<bodyText confidence="0.989917">where α is a weight, α &gt; 1. Direct maximization of the objective is problematic, as it would require summation over all the 2m latent vectors z. Instead we use a meanfield approximation. Similarly, an efficient approximate inference algorithm is used to compute arg maxy P(y|x, 0) at testing time. The approximations are described in Section 4.</bodyText>
<sectionHeader confidence="0.964095" genericHeader="method">
3 Constraints on Inter-Domain Variability
</sectionHeader>
<bodyText confidence="0.999813571428571">As we discussed in the introduction, our goal is to provide a method for domain adaptation based on semi-supervised learning of models with distributed representations. In this section, we first discuss the shortcomings of domain adaptation with the above-described semi-supervised approach and motivate constraints on inter-domain variability of tation z, and the input distributions for both domains PS(z) and PT (z), and is defined as the induced shared representation.</bodyText>
<figure confidence="0.9764192">
...
...
|x |vxj0+ Xn v0izi+ |x|,n X vxjizi).
X i=1 j,i=1
j=1
</figure>
<page confidence="0.991206">
64
</page>
<bodyText confidence="0.9983688">Then we propose a specific form of this constraint based on the Kullback-Leibler (KL) divergence.</bodyText>
<subsectionHeader confidence="0.995772">
3.1 Motivation for the Constraints
</subsectionHeader>
<bodyText confidence="0.999978372093023">Each latent variable zi encodes a cluster or a combination of elementary features xj. At least some of these clusters, when induced by maximizing the likelihood L(θ, α) with sufficiently large α, will be useful for the classification task on the source domain. However, when the domains are substantially different, these predictive clusters are likely to be specific only to the source domain. For example, consider moving from reviews of electronics to book reviews: the cluster of features related to equipment reliability and warranty service will not generalize to books. The corresponding latent variable will always be inactive on the books domain (or always active, if negative correlation is induced during learning). Equivalently, the marginal distribution of this variable will be very different for both domains. Note that the classifier, defined by the vector w, is only trained on the labeled source examples {x(l), y(l)Jl∈SL and therefore it will rely on such latent variables, even though they do not generalize to the target domain. Clearly, the accuracy of such classifier will drop when it is applied to target domain examples. To tackle this issue, we introduce a regularizing term which penalizes differences in the marginal distributions between the domains. In fact, we do not need to consider the behavior of the classifier to understand the rationale behind the introduction of the regularizer. Intuitively, when adapting between domains, we are interested in representations z which explain domain-independent regularities rather than in modeling inter-domain differences. The regularizer favors models which focus on the former type of phenomena rather than the latter. Another motivation for the form of regularization we propose originates from theoretical analysis of the domain adaptation problems (Ben-David et al., 2010; Mansour et al., 2009; Blitzer et al., 2007). Under the assumption that there exists a domainindependent scoring function, these analyses show that the drop in accuracy is upper-bounded by the quantity called discrepancy distance. The discrepancy distance is dependent on the feature represenwhere f and f0 are arbitrary linear classifiers in the feature representation z.</bodyText>
<equation confidence="0.909948">
dz(S,T)=max |EPS[f(z)�=f0(z)]−EPT [f(z)�=f0(z)]|,
�,��
</equation>
<bodyText confidence="0.999971454545455">The quantity EP[f(z)7�f0(z)] measures the probability mass assigned to examples where f and f0 disagree. Then the discrepancy distance is the maximal change in the size of this disagreement set due to transfer between the domains. For a more restricted class of classifiers which rely only on any single feature2 zi, the distance is equal to the maximum over the change in the distributions P(zi). Consequently, for arbitrary linear classifiers we have:</bodyText>
<equation confidence="0.978513">
|EPS[zi = 1] − EPT [zi = 1]|.
</equation>
<bodyText confidence="0.999935866666667">It follows that low inter-domain variability of the marginal distributions of latent variables is a necessary condition for low discrepancy distance. Minimizing the difference in the marginal distributions can be regarded as a coarse approximation to the minimization of the distance. However, we have to concede that the above argument is fairly informal, as the generalization bounds do not directly apply to our case: (1) our feature representation is learned from the same data as the classifier, (2) we cannot guarantee that the existence of a domainindependent scoring function is preserved under the learned transformation x—*z and (3) in our setting we have access not only to samples from P(z|x, θ) but also to the distribution itself.</bodyText>
<subsectionHeader confidence="0.999697">
3.2 The Expectation Criterion
</subsectionHeader>
<bodyText confidence="0.9996213">Though the above argument suggests a specific form of the regularizing term, we believe that the penalizer should not be very sensitive to small differences in the marginal distributions, as useful variables (clusters) are likely to have somewhat different marginal distributions in different domains, but it should severely penalize extreme differences. To achieve this goal we instead propose to use the symmetrized Kullback-Leibler (KL) divergence between the marginal distributions as the penalty. The derivative of the symmetrized KL divergence is large when one of the marginal distributions is concentrated at 0 or 1 with another distribution still having high entropy, and therefore such configurations are severely penalized.3 Formally, the regularizer G(θ) is defined as</bodyText>
<footnote confidence="0.568526">
2We consider only binary features here.
</footnote>
<equation confidence="0.93855">
dz(S,T) &gt; max
i=1,...,m
</equation>
<page confidence="0.981358">
65
</page>
<equation confidence="0.9925055">
m
G(θ) = D(PS(zi|θ)||PT(zi|θ))
i=1
+D(PT(zi|θ)||PS(zi|θ)), (1)
</equation>
<bodyText confidence="0.999856">where PS(zi) and PT(zi) stand for the training sample estimates of the marginal distributions of latent features, for instance:</bodyText>
<equation confidence="0.996191">
1 PT(zi = 1|θ) =
 |TU |� P(zi = 1|x(l), θ).
lETU
</equation>
<bodyText confidence="0.999936666666667">We augment the multi-conditional log-likelihood L(θ, α) with the weighted regularization term G(θ) to get the composite objective function:</bodyText>
<equation confidence="0.824676">
LR(θ, α, β) = L(θ, α) − βG(θ), β &gt; 0.
</equation>
<bodyText confidence="0.991798269230769">Note that this regularization term can be regarded as a form of the generalized expectation (GE) criteria (Mann and McCallum, 2010), where GE criteria are normally defined as KL divergences between a prior expectation of some feature and the expectation of this feature given by the model, where the prior expectation is provided by the model designer as a form of weak supervision. In our case, both expectations are provided by the model but on different domains. Note that the proposed regularizer can be trivially extended to support the multi-domain case (Mansour et al., 2008) by considering symmetrized KL divergences for every pair of domains or regularizing the distributions for every domain towards their average. More powerful regularization terms can also be motivated by minimization of the discrepancy distance but their optimization is likely to be expensive, whereas LR(θ, α, β) can be optimized efficiently. 3An alternative is to use the Jensen-Shannon (JS) divergence, however, our preliminary experiments seem to suggest that the symmetrized KL divergence is preferable. Though the two divergences are virtually equivalent when the distributions are very similar (their ratio tends to a constant as the distributions go closer), the symmetrized KL divergence stronger penalizes extreme differences and this is important for our purposes.</bodyText>
<sectionHeader confidence="0.940469" genericHeader="method">
4 Learning and Inference
</sectionHeader>
<bodyText confidence="0.999984294117647">In this section we describe an approximate learning algorithm based on the mean-field approximation. Though we believe that our approach is independent of the specific learning algorithm, we provide the description for completeness. We also describe a simple approximate algorithm for computing P(y|x, θ) at test time. The stochastic gradient descent algorithm iterates over examples and updates the weight vector based on the contribution of every considered example to the objective function LR(θ, α, β). To compute these updates we need to approximate gradients of Vθ log P(y(l)|x(l), θ) (l E SL), Vθ log P(x(l)|θ) (l E SL U SU U TU) as well as to estimate the contribution of a given example to the gradient of the regularizer VθG(θ). In the next sections we will describe how each of these terms can be estimated.</bodyText>
<subsectionHeader confidence="0.977073">
4.1 Conditional Likelihood Term
</subsectionHeader>
<bodyText confidence="0.989221">We start by explaining the mean-field approximation of log P(y|x, θ). First, we compute the means µ = (µ1, ... , µm):</bodyText>
<equation confidence="0.9968055">
µi = P(zi = 1|x, v) = σ(v0i + E���
j=1 vx-i).
</equation>
<bodyText confidence="0.998441">Now we can substitute them instead of z to approximate the conditional probability of the label:</bodyText>
<equation confidence="0.998356">
P(y = 1|x, θ) =Ez P(y|z, w)P(z|x, v)
a σ(w0 + Emi=1 wiµi).
</equation>
<bodyText confidence="0.999956916666667">We use this estimate both at testing time and also to compute gradients Vθ log P(y(l)|x(l), θ) during learning. The gradients can be computed efficiently using a form of back-propagation. Note that with this approximation, we do not need to normalize over the feature space, which makes the model very efficient at classification time. This approximation is equivalent to the computation of the two-layer perceptron with the soft-max activation function (Bishop, 1995). However, the above derivation provides a probabilistic interpretation of the hidden layer.</bodyText>
<subsectionHeader confidence="0.959307">
4.2 Unlabeled Likelihood Term
</subsectionHeader>
<bodyText confidence="0.7770095">In this section, we describe how the unlabeled likelihood term is optimized in our stochastic learning algorithm.</bodyText>
<page confidence="0.961273">
66
</page>
<bodyText confidence="0.946527333333333">First, we note that, given the directed nature of the arcs between z and y, the weights w do not affect the probability of input x, that is</bodyText>
<equation confidence="0.737707">
P(x|θ) = P(x|v).
</equation>
<bodyText confidence="0.999833333333333">Instead of directly approximating the gradient ∇v log P(x(l)|v), we use a deterministic version of the Contrastive Divergence (CD) algorithm, equivalent to the mean-field approximation of the reconstruction error used in training autoassociaters (Bengio and Delalleau, 2007). The CD-based estimators are biased estimators but are guaranteed to converge. Intuitively, maximizing the likelihood of unlabeled data is closely related to minimizing the reconstruction error, that is training a model to discover such mapping parameters u that z encodes all the necessary information to accurately reproduce x(l) from z for every training example x(l). Formally, the meanfield approximation to the negated reconstruction error is defined as where the means, µi = P(zi = 1|x(l), v), are computed as in the preceding section.</bodyText>
<equation confidence="0.790731">
L(x(l), v) = log P(x(l)|µ, v),
</equation>
<bodyText confidence="0.978299583333333">Note that when computing the gradient of ∇v L, we need to take into account both the forward and backward mappings: the computation of the means µ from x(l) and the computation of the log-probability of x(l) given the means µ: dL ∂ L� ∂ L� dµi = + dvki ∂vki ∂µi .dvki</bodyText>
<subsectionHeader confidence="0.998807">
4.3 Regularization Term
</subsectionHeader>
<bodyText confidence="0.999834125">The criterion G(θ) is also independent of the classifier parameters w, i.e. G(θ) = G(v), and our goal is to compute the contribution of a considered example l to the gradient ∇vG(v). The regularizer G(v) is defined as in equation (1) and it is a function of the sample-based domainspecific marginal distributions of latent variables PS and PT:</bodyText>
<equation confidence="0.9977255">
PT(zi = 1|θ) =  |1 TU|
l� µZl),
</equation>
<bodyText confidence="0.9979235">where the means µ(l) i= P(zi = 1|x(l), v); PS can be re-written analogously. G(v) is dependent on the parameters v only via the mean activations of the latent variables µ(l), and contribution of each example l can be computed by straightforward differentiation:</bodyText>
<equation confidence="0.9972415">
1 − p
−log 1 − p0
1 − p0 dµ(l)
+ 1 − p) dvki
</equation>
<bodyText confidence="0.993484363636364">where p = PS(zi = 1|θ) and p0 = PT(zi = 1|θ) if l is from the source domain, and, inversely, p = PT(zi = 1|θ) and p0 = PS(zi = 1|θ), otherwise. One problem with the above expression is that the exact computation of PS and PT requires recomputation of the means µ(l) for all the examples after each update of the parameters, resulting in O(|SL ∪ SU ∪ TU|2) complexity of each iteration of stochastic gradient descent. Instead, we shuffle examples and use amortization; we approximate PS at update t by:</bodyText>
<equation confidence="0.9277545">
�
(t) (1−γ) P( Vi=1)+γµi, l∈SL∪ SU
PS (zi = 1) =t−1
Ps )(zi = 1), otherwise,
</equation>
<bodyText confidence="0.9977475">where l is an example considered at update t. The approximation PT is computed analogously.</bodyText>
<sectionHeader confidence="0.996847" genericHeader="evaluation and result">
5 Empirical Evaluation
</sectionHeader>
<bodyText confidence="0.9999642">In this section we empirically evaluate our approach on the sentiment classification task. We start with the description of the experimental set-up and the baselines, then we present the results and discuss the utility of the constraint on inter-domain variability.</bodyText>
<subsectionHeader confidence="0.985557">
5.1 Experimental setting
</subsectionHeader>
<bodyText confidence="0.999955384615385">To evaluate our approach, we consider the same dataset as the one used to evaluate the SCL method (Blitzer et al., 2007). The dataset is composed of labeled and unlabeled reviews of four different product types: books, DVDs, electronics and kitchen appliances. For each domain, the dataset contains 1,000 labeled positive reviews and 1,000 labeled negative reviews, as well as several thousands of unlabeled examples (4,919 reviews per domain in average: ranging from 3,685 for DVDs to 5,945 for kitchen appliances). As in Blitzer et al. (2007), we randomly split each labelled portion into 1,600 examples for training and 400 examples for testing.</bodyText>
<equation confidence="0.777616714285714">
dG(l)(v)
dvki
=(log p
p0
p0
−
p
</equation>
<page confidence="0.878413">
67
</page>
<figure confidence="0.991350024390244">
85
80
75
70
72.7
70.8
75.6
74.7
76.5
83.3
73.3
74.6
74.8
76.2
75.4
82.8
77.6
74.6
76.0
78.9
80.2
85.8
79.0
77.7
83.2
82.1
80.0
86.5
75.6
73.9
76.6
77.9
78.8
84.6
Base
NoReg
Reg
NoReg+
Reg+
In-domain
Books DVD Electronics Kitchen Average
</figure>
<figureCaption confidence="0.998266">
Figure 2: Averages accuracies when transferring to books, DVD, electronics and kitchen appliances domains, and
average accuracy over all 12 domain pairs.
</figureCaption>
<bodyText confidence="0.99997320338983">We evaluate the performance of our domainadaptation approach on every ordered pair of domains. For every pair, the semi-supervised methods use labeled data from the source domain and unlabeled data from both domains. We compare them with two supervised methods: a supervised model (Base) which is trained on the source domain data only, and another supervised model (Indomain) which is learned on the labeled data from the target domain. The Base model can be regarded as a natural baseline model, whereas the In-domain model is essentially an upper-bound for any domainadaptation method. All the methods, supervised and semi-supervised, are based on the model described in Section 2. Instead of using the full set of bigram and unigram counts as features (Blitzer et al., 2007), we use a frequency cut-off of 30 to remove infrequent ngrams. This does not seem to have an adverse effect on the accuracy but makes learning very efficient: the average training time for the semi-supervised methods was about 20 minutes on a standard PC. We coarsely tuned the parameters of the learning methods using a form of cross-validation. Both the parameter of the multi-conditional objective α (see Section 2) and the weighting for the constraint Q (see Section 3.2) were set to 5. We used 25 iterations of stochastic gradient descent. The initial learning rate and the weight decay (the inverse squared variance of the Gaussian prior) were set to 0.01, and both parameters were reduced by the factor of 2 every iteration the objective function estimate went down. The size of the latent representation was equal to 10. The stochastic weight updates were amortized with the momentum ('y) of 0.99. We trained the model both without regularization of the domain variability (NoReg, Q = 0), and with the regularizing term (Reg). For the SCL method to produce an accurate classifier for the target domain it is necessary to train a classifier using both the induced shared representation and the initial nontransformed representation. In our case, due to joint learning and non-convexity of the learning problem, this approach would be problematic.4 Instead, we combine predictions of the semi-supervised models Reg and NoReg with the baseline out-of-domain model (Base) using the product-of-experts combination (Hinton, 2002), the corresponding methods are called Reg+ and NoReg+, respectively. In all our models, we augmented the vector z with an additional component set to 0 for examples in the source domain and to 1 for the target domain examples. In this way, we essentially subtracted a unigram domain-specific model from our latent variable model in the hope that this will further reduce the domain dependence of the rest of the model parameters. In preliminary experiments, this modification was beneficial for all the models including the non-constrained one (NoReg).</bodyText>
<subsectionHeader confidence="0.970495">
5.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.993610333333333">The results of all the methods are presented in Figure 2. The 4 leftmost groups of results correspond to a single target domain, and therefore each of them is an average over experiments on 3 domainpairs, for instance, the group Books represents an average over adaptation experiments DVDs-*books, electronics-*books, kitchen-*books.</bodyText>
<footnote confidence="0.995505">
4The latent variables are not likely to learn any useful map-
ping in the presence of observable features. Special training
regimes may be used to attempt to circumvent this problem.
</footnote>
<page confidence="0.999171">
68
</page>
<bodyText confidence="0.999805708333333">The rightmost group of the results corresponds to the average over all 12 experiments. First, observe that the total drop in the accuracy when moving to the target domain is 8.9%: from 84.6% demonstrated by the In-domain classifier to 75.6% shown by the non-adapted Base classifier. For convenience, we also present the errors due to transfer in a separate Table 1: our best method (Reg+) achieves 35% relative reduction of this loss, decreasing the gap to 5.7%. Now, let us turn to the question of the utility of the constraints. First, observe that the non-regularized version of the model (NoReg) often fails to outperform the baseline and achieves the scores considerably worse than the results of the regularized version (2.6% absolute difference). We believe that this happens because the clusters induced when optimizing the non-regularized learning objective are often domain-specific. The regularized model demonstrates substantially better results slightly beating the baseline in most cases. Still, to achieve a larger decrease of the domain-adaptation error, it was necessary to use the combined models, Reg+ and NoReg+. Here, again, the regularized model substantially outperforms the non-regularized one (35% against 26% relative error reduction for Reg+ and NoReg+, respectively). In Table 1, we also compare the results of our method with the results of the best version of the SCL method (SCL-MI) reported in Blitzer et al.(2007). The average error reductions for our method Reg+ and for the SCL method are virtually equal. However, formally, these two numbers are not directly comparable. First, the random splits are different, though this is unlikely to result in any significant difference, as the split proportions are the same and the test sets are sufficiently large. Second, the absolute scores achieved in Blitzer et al.(2007) are slightly worse than those demonstrated in our experiments both for supervised and semi-supervised methods. In absolute terms, our Reg+ method outperforms the SCL method by more than 1%: 75.6% against 74.5%, in average. This is probably due to the difference in the used learning methods: optimization of the Huber loss vs.</bodyText>
<table confidence="0.999144166666667">
D Base NoReg Reg NoReg+ Reg+ SCL-MI
B 10.6 12.4 7.7 8.6 6.7 5.8
D 9.5 8.2 8.0 6.6 7.3 6.1
E 8.2 13.0 9.7 6.8 5.5 5.5
K 7.5 8.8 6.5 4.4 3.3 5.6
Av 8.9 10.6 8.0 6.6 5.7 5.8
</table>
<tableCaption confidence="0.933364666666667">
Table 1: Drop in the accuracy score due to the transfer
for the 4 domains: (B)ooks, (D)VD, (E)electronics and
(K)itchen appliances, and in average over the domains.
</tableCaption>
<bodyText confidence="0.999947578947369">our latent variable model.5 This comparison suggests that our domain-adaptation method is a viable alternative to SCL. Also, it is important to point out that the SCL method uses auxiliary tasks to induce the shared feature representation, these tasks are constructed on the basis of unlabeled data. The auxiliary tasks and the original problem should be closely related, namely they should have the same (or similar) set of predictive features. Defining such tasks can be a challenging engineering problem. On the sentiment classification task in order to construct them two steps need to be performed: (1) a set of words correlated with the sentiment label is selected, and, then (2) prediction of each such word is regarded a distinct auxiliary problem. For many other domains (e.g., parsing (Plank, 2009)) the construction of an effective set of auxiliary tasks is still an open problem.</bodyText>
<sectionHeader confidence="0.999966" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999911461538462">There is a growing body of work on domain adaptation. In this paper, we focus on the class of methods which induce a shared feature representation. Another popular class of domain-adaptation techniques assume that the input distributions P(x) for the source and the target domain share support, that is every example x which has a non-zero probability on the target domain must have also a non-zero probability on the source domain, and vice-versa. Such methods tackle domain adaptation by instance re-weighting (Bickel et al., 2007; Jiang and Zhai, 2007), or, similarly, by feature re-weighting (Satpal and Sarawagi, 2007). In NLP, most features are word-based and lexicons are very different for different domains, therefore such assumptions are likely to be overly restrictive.</bodyText>
<footnote confidence="0.99869375">
5The drop in accuracy for the SCL method in Table 1 is is
computed with respect to the less accurate supervised in-domain
classifier considered in Blitzer et al. (2007), otherwise, the com-
puted drop would be larger.
</footnote>
<page confidence="0.999067">
69
</page>
<bodyText confidence="0.999941189189189">Various semi-supervised techniques for domainadaptation have also been considered, one example being self-training (McClosky et al., 2006). However, their behavior in the domain-adaptation setting is not well-understood. Semi-supervised learning with distributed representations and its application to domain adaptation has previously been considered in (Huang and Yates, 2009), but no attempt has been made to address problems specific to the domain-adaptation setting. Similar approaches has also been considered in the context of topic models (Xue et al., 2008), however the preference towards induction of domain-independent topics was not explicitly encoded in the learning objective or model priors. A closely related method to ours is that of (Druck and McCallum, 2010) which performs semi-supervised learning with posterior regularization (Ganchev et al., 2010). Our approach differs from theirs in many respects. First, they do not focus on the domain-adaptation setting and do not attempt to define constraints to prevent the model from learning domain-specific information. Second, their expectation constraints are estimated from labeled data, whereas we are trying to match expectations computed on unlabeled data for two domains. This approach bears some similarity to the adaptation methods standard for the setting where labelled data is available for both domains (Chelba and Acero, 2004; Daum´e and Marcu, 2006). However, instead of ensuring that the classifier parameters are similar across domains, we favor models resulting in similar marginal distributions of latent variables.</bodyText>
<sectionHeader confidence="0.99812" genericHeader="other">
7 Discussion and Conclusions
</sectionHeader>
<bodyText confidence="0.99928147368421">In this paper we presented a domain-adaptation method based on semi-supervised learning with distributed representations coupled with constraints favoring domain-independence of modeled phenomena. Our approach results in competitive domainadaptation performance on the sentiment classification task, rivalling that of the state-of-the-art SCL method (Blitzer et al., 2007). Both of these methods induce a shared feature representation but unlike SCL our method does not require construction of any auxiliary tasks in order to induce this representation. The primary area of the future work is to apply our method to structured prediction problems in NLP, such as syntactic parsing or semantic role labeling, where construction of auxiliary tasks proved problematic. Another direction is to favor domaininvariability not only of the expectations of individual variables but rather those of constraint functions involving latent variables, features and labels.</bodyText>
<sectionHeader confidence="0.9964" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999078">The author acknowledges the support of the Cluster of Excellence on Multimodal Computing and Interaction at Saarland University and thanks the anonymous reviewers for their helpful comments and suggestions.</bodyText>
<sectionHeader confidence="0.999399" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99940265625">
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2010. A theory of learning from different
domains. Machine Learning, 79:151–175.
Yoshua Bengio and Olivier Delalleau. 2007. Justify-
ing and generalizing contrastive divergence. Techni-
cal Report TR 1311, Department IRO, University of
Montreal, November.
S. Bickel, M. Br¨ueckner, and T. Scheffer. 2007. Dis-
criminative learning for differing training and test dis-
tributions. In Proc. of the International Conference on
Machine Learning (ICML), pages 81–88.
Christopher M. Bishop. 1995. Neural Networks for Pat-
tern Recognition. Oxford University Press, Oxford,
UK.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proc. of EMNLP.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
Proc. 45th Meeting of Association for Computational
Linguistics (ACL), Prague, Czech Republic.
John Blitzer, Koby Crammer, Alex Kulesza, Fernando
Pereira, and Jennifer Wortman. 2008. Learning
bounds for domain adaptation. In Proc. Advances In
Neural Information Processing Systems (NIPS ’07).
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy capitalizer: Little data can help a
lot. In Proc. of the Conference on Empirical Meth-
ods for Natural Language Processing (EMNLP), pages
285–292.
</reference>
<page confidence="0.967244">
70
</page>
<reference confidence="0.999563804123712">
R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing: Deep neural networks
with multitask learning. In International Conference
on Machine Learning, ICML.
Hal Daum´e and Daniel Marcu. 2006. Domain adaptation
for statistical classifiers. Journal of Artificial Intelli-
gence, 26:101–126.
Gregory Druck and Andrew McCallum. 2010. High-
performance semi-supervised learning using discrim-
inatively constrained generative models. In Proc. of
the International Conference on Machine Learning
(ICML), Haifa, Israel.
Kuzman Ganchev, Joao Graca, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. Journal of Machine
Learning Research (JMLR), pages 2001–2049.
Andrea Gesmundo, James Henderson, Paola Merlo, and
Ivan Titov. 2009. Latent variable model of syn-
chronous syntactic-semantic parsing for multiple lan-
guages. In CoNLL 2009 Shared Task.
Zoubin Ghahramani and Michael I. Jordan. 1997. Fac-
torial hidden Markov models. Machine Learning,
29:245–273.
G. E. Hinton and R. R. Salakhutdinov. 2006. Reducing
the dimensionality of data with neural networks. Sci-
ence, 313:504–507.
Geoffrey E. Hinton. 2002. Training Products of Experts
by Minimizing Contrastive Divergence. Neural Com-
putation, 14:1771–1800.
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised se-
quence labeling. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in nlp. In Proc. of the
Annual Meeting of the ACL, pages 264–271, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Gideon S. Mann and Andrew McCallum. 2010. General-
ized expectation criteria for semi-supervised learning
with weakly labeled data. Journal of Machine Learn-
ing Research, 11:955–984.
Yishay Mansour, Mehryar Mohri, and Afshin Ros-
tamizadeh. 2008. Domain adaptation with multiple
sources. In Advances in Neural Information Process-
ing Systems.
Yishay Mansour, Mehryar Mohri, and Afshin Ros-
tamizadeh. 2009. Domain adaptation: Learning
bounds and algorithms. In Proceedings of The 22nd
Annual Conference on Learning Theory (COLT 2009),
Montreal, Canada.
Andrew McCallum, Chris Pal, Greg Druck, and Xuerui
Wang. 2006. Multi-conditional learning: Genera-
tive/discriminative training for clustering and classifi-
cation. In AAAI.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In Proc. of the Annual Meeting of the ACL and
the International Conference on Computational Lin-
guistics, Sydney, Australia.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning
techniques. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Barbara Plank. 2009. Structural correspondence learning
for parse disambiguation. In Proceedings of the Stu-
dent Research Workshop at EACL 2009, pages 37–45,
Athens, Greece, April. Association for Computational
Linguistics.
Sandeepkumar Satpal and Sunita Sarawagi. 2007. Do-
main adaptation of conditional probability models via
feature subsetting. In Proceedings of 11th European
Conference on Principles and Practice of Knowledge
Discovery in Databases (PKDD), Warzaw, Poland.
Lawrence K. Saul, Tommi Jaakkola, and Michael I. Jor-
dan. 1996. Mean field theory for sigmoid belief
networks. Journal of Artificial Intelligence Research,
4:61–76.
Paul Smolensky. 1986. Information processing in dy-
namical systems: foundations of harmony theory. In
D. Rumehart and J McCelland, editors, Parallel dis-
tributed processing: explorations in the microstruc-
tures of cognition, volume 1 : Foundations, pages 194–
281. MIT Press.
Ivan Titov and James Henderson. 2007a. Constituent
parsing with Incremental Sigmoid Belief Networks. In
Proc. 45th Meeting of Association for Computational
Linguistics (ACL), pages 632–639, Prague, Czech Re-
public.
Ivan Titov and James Henderson. 2007b. Fast and robust
multilingual dependency parsing with a generative la-
tent variable model. In Proc. of the CoNLL shared
task, Prague, Czech Republic.
G.-R. Xue, W. Dai, Q. Yang, and Y. Yu. 2008. Topic-
bridged PLSA for cross-domain text classification. In
Proceedings of the SIGIR Conference.
</reference>
<page confidence="0.999131">
71
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant confidence="0.404734" no="0">
<title confidence="0.999395">Domain Adaptation by Constraining Inter-Domain of Latent Feature Representation</title>
<author confidence="0.985776">Ivan</author>
<affiliation confidence="0.870668">Saarland</affiliation>
<address confidence="0.438237">Saarbruecken,</address>
<email confidence="0.994966">titov@mmci.uni-saarland.de</email>
<abstract confidence="0.999640791666667">We consider a semi-supervised setting for domain adaptation where only unlabeled data is available for the target domain. One way to tackle this problem is to train a generative model with latent variables on the mixture of data from the source and target domains. Such a model would cluster features in both domains and ensure that at least some of the latent variables are predictive of the label on the source domain. The danger is that these predictive clusters will consist of features specific to the source domain only and, consequently, a classifier relying on such clusters would perform badly on the target domain. We introduce a constraint enforcing that marginal distributions of each cluster (i.e., each latent variable) do not vary significantly across domains. We show that this constraint is effective on the sentiment classification task (Pang et al., 2002), resulting in scores similar to the ones obtained by the structural correspondence methods (Blitzer et al., 2007) without the need to engineer auxiliary tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shai Ben-David</author>
<author>John Blitzer</author>
<author>Koby Crammer</author>
<author>Alex Kulesza</author>
<author>Fernando Pereira</author>
<author>Jennifer Wortman Vaughan</author>
</authors>
<title>A theory of learning from different domains.</title>
<date>2010</date>
<booktitle>Machine Learning,</booktitle>
<pages>79--151</pages>
<contexts>
<context citStr="Ben-David et al., 2010" endWordPosition="760" position="4821" startWordPosition="757"> affected when tested on the target domain. Intuitively, one would want the model to induce only those features which generalize between domains. We encode this intuition by introducing a term in the learning objective which regularizes inter-domain difference in marginal distributions of each latent variable. Another, though conceptually similar, argument for our method is coming from theoretical results which postulate that the drop in accuracy of an adapted classifier is dependent on the discrepancy distance between the source and target domains (Blitzer et al., 2008; Mansour et al., 2009; Ben-David et al., 2010). Roughly, the discrepancy distance is small when linear classifiers cannot distinguish between examples from different domains. A necessary condition for this is that the feature expectations do not vary significantly across domains. Therefore, our approach can be regarded as minimizing a coarse approximation of the discrepancy distance. The introduced term regularizes model expectations and it can be viewed as a form of a generalized expectation (GE) criterion (Mann and McCallum, 2010). Unlike the standard GE criterion, where a model designer defines the prior for a model expectation, our cr</context>
<context citStr="Ben-David et al., 2010" endWordPosition="2282" position="14193" startWordPosition="2279">in the marginal distributions between the domains. In fact, we do not need to consider the behavior of the classifier to understand the rationale behind the introduction of the regularizer. Intuitively, when adapting between domains, we are interested in representations z which explain domain-independent regularities rather than in modeling inter-domain differences. The regularizer favors models which focus on the former type of phenomena rather than the latter. Another motivation for the form of regularization we propose originates from theoretical analysis of the domain adaptation problems (Ben-David et al., 2010; Mansour et al., 2009; Blitzer et al., 2007). Under the assumption that there exists a domainindependent scoring function, these analyses show that the drop in accuracy is upper-bounded by the quantity called discrepancy distance. The discrepancy distance is dependent on the feature represendz(S,T)=max |EPS[f(z)�=f0(z)]−EPT [f(z)�=f0(z)]|, �,�� where f and f0 are arbitrary linear classifiers in the feature representation z. The quantity EP[f(z)7�f0(z)] measures the probability mass assigned to examples where f and f0 disagree. Then the discrepancy distance is the maximal change in the size of</context>
</contexts>
<marker>Ben-David, Blitzer, Crammer, Kulesza, Pereira, Vaughan, 2010</marker>
<rawString>Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. 2010. A theory of learning from different domains. Machine Learning, 79:151–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Olivier Delalleau</author>
</authors>
<title>Justifying and generalizing contrastive divergence.</title>
<date>2007</date>
<tech>Technical Report TR 1311,</tech>
<institution>Department IRO, University of Montreal,</institution>
<contexts>
<context citStr="Bengio and Delalleau, 2007" endWordPosition="908" position="5746" startWordPosition="905">approximation of the discrepancy distance. The introduced term regularizes model expectations and it can be viewed as a form of a generalized expectation (GE) criterion (Mann and McCallum, 2010). Unlike the standard GE criterion, where a model designer defines the prior for a model expectation, our criterion postulates that the model expectations should be similar across domains. In our experiments, we use a form of Harmonium Model (Smolensky, 1986) with a single layer of binary latent variables. Though exact inference with this class of models is infeasible we use an efficient approximation (Bengio and Delalleau, 2007), which can be regarded either as a mean-field approximation to the reconstruction error or a deterministic version of the Contrastive Divergence sampling method (Hinton, 2002). Though such an estimator is biased, in practice, it yields accurate models. We explain how the introduced regularizer can be integrated into the stochastic gradient descent learning algorithm for our model. We evaluate our approach on adapting sentiment classifiers on 4 domains: books, DVDs, electronics and kitchen appliances (Blitzer et al., 2007). The loss due to transfer to a new domain is very significant for this </context>
<context citStr="Bengio and Delalleau, 2007" endWordPosition="3360" position="20839" startWordPosition="3356">robabilistic interpretation of the hidden layer. 4.2 Unlabeled Likelihood Term In this section, we describe how the unlabeled likelihood term is optimized in our stochastic learning 66 algorithm. First, we note that, given the directed nature of the arcs between z and y, the weights w do not affect the probability of input x, that is P(x|θ) = P(x|v). Instead of directly approximating the gradient ∇v log P(x(l)|v), we use a deterministic version of the Contrastive Divergence (CD) algorithm, equivalent to the mean-field approximation of the reconstruction error used in training autoassociaters (Bengio and Delalleau, 2007). The CD-based estimators are biased estimators but are guaranteed to converge. Intuitively, maximizing the likelihood of unlabeled data is closely related to minimizing the reconstruction error, that is training a model to discover such mapping parameters u that z encodes all the necessary information to accurately reproduce x(l) from z for every training example x(l). Formally, the meanfield approximation to the negated reconstruction error is defined as L(x(l), v) = log P(x(l)|µ, v), where the means, µi = P(zi = 1|x(l), v), are computed as in the preceding section. Note that when computing </context>
</contexts>
<marker>Bengio, Delalleau, 2007</marker>
<rawString>Yoshua Bengio and Olivier Delalleau. 2007. Justifying and generalizing contrastive divergence. Technical Report TR 1311, Department IRO, University of Montreal, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bickel</author>
<author>M Br¨ueckner</author>
<author>T Scheffer</author>
</authors>
<title>Discriminative learning for differing training and test distributions.</title>
<date>2007</date>
<booktitle>In Proc. of the International Conference on Machine Learning (ICML),</booktitle>
<pages>81--88</pages>
<marker>Bickel, Br¨ueckner, Scheffer, 2007</marker>
<rawString>S. Bickel, M. Br¨ueckner, and T. Scheffer. 2007. Discriminative learning for differing training and test distributions. In Proc. of the International Conference on Machine Learning (ICML), pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<title>Neural Networks for Pattern Recognition.</title>
<date>1995</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford, UK.</location>
<contexts>
<context citStr="Bishop, 1995" endWordPosition="3253" position="20168" startWordPosition="3252">substitute them instead of z to approximate the conditional probability of the label: P(y = 1|x, θ) =Ez P(y|z, w)P(z|x, v) a σ(w0 + Emi=1 wiµi). We use this estimate both at testing time and also to compute gradients Vθ log P(y(l)|x(l), θ) during learning. The gradients can be computed efficiently using a form of back-propagation. Note that with this approximation, we do not need to normalize over the feature space, which makes the model very efficient at classification time. This approximation is equivalent to the computation of the two-layer perceptron with the soft-max activation function (Bishop, 1995). However, the above derivation provides a probabilistic interpretation of the hidden layer. 4.2 Unlabeled Likelihood Term In this section, we describe how the unlabeled likelihood term is optimized in our stochastic learning 66 algorithm. First, we note that, given the directed nature of the arcs between z and y, the weights w do not affect the probability of input x, that is P(x|θ) = P(x|v). Instead of directly approximating the gradient ∇v log P(x(l)|v), we use a deterministic version of the Contrastive Divergence (CD) algorithm, equivalent to the mean-field approximation of the reconstruct</context>
</contexts>
<marker>Bishop, 1995</marker>
<rawString>Christopher M. Bishop. 1995. Neural Networks for Pattern Recognition. Oxford University Press, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context citStr="Blitzer et al., 2006" endWordPosition="289" position="1823" startWordPosition="286">liary tasks. 1 Introduction Supervised learning methods have become a standard tool in natural language processing, and large training sets have been annotated for a wide variety of tasks. However, most learning algorithms operate under assumption that the learning data originates from the same distribution as the test data, though in practice this assumption is often violated. This difference in the data distributions normally results in a significant drop in accuracy. To address this problem a number of domain-adaptation methods has recently been proposed (see e.g., (Daum´e and Marcu, 2006; Blitzer et al., 2006; Bickel et al., 2007)). In addition to the labeled data from the source domain, they also exploit small amounts of labeled data and/or unlabeled data from the target domain to estimate a more predictive model for the target domain. In this paper we focus on a more challenging and arguably more realistic version of the domainadaptation problem where only unlabeled data is available for the target domain. One of the most promising research directions on domain adaptation for this setting is based on the idea of inducing a shared feature representation (Blitzer et al., 2006), that is mapping fro</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Proc. 45th Meeting of Association for Computational Linguistics (ACL),</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context citStr="Blitzer et al., 2007" endWordPosition="184" position="1169" startWordPosition="181">les are predictive of the label on the source domain. The danger is that these predictive clusters will consist of features specific to the source domain only and, consequently, a classifier relying on such clusters would perform badly on the target domain. We introduce a constraint enforcing that marginal distributions of each cluster (i.e., each latent variable) do not vary significantly across domains. We show that this constraint is effective on the sentiment classification task (Pang et al., 2002), resulting in scores similar to the ones obtained by the structural correspondence methods (Blitzer et al., 2007) without the need to engineer auxiliary tasks. 1 Introduction Supervised learning methods have become a standard tool in natural language processing, and large training sets have been annotated for a wide variety of tasks. However, most learning algorithms operate under assumption that the learning data originates from the same distribution as the test data, though in practice this assumption is often violated. This difference in the data distributions normally results in a significant drop in accuracy. To address this problem a number of domain-adaptation methods has recently been proposed (s</context>
<context citStr="Blitzer et al., 2007" endWordPosition="988" position="6274" startWordPosition="985"> class of models is infeasible we use an efficient approximation (Bengio and Delalleau, 2007), which can be regarded either as a mean-field approximation to the reconstruction error or a deterministic version of the Contrastive Divergence sampling method (Hinton, 2002). Though such an estimator is biased, in practice, it yields accurate models. We explain how the introduced regularizer can be integrated into the stochastic gradient descent learning algorithm for our model. We evaluate our approach on adapting sentiment classifiers on 4 domains: books, DVDs, electronics and kitchen appliances (Blitzer et al., 2007). The loss due to transfer to a new domain is very significant for this task: in our experiments it was approaching 9%, in average, for the non-adapted model. Our regularized model achieves 35% average relative error reduction with respect to the nonadapted classifier, whereas the non-regularized version demonstrates a considerably smaller reduction of 26%. Both the achieved error reduction and the absolute score match the results reported in (Blitzer et al., 2007) for the best version1 of the SCL method (SCL-MI, 36%), suggesting that our approach is a viable alternative to SCL. The rest of th</context>
<context citStr="Blitzer et al., 2007" endWordPosition="2290" position="14238" startWordPosition="2287">ins. In fact, we do not need to consider the behavior of the classifier to understand the rationale behind the introduction of the regularizer. Intuitively, when adapting between domains, we are interested in representations z which explain domain-independent regularities rather than in modeling inter-domain differences. The regularizer favors models which focus on the former type of phenomena rather than the latter. Another motivation for the form of regularization we propose originates from theoretical analysis of the domain adaptation problems (Ben-David et al., 2010; Mansour et al., 2009; Blitzer et al., 2007). Under the assumption that there exists a domainindependent scoring function, these analyses show that the drop in accuracy is upper-bounded by the quantity called discrepancy distance. The discrepancy distance is dependent on the feature represendz(S,T)=max |EPS[f(z)�=f0(z)]−EPT [f(z)�=f0(z)]|, �,�� where f and f0 are arbitrary linear classifiers in the feature representation z. The quantity EP[f(z)7�f0(z)] measures the probability mass assigned to examples where f and f0 disagree. Then the discrepancy distance is the maximal change in the size of this disagreement set due to transfer betwee</context>
<context citStr="Blitzer et al., 2007" endWordPosition="3835" position="23494" startWordPosition="3832">S at update t by: � (t) (1−γ) P( Vi=1)+γµi, l∈SL∪ SU PS (zi = 1) =t−1 Ps )(zi = 1), otherwise, where l is an example considered at update t. The approximation PT is computed analogously. 5 Empirical Evaluation In this section we empirically evaluate our approach on the sentiment classification task. We start with the description of the experimental set-up and the baselines, then we present the results and discuss the utility of the constraint on inter-domain variability. 5.1 Experimental setting To evaluate our approach, we consider the same dataset as the one used to evaluate the SCL method (Blitzer et al., 2007). The dataset is composed of labeled and unlabeled reviews of four different product types: books, DVDs, electronics and kitchen appliances. For each domain, the dataset contains 1,000 labeled positive reviews and 1,000 labeled negative reviews, as well as several thousands of unlabeled examples (4,919 reviews per domain in average: ranging from 3,685 for DVDs to 5,945 for kitchen appliances). As in Blitzer et al. (2007), we randomly split each labelled portion into 1,600 examples for training and 400 examples for testing. dG(l)(v) dvki =(log p p0 p0 − p 67 85 80 75 70 72.7 70.8 75.6 74.7 76.5</context>
<context citStr="Blitzer et al., 2007" endWordPosition="4126" position="25227" startWordPosition="4123">rom the source domain and unlabeled data from both domains. We compare them with two supervised methods: a supervised model (Base) which is trained on the source domain data only, and another supervised model (Indomain) which is learned on the labeled data from the target domain. The Base model can be regarded as a natural baseline model, whereas the In-domain model is essentially an upper-bound for any domainadaptation method. All the methods, supervised and semi-supervised, are based on the model described in Section 2. Instead of using the full set of bigram and unigram counts as features (Blitzer et al., 2007), we use a frequency cut-off of 30 to remove infrequent ngrams. This does not seem to have an adverse effect on the accuracy but makes learning very efficient: the average training time for the semi-supervised methods was about 20 minutes on a standard PC. We coarsely tuned the parameters of the learning methods using a form of cross-validation. Both the parameter of the multi-conditional objective α (see Section 2) and the weighting for the constraint Q (see Section 3.2) were set to 5. We used 25 iterations of stochastic gradient descent. The initial learning rate and the weight decay (the in</context>
<context citStr="Blitzer et al. (2007)" endWordPosition="4791" position="29305" startWordPosition="4788">g the non-regularized learning objective are often domain-specific. The regularized model demonstrates substantially better results slightly beating the baseline in most cases. Still, to achieve a larger decrease of the domain-adaptation error, it was necessary to use the combined models, Reg+ and NoReg+. Here, again, the regularized model substantially outperforms the non-regularized one (35% against 26% relative error reduction for Reg+ and NoReg+, respectively). In Table 1, we also compare the results of our method with the results of the best version of the SCL method (SCL-MI) reported in Blitzer et al. (2007). The average error reductions for our method Reg+ and for the SCL method are virtually equal. However, formally, these two numbers are not directly comparable. First, the random splits are different, though this is unlikely to result in any significant difference, as the split proportions are the same and the test sets are sufficiently large. Second, the absolute scores achieved in Blitzer et al. (2007) are slightly worse than those demonstrated in our experiments both for supervised and semi-supervised methods. In absolute terms, our Reg+ method outperforms the SCL method by more than 1%: 75</context>
<context citStr="Blitzer et al. (2007)" endWordPosition="5266" position="32098" startWordPosition="5263">hniques assume that the input distributions P(x) for the source and the target domain share support, that is every example x which has a non-zero probability on the target domain must have also a non-zero probability on the source domain, and vice-versa. Such methods tackle domain adaptation by instance re-weighting (Bickel et al., 2007; Jiang and Zhai, 2007), or, similarly, by feature re-weighting (Satpal and Sarawagi, 2007). In NLP, most features 5The drop in accuracy for the SCL method in Table 1 is is computed with respect to the less accurate supervised in-domain classifier considered in Blitzer et al. (2007), otherwise, the computed drop would be larger. 69 are word-based and lexicons are very different for different domains, therefore such assumptions are likely to be overly restrictive. Various semi-supervised techniques for domainadaptation have also been considered, one example being self-training (McClosky et al., 2006). However, their behavior in the domain-adaptation setting is not well-understood. Semi-supervised learning with distributed representations and its application to domain adaptation has previously been considered in (Huang and Yates, 2009), but no attempt has been made to addr</context>
<context citStr="Blitzer et al., 2007" endWordPosition="5591" position="34284" startWordPosition="5588"> (Chelba and Acero, 2004; Daum´e and Marcu, 2006). However, instead of ensuring that the classifier parameters are similar across domains, we favor models resulting in similar marginal distributions of latent variables. 7 Discussion and Conclusions In this paper we presented a domain-adaptation method based on semi-supervised learning with distributed representations coupled with constraints favoring domain-independence of modeled phenomena. Our approach results in competitive domainadaptation performance on the sentiment classification task, rivalling that of the state-of-the-art SCL method (Blitzer et al., 2007). Both of these methods induce a shared feature representation but unlike SCL our method does not require construction of any auxiliary tasks in order to induce this representation. The primary area of the future work is to apply our method to structured prediction problems in NLP, such as syntactic parsing or semantic role labeling, where construction of auxiliary tasks proved problematic. Another direction is to favor domaininvariability not only of the expectations of individual variables but rather those of constraint functions involving latent variables, features and labels. Acknowledgeme</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Proc. 45th Meeting of Association for Computational Linguistics (ACL), Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Koby Crammer</author>
<author>Alex Kulesza</author>
<author>Fernando Pereira</author>
<author>Jennifer Wortman</author>
</authors>
<title>Learning bounds for domain adaptation.</title>
<date>2008</date>
<booktitle>In Proc. Advances In Neural Information Processing Systems (NIPS ’07).</booktitle>
<contexts>
<context citStr="Blitzer et al., 2008" endWordPosition="752" position="4774" startWordPosition="749">elying on this latent variable will be badly affected when tested on the target domain. Intuitively, one would want the model to induce only those features which generalize between domains. We encode this intuition by introducing a term in the learning objective which regularizes inter-domain difference in marginal distributions of each latent variable. Another, though conceptually similar, argument for our method is coming from theoretical results which postulate that the drop in accuracy of an adapted classifier is dependent on the discrepancy distance between the source and target domains (Blitzer et al., 2008; Mansour et al., 2009; Ben-David et al., 2010). Roughly, the discrepancy distance is small when linear classifiers cannot distinguish between examples from different domains. A necessary condition for this is that the feature expectations do not vary significantly across domains. Therefore, our approach can be regarded as minimizing a coarse approximation of the discrepancy distance. The introduced term regularizes model expectations and it can be viewed as a form of a generalized expectation (GE) criterion (Mann and McCallum, 2010). Unlike the standard GE criterion, where a model designer de</context>
</contexts>
<marker>Blitzer, Crammer, Kulesza, Pereira, Wortman, 2008</marker>
<rawString>John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman. 2008. Learning bounds for domain adaptation. In Proc. Advances In Neural Information Processing Systems (NIPS ’07).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Alex Acero</author>
</authors>
<title>Adaptation of maximum entropy capitalizer: Little data can help a lot.</title>
<date>2004</date>
<booktitle>In Proc. of the Conference on Empirical Methods for Natural Language Processing (EMNLP),</booktitle>
<pages>285--292</pages>
<contexts>
<context citStr="Chelba and Acero, 2004" endWordPosition="5507" position="33687" startWordPosition="5504"> performs semi-supervised learning with posterior regularization (Ganchev et al., 2010). Our approach differs from theirs in many respects. First, they do not focus on the domain-adaptation setting and do not attempt to define constraints to prevent the model from learning domain-specific information. Second, their expectation constraints are estimated from labeled data, whereas we are trying to match expectations computed on unlabeled data for two domains. This approach bears some similarity to the adaptation methods standard for the setting where labelled data is available for both domains (Chelba and Acero, 2004; Daum´e and Marcu, 2006). However, instead of ensuring that the classifier parameters are similar across domains, we favor models resulting in similar marginal distributions of latent variables. 7 Discussion and Conclusions In this paper we presented a domain-adaptation method based on semi-supervised learning with distributed representations coupled with constraints favoring domain-independence of modeled phenomena. Our approach results in competitive domainadaptation performance on the sentiment classification task, rivalling that of the state-of-the-art SCL method (Blitzer et al., 2007). B</context>
</contexts>
<marker>Chelba, Acero, 2004</marker>
<rawString>Ciprian Chelba and Alex Acero. 2004. Adaptation of maximum entropy capitalizer: Little data can help a lot. In Proc. of the Conference on Empirical Methods for Natural Language Processing (EMNLP), pages 285–292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In International Conference on Machine Learning, ICML.</booktitle>
<contexts>
<context citStr="Collobert and Weston, 2008" endWordPosition="1358" position="8677" startWordPosition="1355">rsions which do not exploit labeled data from the target domain. 63 as well as different types of Deep Belief Networks (Hinton and Salakhutdinov, 2006). The power of these methods is in their ability to automatically construct new features from elementary ones provided by the model designer. This feature induction capability is especially desirable for problems where engineering features is a labor-intensive process (e.g., multilingual syntactic parsing (Titov and Henderson, 2007b)), or for multitask learning problems where the nature of interactions between the tasks is not fully understood (Collobert and Weston, 2008; Gesmundo et al., 2009). In this paper we consider classification tasks, namely prediction of sentiment polarity of a user review (Pang et al., 2002), and model the joint distribution of the binary sentiment label y ∈ {0, 1} and the multiset of text features x, xi ∈ X. The hidden variable vector z (zi ∈ {0, 1}, i = 1, ... , m) encodes statistical dependencies between components of x and also dependencies between the label y and the features x. Intuitively, the model can be regarded as a logistic regression classifier with latent features. The model assumes that the features and the latent var</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In International Conference on Machine Learning, ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Domain adaptation for statistical classifiers.</title>
<date>2006</date>
<journal>Journal of Artificial Intelligence,</journal>
<pages>26--101</pages>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>Hal Daum´e and Daniel Marcu. 2006. Domain adaptation for statistical classifiers. Journal of Artificial Intelligence, 26:101–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Druck</author>
<author>Andrew McCallum</author>
</authors>
<title>Highperformance semi-supervised learning using discriminatively constrained generative models.</title>
<date>2010</date>
<booktitle>In Proc. of the International Conference on Machine Learning (ICML),</booktitle>
<location>Haifa, Israel.</location>
<contexts>
<context citStr="Druck and McCallum, 2010" endWordPosition="5411" position="33059" startWordPosition="5408">eir behavior in the domain-adaptation setting is not well-understood. Semi-supervised learning with distributed representations and its application to domain adaptation has previously been considered in (Huang and Yates, 2009), but no attempt has been made to address problems specific to the domain-adaptation setting. Similar approaches has also been considered in the context of topic models (Xue et al., 2008), however the preference towards induction of domain-independent topics was not explicitly encoded in the learning objective or model priors. A closely related method to ours is that of (Druck and McCallum, 2010) which performs semi-supervised learning with posterior regularization (Ganchev et al., 2010). Our approach differs from theirs in many respects. First, they do not focus on the domain-adaptation setting and do not attempt to define constraints to prevent the model from learning domain-specific information. Second, their expectation constraints are estimated from labeled data, whereas we are trying to match expectations computed on unlabeled data for two domains. This approach bears some similarity to the adaptation methods standard for the setting where labelled data is available for both dom</context>
</contexts>
<marker>Druck, McCallum, 2010</marker>
<rawString>Gregory Druck and Andrew McCallum. 2010. Highperformance semi-supervised learning using discriminatively constrained generative models. In Proc. of the International Conference on Machine Learning (ICML), Haifa, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Joao Graca</author>
<author>Jennifer Gillenwater</author>
<author>Ben Taskar</author>
</authors>
<title>Posterior regularization for structured latent variable models.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research (JMLR),</journal>
<pages>2001--2049</pages>
<contexts>
<context citStr="Ganchev et al., 2010" endWordPosition="5423" position="33152" startWordPosition="5420">th distributed representations and its application to domain adaptation has previously been considered in (Huang and Yates, 2009), but no attempt has been made to address problems specific to the domain-adaptation setting. Similar approaches has also been considered in the context of topic models (Xue et al., 2008), however the preference towards induction of domain-independent topics was not explicitly encoded in the learning objective or model priors. A closely related method to ours is that of (Druck and McCallum, 2010) which performs semi-supervised learning with posterior regularization (Ganchev et al., 2010). Our approach differs from theirs in many respects. First, they do not focus on the domain-adaptation setting and do not attempt to define constraints to prevent the model from learning domain-specific information. Second, their expectation constraints are estimated from labeled data, whereas we are trying to match expectations computed on unlabeled data for two domains. This approach bears some similarity to the adaptation methods standard for the setting where labelled data is available for both domains (Chelba and Acero, 2004; Daum´e and Marcu, 2006). However, instead of ensuring that the </context>
</contexts>
<marker>Ganchev, Graca, Gillenwater, Taskar, 2010</marker>
<rawString>Kuzman Ganchev, Joao Graca, Jennifer Gillenwater, and Ben Taskar. 2010. Posterior regularization for structured latent variable models. Journal of Machine Learning Research (JMLR), pages 2001–2049.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Gesmundo</author>
<author>James Henderson</author>
<author>Paola Merlo</author>
<author>Ivan Titov</author>
</authors>
<title>Latent variable model of synchronous syntactic-semantic parsing for multiple languages. In CoNLL</title>
<date>2009</date>
<note>Shared Task.</note>
<contexts>
<context citStr="Gesmundo et al., 2009" endWordPosition="1362" position="8701" startWordPosition="1359">labeled data from the target domain. 63 as well as different types of Deep Belief Networks (Hinton and Salakhutdinov, 2006). The power of these methods is in their ability to automatically construct new features from elementary ones provided by the model designer. This feature induction capability is especially desirable for problems where engineering features is a labor-intensive process (e.g., multilingual syntactic parsing (Titov and Henderson, 2007b)), or for multitask learning problems where the nature of interactions between the tasks is not fully understood (Collobert and Weston, 2008; Gesmundo et al., 2009). In this paper we consider classification tasks, namely prediction of sentiment polarity of a user review (Pang et al., 2002), and model the joint distribution of the binary sentiment label y ∈ {0, 1} and the multiset of text features x, xi ∈ X. The hidden variable vector z (zi ∈ {0, 1}, i = 1, ... , m) encodes statistical dependencies between components of x and also dependencies between the label y and the features x. Intuitively, the model can be regarded as a logistic regression classifier with latent features. The model assumes that the features and the latent variable vector are generat</context>
</contexts>
<marker>Gesmundo, Henderson, Merlo, Titov, 2009</marker>
<rawString>Andrea Gesmundo, James Henderson, Paola Merlo, and Ivan Titov. 2009. Latent variable model of synchronous syntactic-semantic parsing for multiple languages. In CoNLL 2009 Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zoubin Ghahramani</author>
<author>Michael I Jordan</author>
</authors>
<date>1997</date>
<booktitle>Factorial hidden Markov models. Machine Learning,</booktitle>
<pages>29--245</pages>
<contexts>
<context citStr="Ghahramani and Jordan, 1997" endWordPosition="1246" position="7937" startWordPosition="1243">iments. In Section 5 we provide an empirical evaluation of the proposed method. We conclude in Section 6 with further examination of the related work. 2 The Latent Variable Model The adaptation method advocated in this paper is applicable to any joint probabilistic model which uses distributed representations, i.e. vectors of latent variables, to abstract away from hand-crafted features. These models, for example, include Restricted Boltzmann Machines (Smolensky, 1986; Hinton, 2002) and Sigmoid Belief Networks (SBNs) (Saul et al., 1996) for classification and regression tasks, Factorial HMMs (Ghahramani and Jordan, 1997) for sequence labeling problems, Incremental SBNs for parsing problems (Titov and Henderson, 2007a), 1Among the versions which do not exploit labeled data from the target domain. 63 as well as different types of Deep Belief Networks (Hinton and Salakhutdinov, 2006). The power of these methods is in their ability to automatically construct new features from elementary ones provided by the model designer. This feature induction capability is especially desirable for problems where engineering features is a labor-intensive process (e.g., multilingual syntactic parsing (Titov and Henderson, 2007b)</context>
</contexts>
<marker>Ghahramani, Jordan, 1997</marker>
<rawString>Zoubin Ghahramani and Michael I. Jordan. 1997. Factorial hidden Markov models. Machine Learning, 29:245–273.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Hinton</author>
<author>R R Salakhutdinov</author>
</authors>
<title>Reducing the dimensionality of data with neural networks.</title>
<date>2006</date>
<journal>Science,</journal>
<pages>313--504</pages>
<contexts>
<context citStr="Hinton and Salakhutdinov, 2006" endWordPosition="1287" position="8202" startWordPosition="1284">tic model which uses distributed representations, i.e. vectors of latent variables, to abstract away from hand-crafted features. These models, for example, include Restricted Boltzmann Machines (Smolensky, 1986; Hinton, 2002) and Sigmoid Belief Networks (SBNs) (Saul et al., 1996) for classification and regression tasks, Factorial HMMs (Ghahramani and Jordan, 1997) for sequence labeling problems, Incremental SBNs for parsing problems (Titov and Henderson, 2007a), 1Among the versions which do not exploit labeled data from the target domain. 63 as well as different types of Deep Belief Networks (Hinton and Salakhutdinov, 2006). The power of these methods is in their ability to automatically construct new features from elementary ones provided by the model designer. This feature induction capability is especially desirable for problems where engineering features is a labor-intensive process (e.g., multilingual syntactic parsing (Titov and Henderson, 2007b)), or for multitask learning problems where the nature of interactions between the tasks is not fully understood (Collobert and Weston, 2008; Gesmundo et al., 2009). In this paper we consider classification tasks, namely prediction of sentiment polarity of a user r</context>
</contexts>
<marker>Hinton, Salakhutdinov, 2006</marker>
<rawString>G. E. Hinton and R. R. Salakhutdinov. 2006. Reducing the dimensionality of data with neural networks. Science, 313:504–507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
</authors>
<title>Training Products of Experts by Minimizing Contrastive Divergence.</title>
<date>2002</date>
<journal>Neural Computation,</journal>
<pages>14--1771</pages>
<contexts>
<context citStr="Hinton, 2002" endWordPosition="935" position="5922" startWordPosition="934">010). Unlike the standard GE criterion, where a model designer defines the prior for a model expectation, our criterion postulates that the model expectations should be similar across domains. In our experiments, we use a form of Harmonium Model (Smolensky, 1986) with a single layer of binary latent variables. Though exact inference with this class of models is infeasible we use an efficient approximation (Bengio and Delalleau, 2007), which can be regarded either as a mean-field approximation to the reconstruction error or a deterministic version of the Contrastive Divergence sampling method (Hinton, 2002). Though such an estimator is biased, in practice, it yields accurate models. We explain how the introduced regularizer can be integrated into the stochastic gradient descent learning algorithm for our model. We evaluate our approach on adapting sentiment classifiers on 4 domains: books, DVDs, electronics and kitchen appliances (Blitzer et al., 2007). The loss due to transfer to a new domain is very significant for this task: in our experiments it was approaching 9%, in average, for the non-adapted model. Our regularized model achieves 35% average relative error reduction with respect to the n</context>
<context citStr="Hinton, 2002" endWordPosition="1226" position="7796" startWordPosition="1225">way to address the discovered limitations. Section 4 describes approximate learning and inference algorithms used in our experiments. In Section 5 we provide an empirical evaluation of the proposed method. We conclude in Section 6 with further examination of the related work. 2 The Latent Variable Model The adaptation method advocated in this paper is applicable to any joint probabilistic model which uses distributed representations, i.e. vectors of latent variables, to abstract away from hand-crafted features. These models, for example, include Restricted Boltzmann Machines (Smolensky, 1986; Hinton, 2002) and Sigmoid Belief Networks (SBNs) (Saul et al., 1996) for classification and regression tasks, Factorial HMMs (Ghahramani and Jordan, 1997) for sequence labeling problems, Incremental SBNs for parsing problems (Titov and Henderson, 2007a), 1Among the versions which do not exploit labeled data from the target domain. 63 as well as different types of Deep Belief Networks (Hinton and Salakhutdinov, 2006). The power of these methods is in their ability to automatically construct new features from elementary ones provided by the model designer. This feature induction capability is especially desi</context>
<context citStr="Hinton, 2002" endWordPosition="4378" position="26759" startWordPosition="4377">he model both without regularization of the domain variability (NoReg, Q = 0), and with the regularizing term (Reg). For the SCL method to produce an accurate classifier for the target domain it is necessary to train a classifier using both the induced shared representation and the initial nontransformed representation. In our case, due to joint learning and non-convexity of the learning problem, this approach would be problematic.4 Instead, we combine predictions of the semi-supervised models Reg and NoReg with the baseline out-of-domain model (Base) using the product-of-experts combination (Hinton, 2002), the corresponding methods are called Reg+ and NoReg+, respectively. In all our models, we augmented the vector z with an additional component set to 0 for examples in the source domain and to 1 for the target domain examples. In this way, we essentially subtracted a unigram domain-specific model from our latent variable model in the hope that this will further reduce the domain dependence of the rest of the model parameters. In preliminary experiments, this modification was beneficial for all the models including the non-constrained one (NoReg). 5.2 Results and Discussion The results of all </context>
</contexts>
<marker>Hinton, 2002</marker>
<rawString>Geoffrey E. Hinton. 2002. Training Products of Experts by Minimizing Contrastive Divergence. Neural Computation, 14:1771–1800.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Alexander Yates</author>
</authors>
<title>Distributional representations for handling sparsity in supervised sequence labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context citStr="Huang and Yates, 2009" endWordPosition="5347" position="32660" startWordPosition="5344">sed in-domain classifier considered in Blitzer et al. (2007), otherwise, the computed drop would be larger. 69 are word-based and lexicons are very different for different domains, therefore such assumptions are likely to be overly restrictive. Various semi-supervised techniques for domainadaptation have also been considered, one example being self-training (McClosky et al., 2006). However, their behavior in the domain-adaptation setting is not well-understood. Semi-supervised learning with distributed representations and its application to domain adaptation has previously been considered in (Huang and Yates, 2009), but no attempt has been made to address problems specific to the domain-adaptation setting. Similar approaches has also been considered in the context of topic models (Xue et al., 2008), however the preference towards induction of domain-independent topics was not explicitly encoded in the learning objective or model priors. A closely related method to ours is that of (Druck and McCallum, 2010) which performs semi-supervised learning with posterior regularization (Ganchev et al., 2010). Our approach differs from theirs in many respects. First, they do not focus on the domain-adaptation setti</context>
</contexts>
<marker>Huang, Yates, 2009</marker>
<rawString>Fei Huang and Alexander Yates. 2009. Distributional representations for handling sparsity in supervised sequence labeling. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Instance weighting for domain adaptation in nlp.</title>
<date>2007</date>
<booktitle>In Proc. of the Annual Meeting of the ACL,</booktitle>
<pages>264--271</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context citStr="Jiang and Zhai, 2007" endWordPosition="5223" position="31838" startWordPosition="5220">e set of auxiliary tasks is still an open problem. 6 Related Work There is a growing body of work on domain adaptation. In this paper, we focus on the class of methods which induce a shared feature representation. Another popular class of domain-adaptation techniques assume that the input distributions P(x) for the source and the target domain share support, that is every example x which has a non-zero probability on the target domain must have also a non-zero probability on the source domain, and vice-versa. Such methods tackle domain adaptation by instance re-weighting (Bickel et al., 2007; Jiang and Zhai, 2007), or, similarly, by feature re-weighting (Satpal and Sarawagi, 2007). In NLP, most features 5The drop in accuracy for the SCL method in Table 1 is is computed with respect to the less accurate supervised in-domain classifier considered in Blitzer et al. (2007), otherwise, the computed drop would be larger. 69 are word-based and lexicons are very different for different domains, therefore such assumptions are likely to be overly restrictive. Various semi-supervised techniques for domainadaptation have also been considered, one example being self-training (McClosky et al., 2006). However, their </context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in nlp. In Proc. of the Annual Meeting of the ACL, pages 264–271, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon S Mann</author>
<author>Andrew McCallum</author>
</authors>
<title>Generalized expectation criteria for semi-supervised learning with weakly labeled data.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>11--955</pages>
<contexts>
<context citStr="Mann and McCallum, 2010" endWordPosition="837" position="5313" startWordPosition="833"> the discrepancy distance between the source and target domains (Blitzer et al., 2008; Mansour et al., 2009; Ben-David et al., 2010). Roughly, the discrepancy distance is small when linear classifiers cannot distinguish between examples from different domains. A necessary condition for this is that the feature expectations do not vary significantly across domains. Therefore, our approach can be regarded as minimizing a coarse approximation of the discrepancy distance. The introduced term regularizes model expectations and it can be viewed as a form of a generalized expectation (GE) criterion (Mann and McCallum, 2010). Unlike the standard GE criterion, where a model designer defines the prior for a model expectation, our criterion postulates that the model expectations should be similar across domains. In our experiments, we use a form of Harmonium Model (Smolensky, 1986) with a single layer of binary latent variables. Though exact inference with this class of models is infeasible we use an efficient approximation (Bengio and Delalleau, 2007), which can be regarded either as a mean-field approximation to the reconstruction error or a deterministic version of the Contrastive Divergence sampling method (Hint</context>
<context citStr="Mann and McCallum, 2010" endWordPosition="2773" position="17277" startWordPosition="2770">tions are severely penalized.3 Formally, the regularizer G(θ) is defined as m G(θ) = D(PS(zi|θ)||PT(zi|θ)) i=1 +D(PT(zi|θ)||PS(zi|θ)), (1) where PS(zi) and PT(zi) stand for the training sample estimates of the marginal distributions of latent features, for instance: 1 PT(zi = 1|θ) = |TU |� P(zi = 1|x(l), θ). lETU We augment the multi-conditional log-likelihood L(θ, α) with the weighted regularization term G(θ) to get the composite objective function: LR(θ, α, β) = L(θ, α) − βG(θ), β &gt; 0. Note that this regularization term can be regarded as a form of the generalized expectation (GE) criteria (Mann and McCallum, 2010), where GE criteria are normally defined as KL divergences between a prior expectation of some feature and the expectation of this feature given by the model, where the prior expectation is provided by the model designer as a form of weak supervision. In our case, both expectations are provided by the model but on different domains. Note that the proposed regularizer can be trivially extended to support the multi-domain case (Mansour et al., 2008) by considering symmetrized KL divergences for every pair of domains or regularizing the distributions for every domain towards their average. More p</context>
</contexts>
<marker>Mann, McCallum, 2010</marker>
<rawString>Gideon S. Mann and Andrew McCallum. 2010. Generalized expectation criteria for semi-supervised learning with weakly labeled data. Journal of Machine Learning Research, 11:955–984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yishay Mansour</author>
<author>Mehryar Mohri</author>
<author>Afshin Rostamizadeh</author>
</authors>
<title>Domain adaptation with multiple sources.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context citStr="Mansour et al., 2008" endWordPosition="2849" position="17728" startWordPosition="2846"> LR(θ, α, β) = L(θ, α) − βG(θ), β &gt; 0. Note that this regularization term can be regarded as a form of the generalized expectation (GE) criteria (Mann and McCallum, 2010), where GE criteria are normally defined as KL divergences between a prior expectation of some feature and the expectation of this feature given by the model, where the prior expectation is provided by the model designer as a form of weak supervision. In our case, both expectations are provided by the model but on different domains. Note that the proposed regularizer can be trivially extended to support the multi-domain case (Mansour et al., 2008) by considering symmetrized KL divergences for every pair of domains or regularizing the distributions for every domain towards their average. More powerful regularization terms can also be motivated by minimization of the discrepancy distance but their optimization is likely to be expensive, whereas LR(θ, α, β) can be optimized efficiently. 3An alternative is to use the Jensen-Shannon (JS) divergence, however, our preliminary experiments seem to suggest that the symmetrized KL divergence is preferable. Though the two divergences are virtually equivalent when the distributions are very similar</context>
</contexts>
<marker>Mansour, Mohri, Rostamizadeh, 2008</marker>
<rawString>Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. 2008. Domain adaptation with multiple sources. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yishay Mansour</author>
<author>Mehryar Mohri</author>
<author>Afshin Rostamizadeh</author>
</authors>
<title>Domain adaptation: Learning bounds and algorithms.</title>
<date>2009</date>
<booktitle>In Proceedings of The 22nd Annual Conference on Learning Theory (COLT</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context citStr="Mansour et al., 2009" endWordPosition="756" position="4796" startWordPosition="753">variable will be badly affected when tested on the target domain. Intuitively, one would want the model to induce only those features which generalize between domains. We encode this intuition by introducing a term in the learning objective which regularizes inter-domain difference in marginal distributions of each latent variable. Another, though conceptually similar, argument for our method is coming from theoretical results which postulate that the drop in accuracy of an adapted classifier is dependent on the discrepancy distance between the source and target domains (Blitzer et al., 2008; Mansour et al., 2009; Ben-David et al., 2010). Roughly, the discrepancy distance is small when linear classifiers cannot distinguish between examples from different domains. A necessary condition for this is that the feature expectations do not vary significantly across domains. Therefore, our approach can be regarded as minimizing a coarse approximation of the discrepancy distance. The introduced term regularizes model expectations and it can be viewed as a form of a generalized expectation (GE) criterion (Mann and McCallum, 2010). Unlike the standard GE criterion, where a model designer defines the prior for a </context>
<context citStr="Mansour et al., 2009" endWordPosition="2286" position="14215" startWordPosition="2283">tions between the domains. In fact, we do not need to consider the behavior of the classifier to understand the rationale behind the introduction of the regularizer. Intuitively, when adapting between domains, we are interested in representations z which explain domain-independent regularities rather than in modeling inter-domain differences. The regularizer favors models which focus on the former type of phenomena rather than the latter. Another motivation for the form of regularization we propose originates from theoretical analysis of the domain adaptation problems (Ben-David et al., 2010; Mansour et al., 2009; Blitzer et al., 2007). Under the assumption that there exists a domainindependent scoring function, these analyses show that the drop in accuracy is upper-bounded by the quantity called discrepancy distance. The discrepancy distance is dependent on the feature represendz(S,T)=max |EPS[f(z)�=f0(z)]−EPT [f(z)�=f0(z)]|, �,�� where f and f0 are arbitrary linear classifiers in the feature representation z. The quantity EP[f(z)7�f0(z)] measures the probability mass assigned to examples where f and f0 disagree. Then the discrepancy distance is the maximal change in the size of this disagreement set</context>
</contexts>
<marker>Mansour, Mohri, Rostamizadeh, 2009</marker>
<rawString>Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. 2009. Domain adaptation: Learning bounds and algorithms. In Proceedings of The 22nd Annual Conference on Learning Theory (COLT 2009), Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Chris Pal</author>
<author>Greg Druck</author>
<author>Xuerui Wang</author>
</authors>
<title>Multi-conditional learning: Generative/discriminative training for clustering and classification.</title>
<date>2006</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context citStr="McCallum et al., 2006" endWordPosition="1787" position="11101" startWordPosition="1784">ESUUTU, where SU and TU stand for the unlabeled datasets for the source and target domains, respectively. However, given that, first, amount of unlabeled data |SU ∪ TU |normally vastly exceeds the amount of labeled data |SL |and, second, the number of features for each example |x(l) |is usually large, the label y will have only a minor effect on the mapping from the initial features x to the latent representation z (i.e. on the parameters v). Consequently, the latent representation induced in this way is likely to be inappropriate for the classification task in question. Therefore, we follow (McCallum et al., 2006) and use a multi-conditional objective, a specific form of hybrid learning, to emphasize the importance of labels y: L(0, α)=α X log P(y(l)|x(l), 0)+ X log P(x(l)|0), lESL lESUUTUUSL where α is a weight, α &gt; 1. Direct maximization of the objective is problematic, as it would require summation over all the 2m latent vectors z. Instead we use a meanfield approximation. Similarly, an efficient approximate inference algorithm is used to compute arg maxy P(y|x, 0) at testing time. The approximations are described in Section 4. 3 Constraints on Inter-Domain Variability As we discussed in the introdu</context>
</contexts>
<marker>McCallum, Pal, Druck, Wang, 2006</marker>
<rawString>Andrew McCallum, Chris Pal, Greg Druck, and Xuerui Wang. 2006. Multi-conditional learning: Generative/discriminative training for clustering and classification. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Reranking and self-training for parser adaptation.</title>
<date>2006</date>
<booktitle>In Proc. of the Annual Meeting of the ACL and the International Conference on Computational Linguistics,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context citStr="McClosky et al., 2006" endWordPosition="5312" position="32421" startWordPosition="5309">kel et al., 2007; Jiang and Zhai, 2007), or, similarly, by feature re-weighting (Satpal and Sarawagi, 2007). In NLP, most features 5The drop in accuracy for the SCL method in Table 1 is is computed with respect to the less accurate supervised in-domain classifier considered in Blitzer et al. (2007), otherwise, the computed drop would be larger. 69 are word-based and lexicons are very different for different domains, therefore such assumptions are likely to be overly restrictive. Various semi-supervised techniques for domainadaptation have also been considered, one example being self-training (McClosky et al., 2006). However, their behavior in the domain-adaptation setting is not well-understood. Semi-supervised learning with distributed representations and its application to domain adaptation has previously been considered in (Huang and Yates, 2009), but no attempt has been made to address problems specific to the domain-adaptation setting. Similar approaches has also been considered in the context of topic models (Xue et al., 2008), however the preference towards induction of domain-independent topics was not explicitly encoded in the learning objective or model priors. A closely related method to ours</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Reranking and self-training for parser adaptation. In Proc. of the Annual Meeting of the ACL and the International Conference on Computational Linguistics, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context citStr="Pang et al., 2002" endWordPosition="166" position="1055" startWordPosition="163">domains. Such a model would cluster features in both domains and ensure that at least some of the latent variables are predictive of the label on the source domain. The danger is that these predictive clusters will consist of features specific to the source domain only and, consequently, a classifier relying on such clusters would perform badly on the target domain. We introduce a constraint enforcing that marginal distributions of each cluster (i.e., each latent variable) do not vary significantly across domains. We show that this constraint is effective on the sentiment classification task (Pang et al., 2002), resulting in scores similar to the ones obtained by the structural correspondence methods (Blitzer et al., 2007) without the need to engineer auxiliary tasks. 1 Introduction Supervised learning methods have become a standard tool in natural language processing, and large training sets have been annotated for a wide variety of tasks. However, most learning algorithms operate under assumption that the learning data originates from the same distribution as the test data, though in practice this assumption is often violated. This difference in the data distributions normally results in a signifi</context>
<context citStr="Pang et al., 2002" endWordPosition="1383" position="8827" startWordPosition="1380">ower of these methods is in their ability to automatically construct new features from elementary ones provided by the model designer. This feature induction capability is especially desirable for problems where engineering features is a labor-intensive process (e.g., multilingual syntactic parsing (Titov and Henderson, 2007b)), or for multitask learning problems where the nature of interactions between the tasks is not fully understood (Collobert and Weston, 2008; Gesmundo et al., 2009). In this paper we consider classification tasks, namely prediction of sentiment polarity of a user review (Pang et al., 2002), and model the joint distribution of the binary sentiment label y ∈ {0, 1} and the multiset of text features x, xi ∈ X. The hidden variable vector z (zi ∈ {0, 1}, i = 1, ... , m) encodes statistical dependencies between components of x and also dependencies between the label y and the features x. Intuitively, the model can be regarded as a logistic regression classifier with latent features. The model assumes that the features and the latent variable vector are generated jointly from a globallynormalized model and then the label y is generated from a conditional distribution dependent on z. B</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
</authors>
<title>Structural correspondence learning for parse disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Student Research Workshop at EACL</booktitle>
<pages>37--45</pages>
<location>Athens, Greece,</location>
<contexts>
<context citStr="Plank, 2009" endWordPosition="484" position="3035" startWordPosition="483">m the initial feature representation to a new representation such that (1) examples from both domains ‘look similar’ and (2) an accurate classifier can be trained in this new representation. Blitzer et al. (2006) use auxiliary tasks based on unlabeled data for both domains (called pivot features) and a dimensionality reduction technique to induce such shared representation. The success of their domain-adaptation method (Structural Correspondence Learning, SCL) crucially depends on the choice of the auxiliary tasks, and defining them can be a non-trivial engineering problem for many NLP tasks (Plank, 2009). In this paper, we investigate methods which do not use auxiliary tasks to induce a shared feature representation. We use generative latent variable models (LVMs) learned on all the available data: unlabeled data for both domains and on the labeled data for the source domain. Our LVMs use vectors of latent features 62 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 62–71, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics to represent examples. The latent variables encode regularities observed on unlabeled data f</context>
<context citStr="Plank, 2009" endWordPosition="5112" position="31184" startWordPosition="5111">o induce the shared feature representation, these tasks are constructed on the basis of unlabeled data. The auxiliary tasks and the original problem should be closely related, namely they should have the same (or similar) set of predictive features. Defining such tasks can be a challenging engineering problem. On the sentiment classification task in order to construct them two steps need to be performed: (1) a set of words correlated with the sentiment label is selected, and, then (2) prediction of each such word is regarded a distinct auxiliary problem. For many other domains (e.g., parsing (Plank, 2009)) the construction of an effective set of auxiliary tasks is still an open problem. 6 Related Work There is a growing body of work on domain adaptation. In this paper, we focus on the class of methods which induce a shared feature representation. Another popular class of domain-adaptation techniques assume that the input distributions P(x) for the source and the target domain share support, that is every example x which has a non-zero probability on the target domain must have also a non-zero probability on the source domain, and vice-versa. Such methods tackle domain adaptation by instance re</context>
</contexts>
<marker>Plank, 2009</marker>
<rawString>Barbara Plank. 2009. Structural correspondence learning for parse disambiguation. In Proceedings of the Student Research Workshop at EACL 2009, pages 37–45, Athens, Greece, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandeepkumar Satpal</author>
<author>Sunita Sarawagi</author>
</authors>
<title>Domain adaptation of conditional probability models via feature subsetting.</title>
<date>2007</date>
<booktitle>In Proceedings of 11th European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD), Warzaw,</booktitle>
<contexts>
<context citStr="Satpal and Sarawagi, 2007" endWordPosition="5233" position="31906" startWordPosition="5229">rk There is a growing body of work on domain adaptation. In this paper, we focus on the class of methods which induce a shared feature representation. Another popular class of domain-adaptation techniques assume that the input distributions P(x) for the source and the target domain share support, that is every example x which has a non-zero probability on the target domain must have also a non-zero probability on the source domain, and vice-versa. Such methods tackle domain adaptation by instance re-weighting (Bickel et al., 2007; Jiang and Zhai, 2007), or, similarly, by feature re-weighting (Satpal and Sarawagi, 2007). In NLP, most features 5The drop in accuracy for the SCL method in Table 1 is is computed with respect to the less accurate supervised in-domain classifier considered in Blitzer et al. (2007), otherwise, the computed drop would be larger. 69 are word-based and lexicons are very different for different domains, therefore such assumptions are likely to be overly restrictive. Various semi-supervised techniques for domainadaptation have also been considered, one example being self-training (McClosky et al., 2006). However, their behavior in the domain-adaptation setting is not well-understood. Se</context>
</contexts>
<marker>Satpal, Sarawagi, 2007</marker>
<rawString>Sandeepkumar Satpal and Sunita Sarawagi. 2007. Domain adaptation of conditional probability models via feature subsetting. In Proceedings of 11th European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD), Warzaw, Poland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence K Saul</author>
<author>Tommi Jaakkola</author>
<author>Michael I Jordan</author>
</authors>
<title>Mean field theory for sigmoid belief networks.</title>
<date>1996</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>4--61</pages>
<contexts>
<context citStr="Saul et al., 1996" endWordPosition="1235" position="7851" startWordPosition="1232"> 4 describes approximate learning and inference algorithms used in our experiments. In Section 5 we provide an empirical evaluation of the proposed method. We conclude in Section 6 with further examination of the related work. 2 The Latent Variable Model The adaptation method advocated in this paper is applicable to any joint probabilistic model which uses distributed representations, i.e. vectors of latent variables, to abstract away from hand-crafted features. These models, for example, include Restricted Boltzmann Machines (Smolensky, 1986; Hinton, 2002) and Sigmoid Belief Networks (SBNs) (Saul et al., 1996) for classification and regression tasks, Factorial HMMs (Ghahramani and Jordan, 1997) for sequence labeling problems, Incremental SBNs for parsing problems (Titov and Henderson, 2007a), 1Among the versions which do not exploit labeled data from the target domain. 63 as well as different types of Deep Belief Networks (Hinton and Salakhutdinov, 2006). The power of these methods is in their ability to automatically construct new features from elementary ones provided by the model designer. This feature induction capability is especially desirable for problems where engineering features is a labo</context>
<context citStr="Saul et al., 1996" endWordPosition="1530" position="9667" startWordPosition="1527">omponents of x and also dependencies between the label y and the features x. Intuitively, the model can be regarded as a logistic regression classifier with latent features. The model assumes that the features and the latent variable vector are generated jointly from a globallynormalized model and then the label y is generated from a conditional distribution dependent on z. Both of these distributions, P(x, z) and P(y|z), are parameterized as log-linear models and, consequently, our model can be seen as a combination of an undirected Harmonium model (Smolensky, 1986) and a directed SBN model (Saul et al., 1996). The formal definition is as follows: (1) Draw (x, z) ∼ P(x, z|v), (2) Draw label y ∼ Q(w0 + P'i�1 wizi), where v and w are parameters, Q is the logistic sigmoid function, Q(t) = 1/(1 + e−t), and the joint distribution of (x, z) is given by the Gibbs distribution: P(x, z|v) ∝ exp( Figure 1 presents the corresponding graphical model. Note that the arcs between x and z are undirected, whereas arcs between y and z are directed. The parameters of this model 0 = (v, w) can be estimated by maximizing joint likelihood L(0) of labeled data for the source domain {x(l), y(l)}lESL Figure 1: The latent v</context>
</contexts>
<marker>Saul, Jaakkola, Jordan, 1996</marker>
<rawString>Lawrence K. Saul, Tommi Jaakkola, and Michael I. Jordan. 1996. Mean field theory for sigmoid belief networks. Journal of Artificial Intelligence Research, 4:61–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Smolensky</author>
</authors>
<title>Information processing in dynamical systems: foundations of harmony theory.</title>
<date>1986</date>
<booktitle>Parallel distributed processing: explorations in the microstructures of cognition,</booktitle>
<volume>1</volume>
<pages>194--281</pages>
<editor>In D. Rumehart and J McCelland, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context citStr="Smolensky, 1986" endWordPosition="879" position="5572" startWordPosition="878">ssary condition for this is that the feature expectations do not vary significantly across domains. Therefore, our approach can be regarded as minimizing a coarse approximation of the discrepancy distance. The introduced term regularizes model expectations and it can be viewed as a form of a generalized expectation (GE) criterion (Mann and McCallum, 2010). Unlike the standard GE criterion, where a model designer defines the prior for a model expectation, our criterion postulates that the model expectations should be similar across domains. In our experiments, we use a form of Harmonium Model (Smolensky, 1986) with a single layer of binary latent variables. Though exact inference with this class of models is infeasible we use an efficient approximation (Bengio and Delalleau, 2007), which can be regarded either as a mean-field approximation to the reconstruction error or a deterministic version of the Contrastive Divergence sampling method (Hinton, 2002). Though such an estimator is biased, in practice, it yields accurate models. We explain how the introduced regularizer can be integrated into the stochastic gradient descent learning algorithm for our model. We evaluate our approach on adapting sent</context>
<context citStr="Smolensky, 1986" endWordPosition="1224" position="7781" startWordPosition="1223">variability as a way to address the discovered limitations. Section 4 describes approximate learning and inference algorithms used in our experiments. In Section 5 we provide an empirical evaluation of the proposed method. We conclude in Section 6 with further examination of the related work. 2 The Latent Variable Model The adaptation method advocated in this paper is applicable to any joint probabilistic model which uses distributed representations, i.e. vectors of latent variables, to abstract away from hand-crafted features. These models, for example, include Restricted Boltzmann Machines (Smolensky, 1986; Hinton, 2002) and Sigmoid Belief Networks (SBNs) (Saul et al., 1996) for classification and regression tasks, Factorial HMMs (Ghahramani and Jordan, 1997) for sequence labeling problems, Incremental SBNs for parsing problems (Titov and Henderson, 2007a), 1Among the versions which do not exploit labeled data from the target domain. 63 as well as different types of Deep Belief Networks (Hinton and Salakhutdinov, 2006). The power of these methods is in their ability to automatically construct new features from elementary ones provided by the model designer. This feature induction capability is </context>
<context citStr="Smolensky, 1986" endWordPosition="1521" position="9622" startWordPosition="1520"> encodes statistical dependencies between components of x and also dependencies between the label y and the features x. Intuitively, the model can be regarded as a logistic regression classifier with latent features. The model assumes that the features and the latent variable vector are generated jointly from a globallynormalized model and then the label y is generated from a conditional distribution dependent on z. Both of these distributions, P(x, z) and P(y|z), are parameterized as log-linear models and, consequently, our model can be seen as a combination of an undirected Harmonium model (Smolensky, 1986) and a directed SBN model (Saul et al., 1996). The formal definition is as follows: (1) Draw (x, z) ∼ P(x, z|v), (2) Draw label y ∼ Q(w0 + P'i�1 wizi), where v and w are parameters, Q is the logistic sigmoid function, Q(t) = 1/(1 + e−t), and the joint distribution of (x, z) is given by the Gibbs distribution: P(x, z|v) ∝ exp( Figure 1 presents the corresponding graphical model. Note that the arcs between x and z are undirected, whereas arcs between y and z are directed. The parameters of this model 0 = (v, w) can be estimated by maximizing joint likelihood L(0) of labeled data for the source d</context>
</contexts>
<marker>Smolensky, 1986</marker>
<rawString>Paul Smolensky. 1986. Information processing in dynamical systems: foundations of harmony theory. In D. Rumehart and J McCelland, editors, Parallel distributed processing: explorations in the microstructures of cognition, volume 1 : Foundations, pages 194– 281. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>James Henderson</author>
</authors>
<title>Constituent parsing with Incremental Sigmoid Belief Networks.</title>
<date>2007</date>
<booktitle>In Proc. 45th Meeting of Association for Computational Linguistics (ACL),</booktitle>
<pages>632--639</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context citStr="Titov and Henderson, 2007" endWordPosition="1259" position="8034" startWordPosition="1256">on 6 with further examination of the related work. 2 The Latent Variable Model The adaptation method advocated in this paper is applicable to any joint probabilistic model which uses distributed representations, i.e. vectors of latent variables, to abstract away from hand-crafted features. These models, for example, include Restricted Boltzmann Machines (Smolensky, 1986; Hinton, 2002) and Sigmoid Belief Networks (SBNs) (Saul et al., 1996) for classification and regression tasks, Factorial HMMs (Ghahramani and Jordan, 1997) for sequence labeling problems, Incremental SBNs for parsing problems (Titov and Henderson, 2007a), 1Among the versions which do not exploit labeled data from the target domain. 63 as well as different types of Deep Belief Networks (Hinton and Salakhutdinov, 2006). The power of these methods is in their ability to automatically construct new features from elementary ones provided by the model designer. This feature induction capability is especially desirable for problems where engineering features is a labor-intensive process (e.g., multilingual syntactic parsing (Titov and Henderson, 2007b)), or for multitask learning problems where the nature of interactions between the tasks is not f</context>
</contexts>
<marker>Titov, Henderson, 2007</marker>
<rawString>Ivan Titov and James Henderson. 2007a. Constituent parsing with Incremental Sigmoid Belief Networks. In Proc. 45th Meeting of Association for Computational Linguistics (ACL), pages 632–639, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>James Henderson</author>
</authors>
<title>Fast and robust multilingual dependency parsing with a generative latent variable model.</title>
<date>2007</date>
<booktitle>In Proc. of the CoNLL shared task,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context citStr="Titov and Henderson, 2007" endWordPosition="1259" position="8034" startWordPosition="1256">on 6 with further examination of the related work. 2 The Latent Variable Model The adaptation method advocated in this paper is applicable to any joint probabilistic model which uses distributed representations, i.e. vectors of latent variables, to abstract away from hand-crafted features. These models, for example, include Restricted Boltzmann Machines (Smolensky, 1986; Hinton, 2002) and Sigmoid Belief Networks (SBNs) (Saul et al., 1996) for classification and regression tasks, Factorial HMMs (Ghahramani and Jordan, 1997) for sequence labeling problems, Incremental SBNs for parsing problems (Titov and Henderson, 2007a), 1Among the versions which do not exploit labeled data from the target domain. 63 as well as different types of Deep Belief Networks (Hinton and Salakhutdinov, 2006). The power of these methods is in their ability to automatically construct new features from elementary ones provided by the model designer. This feature induction capability is especially desirable for problems where engineering features is a labor-intensive process (e.g., multilingual syntactic parsing (Titov and Henderson, 2007b)), or for multitask learning problems where the nature of interactions between the tasks is not f</context>
</contexts>
<marker>Titov, Henderson, 2007</marker>
<rawString>Ivan Titov and James Henderson. 2007b. Fast and robust multilingual dependency parsing with a generative latent variable model. In Proc. of the CoNLL shared task, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G-R Xue</author>
<author>W Dai</author>
<author>Q Yang</author>
<author>Y Yu</author>
</authors>
<title>Topicbridged PLSA for cross-domain text classification.</title>
<date>2008</date>
<booktitle>In Proceedings of the SIGIR Conference.</booktitle>
<contexts>
<context citStr="Xue et al., 2008" endWordPosition="5378" position="32847" startWordPosition="5375">uch assumptions are likely to be overly restrictive. Various semi-supervised techniques for domainadaptation have also been considered, one example being self-training (McClosky et al., 2006). However, their behavior in the domain-adaptation setting is not well-understood. Semi-supervised learning with distributed representations and its application to domain adaptation has previously been considered in (Huang and Yates, 2009), but no attempt has been made to address problems specific to the domain-adaptation setting. Similar approaches has also been considered in the context of topic models (Xue et al., 2008), however the preference towards induction of domain-independent topics was not explicitly encoded in the learning objective or model priors. A closely related method to ours is that of (Druck and McCallum, 2010) which performs semi-supervised learning with posterior regularization (Ganchev et al., 2010). Our approach differs from theirs in many respects. First, they do not focus on the domain-adaptation setting and do not attempt to define constraints to prevent the model from learning domain-specific information. Second, their expectation constraints are estimated from labeled data, whereas </context>
</contexts>
<marker>Xue, Dai, Yang, Yu, 2008</marker>
<rawString>G.-R. Xue, W. Dai, Q. Yang, and Y. Yu. 2008. Topicbridged PLSA for cross-domain text classification. In Proceedings of the SIGIR Conference.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>