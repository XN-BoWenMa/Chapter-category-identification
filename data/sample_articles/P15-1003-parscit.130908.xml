<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant confidence="0.000110" no="0">
<title confidence="0.947337">
Encoding Source Language with Convolutional Neural Network for
Machine Translation
</title>
<author confidence="0.982876">
Fandong Meng1 Zhengdong Lu2 Mingxuan Wang1 Hang Li2 Wenbin Jiang1 Qun Liu3,1
</author>
<affiliation confidence="0.983261">
1Key Laboratory of Intelligent Information Processing,
Institute of Computing Technology, Chinese Academy of Sciences
</affiliation>
<email confidence="0.908464">
{mengfandong,wangmingxuan,jiangwenbin,liuqun}@ict.ac.cn
</email>
<address confidence="0.445478">
2Noah’s Ark Lab, Huawei Technologies
</address>
<email confidence="0.921978">
{Lu.Zhengdong,HangLi.HL}@huawei.com
</email>
<affiliation confidence="0.541042">
3ADAPT Centre, School of Computing, Dublin City University
</affiliation>
<sectionHeader confidence="0.976656" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999952592592593">The recently proposed neural network joint model (NNJM) (Devlin et al., 2014) augments the n-gram target language model with a heuristically chosen source context window, achieving state-of-the-art performance in SMT. In this paper, we give a more systematic treatment by summarizing the relevant source information through a convolutional architecture guided by the target information. With different guiding signals during decoding, our specifically designed convolution+gating architectures can pinpoint the parts of a source sentence that are relevant to predicting a target word, and fuse them with the context of entire source sentence to form a unified representation. This representation, together with target language words, are fed to a deep neural network (DNN) to form a stronger NNJM. Experiments on two NIST Chinese-English translation tasks show that the proposed model can achieve significant improvements over the previous NNJM by up to +1.08 BLEU points on average.</bodyText>
<sectionHeader confidence="0.998883" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99930518">Learning of continuous space representation for source language has attracted much attention in both traditional statistical machine translation (SMT) and neural machine translation (NMT). Various models, mostly neural network-based, have been proposed for representing the source sentence, mainly as the encoder part in an encoder-decoder framework (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014). There has been some quite recent work on encoding only “relevant” part of source sentence during the decoding process, most notably neural network joint model (NNJM) in (Devlin et al., 2014), which extends the n-grams target language model by additionally taking a fixed-length window of source sentence, achieving state-of-the-art performance in statistical machine translation. In this paper, we propose novel convolutional architectures to dynamically encode the relevant information in the source language. Our model covers the entire source sentence, but can effectively find and properly summarize the relevant parts, guided by the information from the target language. With the guiding signals during decoding, our specifically designed convolution architectures can pinpoint the parts of a source sentence that are relevant to predicting a target word, and fuse them with the context of entire source sentence to form a unified representation. This representation, together with target words, are fed to a deep neural network (DNN) to form a stronger NNJM. Since our proposed joint model is purely lexicalized, it can be integrated into any SMT decoder as a feature. Two variants of the joint model are also proposed, with coined name tagCNN and inCNN, with different guiding signals used from the decoding process. We integrate the proposed joint models into a state-of-the-art dependency-to-string translation system (Xie et al., 2011) to evaluate their effectiveness. Experiments on NIST Chinese-English translation tasks show that our model is able to achieve significant improvements of +2.0 BLEU points on average over the baseline. Our model also outperforms Devlin et al. (2014)’s NNJM by up to +1.08 BLEU points.</bodyText>
<page confidence="0.941346">
20
</page>
<note confidence="0.984311">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 20–30,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figure confidence="0.90179">
(a) tagCNN (b) inCNN
</figure>
<figureCaption confidence="0.999965">
Figure 1: Illustration for joint LM based on CNN encoder.
</figureCaption>
<bodyText confidence="0.999664333333333">RoadMap: In the remainder of this paper, we start with a brief overview of joint language model in Section 2, while the convolutional encoders, as the key component of which, will be described in detail in Section 3. Then in Section 4 we discuss the decoding algorithm with the proposed models. The experiment results are reported in Section 5, followed by Section 6 and 7 for related work and conclusion.</bodyText>
<sectionHeader confidence="0.990516" genericHeader="method">
2 Joint Language Model
</sectionHeader>
<bodyText confidence="0.999888692307692">Our joint model with CNN encoders can be illustrated in Figure 1 (a) &amp; (b), which consists 1) a CNN encoder, namely tagCNN or inCNN, to represent the information in the source sentences, and 2) an NN-based model for predicting the next words, with representations from CNN encoders and the history words in target sentence as inputs. In the joint language model, the probability of the target word en, given previous k target words {en−k, · · ·, en−1} and the representations from CNN-encoders for source sentence 5 are n−k), where 01(5, {a(en)}) stands for the representation given by tagCNN with the set of indexes {a(en)} of source words aligned to the target word en, and 02(5, h({e}n−1 n−k)) stands for the representation from inCNN with the attention signal h({e}n−1 n−k).</bodyText>
<equation confidence="0.96203425">
tagCNN: p(en|01(5, {a(en)}), {e}n−1
n−k)
inCNN: p(en |02(5, h({e}n−1
n−k)), {e}n−1
</equation>
<bodyText confidence="0.951416148148148">Let us use the example in Figure 1, where the task is to translate the Chinese sentence into English. In evaluating a target language sequence “holds parliament and presidential”, with “holds parliament and” as the proceeding words (assume 4-gram LM), and the affiliated source word1 of “presidential” being “Zˇongtˇong” (determined by word alignment), tagCNN generates 01(5, {4}) (the index of “Zˇongtˇong” is 4), and inCNN generates 02(5, h(holds parliament and)). The DNN component then takes &amp;quot;holds parliament and&amp;quot; and (01 or 02) as input to give the conditional probability for next word, e.g., p(&amp;quot;presidential&amp;quot;|01|2, {holds, parliament, and}).</bodyText>
<sectionHeader confidence="0.994526" genericHeader="method">
3 Convolutional Models
</sectionHeader>
<bodyText confidence="0.998993666666667">We start with the generic architecture for convolutional encoder, and then proceed to tagCNN and inCNN as two extensions.</bodyText>
<footnote confidence="0.998228">
1For an aligned target word, we take its aligned source
words as its affiliated source words. And for an unaligned
word, we inherit its affiliation from the closest aligned
word, with preference given to the right (Devlin et al.,
2014). Since the word alignment is of many-to-many,
one target word may has multi affiliated source words.
</footnote>
<page confidence="0.999477">
21
</page>
<figureCaption confidence="0.998563">
Figure 2: Illustration for the CNN encoders.
</figureCaption>
<subsectionHeader confidence="0.98886">
3.1 Generic CNN Encoder
</subsectionHeader>
<bodyText confidence="0.998906384615385">The basic architecture is of a generic CNN encoder is illustrated in Figure 2 (a), which has a fixed architecture consisting of six layers: Layer-0: the input layer, which takes words in the form of embedding vectors. In our work, we set the maximum length of sentences to 40 words. For sentences shorter than that, we put zero padding at the beginning of sentences. Layer-1: a convolution layer after Layer-0, with window size = 3. As will be discussed in Section 3.2 and 3.3, the guiding signal are injected into this layer for “guided version”. Layer-2: a local gating layer after Layer1, which simply takes a weighted sum over feature-maps in non-adjacent window with size = 2. Layer-3: a convolution layer after Layer-2, we perform another convolution with window size = 3. Layer-4: we perform a global gating over feature-maps on Layer-3. Layer-5: fully connected weights that maps the output of Layer-4 to this layer as the final representation.</bodyText>
<subsectionHeader confidence="0.951764">
3.1.1 Convolution
</subsectionHeader>
<bodyText confidence="0.999884714285714">As shown in Figure 2 (a), the convolution in Layer-1 operates on sliding windows of words (width k1), and the similar definition of windows carries over to higher layers. Formally, for source sentence input x = {x1, · · · , xN}, the convolution unit for feature map of type-f (among F` of them) on Layer-` is where</bodyText>
<equation confidence="0.999586666666667">
zi (x) = σ(w(`,f)ˆz(`−1)
(`,f) i + b(`,f)),
` = 1,3, f = 1,2,···,F` (1)
</equation>
<listItem confidence="0.9947105">• zi (x) gives the output of feature map (`,f) of type-f for location i in Layer-`; • w(`,f) is the parameters for f on Layer-`; • σ(·) is the Sigmoid activation function; • ˆz(`−1)</listItem>
<bodyText confidence="0.9543385">i denotes the segment of Layer-`−1 for the convolution at location i , while concatenates the vectors for 3 words from sentence input x.</bodyText>
<equation confidence="0.977206333333333">
ˆz(0) def = T T T T
[xi , xi+1, xi+2]
i
</equation>
<subsectionHeader confidence="0.974774">
3.1.2 Gating
</subsectionHeader>
<bodyText confidence="0.999959071428571">Previous CNNs, including those for NLP tasks (Hu et al., 2014; Kalchbrenner et al., 2014), take a straightforward convolutionpooling strategy, in which the “fusion” decisions (e.g., selecting the largest one in maxpooling) are based on the values of featuremaps. This is essentially a soft template matching, which works for tasks like classification, but harmful for keeping the composition functionality of convolution, which is critical for modeling sentences. In this paper, we propose to use separate gating unit to release the score function duty from the convolution, and let it focus on composition.</bodyText>
<page confidence="0.976883">
22
</page>
<bodyText confidence="0.999940333333333">We take two types of gating: 1) for Layer2, we take a local gating with non-overlapping windows (size = 2) on the feature-maps of convolutional Layer-1 for representation of segments, and 2) for Layer-4, we take a global gating to fuse all the segments for a global representation. We found that this gating strategy can considerably improve the performance of both tagCNN and inCNN over pooling.</bodyText>
<listItem confidence="0.904911105263158">• Local Gating: On Layer-1, for every gating window, we first find its original input (before convolution) on Layer-0, and merge them for the input of the gating network. For example, for the two windows: word (3,4,5) and word (4,5,6) on Layer-0, we use concatenated vector consisting of embedding for word (3,4,5,6) as the input of the local gating network (a logistic regression model) to determine the weight for the convolution result of the two windows (on Layer-1), and the weighted sum are the output of Layer-2. • Global Gating: On Layer-3, for featuremaps at each location i, denoted z(3) i , the global gating network (essentially softmax, parameterized wg), assigns a normalized weight and the gated representation on Layer4 is given by the weighted sum Ei ω(zi(3))zi(3).</listItem>
<equation confidence="0.9454412">
�
ω(z(3)
i ) = ew9 z(3)
i /
j
</equation>
<subsectionHeader confidence="0.992526">
3.1.3 Training of CNN encoders
</subsectionHeader>
<bodyText confidence="0.937772315789474">The CNN encoders, including tagCNN and inCNN that will be discussed right below, are trained in a joint language model described in Section 2, along with the following parameters • the embedding of the words on source and the proceeding words on target; • the parameters for the DNN of joint language model, include the parameters of soft-max for word probability. The training procedure is identical to that of neural network language model, except that the parallel corpus is used instead of a monolingual corpus. We seek to maximize the loglikelihood of training samples, with one sample for every target word in the parallel corpus. Optimization is performed with the conventional back-propagation, implemented as stochastic gradient descent (LeCun et al., 1998) with mini-batches.</bodyText>
<subsectionHeader confidence="0.987118">
3.2 tagCNN
</subsectionHeader>
<bodyText confidence="0.999633428571429">tagCNN inherits the convolution and gating from generic CNN (as described in Section 3.1), with the only modification in the input layer. As shown in Figure 2 (b), in tagCNN, we append an extra tagging bit (0 or 1) to the embedding of words in the input layer to indicate whether it is one of affiliated words</bodyText>
<equation confidence="0.974046666666667">
xi = [xT
(AFF) i 1]T, x(NON-AFF) = [xT j 0]T.
j
</equation>
<bodyText confidence="0.99999125">Those extended word embedding will then be treated as regular word-embedding in the convolutional neural network. This particular encoding strategy can be extended to embed more complicated dependency relation in source language, as will be described in Section 5.4. This particular “tag” will be activated in a parameterized way during the training for predicting the target words. In other words, the supervised signal from the words to predict will find, through layers of back-propagation, the importance of the tag bit in the “affiliated words” in the source language, and learn to put proper weight on it to make tagged words stand out and adjust other parameters in tagCNN accordingly for the optimal predictive performance. In doing so, the joint model can pinpoint the parts of a source sentence that are relevant to predicting a target word through the already learned word alignment.</bodyText>
<subsectionHeader confidence="0.966013">
3.3 inCNN
</subsectionHeader>
<bodyText confidence="0.999970888888889">Unlike tagCNN, which directly tells the location of affiliated words to the CNN encoder, inCNN sends the information about the proceeding words in target side to the convolutional encoder to help retrieve the information relevant for predicting the next word. This is essentially a particular case of attention model, analogous to the automatic alignment mechanism in (Bahdanau et al., 2014), where the attention signal is from the state of a generative recurrent neural network (RNN) as decoder.</bodyText>
<equation confidence="0.7024715">
,
WT z(3)
e 9
j
</equation>
<page confidence="0.789832">
23
</page>
<figure confidence="0.995192916666667">
举行
智利 X1:NN
举行
智利/NN 选举/NN
举行/VV
国会/NN 与/CC
总统/NN
Chinese: 智利 举行 国会 与 总统 选举
English: Chile holds parliament and presidential elections
(a)
Chile holds X1
holds
</figure>
<figureCaption confidence="0.99748375">
Figure 3: Illustration for a dependency tree (a) with three head-dependents relations in shadow,
an example of head-dependents relation rule (b) for the top level of (a), and an example of head
rule (c). “X1:NN” indicates a substitution site that can be replaced by a subtree whose root has
part-of-speech “NN”. The underline denotes a leaf node.
</figureCaption>
<bodyText confidence="0.990726166666666">Basically, the information from proceeding words, denoted as h({e}n−1 n−k), is injected into every convolution window in the source language sentence, as illustrated in Figure 2 (c). More specifically, for the window indexed by t, the input to convolution is given by the concatenated vector ˆzt = [h({e}n−1 n−k), x&gt;t , x&gt;t+1, x&gt;t+2]&gt;. In this work, we use a DNN to transform the vector concatenated from word-embedding for words {en−k · · · , en−k} into h({e}n−1 n−k), with sigmoid activation function. Through layers of convolution and gating, inCNN can 1) retrieve the relevant segments of source sentences, and 2) compose and transform the retrieved segments into representation recognizable by the DNN in predicting the words in target language. Different from that of tagCNN, inCNN uses information from proceeding words, hence provides complementary information in the augmented joint language model of tagCNN. This has been empirically verified when using feature based on tagCNN and that based on inCNN in decoding with greater improvement.</bodyText>
<sectionHeader confidence="0.866778" genericHeader="method">
4 Decoding with the Joint Model
</sectionHeader>
<bodyText confidence="0.999914454545455">Our joint model is purely lexicalized, and therefore can be integrated into any SMT decoders as a feature. For a hierarchical SMT decoder, we adopt the integrating method proposed by Devlin et al. (2014). As inherited from the n-gram language model for performing hierarchical decoding, the leftmost and rightmost n − 1 words from each constituent should be stored in the state space. We extend the state space to also include the indexes of the affiliated source words for each of these edge words. For an aligned target word, we take its aligned source words as its affiliated source words. And for an unaligned word, we use the affiliation heuristic adopted by Devlin et al. (2014). In this paper, we integrate the joint model into the state-of-the-art dependency-to-string machine translation decoder as a case study to test the efficacy of our proposed approaches. We will briefly describe the dependency-to-string translation model and then the description of MT system.</bodyText>
<subsectionHeader confidence="0.992869">
4.1 Dependency-to-String Translation
</subsectionHeader>
<bodyText confidence="0.9999215">In this paper, we use a state-of-the-art dependency-to-string (Xie et al., 2011) decoder (Dep2Str), which is also a hierarchical decoder. This dependency-to-string model employs rules that represent the source side as head-dependents relations and the target side as strings. A head-dependents relation (HDR) is composed of a head and all its dependents in dependency trees. Figure 3 shows a dependency tree (a) with three HDRs (in shadow), an example of HDR rule (b) for the top level of (a), and an example of head rule (c).</bodyText>
<page confidence="0.994027">
24
</page>
<bodyText confidence="0.999859222222222">HDR rules are constructed from head-dependents relations. HDR rules can act as both translation rules and reordering rules. And head rules are used for translating source words. We adopt the decoder proposed by Meng et al.(2013) as a variant of Dep2Str translation that is easier to implement with comparable performance. Basically they extract the HDR rules with GHKM (Galley et al., 2004) algorithm. For the decoding procedure, given a source dependency tree T, the decoder transverses T in post-order. The bottomup chart-based decoding algorithm with cube pruning (Chiang, 2007; Huang and Chiang, 2007) is used to find the k-best items for each node.</bodyText>
<subsectionHeader confidence="0.987359">
4.2 MT Decoder
</subsectionHeader>
<bodyText confidence="0.995799">Following Och and Ney (2002), we use a general loglinear framework. Let d be a derivation that convert a source dependency tree into a target string e. The probability of d is defined as:</bodyText>
<equation confidence="0.9873365">
P(d) a � Oi(d)λi (2)
i
</equation>
<bodyText confidence="0.91946825">where Oi are features defined on derivations and Ai are the corresponding weights. Our decoder contains the following features: Baseline Features:</bodyText>
<listItem confidence="0.994039923076923">• translation probabilities P(t|s) and P(s|t) of HDR rules; • lexical translation probabilities PLEX(t|s) and PLEX(s|t) of HDR rules; • rule penalty exp(−1); • pseudo translation rule penalty exp(−1); • target word penalty exp(|e|); • n-gram language model PLM(e); Proposed Features: • n-gram tagCNN joint language model PTLM(e); • n-gram inCNN joint language model PILM(e).</listItem>
<bodyText confidence="0.9998886">Our baseline decoder contains the first eight features. The pseudo translation rule (constructed according to the word order of a HDR) is to ensure the complete translation when no matched rules is found during decoding. The weights of all these features are tuned via minimum error rate training (MERT) (Och, 2003). For the dependency-to-string decoder, we set rule-threshold and stack-threshold to 10−3, rule-limit to 100, stack-limit to 200.</bodyText>
<sectionHeader confidence="0.999237" genericHeader="evaluation and result">
5 Experiments
</sectionHeader>
<bodyText confidence="0.996406">The experiments in this Section are designed to answer the following questions:</bodyText>
<listItem confidence="0.998406692307692">1. Are our tagCNN and inCNN joint language models able to improve translation quality, and are they complementary to each other? 2. Do inCNN and tagCNN benefit from their guiding signal, compared to a generic CNN? 3. For tagCNN, is it helpful to embed more dependency structure, e.g., dependency head of each affiliated word, as additional information? 4. Can our gating strategy improve the performance over max-pooling?</listItem>
<subsectionHeader confidence="0.983609">
5.1 Setup
</subsectionHeader>
<bodyText confidence="0.9996610625">Data: Our training data are extracted from LDC data2. We only keep the sentence pairs that the length of source part no longer than 40 words, which covers over 90% of the sentence. The bilingual training data consist of 221K sentence pairs, containing 5.0 million Chinese words and 6.8 million English words. The development set is NIST MT03 (795 sentences) and test sets are MT04 (1499 sentences) and MT05 (917 sentences) after filtering with length limit. Preprocessing: The word alignments are obtained with GIZA++ (Och and Ney, 2003) on the corpora in both directions, using the “growdiag-final-and” balance strategy (Koehn et al., 2003). We adopt SRI Language Modeling</bodyText>
<footnote confidence="0.978578">
2The corpora include LDC2002E18, LDC2003E07,
LDC2003E14, LDC2004T07, LDC2005T06.
</footnote>
<page confidence="0.992705">
25
</page>
<table confidence="0.999851375">
Systems MT04 MT05 Average
Moses 34.33 31.75 33.04
Dep2Str 34.89 32.24 33.57
+ BBN-JM (Devlin et al., 2014) 36.11 32.86 34.49
+ CNN (generic) 36.12* 33.07* 34.60
+ tagCNN 36.33* 33.37* 34.85
+ inCNN 36.92* 33.72* 35.32
+ tagCNN + inCNN 36.94* 34.20* 35.57
</table>
<tableCaption confidence="0.998117">
Table 1: BLEU-4 scores (%) on NIST MT04-test and MT05-test, of Moses (default settings),
dependency-to-string baseline system (Dep2Str), and different features on top of Dep2Str: neural network joint model (BBN-JM), generic CNN, tagCNN, inCNN and the combination of tagCNN and inCNN.</tableCaption>
<bodyText confidence="0.9976494">The boldface numbers and superscript ∗ indicate that the results are significantly better (p&lt;0.01) than those of the BBN-JM and the Dep2Str baseline respectively. “+” stands for adding the corresponding feature to Dep2Str. Toolkit (Stolcke and others, 2002) to train a 4-gram language model with modified KneserNey smoothing on the Xinhua portion of the English Gigaword corpus (306 million words). We parse the Chinese sentences with Stanford Parser into projective dependency trees. Optimization of NN: In training the neural network, we limit the source and target vocabulary to the most frequent 20K words for both Chinese and English, covering approximately 97% and 99% of two corpus respectively. All the out-of-vocabulary words are mapped to a special token UNK. We used stochastic gradient descent to train the joint model, setting the size of minibatch to 500. All joint models used a 3word target history (i.e., 4-gram LM). The dimension of word embedding and the attention signal h({e}n−1 n−k) for inCNN are 100. For the convolution layers (Layer 1 and Layer 3), we apply 100 filters. And the final representation of CNN encoders is a vector with dimension 100. The final DNN layer of our joint model is the standard multi-layer perceptron with softmax at the top layer. Metric: We use the case-insensitive 4gram NIST BLEU3 as our evaluation metric, with statistical significance test with signtest (Collins et al., 2005) between the proposed models and two baselines.</bodyText>
<footnote confidence="0.641764">
3ftp://jaguar.ncsl.nist.gov/mt/
resources/mteval-v11b.pl
</footnote>
<subsectionHeader confidence="0.939141">
5.2 Setting for Model Comparisons
</subsectionHeader>
<bodyText confidence="0.9998993">We use the tagCNN and inCNN joint language models as additional decoding features to a dependency-to-string baseline system (Dep2Str), and compare them to the neural network joint model with 11 source context words (Devlin et al., 2014). We use the implementation of an open source toolkit4 with default configuration except the global settings described in Section 5.1. Since our tagCNN and inCNN models are source-totarget and left-to-right (on target side), we only take the source-to-target and left-to-right type NNJM in (Devlin et al., 2014) in comparison. We call this type NNJM as BBN-JM hereafter. Although the BBN-JM in (Devlin et al., 2014) is originally tested in the hierarchical phrase-based (Chiang, 2007) SMT and stringto-dependency (Shen et al., 2008) SMT, it is fairly versatile and can be readily integrated into Dep2Str.</bodyText>
<subsectionHeader confidence="0.997335">
5.3 The Main Results
</subsectionHeader>
<bodyText confidence="0.999993666666667">The main results of different models are given in Table 1. Before proceeding to more detailed comparison, we first observe that</bodyText>
<listItem confidence="0.994660833333334">• the baseline Dep2Str system gives BLEU 0.5+ higher than the open-source phrasebased system Moses (Koehn et al., 2007); • BBN-JM can give about +0.92 BLEU score over Dep2Str, a result similar as reported in (Devlin et al., 2014).</listItem>
<footnote confidence="0.978286">
4http://nlg.isi.edu/software/nplm/
</footnote>
<page confidence="0.983138">
26
</page>
<table confidence="0.99970875">
Systems MT04 MT05 Average
Dep2str 34.89 32.24 33.57
+tagCNN 36.33 33.37 34.85
+tagCNN dep 36.54 33.61 35.08
</table>
<tableCaption confidence="0.820774666666667">
Table 2: BLEU-4 scores (%) of tagCNN
model with dependency head words as addi-
tional tags (tagCNN dep).
</tableCaption>
<bodyText confidence="0.926577333333333">Clearly from Table 1, tagCNN and inCNN improve upon the Dep2Str baseline by +1.28 and +1.75 BLEU, outperforming BBN-JM in the same setting by respectively +0.36 and +0.83 BLEU, averaged on NIST MT04 and MT05. These indicate that tagCNN and inCNN can individually provide discriminative information in decoding. It is worth noting that inCNN appears to be more informative than the affiliated words suggested by the word alignment (GIZA++). We conjecture that this is due to the following two facts • inCNN avoids the propagation of mistakes and artifacts in the already learned word alignment; • the guiding signal in inCNN provides complementary information to evaluate the translation.</bodyText>
<bodyText confidence="0.99995515">Moreover, when tagCNN and inCNN are both used in decoding, it can further increase its winning margin over BBN-JM to +1.08 BLEU points (in the last row of Table 1), indicating that the two models with different guiding signals are complementary to each other. The Role of Guiding Signal It is slight surprising that the generic CNN can also achieve the gain on BLEU similar to that of BBNJM, since intuitively generic CNN encodes the entire sentence and the representations should in general far from optimal representation for joint language model. The reason, as we conjecture, is CNN yields fairly informative summarization of the sentence (thanks to its sophisticated convolution and gating architecture), which makes up some of its loss on resolution and relevant parts of the source senescence. That said, the guiding signal in both tagCNN and inCNN are crucial to the power of CNN-based encoder, as can be easily seen from the difference between the BLEU scores achieved by generic CNN, tagCNN, and inCNN.</bodyText>
<table confidence="0.999397">
Systems MT04 MT05 Average
Dep2Str 34.89 32.24 33.57
+inCNN 36.92 33.72 35.32
+inCNN-2-pooling 36.33 32.88 34.61
+inCNN-4-pooling 36.46 33.01 34.74
+inCNN-8-pooling 36.57 33.39 34.98
</table>
<tableCaption confidence="0.99445">
Table 3: BLEU-4 scores (%) of inCNN mod-
els implemented with gating strategy and k max-pooling, where k is of {2, 4, 8}.</tableCaption>
<bodyText confidence="0.952210636363636">Indeed, with the signal from the already learned word alignment, tagCNN can gain +0.25 BLEU over its generic counterpart, while for inCNN with the guiding signal from the proceeding words in target, the gain is more saliently +0.72 BLEU.</bodyText>
<subsectionHeader confidence="0.997969">
5.4 Dependency Head in tagCNN
</subsectionHeader>
<bodyText confidence="0.999916944444445">In this section, we study whether tagCNN can further benefit from encoding richer dependency structure in source language in the input. More specifically, the dependency head words can be used to further improve tagCNN model. As described in Section 3.2, in tagCNN, we append a tagging bit (0 or 1) to the embedding of words in the input layer as tags on whether they are affiliated source words. To incorporate dependency head information, we extend the tagging rule in Section 3.2 to add another tagging bit (0 or 1) to the word-embedding for original tagCNN to indicate whether it is part of dependency heads of the affiliated words. For example, if xi is the embedding of an affiliated source word and xj the dependency head of word xi, the extended input of tagCNN would contain</bodyText>
<equation confidence="0.858773333333333">
x(AFF, NON-HEAD) = [xz 1 0]T
xENON-AFF, HEAD) = `[T 0 1]T
xj
</equation>
<bodyText confidence="0.999639666666667">If the affiliated source word is the root of a sentence, we only append 0 as the second tagging bit since the root has no dependency head. From Table 2, with the help of dependency head information, we can improve tagCNN by +0.23 BLEU points averagely on two test sets.</bodyText>
<page confidence="0.994456">
27
</page>
<subsectionHeader confidence="0.997586">
5.5 Gating Vs. Max-pooling
</subsectionHeader>
<bodyText confidence="0.999984826086957">In this section, we investigate to what extent that our gating strategy can improve the translation performance over max pooling, with the comparisons on inCNN model as a case study. For implementation of inCNN with maxpooling, we replace the local-gating (Layer-2) with max-pooling with size 2 (2-pooling for short), and global gating (Layer-4) with k maxpooling (“k-pooling”), where k is of {2, 4, 8}. Then, we use the mean of the outputs of kpooling as the final input of Layer-5. In doing so, we can guarantee the input dimension of Layer-5 is the same as the architecture with gating. From Table 3, we can clearly see that our gating strategy can improve translation performance over max-pooling by 0.34∼0.71 BLEU points. Moreover, we find 8-pooling yields performance better than 2-pooling. We conjecture that this is because the useful relevant parts for translation are mainly concentrated on a few words of the source sentence, which can be better extracted with a larger pool size.</bodyText>
<sectionHeader confidence="0.999965" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999976627906977">The seminal work of neural network language model (NNLM) can be traced to Bengio et al. (2003) on monolingual text. It is recently extended by Devlin et al. (2014) to include additional source context (11 source words) in modeling the target sentence, which is clearly most related to our work, with however two important differences: 1) instead of the ad hoc way of selecting a context window in (Devlin et al., 2014), our model covers the entire source sentence and automatically distill the context relevant for target modeling; 2) our convolutional architecture can effectively leverage guiding signals of vastly different forms and nature from the target. Prior to our model there is also work on representing source sentences with neural networks, including RNN (Cho et al., 2014; Sutskever et al., 2014) and CNN (Kalchbrenner and Blunsom, 2013). These work typically aim to map the entire sentence to a vector, which will be used later by RNN/LSTMbased decoder to generate the target sentence. As demonstrated in Section 5, the representation learnt this way cannot pinpoint the relevant parts of the source sentences (e.g., words or phrases level) and therefore is inferior to be directly integrated into traditional SMT decoders. Our model, especially inCNN, is inspired by is the automatic alignment model proposed in (Bahdanau et al., 2014). As the first effort to apply attention model to machine translation, it sends the state of a decoding RNN as attentional signal to the source end to obtain a weighted sum of embedding of source words as the summary of relevant context. In contrast, inCNN uses 1) a different attention signal extracted from proceeding words in partial translations, and 2) more importantly, a convolutional architecture and therefore a highly nonlinear way to retrieve and summarize the relevant information in source.</bodyText>
<sectionHeader confidence="0.992776" genericHeader="conclusion">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99992325">We proposed convolutional architectures for obtaining a guided representation of the entire source sentence, which can be used to augment the n-gram target language model. With different guiding signals from target side, we devise tagCNN and inCNN, both of which are tested in enhancing a dependency-to-string SMT with +2.0 BLEU points over baseline and +1.08 BLEU points over the state-of-the-art in (Devlin et al., 2014). For future work, we will consider encoding more complex linguistic structures to further enhance the joint model.</bodyText>
<sectionHeader confidence="0.997597" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997327">Meng, Wang, Jiang and Liu are supported by National Natural Science Foundation of China (Contract 61202216). Liu is partially supported by the Science Foundation Ireland (Grant 12/CE/I2267 and 13/RC/2106) as part of the ADAPT Centre at Dublin City University. We sincerely thank the anonymous reviewers for their thorough reviewing and valuable suggestions.</bodyText>
<sectionHeader confidence="0.994426" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.835261">
[Auli et al.2013] Michael Auli, Michel Galley,
Chris Quirk, and Geoffrey Zweig. 2013. Joint
language and translation modeling with recur-
rent neural networks. In Proceedings of the
</reference>
<page confidence="0.991504">
28
</page>
<reference confidence="0.992458333333334">
2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1044–1054,
Seattle, Washington, USA, October.
[Bahdanau et al.2014] Dzmitry Bahdanau,
Kyunghyun Cho, and Yoshua Bengio. 2014.
Neural machine translation by jointly learn-
ing to align and translate. arXiv preprint
arXiv:1409.0473.
[Bengio et al.2003] Yoshua Bengio, Rjean
Ducharme, Pascal Vincent, and Christian
Jauvin. 2003. A neural probabilistic lan-
guage model. Journal OF Machine Learning
Research, 3:1137–1155.
[Chiang2007] David Chiang. 2007. Hierarchical
phrase-based translation. Computational Lin-
guistics, 33(2):201–228.
[Cho et al.2014] Kyunghyun Cho, Bart van Mer-
rienboer, Caglar Gulcehre, Dzmitry Bahdanau,
Fethi Bougares, Holger Schwenk, and Yoshua
Bengio. 2014. Learning phrase representa-
tions using rnn encoder–decoder for statistical
machine translation. In Proceedings of the 2014
Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1724–
1734, Doha, Qatar, October.
[Collins et al.2005] Michael Collins, Philipp
Koehn, and Ivona Kuˇcerov´a. 2005. Clause
restructuring for statistical machine translation.
In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics,
pages 531–540.
[Devlin et al.2014] Jacob Devlin, Rabih Zbib,
Zhongqiang Huang, Thomas Lamar, Richard
Schwartz, and John Makhoul. 2014. Fast and
robust neural network joint models for statistical
machine translation. In Proceedings of the 52nd
Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers),
pages 1370–1380, Baltimore, Maryland, June.
[Galley et al.2004] Michel Galley, Mark Hopkins,
Kevin Knight, and Daniel Marcu. 2004.
What’s in a translation rule. In Proceedings of
HLT/NAACL, volume 4, pages 273–280. Boston.
[Hu et al.2014] Baotian Hu, Zhengdong Lu, Hang
Li, and Qingcai Chen. 2014. Convolutional
neural network architectures for matching natu-
ral language sentences. In NIPS.
[Huang and Chiang2007] Liang Huang and David
Chiang. 2007. Forest rescoring: Faster de-
coding with integrated language models. In
Annual Meeting-Association For Computational
Linguistics, volume 45, pages 144–151.
[Kalchbrenner and Blunsom2013] Nal Kalchbren-
ner and Phil Blunsom. 2013. Recurrent contin-
uous translation models. In Proceedings of the
2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1700–1709,
Seattle, Washington, USA, October.
[Kalchbrenner et al.2014] Nal Kalchbrenner, Ed-
ward Grefenstette, and Phil Blunsom. 2014. A
convolutional neural network for modelling sen-
tences. ACL.
[Klein and Manning2002] Dan Klein and Christo-
pher D Manning. 2002. Fast exact inference
with a factored model for natural language pars-
ing. In Advances in neural information process-
ing systems, volume 15, pages 3–10.
[Koehn et al.2003] Philipp Koehn, Franz Josef Och,
and Daniel Marcu. 2003. Statistical phrase-
based translation. In Proceedings of the 2003
Conference of the North American Chapter
of the Association for Computational Linguis-
tics on Human Language Technology-Volume 1,
pages 48–54.
[Koehn et al.2007] Philipp Koehn, Hieu Hoang,
Alexandra Birch, Chris Callison-Burch, Mar-
cello Federico, Nicola Bertoldi, Brooke Cowan,
Wade Shen, Christine Moran, Richard Zens,
Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics Compan-
ion Volume Proceedings of the Demo and Poster
Sessions, pages 177–180, Prague, Czech Repub-
lic, June.
[LeCun et al.1998] Y. LeCun, L. Bottou, G. Orr, and
K. Muller. 1998. Efficient backprop. In Neural
Networks: Tricks of the trade. Springer.
[Meng et al.2013] Fandong Meng, Jun Xie, Linfeng
Song, Yajuan L¨u, and Qun Liu. 2013. Trans-
lation with source constituency and dependency
trees. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1066–1076, Seattle, Washington,
USA, October.
[Och and Ney2002] Franz Josef Och and Hermann
Ney. 2002. Discriminative training and max-
imum entropy models for statistical machine
translation. In Proceedings of the 40th Annual
Meeting on Association for Computational Lin-
guistics, pages 295–302.
[Och and Ney2003] Franz Josef Och and Hermann
Ney. 2003. A systematic comparison of vari-
ous statistical alignment models. Computational
linguistics, 29(1):19–51.
[Och2003] Franz Josef Och. 2003. Minimum error
rate training in statistical machine translation. In
Proceedings of the 41st Annual Meeting on As-
sociation for Computational Linguistics-Volume
1, pages 160–167.
</reference>
<page confidence="0.973814">
29
</page>
<reference confidence="0.999246578947368">
[Shen et al.2008] Libin Shen, Jinxi Xu, and Ralph
Weischedel. 2008. A new string-to-dependency
machine translation algorithm with a target de-
pendency language model. In Proceedings of
ACL-08: HLT, pages 577–585.
[Stolcke and others2002] Andreas Stolcke et al.
2002. Srilm-an extensible language modeling
toolkit. In Proceedings of the international
conference on spoken language processing, vol-
ume 2, pages 901–904.
[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals,
and Quoc V. Le. 2014. Sequence to se-
quence learning with neural networks. CoRR,
abs/1409.3215.
[Xie et al.2011] Jun Xie, Haitao Mi, and Qun Liu.
2011. A novel dependency-to-string model for
statistical machine translation. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, pages 216–226.
</reference>
<page confidence="0.998805">
30
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant confidence="0.421620" no="0">
<title confidence="0.998406">Encoding Source Language with Convolutional Neural Network Machine Translation</title>
<author confidence="0.962287">Zhengdong Mingxuan Hang Wenbin Qun</author>
<affiliation confidence="0.9394565">Laboratory of Intelligent Information Institute of Computing Technology, Chinese Academy of</affiliation>
<author confidence="0.538291">Ark Lab</author>
<author confidence="0.538291">Huawei</author>
<affiliation confidence="0.990927">Centre, School of Computing, Dublin City University</affiliation>
<abstract confidence="0.996198178571429">The recently proposed neural network joint model (NNJM) (Devlin et al., 2014) augments the n-gram target language model with a heuristically chosen source context window, achieving state-of-the-art performance in SMT. In this paper, we give a more systematic treatment by summarizing the relevant source information through a convolutional architecture guided by target information. With ferent guiding signals during decoding, our specifically designed convolution+gating architectures can pinpoint the parts of a source sentence that are relevant to predicting a target word, and fuse them with the context of entire source sentence to form a unified representation. This representation, together with target language words, are fed to a deep neural network (DNN) to form a stronger NNJM. Experiments on two NIST Chinese-English translation tasks show that the proposed model can achieve significant improvements over the previous NNJM by up to +1.08 BLEU points on average.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Michel Galley</author>
<author>Chris Quirk</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Joint language and translation modeling with recurrent neural networks.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1044--1054</pages>
<location>Seattle, Washington, USA,</location>
<marker>[Auli et al.2013]</marker>
<rawString>Michael Auli, Michel Galley, Chris Quirk, and Geoffrey Zweig. 2013. Joint language and translation modeling with recurrent neural networks. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1044–1054, Seattle, Washington, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dzmitry Bahdanau</author>
<author>Kyunghyun Cho</author>
<author>Yoshua Bengio</author>
</authors>
<title>Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.</title>
<date>2014</date>
<marker>[Bahdanau et al.2014]</marker>
<rawString>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Rjean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal OF Machine Learning Research,</journal>
<pages>3--1137</pages>
<marker>[Bengio et al.2003]</marker>
<rawString>Yoshua Bengio, Rjean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal OF Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>[Chiang2007]</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyunghyun Cho</author>
<author>Bart van Merrienboer</author>
<author>Caglar Gulcehre</author>
<author>Dzmitry Bahdanau</author>
<author>Fethi Bougares</author>
<author>Holger Schwenk</author>
<author>Yoshua Bengio</author>
</authors>
<title>Learning phrase representations using rnn encoder–decoder for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1724--1734</pages>
<location>Doha, Qatar,</location>
<marker>[Cho et al.2014]</marker>
<rawString>Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder–decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724– 1734, Doha, Qatar, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kuˇcerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>531--540</pages>
<marker>[Collins et al.2005]</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a. 2005. Clause restructuring for statistical machine translation. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 531–540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
<author>Zhongqiang Huang</author>
<author>Thomas Lamar</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Fast and robust neural network joint models for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1370--1380</pages>
<location>Baltimore, Maryland,</location>
<marker>[Devlin et al.2014]</marker>
<rawString>Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1370–1380, Baltimore, Maryland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<volume>4</volume>
<pages>273--280</pages>
<location>Boston.</location>
<marker>[Galley et al.2004]</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule. In Proceedings of HLT/NAACL, volume 4, pages 273–280. Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baotian Hu</author>
<author>Zhengdong Lu</author>
<author>Hang Li</author>
<author>Qingcai Chen</author>
</authors>
<title>Convolutional neural network architectures for matching natural language sentences.</title>
<date>2014</date>
<booktitle>In NIPS.</booktitle>
<marker>[Hu et al.2014]</marker>
<rawString>Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional neural network architectures for matching natural language sentences. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Annual Meeting-Association For Computational Linguistics,</booktitle>
<volume>45</volume>
<pages>144--151</pages>
<marker>[Huang and Chiang2007]</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Annual Meeting-Association For Computational Linguistics, volume 45, pages 144–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent continuous translation models.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1700--1709</pages>
<location>Seattle, Washington, USA,</location>
<marker>[Kalchbrenner and Blunsom2013]</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700–1709, Seattle, Washington, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>A convolutional neural network for modelling sentences.</title>
<date>2014</date>
<publisher>ACL.</publisher>
<marker>[Kalchbrenner et al.2014]</marker>
<rawString>Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2002</date>
<booktitle>In Advances in neural information processing systems,</booktitle>
<volume>15</volume>
<pages>3--10</pages>
<marker>[Klein and Manning2002]</marker>
<rawString>Dan Klein and Christopher D Manning. 2002. Fast exact inference with a factored model for natural language parsing. In Advances in neural information processing systems, volume 15, pages 3–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrasebased translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume</booktitle>
<volume>1</volume>
<pages>48--54</pages>
<marker>[Koehn et al.2003]</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrasebased translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 48–54.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic,</location>
<marker>[Koehn et al.2007]</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y LeCun</author>
<author>L Bottou</author>
<author>G Orr</author>
<author>K Muller</author>
</authors>
<title>Efficient backprop.</title>
<date>1998</date>
<booktitle>In Neural Networks: Tricks of the trade.</booktitle>
<publisher>Springer.</publisher>
<marker>[LeCun et al.1998]</marker>
<rawString>Y. LeCun, L. Bottou, G. Orr, and K. Muller. 1998. Efficient backprop. In Neural Networks: Tricks of the trade. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fandong Meng</author>
<author>Jun Xie</author>
<author>Linfeng Song</author>
<author>Yajuan L¨u</author>
<author>Qun Liu</author>
</authors>
<title>Translation with source constituency and dependency trees.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1066--1076</pages>
<location>Seattle, Washington, USA,</location>
<marker>[Meng et al.2013]</marker>
<rawString>Fandong Meng, Jun Xie, Linfeng Song, Yajuan L¨u, and Qun Liu. 2013. Translation with source constituency and dependency trees. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1066–1076, Seattle, Washington, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>295--302</pages>
<marker>[Och and Ney2002]</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational linguistics,</journal>
<pages>29--1</pages>
<marker>[Och and Ney2003]</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>160--167</pages>
<marker>[Och2003]</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>577--585</pages>
<marker>[Shen et al.2008]</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings of ACL-08: HLT, pages 577–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the international conference on spoken language processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<marker>[Stolcke and others2002]</marker>
<rawString>Andreas Stolcke et al. 2002. Srilm-an extensible language modeling toolkit. In Proceedings of the international conference on spoken language processing, volume 2, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc V Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<tech>CoRR, abs/1409.3215.</tech>
<marker>[Sutskever et al.2014]</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. CoRR, abs/1409.3215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Xie</author>
<author>Haitao Mi</author>
<author>Qun Liu</author>
</authors>
<title>A novel dependency-to-string model for statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>216--226</pages>
<marker>[Xie et al.2011]</marker>
<rawString>Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel dependency-to-string model for statistical machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 216–226.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>