<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant confidence="0.000003" no="0">
<title confidence="0.988654">
Grounded Language Learning from Video Described with Sentences
</title>
<author confidence="0.891237">
Haonan Yu and Jeffrey Mark Siskind
</author>
<affiliation confidence="0.8533505">
Purdue University
School of Electrical and Computer Engineering
</affiliation>
<address confidence="0.730013">
465 Northwestern Ave.
West Lafayette, IN 47907-2035 USA
</address>
<email confidence="0.997622">
haonan@haonanyu.com, qobi@purdue.edu
</email>
<sectionHeader confidence="0.994767" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999987428571429">We present a method that learns representations for word meanings from short video clips paired with sentences. Unlike prior work on learning language from symbolic input, our input consists of video of people interacting with multiple complex objects in outdoor environments. Unlike prior computer-vision approaches that learn from videos with verb labels or images with noun labels, our labels are sentences containing nouns, verbs, prepositions, adjectives, and adverbs. The correspondence between words and concepts in the video is learned in an unsupervised fashion, even when the video depicts simultaneous events described by multiple sentences or when different aspects of a single event are described with multiple sentences. The learned word meanings can be subsequently used to automatically generate description of new video.</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999976719298246">People learn language through exposure to a rich perceptual context. Language is grounded by mapping words, phrases, and sentences to meaning representations referring to the world. Siskind (1996) has shown that even with referential uncertainty and noise, a system based on crosssituational learning can robustly acquire a lexicon, mapping words to word-level meanings from sentences paired with sentence-level meanings. However, it did so only for symbolic representations of wordand sentence-level meanings that were not perceptually grounded. An ideal system would not require detailed word-level labelings to acquire word meanings from video but rather could learn language in a largely unsupervised fashion, just as a child does, from video paired with sentences. There has been recent research on grounded language learning. Roy (2002) pairs training sentences with vectors of real-valued features extracted from synthesized images which depict 2D blocks-world scenes, to learn a specific set of features for adjectives, nouns, and adjuncts. Yu and Ballard (2004) paired training images containing multiple objects with spoken name candidates for the objects to find the correspondence between lexical items and visual features. Dominey and Boucher (2005) paired narrated sentences with symbolic representations of their meanings, automatically extracted from video, to learn object names, spatial-relation terms, and event names as a mapping from the grammatical structure of a sentence to the semantic structure of the associated meaning representation. Chen and Mooney (2008) learned the language of sportscasting by determining the mapping between game commentaries and the meaning representations output by a rulebased simulation of the game. Kwiatkowski et al. (2012) present an approach that learns Montaguegrammar representations of word meanings together with a combinatory categorial grammar (CCG) from child-directed sentences paired with first-order formulas that represent their meaning. Although most of these methods succeed in learning word meanings from sentential descriptions they do so only for symbolic or simple visual input (often synthesized); they fail to bridge the gap between language and computer vision, i.e., they do not attempt to extract meaning representations from complex visual scenes. On the other hand, there has been research on training object and event models from large corpora of complex images and video in the computer-vision community (Kuznetsova et al., 2012; Sadanand and Corso, 2012; Kulkarni et al., 2011; Ordonez et al., 2011; Yao et al., 2010). However, most such work requires training data that labels individual concepts with individual words (i.e., objects delineated via bounding boxes in images as nouns and events that occur in short video clips as verbs).</bodyText>
<page confidence="0.988556">
53
</page>
<note confidence="0.91387">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 53–63,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.998569147058824">There is no attempt to model phrasal or sentential meaning, let alone acquire the object or event models from training data labeled with phrasal or sentential annotations. Moreover, such work uses distinct representations for different parts of speech; i.e., object and event recognizers use different representations. In this paper, we present a method that learns representations for word meanings from short video clips paired with sentences. Our work differs from prior work in three ways. First, our input consists of realistic video filmed in an outdoor environment. Second, we learn the entire lexicon, including nouns, verbs, prepositions, adjectives, and adverbs, simultaneously from video described with whole sentences. Third we adopt a uniform representation for the meanings of words in all parts of speech, namely Hidden Markov Models (HMMs) whose states and distributions allow for multiple possible interpretations of a word or a sentence in an ambiguous perceptual context. We employ the following representation to ground the meanings of words, phrases, and sentences in video clips. We first run an object detector on each video frame to yield a set of detections, each a subregion of the frame. In principle, the object detector need just detect the objects rather than classify them. In practice, we employ a collection of class-, shape-, pose-, and viewpoint-specific detectors and pool the detections to account for objects whose shape, pose, and viewpoint may vary over time. Our methods can learn to associate a single noun with detections produced by multiple detectors. We then string together detections from individual frames to yield tracks for objects that temporally span the video clip. We associate a feature vector with each frame (detection) of each such track. This feature vector can encode image features (including the identity of the particular detector that produced that detection) that correlate with object class; region color, shape, and size features that correlate with object properties; and motion features, such as linear and angular object position, velocity, and acceleration, that correlate with event properties. We also compute features between pairs of tracks to encode the relative position and motion of the pairs of objects that participate in events that involve two participants. In principle, we can also compute features between tuples of any number of tracks. Following Yamoto et al.(1992), Siskind and Morris (1996), and Starner et al.(1998), we represent the meaning of an intransitive verb, like jump, as a two-state HMM over the velocity-direction feature, modeling the requirement that the participant move upward then downward. We represent the meaning of a transitive verb, like pick up, as a two-state HMM over both single-object and object-pair features: the agent moving toward the patient while the patient is as rest, followed by the agent moving together with the patient. We extend this general approach to other parts of speech. Nouns, like person, can be represented as one-state HMMs over image features that correlate with the object classes denoted by those nouns. Adjectives, like red, round, and big, can be represented as one-state HMMs over region color, shape, and size features that correlate with object properties denoted by such adjectives. Adverbs, like quickly, can be represented as one-state HMMs over object-velocity features. Intransitive prepositions, like leftward, can be represented as one-state HMMs over velocity-direction features. Static transitive prepositions, like to the left of, can be represented as one-state HMMs over the relative position of a pair of objects. Dynamic transitive prepositions, like towards, can be represented as HMMs over the changing distance between a pair of objects. Note that with this formulation, the representation of a verb, like approach, might be the same as a dynamic transitive preposition, like towards. While it might seem like overkill to represent the meanings of words as one-stateHMMs, in practice, we often instead encode such concepts with multiple states to allow for temporal variation in the associated features due to changing pose and viewpoint as well as deal with noise and occlusion. Moreover, the general framework of modeling word meanings as temporally variant time series via multi-state HMMs allows one to model denominalized verbs, i.e., nouns that denote events, as in The jump was fast. Our HMMs are parameterized with varying arity. Some, like jump(α), person(α), red(α), round(α), big(α), quickly(α), and leftward(α) have one argument, while others, like pick-up(α, β), to-the-left-of(α, β), and towards(α,β), have two arguments (In principle, any arity can be supported.). HMMs are instantiated by mapping their arguments to tracks.This involves computing the associated feature vector for that HMM over the detections in the tracks chosen to fill its arguments.</bodyText>
<page confidence="0.996979">
54
</page>
<bodyText confidence="0.999966803921569">This is done with a two-step process to support compositional semantics. The meaning of a multi-word phrase or sentence is represented as a joint likelihood of the HMMs for the words in that phrase or sentence. Compositionality is handled by linking or coindexing the arguments of the conjoined HMMs. Thus a sentence like The person to the left of the backpack approached the trashcan would be represented as a conjunction of person(p0), to-the-left-of(p0, p1), backback(p1), approached(p0, p2), and trash-can(p2) over the three participants p0, p1, and p2. This whole sentence is then grounded in a particular video by mapping these participants to particular tracks and instantiating the associated HMMs over those tracks, by computing the feature vectors for each HMM from the tracks chosen to fill its arguments. Our algorithm makes six assumptions. First, we assume that we know the part of speech Cm associated with each lexical entry m, along with the part-of-speech dependent number of states I, in the HMMs used to represent word meanings in that part of speech, the part-of-speech dependent number of features N, in the feature vectors used by HMMs to represent word meanings in that part of speech, and the part-of-speech dependent feature-vector computation Φ, used to compute the features used by HMMs to represent word meanings in that part of speech. Second, we pair individual sentences each with a short video clip that depicts that sentence. The algorithm is not able to determine the alignment between multiple sentences and longer video segments. Note that there is no requirement that the video depict only that sentence. Other objects may be present and other events may occur. In fact, nothing precludes a training corpus with multiple copies of the same video, each paired with a different sentence describing a different aspect of that video. Moreover, our algorithm potentially can handle a small amount of noise, where a video clip is paired with an incorrect sentence that the video does not depict. Third, we assume that we already have (pre-trained) low-level object detectors capable of detecting instances of our target event participants in individual frames of the video. We allow such detections to be unreliable; our method can handle a moderate amount of false positives and false negatives. We do not need to know the mapping from these object-detection classes to words; our algorithm determines that. Fourth, we assume that we know the arity of each word in the corpus, i.e., the number of arguments that that word takes. For example, we assume that we know that the word person(α) takes one argument and the word approached(α, β) takes two arguments. Fifth, we assume that we know the total number of distinct participants that collectively fill all of the arguments for all of the words in each training sentence. For example, for the sentence The person to the left of the backpack approached the trash-can, we assume that we know that there are three distinct objects that participate in the event denoted. Sixth, we assume that we know the argument-to-participant mapping for each training sentence. Thus, for example, for the above sentence we would know person(p0), to-the-left-of(p0, p1), backback(p1), approached(p0, p2), and trash-can(p2). The latter two items can be determined by parsing the sentence, which is what we do. One can imagine learning the ability to automatically perform the latter two items, and even the fourth item above, by learning the grammar and the part of speech of each word, such as done by Kwiatkowski et al.(2012). We leave such for future work. Figure 1 illustrates a single frame from a potential training sample provided as input to our learner. It consists of a video clip paired with a sentence, where the arguments of the words in the sentence are mapped to participants. From a sequence of such training samples, our learner determines the objects tracks and the mapping from participants to those tracks, together with the meanings of the words. The remainder of the paper is organized as follows. Section 2 generally describes our problem of lexical acquisition from video. Section 3 introduces our work on the sentence tracker, a method for jointly tracking the motion of multiple objects in a video that participate in a sententiallyspecified event. Section 4 elaborates on the details of our problem formulation in the context of this sentence tracker. Section 5 describes how to generalize and extend the sentence tracker so that it can be used to support lexical acquisition. We demonstrate this lexical acquisition algorithm on a small example in Section 6. Finally, we conclude with a discussion in Section 7.</bodyText>
<page confidence="0.990824">
55
</page>
<bodyText confidence="0.378164">The person to the left of the backpack carried the trash-can towards the chair.</bodyText>
<figureCaption confidence="0.8285845">
Figure 1: An illustration of our problem. Each
word in the sentence has one or more arguments
</figureCaption>
<bodyText confidence="0.968734166666667">(α and possibly β), each argument of each word is assigned to a participant (p0, ... , p3) in the event described by the sentence, and each participant can be assigned to any object track in the video. This figure shows a possible (but erroneous) interpretation of the sentence where the mapping is: p0 �-+ Track 3, p1 �-+ Track 0, p2 �-+ Track 1, and p3 �-+ Track 2, which might (incorrectly) lead the learner to conclude that the word person maps to the backpack, the word backpack maps to the chair, the word trash-can maps to the trash-can, and the word chair maps to the person.</bodyText>
<sectionHeader confidence="0.980405" genericHeader="method">
2 General Problem Formulation
</sectionHeader>
<bodyText confidence="0.999979209302326">Throughout this paper, lowercase letters are used for variables or hidden quantities while uppercase ones are used for constants or observed quantities. We are given a lexicon {1, ... , M}, letting m denote a lexical entry. We are given a sequence D = (D1, ... , DR) of video clips Dr, each paired with a sentence Sr from a sequence S = (S1, ... , SR) of sentences. We refer to Dr paired with Sr as a training sample. Each sentence Sr is a sequence (Sr,1, . . . , Sr,Lr) of words Sr,l, each an entry from the lexicon. A given entry may potentially appear in multiple sentences and even multiple times in a given sentence. For example, the third word in the first sentence might be the same entry as the second word in the fourth sentence, in which case S1,3 = S4,2. This is what allows cross-situational learning in our algorithm. Let us assume, for a moment, that we can process each video clip Dr to yield a sequence (τr,1, . . . , τr,Ur)of object tracks τr,u. Let us also assume that Dr is paired with a sentence Sr = The person approached the chair, specified to have two participants, pr,0 and pr,1, with the mapping person(pr,0), chair(pr,1), and approached(pr,0, pr,1). Let us further assume, for a moment, that we are given a mapping from participants to object tracks, say pr,0 �-+ τr,39 and pr,1 �-+ τr,51. This would allow us to instantiate the HMMs with object tracks for a given video clip: person(τr,39), chair(τr,51), and approached(τr,39,τr,51). Let us further assume that we can score each such instantiated HMM and aggregate the scores for all of the words in a sentence to yield a sentence score and further aggregate the scores for all of the sentences in the corpus to yield a corpus score. However, we don’t know the parameters of the HMMs. These constitute the unknown meanings of the words in our corpus which we wish to learn. The problem is to simultaneously determine (a) those parameters along with (b) the object tracks and (c) the mapping from participants to object tracks. We do this by finding (a)–(c) that maximizes the corpus score.</bodyText>
<sectionHeader confidence="0.977095" genericHeader="method">
3 The Sentence Tracker
</sectionHeader>
<bodyText confidence="0.999919296296296">Barbu et al. (2012a) presented a method that first determines object tracks from a single video clip and then uses these fixed tracks with HMMs to recognize actions corresponding to verbs and construct sentential descriptions with templates. Later Barbu et al. (2012b) addressed the problem of solving (b) and (c), for a single object track constrained by a single intransitive verb, without solving (a), in the context of a single video clip. Our group has generalized this work to yield an algorithm called the sentence tracker which operates by way of a factorial HMM framework. We introduce that here as the foundation of our extension. Each video clip Dr contains Tr frames. We run an object detector on each frame to yield a set Dtr of detections. Since our object detector is unreliable, we bias it to have high recall but low precision, yielding multiple detections in each frame. We form an object track by selecting a single detection for that track for each frame. For a moment, let us consider a single video clip with length T, with detections Dt in frame t. Further, let us assume that we seek a single object track in that video clip. Let jt denote the index of the detection from Dt in frame t that is selected to form the track. The object detector scores each detection. Let F(Dt, jt) denote that score. Moreover, we wish the track to be temporally coherent; we want the objects in a track to move smoothly over time and not jump around the field of view.</bodyText>
<figure confidence="0.996910571428572">
0
1
2
3
α α β α α β α α β α
p� p� p� p� p� p� p� p� pc pc
Track 3 Track 0 Track 1 Track 2
</figure>
<page confidence="0.983873">
56
</page>
<bodyText confidence="0.999799686746988">Let G(Dt−1, jt−1,Dt, jt) denote some measure of coherence between two detections in adjacent frames.(One possible such measure is consistency of the displacement of Dt relative to Dt−1 with the velocity of Dt−1 computed from the image by optical flow.) One can select the detections to yield a track that maximizes both the aggregate detection score and the aggregate temporal coherence score. This can be determined with the Viterbi (1967) algorithm and is known as detection-based tracking (Viterbi, 1971). Recall that we model the meaning of an intransitive verb as an HMM over a time series of features extracted for its participant in each frame. Let λ denote the parameters of this HMM, (q1, ..., qT ) denote the sequence of states qt that leads to an observed track, B(Dt, jt, qt, λ) denote the conditional log probability of observing the feature vector associated with the detection selected by jt among the detections Dt in frame t, given that the HMM is in state qt, and A(qt−1, qt, λ) denote the log transition probability of the HMM. For a given track (j1, ..., jT ), the state sequence that yields the maximal likelihood is given by: which can also be found by the Viterbi algorithm. A given video clip may depict multiple objects, each moving along its own trajectory. There may be both a person jumping and a ball rolling. How are we to select one track over the other? The key insight of the sentence tracker is to bias the selection of a track so that it matches an HMM. This is done by combining the cost function of Eq.1 with the cost function of Eq.2 to yield Eq.3, which can also be determined using the Viterbi algorithm. This is done by forming the cross product of the two lattices. This jointly selects the optimal detections to form the track, together with the optimal state sequence, and scores that combination. While we formulated the above around a single track and a word that contains a single participant, it is straightforward to extend this so that it supports multiple tracks and words of higher arity by forming a larger cross product. When doing so, we generalize jt to denote a sequence of detections from Dt, one for each of the tracks. We further need to generalize F so that it computes the joint score of a sequence of detections, one for each track, G so that it computes the joint measure of coherence between a sequence of pairs of detections in two adjacent frames, and B so that it computes the joint conditional log probability of observing the feature vectors associated with the sequence of detections selected by jt. When doing this, note that Eqs.1 and 3 maximize over j1, ..., jT which denotes T sequences of detection indices, rather than T individual indices. It is further straightforward to extend the above to support a sequence (S1, ... , SL) of words Sl denoting a sentence, each of which applies to different subsets of the multiple tracks, again by forming a larger cross product. When doing so, we generalize qt to denote a sequence (qt1, ... , qtL) of states qtl, one for each word l in the sentence, and use ql to denote the sequence (q1l , ... , qi) and q to denote the sequence (q1, ... , qL). We further need to generalize B so that it computes the joint conditional log probability of observing the feature vectors for the detections in the tracks that are assigned to the arguments of the HMM for each word in the sentence and A so that it computes the joint log transition probability for the HMMs for all words in the sentence. This allows selection of an optimal sequence of tracks that yields the highest score for the sentential meaning of a sequence of words. Modeling the meaning of a sentence through a sequence of words whose meanings are modeled by HMMs, defines a factorial HMM for that sentence, since the overall Markov process for that sentence can be factored into independent component processes (Brand et al., 1997; Zhong and Ghosh, 2001) for the individual words.</bodyText>
<equation confidence="0.561033">
XT F(Dt, jt)
t=1
+
XT G(Dt−1, jt−1, Dt, jt)
t=2
max
j1,...,jT
⎛
⎜ ⎜ ⎜ ⎜ ⎝
⎞
⎠ ⎟ ⎟ ⎟ ⎟
q1,...,qT
max ⎛
⎜ ⎜ ⎜ ⎜ ⎝
XT B(Dt, jt, qt, λ)
t=1
T
+X A(qt−1, qt, λ)
t=2
⎞
⎠⎟⎟⎟⎟
F(Dt, jt)
+ B(Dt, jt, qt, λ)
T
G(Dt−1, jt−1, Dt, jt)
+ A(qt−1, qt, λ)
XT
t=1
X
+
t=2
⎞
⎠ ⎟ ⎟ ⎟ ⎟ ⎟
max
j1,...,jT
q1,...,qT
⎛
⎜ ⎜ ⎜ ⎜ ⎜ ⎝
</equation>
<page confidence="0.988455">
57
</page>
<bodyText confidence="0.999684">In this view, q denotes the state sequence for the combined factorial HMM and ql denotes the factor of that state sequence for word l. The remainder of this paper wraps this sentence tracker in Baum Welch (Baum et al., 1970; Baum, 1972).</bodyText>
<sectionHeader confidence="0.998151" genericHeader="method">
4 Detailed Problem Formulation
</sectionHeader>
<bodyText confidence="0.998933644444445">We adapt the sentence tracker to training a corpus of R video clips, each paired with a sentence. Thus we augment our notation, generalizing jt to jtr and qt l to qtr,l. Below, we use jr to denote (j1r , ..., jTr r ), j to denote (j1, ... , jR), qr,l to denote (q1r,l, . . . , qTr r,l ), qr to denote (qr,1, ... , qr,Lr), and q to denote (q1, ... , qR). We use discrete features, namely natural numbers, in our feature vectors, quantized by a binning process. We assume the part of speech of entry m is known as Cm. The length of the feature vector may vary across parts of speech. Let Nc denote the length of the feature vector for part of speech c, xr,l denote the time-series (x1r,l, ... , xTr r,l) of feature vectors xtr,l, associated with Sr,l (which recall is some entry m), and xr denote the sequence (xr,1, ... , xr,Lr). We assume that we are given a function 41)c(Dt r, jtr) that computes the feature vector xtr,l for the word Sr,l whose part of speech is CSr,l = c. Note that we allow 41) to be dependent on c allowing different features to be computed for different parts of speech, since we can determine m and thus Cm from Sr,l. We choose to have Nc and 41)c depend on the part of speech c and not on the entry m since doing so would be tantamount to encoding the to-be-learned word meaning in the provided feature-vector computation. The goal of training is to find a sequence λ = (λ1, ... , λM) of parameters λm that best explains the R training samples. The parameters λm constitute the meaning of the entry m in the lexicon. Collectively, these are the initial state probabilities am0,k, for 1 ≤ k ≤ ICm, the transition probabilities ami,k, for 1 ≤ i, k ≤ ICm, and the output probabilities bmi,n(x), for 1 ≤ i ≤ ICm and 1 ≤ n ≤ NCm, where ICm denotes the number of states in the HMM for entry m. Like before, we could have a distinct Im for each entry m but instead have ICm depend only on the part of speech of entry m, and assume that we know the fixed I for each part of speech. In our case, bmi,n is a discrete distribution because the features are binned.</bodyText>
<sectionHeader confidence="0.955837" genericHeader="method">
5 The Learning Algorithm
</sectionHeader>
<bodyText confidence="0.999974">Instantiating the above approach requires a definition for what it means to best explain the R training samples. Towards this end, we define the score of a video clip Dr paired with sentence Sr given the parameter set λ to characterize how well this training sample is explained. While the cost function in Eq. 3 may qualify as a score, it is easier to fit a likelihood calculation into the Baum-Welch framework than a MAP estimate. Thus we replace the max in Eq. 3 with a P and redefine our scoring function as follows:</bodyText>
<equation confidence="0.98612">
L(Dr; Sr, λ) = X P(jr|Dr)P(xr|Sr, λ) (4)
jr
</equation>
<bodyText confidence="0.976902153846154">The score in Eq. 4 can be interpreted as an expectation of the HMM likelihood over all possible mappings from participants to all possible tracks. By definition, P(jr|Dr) = V (Dr,jr) �j�rV(Dr,j,r), where the numerator is the score of a particular track sequence jr while the denominator sums the scores over all possible track sequences. The log of the numerator V (Dr, jr) is simply Eq. 1 without the max. The log of the denominator can be computed efficiently by the forward algorithm (Baum and Petrie, 1966). The likelihood for a factorial HMM can be computed as:</bodyText>
<equation confidence="0.503735">
P(xr|Sr, λ) = X Y P(xr,l, qr,l|Sr,l, λ) (5)
qr l
</equation>
<bodyText confidence="0.99937725">i.e., summing the likelihoods for all possible state sequences. Each summand is simply the joint likelihood for all the words in the sentence conditioned on a state sequence qr. For HMMs we have</bodyText>
<equation confidence="0.4603962">
YP (xr,l, qr,l|Sr,l, λ) = (6)
t a qt−1 Y r,l,qt r,l
Sr l
bqt n(xtr,l,n)
n
</equation>
<bodyText confidence="0.9971145">Finally, for a training corpus of R samples, we seek to maximize the joint score:</bodyText>
<equation confidence="0.9821105">
L(D; S, λ) = Y L(Dr; Sr, λ) (7)
r
</equation>
<bodyText confidence="0.999853">A local maximum can be found by employing the Baum-Welch algorithm (Baum et al., 1970; Baum, 1972). By constructing an auxiliary function (Bilmes, 1997), one can derive the reestimation formulas in Eq. 8, where xtr,l,n = h denotes the selection of all possible jtr such that the nth feature computed by ΦC„(Dtr, jtr) is h. The coefficients θmi and ψmi,n are for normalization.</bodyText>
<page confidence="0.963536">
58
</page>
<equation confidence="0.936292761904762">
am i,k = θm i XR XLr XTr
r=1 l=1 t=1
s.t.Sr�J=m
bmi,n(h) =ψmi,n
s.t.Sr�J=m
XLr
l=1
XR
r=1
L(qt−1
r,l = i, qtr,l = k, Dr; Sr, λ0)
L(Dr; Sr, λ0)
 |{z }
ξ(r,l,i,k,t)
L(qtr,l = i, xtr,l,n = h, Dr; Sr, λ0)
L(Dr; Sr, λ0)
 |{z }
γ(r,l,n,i,h,t)
XTr
t=1
(8)
</equation>
<bodyText confidence="0.998671642857143">The reestimation formulas involve occurrence counting. However, since we use a factorial HMM that involves a cross-product lattice and use a scoring function derived from Eq.3 that incorporates both tracking (Eq.1) and word models (Eq.2), we need to count the frequency of transitions in the whole cross-product lattice. As an example of such cross-product occurrence counting, when counting the transitions from state i to k for the lth word from frame t − 1 to t, i.e., ξ(r, l, i, k, t), we need to count all the possible paths through the adjacent factorial states (jt−1 r,l = i and qt r,l = k. Similarly, when counting the frequency of being at state i while observing h as the nth feature in frame t for the lth word of entry m, i.e., γ(r,l, n, i, h, t), we need to count all the possible paths through the factorial state (jtr, qtr,1, ..., qt r,Lr) such that qtr,l = i and the nth feature computed by ΦC„(Dtr, jtr) is h. The reestimation of a single component HMM can depend on the previous estimate for other component HMMs.</bodyText>
<equation confidence="0.76995025">
r , qt−1
r,1 , . . . ,qt−1
r,Lr)
and (jtr, qtr,1, ... , qtr,Lr) such that qt−1
</equation>
<bodyText confidence="0.989588909090909">This dependence happens because of the argument-to-participant mapping which coindexes arguments of different component HMMs to the same track. It is precisely this dependence that leads to cross-situational learning of two kinds: both inter-sentential and intra-sentential. Acquisition of a word meaning is driven across sentences by entries that appear in more than one training sample and within sentences by the requirement that the meanings of all of the individual words in a sentence be consistent with the collective sentential meaning.</bodyText>
<sectionHeader confidence="0.996654" genericHeader="evaluation and result">
6 Experiment
</sectionHeader>
<bodyText confidence="0.999944">We filmed 61 video clips (each 3–5 seconds at 640×480 resolution and 40 fps) that depict a variety of different compound events. Each clip depicts multiple simultaneous events between some</bodyText>
<equation confidence="0.992725363636364">
S → NP VP
NP → D N [PP]
D → the
N → person  |backpack  |trash-can  |chair
PP → PNP
P → to the left of  |to the right of
VP → V NP [ADV] [PPM]
V → picked up  |put down  |carried  |approached
ADV → quickly  |slowly
PPM → PM NP
PM → towards  |away from
</equation>
<tableCaption confidence="0.908034">
Table 1: The grammar used for our annotation and
generation.</tableCaption>
<listItem confidence="0.4249125">Our lexicon contains 1 determiner, 4 nouns, 2 spatial relation prepositions, 4 verbs, 2 adverbs, and 2 motion prepositions for a total of 15 lexical entries over 6 parts of speech.</listItem>
<bodyText confidence="0.999774346153846">subset of four objects: a person, a backpack, a chair, and a trash-can. These clips were filmed in three different outdoor environments which we use for cross validation. We manually annotated each video with several sentences that describe what occurs in that video. The sentences were constrained to conform to the grammar in Table 1. Our corpus of 159 training samples pairs some videos with more than one sentence and some sentences with more than one video, with an average of 2.6 sentences per video 1. We model and learn the semantics of all words except determiners. Table 2 specifies the arity, the state number Ic, and the features computed by Φc for the semantic models for words of each part of speech c. While we specify a different subset of features for each part of speech, we presume that, in principle, with enough training data, we could include all features in all parts of speech and automatically learn which ones are noninformative and lead to uniform distributions. We use an off-the-shelf object detector (Felzenszwalb et al., 2010a; Felzenszwalb et al., 2010b) which outputs detections in the form of scored axis-aligned rectangles. We trained four object detectors, one for each of the four object classes in our corpus: person, backpack, chair, and trashcan.</bodyText>
<footnote confidence="0.999217333333333">
1Our code, videos, and sentential annotations are
available at http://haonanyu.com/research/
acl2013/.
</footnote>
<page confidence="0.996142">
59
</page>
<table confidence="0.764897">
C arity I. Φ.
N 1 1 α detector index
</table>
<tableCaption confidence="0.83671">
Table 2: Arguments and model configurations for
different parts of speech c. VEL stands for veloc-
ity, MAG for magnitude, ORIENT for orientation,
and DIST for distance.
</tableCaption>
<bodyText confidence="0.999753109589041">For each frame, we pick the two highestscoring detections produced by each object detector and pool the results yielding eight detections per frame. Having a larger pool of detections per frame can better compensate for false negatives in the object detection and potentially yield smoother tracks but it increases the size of the lattice and the concomitant running time and does not lead to appreciably better performance on our corpus. We compute continuous features, such as velocity, distance, size ratio, and x-position solely from the detection rectangles and quantize the features into bins as follows: velocity To reduce noise, we compute the velocity of a participant by averaging the optical flow in the detection rectangle. The velocity magnitude is quantized into 5 levels: absolutely stationary, stationary, moving, fast moving, and quickly. The velocity orientation is quantized into 4 directions: left, up, right, and down.distance We compute the Euclidean distance between the detection centers of two participants, which is quantized into 3 levels: near, normal, and far away.size ratio We compute the ratio of detection area of the first participant to the detection area of the second participant, quantized into 2 possibilities: larger/smaller than.x-position We compute the difference between the x-coordinates of the participants, quantized into 2 possibilities: to the left/right of. The binning process was determined by a preprocessing step that clustered a subset of the training data. We also incorporate the index of the detector that produced the detection as a feature. The particular features computed for each part of speech are given in Table 2. Note that while we use English phrases, like to the left of, to refer to particular bins of particular features, and we have object detectors which we train on samples of a particular object class such as backpack, such phrases are only mnemonic of the clustering and object-detector training process. We do not have a fixed correspondence between the lexical entries and any particular feature value. Moreover, that correspondence need not be oneto-one: a given lexical entry may correspond to a (time variant) constellation of feature values and any given feature value may participate in the meaning of multiple lexical entries. We perform a three-fold cross validation, taking the test data for each fold to be the videos filmed in a given outdoor environment and the training data for that fold to be all training samples that contain other videos. For testing, we hand selected 24 sentences generated by the grammar in Table 1, where each sentence is true for at least one test video. Half of these sentences (designated NV) contain only nouns and verbs while the other half (designated ALL) contain other parts of speech. The latter are longer and more complicated than the former. We score each testing video paired with every sentence in both NV and ALL. To evaluate our results, we manually annotated the correctness of each such pair. Video-sentence pairs could be scored with Eq.4. However, the score depends on the sentence length, the collective numbers of states and features in the HMMs for words in that sentence, and the length of the video clip. To render the scores comparable across such variation we incorporate a sentence prior to the per-frame score:</bodyText>
<equation confidence="0.871623">
ˆL(Dr, Sr; A) = [L(Dr; Sr, A)] Tr π(Sr) (9)
</equation>
<bodyText confidence="0.874667">where</bodyText>
<equation confidence="0.706552166666667">
π(Sr) =
E(ICSr,l )
NCSr,l
�
+
n=1
</equation>
<bodyText confidence="0.999832">In the above, ZCSr,l,n is the number of bins for the nth feature of Sr,l of part of speech CSr,l and E(Y ) = − Ey 1 1 log 1 = log Y is the entropy of a uniform distribution over Y bins. This prior prefers longer sentences which describe more information in the video.</bodyText>
<figure confidence="0.989467047619048">
α VEL MAG
α VEL ORIENT
β VEL MAG
β VEL ORIENT
α-β DIST
α-β size ratio
P 2 1 α-β x-position
ADV 1 3 α VEL MAG
3 α VEL MAG
PM 2
α-β DIST
V 2 3
⎛
⎜ ⎜ ⎝
exp
�Lr
l=1
E(ZCSr,l,n)
⎞
⎠ ⎟ ⎟
(10)
</figure>
<page confidence="0.930932">
60
</page>
<table confidence="0.907665333333333">
CHANCE BLIND OUR HAND
NV 0.155 0.265 0.545 0.748
ALL 0.099 0.198 0.639 0.786
</table>
<tableCaption confidence="0.998457">
Table 3: F1 scores of different methods.
</tableCaption>
<figureCaption confidence="0.9907265">
Figure 2: ROC curves of trained models and hand-
written models.
</figureCaption>
<bodyText confidence="0.99999025">The scores are thresholded to decide hits, which together with the manual annotations, can generate TP, TN, FP, and FN counts. We select the threshold that leads to the maximal F1 score on the training set, use this threshold to compute F1 scores on the test set in each fold, and average F1 scores across the folds. The F1 scores are listed in the column labeled Our in Table 3. For comparison, we also report F1 scores for three baselines: Chance, Blind, and Hand. The Chance baseline randomly classifies a video-sentence pair as a hit with probability 0.5. The Blind baseline determines hits by potentially looking at the sentence but never looking at the video. We can find an upper bound on the F1 score that any blind method could have on each of our test sets by solving a 0-1 fractional programming problem (Dinkelbach, 1967) (see Appendix A for details). The Hand baseline determines hits with hand-coded HMMs, carefully designed to yield what we believe is near-optimal performance. As can be seen from Table 3, our trained models perform substantially better than the Chance and Blind baselines and approach the performance of the ideal Hand baseline. One can further see from the ROC curves in Figure 2, comparing the trained and hand-written models on both NV and ALL, that the trained models are close to optimal. Note that performance on ALL exceeds that on NV with the trained models. This is because longer sentences with varied parts of speech incorporate more information into the scoring process.</bodyText>
<sectionHeader confidence="0.996434" genericHeader="conclusion">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999983388888889">We presented a method that learns word meanings from video paired with sentences. Unlike prior work, our method deals with realistic video scenes labeled with whole sentences, not individual words labeling hand delineated objects or events. The experiment shows that it can correctly learn the meaning representations in terms of HMM parameters for our lexical entries, from highly ambiguous training data. Our maximumlikelihood method makes use of only positive sentential labels. As such, it might require more training data for convergence than a method that also makes use of negative training sentences that are not true of a given video. Such can be handled with discriminative training, a topic we plan to address in the future. We believe that this will allow learning larger lexicons from more complex video without excessive amounts of training data.</bodyText>
<sectionHeader confidence="0.998465" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999952">This research was sponsored by the Army Research Laboratory and was accomplished under Cooperative Agreement Number W911NF-10-20060. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either express or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes, notwithstanding any copyright notation herein.</bodyText>
<subsectionHeader confidence="0.774862">
A An Upper Bound on the F1 Score of
any Blind Method
</subsectionHeader>
<bodyText confidence="0.999962625">A Blind algorithm makes identical decisions on the same sentence paired with different video clips. An optimal algorithm will try to find a decision si for each test sentence i that maximizes the F1 score. Suppose, the ground-truth yields FPi false positives and TPi true positives on the test set when si = 1. Also suppose that setting si = 0 yields FNi false negatives. Then the F1 score is • v • Δ</bodyText>
<equation confidence="0.9246288">
1
F1 =
�i siFPi + (1 − si)FNi
1 +
�i 2siTPi
</equation>
<bodyText confidence="0.9968885">Thus we want to minimize the term A. This is an instance of a 0-1 fractional programming problem which can be solved by binary search or Dinkelbach’s algorithm (Dinkelbach, 1967).</bodyText>
<page confidence="0.999015">
61
</page>
<sectionHeader confidence="0.993908" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999923603773584">
A. Barbu, A. Bridge, Z. Burchill, D. Coroian, S. Dick-
inson, S. Fidler, A. Michaux, S. Mussman, N. Sid-
dharth, D. Salvi, L. Schmidt, J. Shangguan, J. M.
Siskind, J. Waggoner, S. Wang, J. Wei, Y. Yin, and
Z. Zhang. 2012a. Video in sentences out. In Pro-
ceedings of the Twenty-Eighth Conference on Un-
certainty in Artificial Intelligence, pages 102–112.
A. Barbu, N. Siddharth, A. Michaux, and J. M. Siskind.
2012b. Simultaneous object detection, tracking, and
event recognition. Advances in Cognitive Systems,
2:203–220, December.
L. E. Baum and T. Petrie. 1966. Statistical inference
for probabilistic functions of finite state Markov
chains. The Annals of Mathematical Statistics,
37:1554–1563.
L. E. Baum, T. Petrie, G. Soules, and N. Weiss. 1970.
A maximization technique occuring in the statistical
analysis of probabilistic functions of Markov chains.
The Annals of Mathematical Statistics, 41(1):164–
171.
L. E. Baum. 1972. An inequality and associated maxi-
mization technique in statistical estimation of proba-
bilistic functions of a Markov process. Inequalities,
3:1–8.
J. Bilmes. 1997. A gentle tutorial of the EM algorithm
and its application to parameter estimation for Gaus-
sian mixture and hidden Markov models. Technical
Report TR-97-021, ICSI.
M. Brand, N. Oliver, and A. Pentland. 1997. Coupled
hidden Markov models for complex action recog-
nition. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages
994–999.
D. L. Chen and R. J. Mooney. 2008. Learning to
sportscast: A test of grounded language acquisition.
In Proceedings of the 25th International Conference
on Machine Learning.
W. Dinkelbach. 1967. On nonlinear fractional pro-
gramming. Management Science, 13(7):492–498.
P. F. Dominey and J.-D. Boucher. 2005. Learning to
talk about events from narrated video in a construc-
tion grammar framework. Artificial Intelligence,
167(12):31–61.
P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and
D. Ramanan. 2010a. Object detection with discrim-
inatively trained part-based models. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
32(9):1627–1645.
P. F. Felzenszwalb, R. B. Girshick, and D. A.
McAllester. 2010b. Cascade object detection with
deformable part models. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recog-
nition, pages 2241–2248.
G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C.
Berg, and T. L. Berg. 2011. Baby talk: Understand-
ing and generating simple image descriptions. In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 1601–1608.
P. Kuznetsova, V. Ordonez, A. C. Berg, T. L. Berg, and
Y. Choi. 2012. Collective generation of natural im-
age descriptions. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics: Long Papers - Volume 1, pages 359–368.
T. Kwiatkowski, S. Goldwater, L. Zettlemoyer, and
M. Steedman. 2012. A probabilistic model of syn-
tactic and semantic acquisition from child-directed
utterances and their meanings. In Proceedings of the
13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 234–
244.
V. Ordonez, G. Kulkarni, and T. L. Berg. 2011.
Im2text: Describing images using 1 million cap-
tioned photographs. In Proceedings of Neural In-
formation Processing Systems.
D. Roy. 2002. Learning visually-grounded words
and syntax for a scene description task. Computer
Speech and Language, 16:353–385.
S. Sadanand and J. J. Corso. 2012. Action bank: A
high-level representation of activity in video. In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 1234–1241.
J. M. Siskind and Q. Morris. 1996. A maximum-
likelihood approach to visual event classification. In
Proceedings of the Fourth European Conference on
Computer Vision, pages 347–360.
J. M. Siskind. 1996. A computational study of cross-
situational techniques for learning word-to-meaning
mappings. Cognition, 61:39–91.
T. Starner, J. Weaver, and A. Pentland. 1998. Real-
time American Sign Language recognition using
desk and wearable computer based video. IEEE
Transactions on Pattern Analysis and Machine In-
telligence, 20(12):1371–1375.
A. J. Viterbi. 1967. Error bounds for convolutional
codes and an asymtotically optimum decoding algo-
rithm. IEEE Transactions on Information Theory,
13:260–267.
A. Viterbi. 1971. Convolutional codes and their per-
formance in communication systems. IEEE Trans-
actions on Communication Technology, 19(5):751–
772.
J. Yamoto, J. Ohya, and K. Ishii. 1992. Recogniz-
ing human action in time-sequential images using
hidden Markov model. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recog-
nition, pages 379–385.
</reference>
<page confidence="0.980763">
62
</page>
<reference confidence="0.965179545454545">
B. Z. Yao, X. Yang, L. Lin, M. W. Lee, and S.-C. Zhu.
2010. I2T: Image parsing to text description. Pro-
ceedings of the IEEE, 98(8):1485–1508, August.
C. Yu and D. H. Ballard. 2004. On the integration of
grounding language and learning objects. In Pro-
ceedings of the 19th National Conference on Artifi-
cal intelligence, pages 488–493.
S. Zhong and J. Ghosh. 2001. A new formulation of
coupled hidden Markov models. Technical report,
Department of Electrical and Computer Engineer-
ing, The University of Texas at Austin.
</reference>
<page confidence="0.999516">
63
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant confidence="0.489940" no="0">
<title confidence="0.999803">Grounded Language Learning from Video Described with Sentences</title>
<author confidence="0.997532">Yu Mark</author>
<affiliation confidence="0.75279">Purdue School of Electrical and Computer</affiliation>
<address confidence="0.992613">465 Northwestern West Lafayette, IN 47907-2035</address>
<abstract confidence="0.999204227272727">We present a method that learns representations for word meanings from short video clips paired with sentences. Unlike prior work on learning language from symbolic input, our input consists of video of people interacting with multiple complex objects in outdoor environments. Unlike prior computer-vision approaches that learn from videos with verb labels or images with noun labels, our labels are sentences containing nouns, verbs, prepositions, adjectives, and adverbs. The correspondence between words and concepts in the video is learned in an unsupervised fashion, even when the video depicts simultaneous events described by multiple sentences or when different aspects of a single event are described with multiple sentences. The learned word meanings can be subsequently used to automatically generate description of new video.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Barbu</author>
<author>A Bridge</author>
<author>Z Burchill</author>
<author>D Coroian</author>
<author>S Dickinson</author>
<author>S Fidler</author>
<author>A Michaux</author>
<author>S Mussman</author>
<author>N Siddharth</author>
<author>D Salvi</author>
<author>L Schmidt</author>
<author>J Shangguan</author>
<author>J M Siskind</author>
<author>J Waggoner</author>
<author>S Wang</author>
<author>J Wei</author>
<author>Y Yin</author>
<author>Z Zhang</author>
</authors>
<title>Video in sentences out.</title>
<date>2012</date>
<booktitle>In Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence,</booktitle>
<pages>102--112</pages>
<contexts>
<context citStr="Barbu et al. (2012" endWordPosition="2731" position="16680" startWordPosition="2728"> can score each such instantiated HMM and aggregate the scores for all of the words in a sentence to yield a sentence score and further aggregate the scores for all of the sentences in the corpus to yield a corpus score. However, we don’t know the parameters of the HMMs. These constitute the unknown meanings of the words in our corpus which we wish to learn. The problem is to simultaneously determine (a) those parameters along with (b) the object tracks and (c) the mapping from participants to object tracks. We do this by finding (a)–(c) that maximizes the corpus score. 3 The Sentence Tracker Barbu et al. (2012a) presented a method that first determines object tracks from a single video clip and then uses these fixed tracks with HMMs to recognize actions corresponding to verbs and construct sentential descriptions with templates. Later Barbu et al. (2012b) addressed the problem of solving (b) and (c), for a single object track constrained by a single intransitive verb, without solving (a), in the context of a single video clip. Our group has generalized this work to yield an algorithm called the sentence tracker which operates by way of a factorial HMM framework. We introduce that here as the founda</context>
</contexts>
<marker>Barbu, Bridge, Burchill, Coroian, Dickinson, Fidler, Michaux, Mussman, Siddharth, Salvi, Schmidt, Shangguan, Siskind, Waggoner, Wang, Wei, Yin, Zhang, 2012</marker>
<rawString>A. Barbu, A. Bridge, Z. Burchill, D. Coroian, S. Dickinson, S. Fidler, A. Michaux, S. Mussman, N. Siddharth, D. Salvi, L. Schmidt, J. Shangguan, J. M. Siskind, J. Waggoner, S. Wang, J. Wei, Y. Yin, and Z. Zhang. 2012a. Video in sentences out. In Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence, pages 102–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Barbu</author>
<author>N Siddharth</author>
<author>A Michaux</author>
<author>J M Siskind</author>
</authors>
<title>Simultaneous object detection, tracking, and event recognition.</title>
<date>2012</date>
<booktitle>Advances in Cognitive Systems,</booktitle>
<pages>2--203</pages>
<contexts>
<context citStr="Barbu et al. (2012" endWordPosition="2731" position="16680" startWordPosition="2728"> can score each such instantiated HMM and aggregate the scores for all of the words in a sentence to yield a sentence score and further aggregate the scores for all of the sentences in the corpus to yield a corpus score. However, we don’t know the parameters of the HMMs. These constitute the unknown meanings of the words in our corpus which we wish to learn. The problem is to simultaneously determine (a) those parameters along with (b) the object tracks and (c) the mapping from participants to object tracks. We do this by finding (a)–(c) that maximizes the corpus score. 3 The Sentence Tracker Barbu et al. (2012a) presented a method that first determines object tracks from a single video clip and then uses these fixed tracks with HMMs to recognize actions corresponding to verbs and construct sentential descriptions with templates. Later Barbu et al. (2012b) addressed the problem of solving (b) and (c), for a single object track constrained by a single intransitive verb, without solving (a), in the context of a single video clip. Our group has generalized this work to yield an algorithm called the sentence tracker which operates by way of a factorial HMM framework. We introduce that here as the founda</context>
</contexts>
<marker>Barbu, Siddharth, Michaux, Siskind, 2012</marker>
<rawString>A. Barbu, N. Siddharth, A. Michaux, and J. M. Siskind. 2012b. Simultaneous object detection, tracking, and event recognition. Advances in Cognitive Systems, 2:203–220, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L E Baum</author>
<author>T Petrie</author>
</authors>
<title>Statistical inference for probabilistic functions of finite state Markov chains.</title>
<date>1966</date>
<booktitle>The Annals of Mathematical Statistics,</booktitle>
<pages>37--1554</pages>
<contexts>
<context citStr="Baum and Petrie, 1966" endWordPosition="4513" position="25953" startWordPosition="4510">max in Eq. 3 with a P and redefine our scoring function as follows: L(Dr; Sr, λ) = X P(jr|Dr)P(xr|Sr, λ) (4) jr The score in Eq. 4 can be interpreted as an expectation of the HMM likelihood over all possible mappings from participants to all possible tracks. By definition, P(jr|Dr) = V (Dr,jr) �j�rV(Dr,j,r), where the numerator is the score of a particular track sequence jr while the denominator sums the scores over all possible track sequences. The log of the numerator V (Dr, jr) is simply Eq. 1 without the max. The log of the denominator can be computed efficiently by the forward algorithm (Baum and Petrie, 1966). The likelihood for a factorial HMM can be computed as: P(xr|Sr, λ) = X Y P(xr,l, qr,l|Sr,l, λ) (5) qr l i.e., summing the likelihoods for all possible state sequences. Each summand is simply the joint likelihood for all the words in the sentence conditioned on a state sequence qr. For HMMs we have YP (xr,l, qr,l|Sr,l, λ) = (6) t a qt−1 Y r,l,qt r,l Sr l bqt n(xtr,l,n) n Finally, for a training corpus of R samples, we seek to maximize the joint score: L(D; S, λ) = Y L(Dr; Sr, λ) (7) r A local maximum can be found by employing the Baum-Welch algorithm (Baum et al., 1970; Baum, 1972). By constr</context>
</contexts>
<marker>Baum, Petrie, 1966</marker>
<rawString>L. E. Baum and T. Petrie. 1966. Statistical inference for probabilistic functions of finite state Markov chains. The Annals of Mathematical Statistics, 37:1554–1563.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L E Baum</author>
<author>T Petrie</author>
<author>G Soules</author>
<author>N Weiss</author>
</authors>
<title>A maximization technique occuring in the statistical analysis of probabilistic functions of Markov chains.</title>
<date>1970</date>
<journal>The Annals of Mathematical Statistics,</journal>
<volume>41</volume>
<issue>1</issue>
<pages>171</pages>
<contexts>
<context citStr="Baum et al., 1970" endWordPosition="3886" position="22729" startWordPosition="3883"> t=1 + XT G(Dt−1, jt−1, Dt, jt) t=2 max j1,...,jT ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ q1,...,qT max ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ XT B(Dt, jt, qt, λ) t=1 T +X A(qt−1, qt, λ) t=2 ⎞ ⎠⎟⎟⎟⎟ F(Dt, jt) + B(Dt, jt, qt, λ) T G(Dt−1, jt−1, Dt, jt) + A(qt−1, qt, λ) XT t=1 X + t=2 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ max j1,...,jT q1,...,qT ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ 57 pendent component processes (Brand et al., 1997; Zhong and Ghosh, 2001) for the individual words. In this view, q denotes the state sequence for the combined factorial HMM and ql denotes the factor of that state sequence for word l. The remainder of this paper wraps this sentence tracker in Baum Welch (Baum et al., 1970; Baum, 1972). 4 Detailed Problem Formulation We adapt the sentence tracker to training a corpus of R video clips, each paired with a sentence. Thus we augment our notation, generalizing jt to jtr and qt l to qtr,l. Below, we use jr to denote (j1r , ..., jTr r ), j to denote (j1, ... , jR), qr,l to denote (q1r,l, . . . , qTr r,l ), qr to denote (qr,1, ... , qr,Lr), and q to denote (q1, ... , qR). We use discrete features, namely natural numbers, in our feature vectors, quantized by a binning process. We assume the part of speech of entry m is known as Cm. The length of the feature vector may v</context>
<context citStr="Baum et al., 1970" endWordPosition="4626" position="26529" startWordPosition="4623">forward algorithm (Baum and Petrie, 1966). The likelihood for a factorial HMM can be computed as: P(xr|Sr, λ) = X Y P(xr,l, qr,l|Sr,l, λ) (5) qr l i.e., summing the likelihoods for all possible state sequences. Each summand is simply the joint likelihood for all the words in the sentence conditioned on a state sequence qr. For HMMs we have YP (xr,l, qr,l|Sr,l, λ) = (6) t a qt−1 Y r,l,qt r,l Sr l bqt n(xtr,l,n) n Finally, for a training corpus of R samples, we seek to maximize the joint score: L(D; S, λ) = Y L(Dr; Sr, λ) (7) r A local maximum can be found by employing the Baum-Welch algorithm (Baum et al., 1970; Baum, 1972). By constructing an auxiliary function (Bilmes, 1997), one can derive the reestimation formulas in Eq. 8, where xtr,l,n = h denotes the selection of all possible jtr such that the nth 58 am i,k = θm i XR XLr XTr r=1 l=1 t=1 s.t.Sr�J=m bmi,n(h) =ψmi,n s.t.Sr�J=m XLr l=1 XR r=1 L(qt−1 r,l = i, qtr,l = k, Dr; Sr, λ0) L(Dr; Sr, λ0) |{z } ξ(r,l,i,k,t) L(qtr,l = i, xtr,l,n = h, Dr; Sr, λ0) L(Dr; Sr, λ0) |{z } γ(r,l,n,i,h,t) XTr t=1 (8) feature computed by ΦC„(Dtr, jtr) is h. The coefficients θmi and ψmi,n are for normalization. The reestimation formulas involve occurrence counting. How</context>
</contexts>
<marker>Baum, Petrie, Soules, Weiss, 1970</marker>
<rawString>L. E. Baum, T. Petrie, G. Soules, and N. Weiss. 1970. A maximization technique occuring in the statistical analysis of probabilistic functions of Markov chains. The Annals of Mathematical Statistics, 41(1):164– 171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L E Baum</author>
</authors>
<title>An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process.</title>
<date>1972</date>
<journal>Inequalities,</journal>
<volume>3</volume>
<contexts>
<context citStr="Baum, 1972" endWordPosition="3888" position="22742" startWordPosition="3887">t−1, Dt, jt) t=2 max j1,...,jT ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ q1,...,qT max ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ XT B(Dt, jt, qt, λ) t=1 T +X A(qt−1, qt, λ) t=2 ⎞ ⎠⎟⎟⎟⎟ F(Dt, jt) + B(Dt, jt, qt, λ) T G(Dt−1, jt−1, Dt, jt) + A(qt−1, qt, λ) XT t=1 X + t=2 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ max j1,...,jT q1,...,qT ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ 57 pendent component processes (Brand et al., 1997; Zhong and Ghosh, 2001) for the individual words. In this view, q denotes the state sequence for the combined factorial HMM and ql denotes the factor of that state sequence for word l. The remainder of this paper wraps this sentence tracker in Baum Welch (Baum et al., 1970; Baum, 1972). 4 Detailed Problem Formulation We adapt the sentence tracker to training a corpus of R video clips, each paired with a sentence. Thus we augment our notation, generalizing jt to jtr and qt l to qtr,l. Below, we use jr to denote (j1r , ..., jTr r ), j to denote (j1, ... , jR), qr,l to denote (q1r,l, . . . , qTr r,l ), qr to denote (qr,1, ... , qr,Lr), and q to denote (q1, ... , qR). We use discrete features, namely natural numbers, in our feature vectors, quantized by a binning process. We assume the part of speech of entry m is known as Cm. The length of the feature vector may vary across pa</context>
<context citStr="Baum, 1972" endWordPosition="4628" position="26542" startWordPosition="4627">Baum and Petrie, 1966). The likelihood for a factorial HMM can be computed as: P(xr|Sr, λ) = X Y P(xr,l, qr,l|Sr,l, λ) (5) qr l i.e., summing the likelihoods for all possible state sequences. Each summand is simply the joint likelihood for all the words in the sentence conditioned on a state sequence qr. For HMMs we have YP (xr,l, qr,l|Sr,l, λ) = (6) t a qt−1 Y r,l,qt r,l Sr l bqt n(xtr,l,n) n Finally, for a training corpus of R samples, we seek to maximize the joint score: L(D; S, λ) = Y L(Dr; Sr, λ) (7) r A local maximum can be found by employing the Baum-Welch algorithm (Baum et al., 1970; Baum, 1972). By constructing an auxiliary function (Bilmes, 1997), one can derive the reestimation formulas in Eq. 8, where xtr,l,n = h denotes the selection of all possible jtr such that the nth 58 am i,k = θm i XR XLr XTr r=1 l=1 t=1 s.t.Sr�J=m bmi,n(h) =ψmi,n s.t.Sr�J=m XLr l=1 XR r=1 L(qt−1 r,l = i, qtr,l = k, Dr; Sr, λ0) L(Dr; Sr, λ0) |{z } ξ(r,l,i,k,t) L(qtr,l = i, xtr,l,n = h, Dr; Sr, λ0) L(Dr; Sr, λ0) |{z } γ(r,l,n,i,h,t) XTr t=1 (8) feature computed by ΦC„(Dtr, jtr) is h. The coefficients θmi and ψmi,n are for normalization. The reestimation formulas involve occurrence counting. However, since w</context>
</contexts>
<marker>Baum, 1972</marker>
<rawString>L. E. Baum. 1972. An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process. Inequalities, 3:1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bilmes</author>
</authors>
<title>A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and hidden Markov models.</title>
<date>1997</date>
<tech>Technical Report TR-97-021, ICSI.</tech>
<contexts>
<context citStr="Bilmes, 1997" endWordPosition="4636" position="26596" startWordPosition="4635">al HMM can be computed as: P(xr|Sr, λ) = X Y P(xr,l, qr,l|Sr,l, λ) (5) qr l i.e., summing the likelihoods for all possible state sequences. Each summand is simply the joint likelihood for all the words in the sentence conditioned on a state sequence qr. For HMMs we have YP (xr,l, qr,l|Sr,l, λ) = (6) t a qt−1 Y r,l,qt r,l Sr l bqt n(xtr,l,n) n Finally, for a training corpus of R samples, we seek to maximize the joint score: L(D; S, λ) = Y L(Dr; Sr, λ) (7) r A local maximum can be found by employing the Baum-Welch algorithm (Baum et al., 1970; Baum, 1972). By constructing an auxiliary function (Bilmes, 1997), one can derive the reestimation formulas in Eq. 8, where xtr,l,n = h denotes the selection of all possible jtr such that the nth 58 am i,k = θm i XR XLr XTr r=1 l=1 t=1 s.t.Sr�J=m bmi,n(h) =ψmi,n s.t.Sr�J=m XLr l=1 XR r=1 L(qt−1 r,l = i, qtr,l = k, Dr; Sr, λ0) L(Dr; Sr, λ0) |{z } ξ(r,l,i,k,t) L(qtr,l = i, xtr,l,n = h, Dr; Sr, λ0) L(Dr; Sr, λ0) |{z } γ(r,l,n,i,h,t) XTr t=1 (8) feature computed by ΦC„(Dtr, jtr) is h. The coefficients θmi and ψmi,n are for normalization. The reestimation formulas involve occurrence counting. However, since we use a factorial HMM that involves a cross-product la</context>
</contexts>
<marker>Bilmes, 1997</marker>
<rawString>J. Bilmes. 1997. A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and hidden Markov models. Technical Report TR-97-021, ICSI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Brand</author>
<author>N Oliver</author>
<author>A Pentland</author>
</authors>
<title>Coupled hidden Markov models for complex action recognition.</title>
<date>1997</date>
<booktitle>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>994--999</pages>
<contexts>
<context citStr="Brand et al., 1997" endWordPosition="3837" position="22455" startWordPosition="3834">e sentential meaning of a sequence of words. Modeling the meaning of a sentence through a sequence of words whose meanings are modeled by HMMs, defines a factorial HMM for that sentence, since the overall Markov process for that sentence can be factored into indeXT F(Dt, jt) t=1 + XT G(Dt−1, jt−1, Dt, jt) t=2 max j1,...,jT ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ q1,...,qT max ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ XT B(Dt, jt, qt, λ) t=1 T +X A(qt−1, qt, λ) t=2 ⎞ ⎠⎟⎟⎟⎟ F(Dt, jt) + B(Dt, jt, qt, λ) T G(Dt−1, jt−1, Dt, jt) + A(qt−1, qt, λ) XT t=1 X + t=2 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ max j1,...,jT q1,...,qT ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ 57 pendent component processes (Brand et al., 1997; Zhong and Ghosh, 2001) for the individual words. In this view, q denotes the state sequence for the combined factorial HMM and ql denotes the factor of that state sequence for word l. The remainder of this paper wraps this sentence tracker in Baum Welch (Baum et al., 1970; Baum, 1972). 4 Detailed Problem Formulation We adapt the sentence tracker to training a corpus of R video clips, each paired with a sentence. Thus we augment our notation, generalizing jt to jtr and qt l to qtr,l. Below, we use jr to denote (j1r , ..., jTr r ), j to denote (j1, ... , jR), qr,l to denote (q1r,l, . . . , qTr</context>
</contexts>
<marker>Brand, Oliver, Pentland, 1997</marker>
<rawString>M. Brand, N. Oliver, and A. Pentland. 1997. Coupled hidden Markov models for complex action recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 994–999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L Chen</author>
<author>R J Mooney</author>
</authors>
<title>Learning to sportscast: A test of grounded language acquisition.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning.</booktitle>
<contexts>
<context citStr="Chen and Mooney (2008)" endWordPosition="403" position="2704" startWordPosition="400">orld scenes, to learn a specific set of features for adjectives, nouns, and adjuncts. Yu and Ballard (2004) paired training images containing multiple objects with spoken name candidates for the objects to find the correspondence between lexical items and visual features. Dominey and Boucher (2005) paired narrated sentences with symbolic representations of their meanings, automatically extracted from video, to learn object names, spatial-relation terms, and event names as a mapping from the grammatical structure of a sentence to the semantic structure of the associated meaning representation. Chen and Mooney (2008) learned the language of sportscasting by determining the mapping between game commentaries and the meaning representations output by a rulebased simulation of the game. Kwiatkowski et al. (2012) present an approach that learns Montaguegrammar representations of word meanings together with a combinatory categorial grammar (CCG) from child-directed sentences paired with first-order formulas that represent their meaning. Although most of these methods succeed in learning word meanings from sentential descriptions they do so only for symbolic or simple visual input (often synthesized); they fail </context>
</contexts>
<marker>Chen, Mooney, 2008</marker>
<rawString>D. L. Chen and R. J. Mooney. 2008. Learning to sportscast: A test of grounded language acquisition. In Proceedings of the 25th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Dinkelbach</author>
</authors>
<title>On nonlinear fractional programming.</title>
<date>1967</date>
<journal>Management Science,</journal>
<volume>13</volume>
<issue>7</issue>
<contexts>
<context citStr="Dinkelbach, 1967" endWordPosition="6287" position="35930" startWordPosition="6286">threshold to compute F1 scores on the test set in each fold, and average F1 scores across the folds. The F1 scores are listed in the column labeled Our in Table 3. For comparison, we also report F1 scores for three baselines: Chance, Blind, and Hand. The Chance baseline randomly classifies a video-sentence pair as a hit with probability 0.5. The Blind baseline determines hits by potentially looking at the sentence but never looking at the video. We can find an upper bound on the F1 score that any blind method could have on each of our test sets by solving a 0-1 fractional programming problem (Dinkelbach, 1967) (see Appendix A for details). The Hand baseline determines hits with hand-coded HMMs, carefully designed to yield what we believe is near-optimal performance. As can be seen from Table 3, our trained models perform substantially better than the Chance and Blind baselines and approach the performance of the ideal Hand baseline. One can further see from the ROC curves in Figure 2, comparing the trained and hand-written models on both NV and ALL, that the trained models are close to optimal. Note that performance on ALL exceeds that on NV with the trained models. This is because longer sentences</context>
</contexts>
<marker>Dinkelbach, 1967</marker>
<rawString>W. Dinkelbach. 1967. On nonlinear fractional programming. Management Science, 13(7):492–498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Dominey</author>
<author>J-D Boucher</author>
</authors>
<title>Learning to talk about events from narrated video in a construction grammar framework.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>167</volume>
<issue>12</issue>
<contexts>
<context citStr="Dominey and Boucher (2005)" endWordPosition="357" position="2381" startWordPosition="354">from video but rather could learn language in a largely unsupervised fashion, just as a child does, from video paired with sentences. There has been recent research on grounded language learning. Roy (2002) pairs training sentences with vectors of real-valued features extracted from synthesized images which depict 2D blocks-world scenes, to learn a specific set of features for adjectives, nouns, and adjuncts. Yu and Ballard (2004) paired training images containing multiple objects with spoken name candidates for the objects to find the correspondence between lexical items and visual features. Dominey and Boucher (2005) paired narrated sentences with symbolic representations of their meanings, automatically extracted from video, to learn object names, spatial-relation terms, and event names as a mapping from the grammatical structure of a sentence to the semantic structure of the associated meaning representation. Chen and Mooney (2008) learned the language of sportscasting by determining the mapping between game commentaries and the meaning representations output by a rulebased simulation of the game. Kwiatkowski et al. (2012) present an approach that learns Montaguegrammar representations of word meanings </context>
</contexts>
<marker>Dominey, Boucher, 2005</marker>
<rawString>P. F. Dominey and J.-D. Boucher. 2005. Learning to talk about events from narrated video in a construction grammar framework. Artificial Intelligence, 167(12):31–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Felzenszwalb</author>
<author>R B Girshick</author>
<author>D McAllester</author>
<author>D Ramanan</author>
</authors>
<title>Object detection with discriminatively trained part-based models.</title>
<date>2010</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>32</volume>
<issue>9</issue>
<contexts>
<context citStr="Felzenszwalb et al., 2010" endWordPosition="5340" position="30474" startWordPosition="5336">ences with more than one video, with an average of 2.6 sentences per video 1. We model and learn the semantics of all words except determiners. Table 2 specifies the arity, the state number Ic, and the features computed by Φc for the semantic models for words of each part of speech c. While we specify a different subset of features for each part of speech, we presume that, in principle, with enough training data, we could include all features in all parts of speech and automatically learn which ones are noninformative and lead to uniform distributions. We use an off-the-shelf object detector (Felzenszwalb et al., 2010a; Felzenszwalb et al., 2010b) which outputs detections in the form of scored axis-aligned rectangles. We trained four object detectors, one for each of the four object classes in 1Our code, videos, and sentential annotations are available at http://haonanyu.com/research/ acl2013/. 59 C arity I. Φ. N 1 1 α detector index Table 2: Arguments and model configurations for different parts of speech c. VEL stands for velocity, MAG for magnitude, ORIENT for orientation, and DIST for distance. our corpus: person, backpack, chair, and trashcan. For each frame, we pick the two highestscoring detections </context>
</contexts>
<marker>Felzenszwalb, Girshick, McAllester, Ramanan, 2010</marker>
<rawString>P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. 2010a. Object detection with discriminatively trained part-based models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(9):1627–1645.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Felzenszwalb</author>
<author>R B Girshick</author>
<author>D A McAllester</author>
</authors>
<title>Cascade object detection with deformable part models.</title>
<date>2010</date>
<booktitle>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>2241--2248</pages>
<contexts>
<context citStr="Felzenszwalb et al., 2010" endWordPosition="5340" position="30474" startWordPosition="5336">ences with more than one video, with an average of 2.6 sentences per video 1. We model and learn the semantics of all words except determiners. Table 2 specifies the arity, the state number Ic, and the features computed by Φc for the semantic models for words of each part of speech c. While we specify a different subset of features for each part of speech, we presume that, in principle, with enough training data, we could include all features in all parts of speech and automatically learn which ones are noninformative and lead to uniform distributions. We use an off-the-shelf object detector (Felzenszwalb et al., 2010a; Felzenszwalb et al., 2010b) which outputs detections in the form of scored axis-aligned rectangles. We trained four object detectors, one for each of the four object classes in 1Our code, videos, and sentential annotations are available at http://haonanyu.com/research/ acl2013/. 59 C arity I. Φ. N 1 1 α detector index Table 2: Arguments and model configurations for different parts of speech c. VEL stands for velocity, MAG for magnitude, ORIENT for orientation, and DIST for distance. our corpus: person, backpack, chair, and trashcan. For each frame, we pick the two highestscoring detections </context>
</contexts>
<marker>Felzenszwalb, Girshick, McAllester, 2010</marker>
<rawString>P. F. Felzenszwalb, R. B. Girshick, and D. A. McAllester. 2010b. Cascade object detection with deformable part models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2241–2248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Kulkarni</author>
<author>V Premraj</author>
<author>S Dhar</author>
<author>S Li</author>
<author>Y Choi</author>
<author>A C Berg</author>
<author>T L Berg</author>
</authors>
<title>Baby talk: Understanding and generating simple image descriptions.</title>
<date>2011</date>
<booktitle>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>1601--1608</pages>
<contexts>
<context citStr="Kulkarni et al., 2011" endWordPosition="553" position="3681" startWordPosition="550">ces paired with first-order formulas that represent their meaning. Although most of these methods succeed in learning word meanings from sentential descriptions they do so only for symbolic or simple visual input (often synthesized); they fail to bridge the gap between language and computer vision, i.e., they do not attempt to extract meaning representations from complex visual scenes. On the other hand, there has been research on training object and event models from large corpora of complex images and video in the computer-vision community (Kuznetsova et al., 2012; Sadanand and Corso, 2012; Kulkarni et al., 2011; Ordonez et al., 2011; Yao et al., 2010). However, most such work requires training data that labels individual concepts with individual words (i.e., ob53 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 53–63, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics jects delineated via bounding boxes in images as nouns and events that occur in short video clips as verbs). There is no attempt to model phrasal or sentential meaning, let alone acquire the object or event models from training data labeled with phrasal or se</context>
</contexts>
<marker>Kulkarni, Premraj, Dhar, Li, Choi, Berg, Berg, 2011</marker>
<rawString>G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C. Berg, and T. L. Berg. 2011. Baby talk: Understanding and generating simple image descriptions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1601–1608.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kuznetsova</author>
<author>V Ordonez</author>
<author>A C Berg</author>
<author>T L Berg</author>
<author>Y Choi</author>
</authors>
<title>Collective generation of natural image descriptions.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers -</booktitle>
<volume>1</volume>
<pages>359--368</pages>
<contexts>
<context citStr="Kuznetsova et al., 2012" endWordPosition="545" position="3632" startWordPosition="542">categorial grammar (CCG) from child-directed sentences paired with first-order formulas that represent their meaning. Although most of these methods succeed in learning word meanings from sentential descriptions they do so only for symbolic or simple visual input (often synthesized); they fail to bridge the gap between language and computer vision, i.e., they do not attempt to extract meaning representations from complex visual scenes. On the other hand, there has been research on training object and event models from large corpora of complex images and video in the computer-vision community (Kuznetsova et al., 2012; Sadanand and Corso, 2012; Kulkarni et al., 2011; Ordonez et al., 2011; Yao et al., 2010). However, most such work requires training data that labels individual concepts with individual words (i.e., ob53 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 53–63, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics jects delineated via bounding boxes in images as nouns and events that occur in short video clips as verbs). There is no attempt to model phrasal or sentential meaning, let alone acquire the object or event mod</context>
</contexts>
<marker>Kuznetsova, Ordonez, Berg, Berg, Choi, 2012</marker>
<rawString>P. Kuznetsova, V. Ordonez, A. C. Berg, T. L. Berg, and Y. Choi. 2012. Collective generation of natural image descriptions. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, pages 359–368.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kwiatkowski</author>
<author>S Goldwater</author>
<author>L Zettlemoyer</author>
<author>M Steedman</author>
</authors>
<title>A probabilistic model of syntactic and semantic acquisition from child-directed utterances and their meanings.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>234--244</pages>
<contexts>
<context citStr="Kwiatkowski et al. (2012)" endWordPosition="433" position="2899" startWordPosition="430">he objects to find the correspondence between lexical items and visual features. Dominey and Boucher (2005) paired narrated sentences with symbolic representations of their meanings, automatically extracted from video, to learn object names, spatial-relation terms, and event names as a mapping from the grammatical structure of a sentence to the semantic structure of the associated meaning representation. Chen and Mooney (2008) learned the language of sportscasting by determining the mapping between game commentaries and the meaning representations output by a rulebased simulation of the game. Kwiatkowski et al. (2012) present an approach that learns Montaguegrammar representations of word meanings together with a combinatory categorial grammar (CCG) from child-directed sentences paired with first-order formulas that represent their meaning. Although most of these methods succeed in learning word meanings from sentential descriptions they do so only for symbolic or simple visual input (often synthesized); they fail to bridge the gap between language and computer vision, i.e., they do not attempt to extract meaning representations from complex visual scenes. On the other hand, there has been research on trai</context>
<context citStr="Kwiatkowski et al. (2012)" endWordPosition="2013" position="12667" startWordPosition="2010"> know that there are three distinct objects that participate in the event denoted. Sixth, we assume that we know the argument-to-participant mapping for each training sentence. Thus, for example, for the above sentence we would know person(p0), to-the-left-of(p0, p1), backback(p1), approached(p0, p2), and trash-can(p2). The latter two items can be determined by parsing the sentence, which is what we do. One can imagine learning the ability to automatically perform the latter two items, and even the fourth item above, by learning the grammar and the part of speech of each word, such as done by Kwiatkowski et al. (2012). We leave such for future work. Figure 1 illustrates a single frame from a potential training sample provided as input to our learner. It consists of a video clip paired with a sentence, where the arguments of the words in the sentence are mapped to participants. From a sequence of such training samples, our learner determines the objects tracks and the mapping from participants to those tracks, together with the meanings of the words. The remainder of the paper is organized as follows. Section 2 generally describes our problem of lexical acquisition from video. Section 3 introduces our work </context>
</contexts>
<marker>Kwiatkowski, Goldwater, Zettlemoyer, Steedman, 2012</marker>
<rawString>T. Kwiatkowski, S. Goldwater, L. Zettlemoyer, and M. Steedman. 2012. A probabilistic model of syntactic and semantic acquisition from child-directed utterances and their meanings. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 234– 244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ordonez</author>
<author>G Kulkarni</author>
<author>T L Berg</author>
</authors>
<title>Im2text: Describing images using 1 million captioned photographs.</title>
<date>2011</date>
<booktitle>In Proceedings of Neural Information Processing Systems.</booktitle>
<contexts>
<context citStr="Ordonez et al., 2011" endWordPosition="557" position="3703" startWordPosition="554">rder formulas that represent their meaning. Although most of these methods succeed in learning word meanings from sentential descriptions they do so only for symbolic or simple visual input (often synthesized); they fail to bridge the gap between language and computer vision, i.e., they do not attempt to extract meaning representations from complex visual scenes. On the other hand, there has been research on training object and event models from large corpora of complex images and video in the computer-vision community (Kuznetsova et al., 2012; Sadanand and Corso, 2012; Kulkarni et al., 2011; Ordonez et al., 2011; Yao et al., 2010). However, most such work requires training data that labels individual concepts with individual words (i.e., ob53 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 53–63, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics jects delineated via bounding boxes in images as nouns and events that occur in short video clips as verbs). There is no attempt to model phrasal or sentential meaning, let alone acquire the object or event models from training data labeled with phrasal or sentential annotations. </context>
</contexts>
<marker>Ordonez, Kulkarni, Berg, 2011</marker>
<rawString>V. Ordonez, G. Kulkarni, and T. L. Berg. 2011. Im2text: Describing images using 1 million captioned photographs. In Proceedings of Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roy</author>
</authors>
<title>Learning visually-grounded words and syntax for a scene description task. Computer Speech and Language,</title>
<date>2002</date>
<pages>16--353</pages>
<contexts>
<context citStr="Roy (2002)" endWordPosition="294" position="1961" startWordPosition="293">uncertainty and noise, a system based on crosssituational learning can robustly acquire a lexicon, mapping words to word-level meanings from sentences paired with sentence-level meanings. However, it did so only for symbolic representations of word- and sentence-level meanings that were not perceptually grounded. An ideal system would not require detailed word-level labelings to acquire word meanings from video but rather could learn language in a largely unsupervised fashion, just as a child does, from video paired with sentences. There has been recent research on grounded language learning. Roy (2002) pairs training sentences with vectors of real-valued features extracted from synthesized images which depict 2D blocks-world scenes, to learn a specific set of features for adjectives, nouns, and adjuncts. Yu and Ballard (2004) paired training images containing multiple objects with spoken name candidates for the objects to find the correspondence between lexical items and visual features. Dominey and Boucher (2005) paired narrated sentences with symbolic representations of their meanings, automatically extracted from video, to learn object names, spatial-relation terms, and event names as a </context>
</contexts>
<marker>Roy, 2002</marker>
<rawString>D. Roy. 2002. Learning visually-grounded words and syntax for a scene description task. Computer Speech and Language, 16:353–385.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sadanand</author>
<author>J J Corso</author>
</authors>
<title>Action bank: A high-level representation of activity in video.</title>
<date>2012</date>
<booktitle>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>1234--1241</pages>
<contexts>
<context citStr="Sadanand and Corso, 2012" endWordPosition="549" position="3658" startWordPosition="546">from child-directed sentences paired with first-order formulas that represent their meaning. Although most of these methods succeed in learning word meanings from sentential descriptions they do so only for symbolic or simple visual input (often synthesized); they fail to bridge the gap between language and computer vision, i.e., they do not attempt to extract meaning representations from complex visual scenes. On the other hand, there has been research on training object and event models from large corpora of complex images and video in the computer-vision community (Kuznetsova et al., 2012; Sadanand and Corso, 2012; Kulkarni et al., 2011; Ordonez et al., 2011; Yao et al., 2010). However, most such work requires training data that labels individual concepts with individual words (i.e., ob53 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 53–63, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics jects delineated via bounding boxes in images as nouns and events that occur in short video clips as verbs). There is no attempt to model phrasal or sentential meaning, let alone acquire the object or event models from training data lab</context>
</contexts>
<marker>Sadanand, Corso, 2012</marker>
<rawString>S. Sadanand and J. J. Corso. 2012. Action bank: A high-level representation of activity in video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1234–1241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Siskind</author>
<author>Q Morris</author>
</authors>
<title>A maximumlikelihood approach to visual event classification.</title>
<date>1996</date>
<booktitle>In Proceedings of the Fourth European Conference on Computer Vision,</booktitle>
<pages>347--360</pages>
<contexts>
<context citStr="Siskind and Morris (1996)" endWordPosition="1018" position="6613" startWordPosition="1015">e identity of the particular detector that produced that detection) that correlate with object class; region color, shape, and size features that correlate with object properties; and motion features, such as linear and angular object position, velocity, and acceleration, that correlate with event properties. We also compute features between pairs of tracks to encode the relative position and motion of the pairs of objects that participate in events that involve two participants. In principle, we can also compute features between tuples of any number of tracks. Following Yamoto et al. (1992), Siskind and Morris (1996), and Starner et al. (1998), we represent the meaning of an intransitive verb, like jump, as a two-state HMM over the velocity-direction feature, modeling the requirement that the participant move upward then downward. We represent the meaning of a transitive verb, like pick up, as a two-state HMM over both single-object and object-pair features: the agent moving toward the patient while the patient is as rest, followed by the agent moving together with the patient. We extend this general approach to other parts of speech. Nouns, like person, can be represented as one-state HMMs over image fea</context>
</contexts>
<marker>Siskind, Morris, 1996</marker>
<rawString>J. M. Siskind and Q. Morris. 1996. A maximumlikelihood approach to visual event classification. In Proceedings of the Fourth European Conference on Computer Vision, pages 347–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Siskind</author>
</authors>
<title>A computational study of crosssituational techniques for learning word-to-meaning mappings.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--39</pages>
<contexts>
<context citStr="Siskind (1996)" endWordPosition="194" position="1313" startWordPosition="193">prepositions, adjectives, and adverbs. The correspondence between words and concepts in the video is learned in an unsupervised fashion, even when the video depicts simultaneous events described by multiple sentences or when different aspects of a single event are described with multiple sentences. The learned word meanings can be subsequently used to automatically generate description of new video. 1 Introduction People learn language through exposure to a rich perceptual context. Language is grounded by mapping words, phrases, and sentences to meaning representations referring to the world. Siskind (1996) has shown that even with referential uncertainty and noise, a system based on crosssituational learning can robustly acquire a lexicon, mapping words to word-level meanings from sentences paired with sentence-level meanings. However, it did so only for symbolic representations of word- and sentence-level meanings that were not perceptually grounded. An ideal system would not require detailed word-level labelings to acquire word meanings from video but rather could learn language in a largely unsupervised fashion, just as a child does, from video paired with sentences. There has been recent re</context>
</contexts>
<marker>Siskind, 1996</marker>
<rawString>J. M. Siskind. 1996. A computational study of crosssituational techniques for learning word-to-meaning mappings. Cognition, 61:39–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Starner</author>
<author>J Weaver</author>
<author>A Pentland</author>
</authors>
<title>Realtime American Sign Language recognition using desk and wearable computer based video.</title>
<date>1998</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>20</volume>
<issue>12</issue>
<contexts>
<context citStr="Starner et al. (1998)" endWordPosition="1023" position="6640" startWordPosition="1020">tector that produced that detection) that correlate with object class; region color, shape, and size features that correlate with object properties; and motion features, such as linear and angular object position, velocity, and acceleration, that correlate with event properties. We also compute features between pairs of tracks to encode the relative position and motion of the pairs of objects that participate in events that involve two participants. In principle, we can also compute features between tuples of any number of tracks. Following Yamoto et al. (1992), Siskind and Morris (1996), and Starner et al. (1998), we represent the meaning of an intransitive verb, like jump, as a two-state HMM over the velocity-direction feature, modeling the requirement that the participant move upward then downward. We represent the meaning of a transitive verb, like pick up, as a two-state HMM over both single-object and object-pair features: the agent moving toward the patient while the patient is as rest, followed by the agent moving together with the patient. We extend this general approach to other parts of speech. Nouns, like person, can be represented as one-state HMMs over image features that correlate with t</context>
</contexts>
<marker>Starner, Weaver, Pentland, 1998</marker>
<rawString>T. Starner, J. Weaver, and A. Pentland. 1998. Realtime American Sign Language recognition using desk and wearable computer based video. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(12):1371–1375.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Viterbi</author>
</authors>
<title>Error bounds for convolutional codes and an asymtotically optimum decoding algorithm.</title>
<date>1967</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>13--260</pages>
<contexts>
<context citStr="Viterbi (1967)" endWordPosition="3099" position="18670" startWordPosition="3098">ack 0 Track 1 Track 2 56 over, we wish the track to be temporally coherent; we want the objects in a track to move smoothly over time and not jump around the field of view. Let G(Dt−1, jt−1,Dt, jt) denote some measure of coherence between two detections in adjacent frames. (One possible such measure is consistency of the displacement of Dt relative to Dt−1 with the velocity of Dt−1 computed from the image by optical flow.) One can select the detections to yield a track that maximizes both the aggregate detection score and the aggregate temporal coherence score. This can be determined with the Viterbi (1967) algorithm and is known as detection-based tracking (Viterbi, 1971). Recall that we model the meaning of an intransitive verb as an HMM over a time series of features extracted for its participant in each frame. Let λ denote the parameters of this HMM, (q1, . . . , qT ) denote the sequence of states qt that leads to an observed track, B(Dt, jt, qt, λ) denote the conditional log probability of observing the feature vector associated with the detection selected by jt among the detections Dt in frame t, given that the HMM is in state qt, and A(qt−1, qt, λ) denote the log transition probability of</context>
</contexts>
<marker>Viterbi, 1967</marker>
<rawString>A. J. Viterbi. 1967. Error bounds for convolutional codes and an asymtotically optimum decoding algorithm. IEEE Transactions on Information Theory, 13:260–267.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Viterbi</author>
</authors>
<title>Convolutional codes and their performance in communication systems.</title>
<date>1971</date>
<journal>IEEE Transactions on Communication Technology,</journal>
<volume>19</volume>
<issue>5</issue>
<pages>772</pages>
<contexts>
<context citStr="Viterbi, 1971" endWordPosition="3109" position="18737" startWordPosition="3108">oherent; we want the objects in a track to move smoothly over time and not jump around the field of view. Let G(Dt−1, jt−1,Dt, jt) denote some measure of coherence between two detections in adjacent frames. (One possible such measure is consistency of the displacement of Dt relative to Dt−1 with the velocity of Dt−1 computed from the image by optical flow.) One can select the detections to yield a track that maximizes both the aggregate detection score and the aggregate temporal coherence score. This can be determined with the Viterbi (1967) algorithm and is known as detection-based tracking (Viterbi, 1971). Recall that we model the meaning of an intransitive verb as an HMM over a time series of features extracted for its participant in each frame. Let λ denote the parameters of this HMM, (q1, . . . , qT ) denote the sequence of states qt that leads to an observed track, B(Dt, jt, qt, λ) denote the conditional log probability of observing the feature vector associated with the detection selected by jt among the detections Dt in frame t, given that the HMM is in state qt, and A(qt−1, qt, λ) denote the log transition probability of the HMM. For a given track (j1, . . . , jT ), the state sequence t</context>
</contexts>
<marker>Viterbi, 1971</marker>
<rawString>A. Viterbi. 1971. Convolutional codes and their performance in communication systems. IEEE Transactions on Communication Technology, 19(5):751– 772.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Yamoto</author>
<author>J Ohya</author>
<author>K Ishii</author>
</authors>
<title>Recognizing human action in time-sequential images using hidden Markov model.</title>
<date>1992</date>
<booktitle>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>379--385</pages>
<contexts>
<context citStr="Yamoto et al. (1992)" endWordPosition="1014" position="6586" startWordPosition="1011">features (including the identity of the particular detector that produced that detection) that correlate with object class; region color, shape, and size features that correlate with object properties; and motion features, such as linear and angular object position, velocity, and acceleration, that correlate with event properties. We also compute features between pairs of tracks to encode the relative position and motion of the pairs of objects that participate in events that involve two participants. In principle, we can also compute features between tuples of any number of tracks. Following Yamoto et al. (1992), Siskind and Morris (1996), and Starner et al. (1998), we represent the meaning of an intransitive verb, like jump, as a two-state HMM over the velocity-direction feature, modeling the requirement that the participant move upward then downward. We represent the meaning of a transitive verb, like pick up, as a two-state HMM over both single-object and object-pair features: the agent moving toward the patient while the patient is as rest, followed by the agent moving together with the patient. We extend this general approach to other parts of speech. Nouns, like person, can be represented as on</context>
</contexts>
<marker>Yamoto, Ohya, Ishii, 1992</marker>
<rawString>J. Yamoto, J. Ohya, and K. Ishii. 1992. Recognizing human action in time-sequential images using hidden Markov model. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 379–385.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Z Yao</author>
<author>X Yang</author>
<author>L Lin</author>
<author>M W Lee</author>
<author>S-C Zhu</author>
</authors>
<title>I2T: Image parsing to text description.</title>
<date>2010</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<volume>98</volume>
<issue>8</issue>
<contexts>
<context citStr="Yao et al., 2010" endWordPosition="561" position="3722" startWordPosition="558">resent their meaning. Although most of these methods succeed in learning word meanings from sentential descriptions they do so only for symbolic or simple visual input (often synthesized); they fail to bridge the gap between language and computer vision, i.e., they do not attempt to extract meaning representations from complex visual scenes. On the other hand, there has been research on training object and event models from large corpora of complex images and video in the computer-vision community (Kuznetsova et al., 2012; Sadanand and Corso, 2012; Kulkarni et al., 2011; Ordonez et al., 2011; Yao et al., 2010). However, most such work requires training data that labels individual concepts with individual words (i.e., ob53 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 53–63, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics jects delineated via bounding boxes in images as nouns and events that occur in short video clips as verbs). There is no attempt to model phrasal or sentential meaning, let alone acquire the object or event models from training data labeled with phrasal or sentential annotations. Moreover, such work</context>
</contexts>
<marker>Yao, Yang, Lin, Lee, Zhu, 2010</marker>
<rawString>B. Z. Yao, X. Yang, L. Lin, M. W. Lee, and S.-C. Zhu. 2010. I2T: Image parsing to text description. Proceedings of the IEEE, 98(8):1485–1508, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Yu</author>
<author>D H Ballard</author>
</authors>
<title>On the integration of grounding language and learning objects.</title>
<date>2004</date>
<booktitle>In Proceedings of the 19th National Conference on Artifical intelligence,</booktitle>
<pages>488--493</pages>
<contexts>
<context citStr="Yu and Ballard (2004)" endWordPosition="330" position="2189" startWordPosition="327">ymbolic representations of word- and sentence-level meanings that were not perceptually grounded. An ideal system would not require detailed word-level labelings to acquire word meanings from video but rather could learn language in a largely unsupervised fashion, just as a child does, from video paired with sentences. There has been recent research on grounded language learning. Roy (2002) pairs training sentences with vectors of real-valued features extracted from synthesized images which depict 2D blocks-world scenes, to learn a specific set of features for adjectives, nouns, and adjuncts. Yu and Ballard (2004) paired training images containing multiple objects with spoken name candidates for the objects to find the correspondence between lexical items and visual features. Dominey and Boucher (2005) paired narrated sentences with symbolic representations of their meanings, automatically extracted from video, to learn object names, spatial-relation terms, and event names as a mapping from the grammatical structure of a sentence to the semantic structure of the associated meaning representation. Chen and Mooney (2008) learned the language of sportscasting by determining the mapping between game commen</context>
</contexts>
<marker>Yu, Ballard, 2004</marker>
<rawString>C. Yu and D. H. Ballard. 2004. On the integration of grounding language and learning objects. In Proceedings of the 19th National Conference on Artifical intelligence, pages 488–493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Zhong</author>
<author>J Ghosh</author>
</authors>
<title>A new formulation of coupled hidden Markov models.</title>
<date>2001</date>
<tech>Technical report,</tech>
<institution>Department of Electrical and Computer Engineering, The University of Texas at Austin.</institution>
<contexts>
<context citStr="Zhong and Ghosh, 2001" endWordPosition="3841" position="22479" startWordPosition="3838"> of a sequence of words. Modeling the meaning of a sentence through a sequence of words whose meanings are modeled by HMMs, defines a factorial HMM for that sentence, since the overall Markov process for that sentence can be factored into indeXT F(Dt, jt) t=1 + XT G(Dt−1, jt−1, Dt, jt) t=2 max j1,...,jT ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ q1,...,qT max ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ XT B(Dt, jt, qt, λ) t=1 T +X A(qt−1, qt, λ) t=2 ⎞ ⎠⎟⎟⎟⎟ F(Dt, jt) + B(Dt, jt, qt, λ) T G(Dt−1, jt−1, Dt, jt) + A(qt−1, qt, λ) XT t=1 X + t=2 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ max j1,...,jT q1,...,qT ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ 57 pendent component processes (Brand et al., 1997; Zhong and Ghosh, 2001) for the individual words. In this view, q denotes the state sequence for the combined factorial HMM and ql denotes the factor of that state sequence for word l. The remainder of this paper wraps this sentence tracker in Baum Welch (Baum et al., 1970; Baum, 1972). 4 Detailed Problem Formulation We adapt the sentence tracker to training a corpus of R video clips, each paired with a sentence. Thus we augment our notation, generalizing jt to jtr and qt l to qtr,l. Below, we use jr to denote (j1r , ..., jTr r ), j to denote (j1, ... , jR), qr,l to denote (q1r,l, . . . , qTr r,l ), qr to denote (qr</context>
</contexts>
<marker>Zhong, Ghosh, 2001</marker>
<rawString>S. Zhong and J. Ghosh. 2001. A new formulation of coupled hidden Markov models. Technical report, Department of Electrical and Computer Engineering, The University of Texas at Austin.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>