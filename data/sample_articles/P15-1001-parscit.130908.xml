<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant confidence="0.000005" no="0">
<title confidence="0.997713">
On Using Very Large Target Vocabulary for
Neural Machine Translation
</title>
<author confidence="0.928362">
S´ebastien Jean Kyunghyun Cho Yoshua Bengio
</author>
<affiliation confidence="0.7659485">
Roland Memisevic Universit´e de Montr´eal
Universit´e de Montr´eal CIFAR Senior Fellow
</affiliation>
<sectionHeader confidence="0.987763" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99991625">Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrasebased statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method based on importance sampling that allows us to use a very large target vocabulary without increasing training complexity. We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to match, and in some cases outperform, the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use an ensemble of a few models with very large target vocabularies, we achieve performance comparable to the state of the art (measured by BLEU) on both the English→German and English→French translation tasks of WMT’14.</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999916765957447">Neural machine translation (NMT) is a recently introduced approach to solving machine translation (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Sutskever et al., 2014). In neural machine translation, one builds a single neural network that reads a source sentence and generates its translation. The whole neural network is jointly trained to maximize the conditional probability of a correct translation given a source sentence, using the bilingual corpus. The NMT models have shown to perform as well as the most widely used conventional translation systems (Sutskever et al., 2014; Bahdanau et al., 2015). Neural machine translation has a number of advantages over the existing statistical machine translation system, specifically, the phrase-based system (Koehn et al., 2003). First, NMT requires a minimal set of domain knowledge. For instance, all of the models proposed in (Sutskever et al., 2014), (Bahdanau et al., 2015) or (Kalchbrenner and Blunsom, 2013) do not assume any linguistic property in both source and target sentences except that they are sequences of words. Second, the whole system is jointly trained to maximize the translation performance, unlike the existing phrase-based system which consists of many separately trained features whose weights are then tuned jointly. Lastly, the memory footprint of the NMT model is often much smaller than the existing system which relies on maintaining large tables of phrase pairs. Despite these advantages and promising results, there is a major limitation in NMT compared to the existing phrase-based approach. That is, the number of target words must be limited. This is mainly because the complexity of training and using an NMT model increases as the number of target words increases. A usual practice is to construct a target vocabulary of the K most frequent words (a socalled shortlist), where K is often in the range of 30k (Bahdanau et al., 2015) to 80k (Sutskever et al., 2014). Any word not included in this vocabulary is mapped to a special token representing an unknown word [UNK]. This approach works well when there are only a few unknown words in the target sentence, but it has been observed that the translation performance degrades rapidly as the number of unknown words increases (Cho et al., 2014a; Bahdanau et al., 2015).</bodyText>
<page confidence="0.812883">
1
</page>
<note confidence="0.970647">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1–10,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999636961538461">In this paper, we propose an approximate training algorithm based on (biased) importance sampling that allows us to train an NMT model with a much larger target vocabulary. The proposed algorithm effectively keeps the computational complexity during training at the level of using only a small subset of the full vocabulary. Once the model with a very large target vocabulary is trained, one can choose to use either all the target words or only a subset of them. We compare the proposed algorithm against the baseline shortlist-based approach in the tasks of English→French and English→German translation using the NMT model introduced in (Bahdanau et al., 2015). The empirical results demonstrate that we can potentially achieve better translation performance using larger vocabularies, and that our approach does not sacrifice too much speed for both training and decoding. Furthermore, we show that the model trained with this algorithm gets the best translation performance yet achieved by single NMT models on the WMT’14 English→French translation task.</bodyText>
<sectionHeader confidence="0.982521" genericHeader="method">
2 Neural Machine Translation and Limited Vocabulary Problem
</sectionHeader>
<bodyText confidence="0.9999542">In this section, we briefly describe an approach to neural machine translation proposed recently in (Bahdanau et al., 2015). Based on this description we explain the issue of limited vocabularies in neural machine translation.</bodyText>
<subsectionHeader confidence="0.978709">
2.1 Neural Machine Translation
</subsectionHeader>
<bodyText confidence="0.999668727272727">Neural machine translation is a recently proposed approach to machine translation, which uses a single neural network trained jointly to maximize the translation performance (Forcada and ˜Neco, 1997; Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014; Bahdanau et al., 2015). Neural machine translation is often implemented as the encoder–decoder network. The encoder reads the source sentence x = (x1, ... , xT) and encodes it into a sequence of hidden states</bodyText>
<equation confidence="0.986561">
h = (h1, ··· ,hT):
ht = f (xt, ht−1) . (1)
</equation>
<bodyText confidence="0.9985775">Then, the decoder, another recurrent neural network, generates a corresponding translation y = where</bodyText>
<equation confidence="0.987968333333333">
(y1, · · · , yT,) based on the encoded sequence of
hidden states h:
p(yt  |y&lt;t, x) a exp {q (yt−1, zt, ct)} , (2)
</equation>
<equation confidence="0.884780333333333">
zt = g (yt−1, zt−1, ct) , (3)
ct = r (zt−1, h1, ... , hT) , (4)
and y&lt;t = (y1, . . . , yt−1).
</equation>
<bodyText confidence="0.99973425">The whole model is jointly trained to maximize the conditional log-probability of the correct translation given a source sentence with respect to the parameters 0 of the model:</bodyText>
<equation confidence="0.710967">
log p(ynt  |yn&lt;t, xn),
</equation>
<bodyText confidence="0.999629">where (xn, yn) is the n-th training pair of sentences, and Tn is the length of the n-th target sentence (yn).</bodyText>
<subsectionHeader confidence="0.620199">
2.1.1 Detailed Description
</subsectionHeader>
<bodyText confidence="0.999850571428571">In this paper, we use a specific implementation of neural machine translation that uses an attention mechanism, as recently proposed in (Bahdanau et al., 2015). In (Bahdanau et al., 2015), the encoder in Eq. (1) is implemented by a bi-directional recurrent neural network such that where h t = f (xt, h t+1) , h t = f (xt, h t−1) .</bodyText>
<equation confidence="0.751663">
ht = [h t; h t] ,
</equation>
<bodyText confidence="0.721704166666667">They used a gated recurrent unit for f (see, e.g., (Cho et al., 2014b)). The decoder, at each time, computes the context vector ct as a convex sum of the hidden states where a is a feedforward neural network with a single hidden layer.</bodyText>
<equation confidence="0.9417328">
(h1, ... , hT) with the coefficients α1, ... , αT
computed by
exp {a (ht, zt−1)}
αt = (5)
Ek exp {a (hk, zt−1)},
</equation>
<bodyText confidence="0.9999072">A new hidden state zt of the decoder in Eq.(3) is computed based on the previous hidden state zt−1, previous generated symbol yt−1 and the computed context vector ct.</bodyText>
<equation confidence="0.996015166666667">
0* = arg max
θ
Tn
t=1
N
n=1
</equation>
<page confidence="0.906601">
2
</page>
<bodyText confidence="0.98760675">The decoder also uses the gated recurrent unit, as the encoder does. The probability of the next target word in Eq.(2) is then computed by where φ is an affine transformation followed by a nonlinear activation, and wt and bt are respectively the target word vector and the target word bias.</bodyText>
<equation confidence="0.997660333333333">
1
p( yt  |y&lt;t, x) = Z exp{w&gt;t φ (yt−1, zt, ct) + bt} ,
(6)
</equation>
<bodyText confidence="0.98236925">Z is the normalization constant computed by where V is the set of all the target words.</bodyText>
<equation confidence="0.9061415">
{ }
exp w&gt; k φ (yt−1, zt, ct) + bk , (7)
</equation>
<bodyText confidence="0.98516">For the detailed description of the implementation, we refer the reader to the appendix of (Bahdanau et al., 2015).</bodyText>
<subsectionHeader confidence="0.9993895">
2.2 Limited Vocabulary Issue and
Conventional Solutions
</subsectionHeader>
<bodyText confidence="0.999974594202899">One of the main difficulties in training this neural machine translation model is the computational complexity involved in computing the target word probability (Eq. (6)). More specifically, we need to compute the dot product between the feature φ (yt−1, zt, ct) and the word vector wt as many times as there are words in a target vocabulary in order to compute the normalization constant (the denominator in Eq. (6)). This has to be done for, on average, 20–30 words per sentence, which easily becomes prohibitively expensive even with a moderate number of possible target words. Furthermore, the memory requirement grows linearly with respect to the number of target words. This has been a major hurdle for neural machine translation, compared to the existing non-parametric approaches such as phrase-based translation systems. Recently proposed neural machine translation models, hence, use a shortlist of 30k to 80k most frequent words (Bahdanau et al., 2015; Sutskever et al., 2014). This makes training more feasible, but comes with a number of problems. First of all, the performance of the model degrades heavily if the translation of a source sentence requires many words that are not included in the shortlist (Cho et al., 2014a). This also affects the performance evaluation of the system which is often measured by BLEU. Second, the first issue becomes more problematic with languages that have a rich set of words such as German or other highly inflected languages. There are two model-specific approaches to this issue of large target vocabulary. The first approach is to stochastically approximate the target word probability. This has been proposed recently in (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013) based on noise-contrastive estimation (Gutmann and Hyvarinen, 2010). In the second approach, the target words are clustered into multiple classes, or hierarchical classes, and the target probability p(yt|y&lt;t, x) is factorized as a product of the class probability p(ct|y&lt;t, x) and the intraclass word probability p(yt|ct, y&lt;t, x). This reduces the number of required dot-products into the sum of the number of classes and the words in a class. These approaches mainly aim at reducing the computational complexity during training, but do not often result in speed-up when decoding a translation during test time.1 Other than these model-specific approaches, there exist translation-specific approaches. A translation-specific approach exploits the properties of the rare target words. For instance, Luong et al. proposed such an approach for neural machine translation (Luong et al., 2015). They replace rare words (the words that are not included in the shortlist) in both source and target sentences into corresponding (OOVn) tokens using the word alignment model. Once a source sentence is translated, each (OOVn) in the translation will be replaced based on the source word marked by the corresponding (OOVn). It is important to note that the modelspecific approaches and the translation-specific approaches are often complementary and can be used together to further improve the translation performance and reduce the computational complexity.</bodyText>
<sectionHeader confidence="0.9823365" genericHeader="method">
3 Approximate Learning Approach to
Very Large Target Vocabulary
</sectionHeader>
<subsectionHeader confidence="0.999167">
3.1 Description
</subsectionHeader>
<bodyText confidence="0.99995925">In this paper, we propose a model-specific approach that allows us to train a neural machine translation model with a very large target vocabulary. With the proposed approach, the computational complexity of training becomes constant with respect to the size of the target vocabulary.</bodyText>
<footnote confidence="0.821903">
1This is due to the fact that the beam search requires the
conditional probability of every target word at each time step
regardless of the parametrization of the output probability.
</footnote>
<equation confidence="0.9875455">
�Z =
k:yk∈V
</equation>
<page confidence="0.949504">
3
</page>
<bodyText confidence="0.999921789473684">Furthermore, the proposed approach allows us to efficiently use a fast computing device with limited memory, such as a GPU, to train a neural machine translation model with a much larger target vocabulary. As mentioned earlier, the computational inefficiency of training a neural machine translation model arises from the normalization constant in Eq.(6). In order to avoid the growing complexity of computing the normalization constant, we propose here to use only a small subset V 0 of the target vocabulary at each update. The proposed approach is based on the earlier work of (Bengio and S´en´ecal, 2008). Let us consider the gradient of the logprobability of the output in Eq.(6). The gradient is composed of a positive and negative part:</bodyText>
<equation confidence="0.998913">
V logp(yt  |y&lt;t, x) (8)
=V£(yt) − � p(yk  |y&lt;t, x)V£(yk),
k:yk∈V
</equation>
<bodyText confidence="0.998623">where we define the energy £ as</bodyText>
<equation confidence="0.879491">
£(yj) = w&gt;j φ (yj−1, zj, cj) + bj.
</equation>
<bodyText confidence="0.974139">The second, or negative, term of the gradient is in essence the expected gradient of the energy:</bodyText>
<equation confidence="0.736361">
EP [V£(y)] , (9)
</equation>
<bodyText confidence="0.999896">where P denotes p(y  |y&lt;t, x). The main idea of the proposed approach is to approximate this expectation, or the negative term of the gradient, by importance sampling with a small number of samples. Given a predefined proposal distribution Q and a set V 0 of samples from Q, we approximate the expectation in Eq. (9) with we update only the vectors associated with the correct word wt and with the sampled words in V 0.</bodyText>
<equation confidence="0.932777">
�
EP [V£(y)] ^
k:yk∈V 0
</equation>
<bodyText confidence="0.990911121212121">Once training is over, we can use the full target vocabulary to compute the output probability of each target word. Although the proposed approach naturally addresses the computational complexity, using this approach naively does not guarantee that the number of parameters being updated for each sentence pair, which includes multiple target words, is bounded nor can be controlled. This becomes problematic when training is done, for instance, on a GPU with limited memory. In practice, hence, we partition the training corpus and define a subset V 0 of the target vocabulary for each partition prior to training. Before training begins, we sequentially examine each target sentence in the training corpus and accumulate unique target words until the number of unique target words reaches the predefined threshold τ. The accumulated vocabulary will be used for this partition of the corpus during training. We repeat this until the end of the training set is reached. Let us refer to the subset of target words used for the i-th partition by V 0 i . This may be understood as having a separate proposal distribution Qi for each partition of the training corpus. The distribution Qi assigns equal probability mass to all the target words included in the subset V 0 i , and zero probability mass to all the other words, i.e.,</bodyText>
<equation confidence="0.971772666666667">
{ 1if yt E Vi0
|Vi0|
0 otherwise.
</equation>
<bodyText confidence="0.9999754">This choice of proposal distribution cancels out the correction term − log Q(yk) from the importance weight in Eqs. (10)–(11), which makes the proposed approach equivalent to approximating the exact output probability in Eq. (6) with where exp {w&gt; I t φ (yt−1, zt, ct) + bt = Ek:yk∈V 0 exp {w&gt;k φ (yt−1, zt, ct) + bk I.</bodyText>
<equation confidence="0.9978188">
E
k0:yk0∈V 0 ωk0
ωk V£(yk),
Qi(yk) =
(10) p(yt  |y&lt;t, x)
</equation>
<equation confidence="0.976294">
ωk = exp {£(yk) − logQ(yk)I . (11)
</equation>
<bodyText confidence="0.999860272727273">This approach allows us to compute the normalization constant during training using only a small subset of the target vocabulary, resulting in much lower computational complexity for each parameter update. Intuitively, at each parameter update, It should be noted that this choice of Q makes the estimator biased. The proposed procedure results in speed up against usual importance sampling, as it exploits the advantage of modern computers in doing matrix-matrix vs matrix-vector multiplications.</bodyText>
<page confidence="0.991734">
4
</page>
<subsectionHeader confidence="0.998735">
3.1.1 Informal Discussion on Consequence
</subsectionHeader>
<bodyText confidence="0.999974473684211">The parametrization of the output probability in Eq. (6) can be understood as arranging the vectors associated with the target words such that the dot product between the most likely, or correct, target word’s vector and the current hidden state is maximized. The exponentiation followed by normalization is simply a process in which the dot products are converted into proper probabilities. As learning continues, therefore, the vectors of all the likely target words tend to align with each other but not with the others. This is achieved exactly by moving the vector of the correct word in the direction of φ (yt−1, zt, ct), while pushing all the other vectors away, which happens when the gradient of the logarithm of the exact output probability in Eq. (6) is maximized. Our approximate approach, instead, moves the word vectors of the correct words and of only a subset of sampled target words (those included in V ').</bodyText>
<subsectionHeader confidence="0.998735">
3.2 Decoding
</subsectionHeader>
<bodyText confidence="0.999987628571429">Once the model is trained using the proposed approximation, we can use the full target vocabulary when decoding a translation given a new source sentence. Although this is advantageous as it allows the trained model to utilize the whole vocabulary when generating a translation, doing so may be too computationally expensive, e.g., for realtime applications. Since training puts the target word vectors in the space so that they align well with the hidden state of the decoder only when they are likely to be a correct word, we can use only a subset of candidate target words during decoding. This is similar to what we do during training, except that at test time, we do not have access to a set of correct target words. The most naive way to select a subset of candidate target words is to take only the top-K most frequent target words, where K can be adjusted to meet the computational requirement. This, however, effectively cancels out the whole purpose of training a model with a very large target vocabulary. Instead, we can use an existing word alignment model to align the source and target words in the training corpus and build a dictionary. With the dictionary, for each source sentence, we construct a target word set consisting of the K-most frequent words (according to the estimated unigram probability) and, using the dictionary, at most K' likely target words for each source word. K and K' may be chosen either to meet the computational requirement or to maximize the translation performance on the development set. We call a subset constructed in either of these ways a candidate list.</bodyText>
<subsectionHeader confidence="0.998794">
3.3 Source Words for Unknown Words
</subsectionHeader>
<bodyText confidence="0.999868904761905">In the experiments, we evaluate the proposed approach with the neural machine translation model called RNNsearch (Bahdanau et al., 2015) (see Sec. 2.1.1). In this model, as a part of decoding process, we obtain the alignments between the target words and source locations via the alignment model in Eq. (5). We can use this feature to infer the source word to which each target word was most aligned (indicated by the largest αt in Eq. (5)). This is especially useful when the model generated an [UNK] token. Once a translation is generated given a source sentence, each [UNK] may be replaced using a translation-specific technique based on the aligned source word. For instance, in the experiment, we try replacing each [UNK] token with the aligned source word or its most likely translation determined by another word alignment model. Other techniques such as transliteration may also be used to further improve the performance (Koehn, 2010).</bodyText>
<sectionHeader confidence="0.999788" genericHeader="evaluation and result">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9998955">We evaluate the proposed approach in English—*French and English—*German translation tasks. We trained the neural machine translation models using only the bilingual, parallel corpora made available as a part of WMT’14. For each pair, the datasets we used are:</bodyText>
<listItem confidence="0.9912648">• English—*French:2 – Common Crawl – News Commentary – Gigaword – Europarl v7 – UN • English—*German: – Common Crawl – News Commentary – Europarl v7</listItem>
<footnote confidence="0.990796333333333">
2The preprocessed data can be found and down-
loaded from http://www-lium.univ-lemans.fr/
˜schwenk/nnmt-shared-task/README.
</footnote>
<page confidence="0.987146">
5
</page>
<table confidence="0.999536428571428">
English-French English-German
Train Test Train Test
15k 93.5 90.8 88.5 83.8
30k 96.0 94.6 91.8 87.9
50k 97.3 96.3 93.7 90.4
500k 99.5 99.3 98.4 96.1
All 100.0 99.6 100.0 97.3
</table>
<tableCaption confidence="0.851550333333333">
Table 1: Data coverage (in %) on target-side cor-
pora for different vocabulary sizes. ”All” refers to
all the tokens in the training set.
</tableCaption>
<bodyText confidence="0.999481894736842">To ensure fair comparison, the English—*French corpus, which comprises approximately 12 million sentences, is identical to the one used in (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Sutskever et al., 2014). As for English—*German, the corpus was preprocessed, in a manner similar to (Peitz et al., 2014; Li et al., 2014), in order to remove many poorly translated sentences. We evaluate the models on the WMT’14 test set (news-test 2014),3 while the concatenation of news-test-2012 and news-test-2013 is used for model selection (development set). Table 1 presents data coverage w.r.t. the vocabulary size, on the target side. Unless mentioned otherwise, all reported BLEU scores (Papineni et al., 2002) are computed with the multi-bleu.perl script4 on the cased tokenized translations.</bodyText>
<subsectionHeader confidence="0.981049">
4.1 Settings
</subsectionHeader>
<bodyText confidence="0.999758416666667">As a baseline for English—*French translation, we use the RNNsearch model proposed by (Bahdanau et al., 2015), with 30k source and target words.5 Another RNNsearch model is trained for English—*German translation with 50k source and target words. For each language pair, we train another set of RNNsearch models with much larger vocabularies of 500k source and target words, using the proposed approach. We call these models RNNsearch-LV. We vary the size of the shortlist used during training (r in Sec. 3.1). We tried</bodyText>
<footnote confidence="0.981520625">
3To compare with previous submissions, we use the fil-
tered test sets.
4https://github.com/moses-smt/
mosesdecoder/blob/master/scripts/
generic/multi-bleu.perl
5The authors of (Bahdanau et al., 2015) gave us access to
their trained models. We chose the best one on the validation
set and resumed training.
</footnote>
<bodyText confidence="0.999766265306123">15k and 30k for English—*French, and 15k and 50k for English—*German. We later report the results for the best performance on the development set, with models generally evaluated every twelve hours. The training speed is approximately the same as for RNNsearch. Using a 780 Ti or Titan Black GPU, we could process 100k mini-batches of 80 sentences in about 29 and 39 hours respectively for r = 15k and r = 50k. For both language pairs, we also trained new models, with r = 15k and r = 50k, by reshuffling the dataset at the beginning of each epoch. While this causes a non-negligible amount of overhead, such a change allows words to be contrasted with different sets of other words each epoch. To stabilize parameters other than the word embeddings, at the end of the training stage, we freeze the word embeddings and tune only the other parameters for approximately two more days after the peak performance on the development set is observed. This helped increase BLEU scores on the development set. We use beam search to generate a translation given a source. During beam search, we keep a set of 12 hypotheses and normalize probabilities by the length of the candidate sentences, as in (Cho et al., 2014a).6 The candidate list is chosen to maximize the performance on the development set, for K E 115k, 30k, 50k} and K' E 110, 201. As explained in Sec. 3.2, we test using a bilingual dictionary to accelerate decoding and to replace unknown words in translations. The bilingual dictionary is built using fast align (Dyer et al., 2013). We use the dictionary only if a word starts with a lowercase letter, and otherwise, we copy the source word directly. This led to better performance on the development sets. Note on ensembles For each language pair, we began training four models from each of which two points corresponding to the best and secondbest performance on the development set were collected. We continued training from each point, while keeping the word embeddings fixed, until the best development performance was reached, and took the model at this point as a single model in an ensemble. This procedure resulted in a total of eight models from which we averaged the length-normalized log-probabilities. Since much of training had been shared, the composition of</bodyText>
<footnote confidence="0.9922385">
6These experimental details differ from (Bahdanau et al.,
2015).
</footnote>
<page confidence="0.996168">
6
</page>
<table confidence="0.973615428571429">
RNNsearch RNNsearch-LV Google Phrase-based SMT
Basic NMT 29.97 (26.58) 32.68 (28.76) 30.6*
+Candidate List – 33.36 (29.32) –
+UNK Replace 33.08 (29.08) 34.11 (29.98) 33.10 33.3∗ 37.03•
+Reshuffle (T=50k) – 34.60 (30.53) –
+Ensemble – 37.19 (31.98) 37.50
(a) English—*French
RNNsearch RNNsearch-LV Phrase-based SMT
Basic NMT 16.46 (17.13) 16.95 (17.85)
+Candidate List – 17.46 (18.00)
+UNK Replace 18.97 (19.16) 18.89 (19.03) 20.67°
+Reshuffle – 19.40 (19.37)
+Ensemble – 21.59 (21.06)
(b) English—*German
</table>
<tableCaption confidence="0.6876395">
Table 2: The translation performances in BLEU obtained by different models on (a) English—*French and
(b) English—*German translation tasks. RNNsearch is the model proposed in (Bahdanau et al., 2015),
</tableCaption>
<bodyText confidence="0.890676">RNNsearch-LV is the RNNsearch trained with the approach proposed in this paper, and Google is the LSTM-based model proposed in (Sutskever et al., 2014). Unless mentioned otherwise, we report singlemodel RNNsearch-LV scores using T = 30k (English—*French) and T = 50k (English—*German). For the experiments we have run ourselves, we show the scores on the development set as well in the brackets. (*) (Sutskever et al., 2014), (o) (Luong et al., 2015), (•) (Durrani et al., 2014), (*) Standard Moses Setting (Cho et al., 2014b), (o) (Buck et al., 2014). such ensembles may be sub-optimal. This is supported by the fact that higher cross-model BLEU scores (Freitag et al., 2014) are observed for models that were partially trained together.</bodyText>
<subsectionHeader confidence="0.998575">
4.2 Translation Performance
</subsectionHeader>
<bodyText confidence="0.9999717">In Table 2, we present the results obtained by the trained models with very large target vocabularies, and alongside them, the previous results reported in (Sutskever et al., 2014), (Luong et al., 2015), (Buck et al., 2014) and (Durrani et al., 2014). Without translation-specific strategies, we can clearly see that the RNNsearch-LV outperforms the baseline RNNsearch. In the case of the English—*French task, RNNsearch-LV approached the performance level of the previous best single neural machine translation (NMT) model, even without any translationspecific techniques (Sec. 3.2–3.3). With these, however, the RNNsearch-LV outperformed it. The performance of the RNNsearch-LV is also better than that of a standard phrase-based translation system (Cho et al., 2014b). Furthermore, by combining 8 models, we were able to achieve a translation performance comparable to the state of the art, measured in BLEU. For English—*German, the RNNsearch-LV outperformed the baseline before unknown word replacement, but after doing so, the two systems performed similarly. We could reach higher largevocabulary single-model performance by reshuffling the dataset, but this step could potentially also help the baseline. In this case, we were able to surpass the previously reported best translation result on this task by building an ensemble of 8 models. With T = 15k, the RNNsearch-LV performance worsened a little, with best BLEU scores, without reshuffling, of 33.76 and 18.59 respectively for English—*French and English—*German. The English—*German ensemble described in this paper has also been used for the shared translation task of the 10th Workshop on Statistical Machine Translation (WMT’15), where it was ranked first in terms of BLEU score. The translations by this ensemble can be found online.7</bodyText>
<subsectionHeader confidence="0.996109">
4.3 Analysis
4.3.1 Decoding Speed
</subsectionHeader>
<bodyText confidence="0.999529333333333">In Table 3, we present the timing information of decoding for different models. Clearly, decoding from RNNsearch-LV with the full target vocab-</bodyText>
<footnote confidence="0.9842225">
7http://matrix.statmt.org/matrix/
output/1774?run_id=4079
</footnote>
<page confidence="0.99854">
7
</page>
<figure confidence="0.6137092">
CPU* GPU°
RNNsearch
RNNsearch-LV
RNNsearch-LV
+Candidate list
</figure>
<tableCaption confidence="0.439383333333333">
Table 3: The average per-word decoding time.
Decoding here does not include parameter load-
ing and unknown word replacement. The baseline
uses 30k words. The candidate list is built with
K = 30k and K' = 10. (*) i7-4820K (single
thread), (o) GTX TITAN Black
ulary is slowest.</tableCaption>
<bodyText confidence="0.990262615384616">If we use a candidate list for decoding each translation, the speed of decoding substantially improves and becomes close to the baseline RNNsearch. A potential issue with using a candidate list is that for each source sentence, we must re-build a target vocabulary and subsequently replace a part of the parameters, which may easily become timeconsuming. We can address this issue, for instance, by building a common candidate list for multiple source sentences. By doing so, we were able to match the decoding speed of the baseline RNNsearch model.</bodyText>
<subsectionHeader confidence="0.970791">
4.3.2 Decoding Target Vocabulary
</subsectionHeader>
<bodyText confidence="0.998838958333333">For English—*French (T = 30k), we evaluate the influence of the target vocabulary when translating the test sentences by using the union of a fixed set of 30k common words and (at most) K' likely candidates for each source word according to the dictionary. Results are presented in Figure 1. With K' = 0 (not shown), the performance of the system is comparable to the baseline when not replacing the unknown words (30.12), but there is not as much improvement when doing so (31.14). As the large vocabulary model does not predict [UNK] as much during training, it is less likely to generate it when decoding, limiting the effectiveness of the post-processing step in this case. With K' = 1, which limits the diversity of allowed uncommon words, BLEU is not as good as with moderately larger K', which indicates that our models can, to some degree, correctly choose between rare alternatives. If we rather use K = 50k, as we did for testing based on validation performance, the improvement over K' = 1 is approximately 0.2 BLEU. When validating the choice of K, we found it to be correlated with the value of T used during</bodyText>
<figureCaption confidence="0.939652">
Figure 1: Single-model test BLEU scores
</figureCaption>
<bodyText confidence="0.915917071428572">(English—*French) with respect to the number of dictionary entries K' allowed for each source word. training. For example, on the English—*French validation set, with T = 15k (and K' = 10), the BLEU score is 29.44 with K = 15k, but drops to 29.19 and 28.84 respectively for K = 30k and 50k. For T = 30k, the score increases moderately from K = 15k to K = 50k. A similar effect was observed for English—*German and on the test sets. As our implementation of importance sampling does not apply the usual correction to the gradient, it seems beneficial for the test vocabularies to resemble those used during training.</bodyText>
<sectionHeader confidence="0.999449" genericHeader="conclusion">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999944">In this paper, we proposed a way to extend the size of the target vocabulary for neural machine translation. The proposed approach allows us to train a model with much larger target vocabulary without any substantial increase in computational complexity. It is based on the earlier work in (Bengio and S´en´ecal, 2008) which used importance sampling to reduce the complexity of computing the normalization constant of the output word probability in neural language models. On English—*French and English—*German translation tasks, we observed that the neural machine translation models trained using the proposed method performed as well as, or better than, those using only limited sets of target words, even when replacing unknown words. As performance of the RNNsearch-LV models increased when only a selected subset of the target vocabulary was used during decoding, this makes the proposed learning algorithm more practical. When measured by BLEU, our models showed translation performance comparable to the state-of-the-art translation systems on both the English→French task and English→German task.</bodyText>
<figure confidence="0.998644125">
BLEU score
34.2
34.0
33.8
33.6
33.4
33.2
33.0
32.8
10 101 10 10
K'
With UNK replacement
Without UNK replacement
0.09 s 0.02 s
0.80 s 0.25 s
0.12 s 0.05 s
</figure>
<page confidence="0.991318">
8
</page>
<bodyText confidence="0.999942058823529">On the English→French task, a model trained with the proposed approach outperformed the best single neural machine translation (NMT) model from (Luong et al., 2015) by approximately 1 BLEU point. The performance of the ensemble of multiple models, despite its relatively less diverse composition, is approximately 0.3 BLEU points away from the best system (Luong et al., 2015). On the English→German task, the best performance of 21.59 BLEU by our model is higher than that of the previous state of the art (20.67) reported in (Buck et al., 2014). Finally, we release the source code used in our experiments to encourage progress in neural machine translation.8</bodyText>
<sectionHeader confidence="0.999157" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999835857142857">The authors would like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012). We acknowledge the support of the following agencies for research funding and computing support: NSERC, Calcul Qu´ebec, Compute Canada, the Canada Research Chairs, CIFAR and Samsung.</bodyText>
<sectionHeader confidence="0.998869" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99731141025641">
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR’2015,
arXiv:1409.0473.
Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu,
James Bergstra, Ian J. Goodfellow, Arnaud Berg-
eron, Nicolas Bouchard, and Yoshua Bengio. 2012.
Theano: new features and speed improvements.
Deep Learning and Unsupervised Feature Learning
NIPS 2012 Workshop.
Yoshua Bengio and Jean-S´ebastien S´en´ecal. 2008.
Adaptive importance sampling to accelerate train-
ing of a neural probabilistic language model. IEEE
Trans. Neural Networks, 19(4):713–722.
James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. 2010. Theano: a CPU and
GPU math expression compiler. In Proceedings
of the Python for Scientific Computing Conference
(SciPy), June. Oral Presentation.
8https://github.com/sebastien-j/LV_
groundhog
Christian Buck, Kenneth Heafield, and Bas van Ooyen.
2014. N-gram counts and language models from the
common crawl. In Proceedings of the Language Re-
sources and Evaluation Conference, Reykjavik, Ice-
land, May.
Kyunghyun Cho, Bart van Merri¨enboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014a. On the
properties of neural machine translation: Encoder–
Decoder approaches. In Eighth Workshop on Syn-
tax, Semantics and Structure in Statistical Transla-
tion, October.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Fethi Bougares, Holger Schwenk, and Yoshua
Bengio. 2014b. Learning phrase representa-
tions using RNN encoder-decoder for statistical ma-
chine translation. In Proceedings of the Empiricial
Methods in Natural Language Processing (EMNLP
2014), October.
Nadir Durrani, Barry Haddow, Philipp Koehn, and
Kenneth Heafield. 2014. Edinburgh’s phrase-based
machine translation systems for WMT-14. In Pro-
ceedings of the Ninth Workshop on Statistical Ma-
chine Translation, pages 97–104. Association for
Computational Linguistics Baltimore, MD, USA.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameter-
ization of IBM Model 2. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 644–648, At-
lanta, Georgia, June. Association for Computational
Linguistics.
Mikel L. Forcada and Ram´on P. ˜Neco. 1997. Re-
cursive hetero-associative memories for translation.
In Jos´e Mira, Roberto Moreno-Diaz, and Joan
Cabestany, editors, Biological and Artificial Compu-
tation: From Neuroscience to Technology, volume
1240 of Lecture Notes in Computer Science, pages
453–462. Springer Berlin Heidelberg.
Markus Freitag, Stephan Peitz, Joern Wuebker, Her-
mann Ney, Matthias Huck, Rico Sennrich, Nadir
Durrani, Maria Nadejde, Philip Williams, Philipp
Koehn, et al. 2014. Eu-bridge MT: Combined ma-
chine translation. In Proc. of the Workshop on Sta-
tistical Machine Translation, pages 105–113.
M. Gutmann and A. Hyvarinen. 2010. Noise-
contrastive estimation: A new estimation principle
for unnormalized statistical models. In Proceedings
of The Thirteenth International Conference on Arti-
ficial Intelligence and Statistics (AISTATS’10).
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
the ACL Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1700–
1709. Association for Computational Linguistics.
</reference>
<page confidence="0.944778">
9
</page>
<reference confidence="0.999785404255319">
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ’03, pages 48–54.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York, NY, USA,
1st edition.
Liangyou Li, Xiaofeng Wu, Santiago Cortes Vaillo, Jun
Xie, Andy Way, and Qun Liu. 2014. The DCU-
ICTCAS MT system at WMT 2014 on German-
English translation task. In Proceedings of the Ninth
Workshop on Statistical Machine Translation, pages
136–141, Baltimore, Maryland, USA, June. Associ-
ation for Computational Linguistics.
Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol
Vinyals, and Wojciech Zaremba. 2015. Addressing
the rare word problem in neural machine translation.
In Proceedings of ACL.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In International Conference
on Learning Representations: Workshops Track.
Andriy Mnih and Koray Kavukcuoglu. 2013. Learning
word embeddings efficiently with noise-contrastive
estimation. In C.J.C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K.Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, pages 2265–2273. Curran Associates, Inc.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 311–318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Stephan Peitz, Joern Wuebker, Markus Freitag, and
Hermann Ney. 2014. The RWTH Aachen German-
English machine translation system for WMT 2014.
In Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, pages 157–162, Baltimore,
Maryland, USA, June. Association for Computa-
tional Linguistics.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In NIPS’2014.
</reference>
<page confidence="0.997798">
10
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant confidence="0.691328" no="0">
<title confidence="0.999876">On Using Very Large Target Vocabulary for Neural Machine Translation</title>
<author confidence="0.998521">S´ebastien Jean Kyunghyun Cho Yoshua Bengio</author>
<affiliation confidence="0.939606">Memisevic de Montr´eal Universit´e de Montr´eal CIFAR Senior Fellow</affiliation>
<abstract confidence="0.991845303030303">Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrasebased statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method based on importance sampling that allows us to use a very large target vocabulary without increasing training complexity. We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to match, and in some cases outperform, the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use an ensemble of a few models with very large target vocabularies, we achieve performance comparable to the state of the art (measured BLEU) on both the translation tasks of WMT’14.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dzmitry Bahdanau</author>
<author>Kyunghyun Cho</author>
<author>Yoshua Bengio</author>
</authors>
<title>Neural machine translation by jointly learning to align and translate.</title>
<date>2015</date>
<booktitle>In ICLR’2015,</booktitle>
<pages>1409--0473</pages>
<contexts>
<context citStr="Bahdanau et al., 2015" endWordPosition="251" position="1601" startWordPosition="248"> The models trained by the proposed approach are empirically found to match, and in some cases outperform, the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use an ensemble of a few models with very large target vocabularies, we achieve performance comparable to the state of the art (measured by BLEU) on both the English→German and English→French translation tasks of WMT’14. 1 Introduction Neural machine translation (NMT) is a recently introduced approach to solving machine translation (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Sutskever et al., 2014). In neural machine translation, one builds a single neural network that reads a source sentence and generates its translation. The whole neural network is jointly trained to maximize the conditional probability of a correct translation given a source sentence, using the bilingual corpus. The NMT models have shown to perform as well as the most widely used conventional translation systems (Sutskever et al., 2014; Bahdanau et al., 2015). Neural machine translation has a number of advantages over the existing statistical machine translation system, specifically, the phra</context>
<context citStr="Bahdanau et al., 2015" endWordPosition="544" position="3378" startWordPosition="541">tly, the memory footprint of the NMT model is often much smaller than the existing system which relies on maintaining large tables of phrase pairs. Despite these advantages and promising results, there is a major limitation in NMT compared to the existing phrase-based approach. That is, the number of target words must be limited. This is mainly because the complexity of training and using an NMT model increases as the number of target words increases. A usual practice is to construct a target vocabulary of the K most frequent words (a socalled shortlist), where K is often in the range of 30k (Bahdanau et al., 2015) to 80k (Sutskever et al., 2014). Any word not included in this vocabulary is mapped to a special token representing an unknown word [UNK]. This approach works well when there are only a few unknown words in the target sentence, but it has been observed 1 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1–10, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics that the translation performance degrades rapidly as the number of unknown words incre</context>
<context citStr="Bahdanau et al., 2015" endWordPosition="758" position="4690" startWordPosition="754">training algorithm based on (biased) importance sampling that allows us to train an NMT model with a much larger target vocabulary. The proposed algorithm effectively keeps the computational complexity during training at the level of using only a small subset of the full vocabulary. Once the model with a very large target vocabulary is trained, one can choose to use either all the target words or only a subset of them. We compare the proposed algorithm against the baseline shortlist-based approach in the tasks of English→French and English→German translation using the NMT model introduced in (Bahdanau et al., 2015). The empirical results demonstrate that we can potentially achieve better translation performance using larger vocabularies, and that our approach does not sacrifice too much speed for both training and decoding. Furthermore, we show that the model trained with this algorithm gets the best translation performance yet achieved by single NMT models on the WMT’14 English→French translation task. 2 Neural Machine Translation and Limited Vocabulary Problem In this section, we briefly describe an approach to neural machine translation proposed recently in (Bahdanau et al., 2015). Based on this desc</context>
<context citStr="Bahdanau et al., 2015" endWordPosition="1108" position="6735" startWordPosition="1105">p {q (yt−1, zt, ct)} , (2) where zt = g (yt−1, zt−1, ct) , (3) ct = r (zt−1, h1, ... , hT) , (4) and y&lt;t = (y1, . . . , yt−1). The whole model is jointly trained to maximize the conditional log-probability of the correct translation given a source sentence with respect to the parameters 0 of the model: log p(ynt |yn&lt;t, xn), where (xn, yn) is the n-th training pair of sentences, and Tn is the length of the n-th target sentence (yn). 2.1.1 Detailed Description In this paper, we use a specific implementation of neural machine translation that uses an attention mechanism, as recently proposed in (Bahdanau et al., 2015). In (Bahdanau et al., 2015), the encoder in Eq. (1) is implemented by a bi-directional recurrent neural network such that ht = [h t; h t] , where h t = f (xt, h t+1) , h t = f (xt, h t−1) . They used a gated recurrent unit for f (see, e.g., (Cho et al., 2014b)). The decoder, at each time, computes the context vector ct as a convex sum of the hidden states (h1, ... , hT) with the coefficients α1, ... , αT computed by exp {a (ht, zt−1)} αt = (5) Ek exp {a (hk, zt−1)}, where a is a feedforward neural network with a single hidden layer. A new hidden state zt of the decoder in Eq. (3) is computed </context>
<context citStr="Bahdanau et al., 2015" endWordPosition="1383" position="8066" startWordPosition="1379">1 N n=1 2 context vector ct. The decoder also uses the gated recurrent unit, as the encoder does. The probability of the next target word in Eq. (2) is then computed by 1 p( yt |y&lt;t, x) = Z exp{w&gt;t φ (yt−1, zt, ct) + bt} , (6) where φ is an affine transformation followed by a nonlinear activation, and wt and bt are respectively the target word vector and the target word bias. Z is the normalization constant computed by { } exp w&gt; k φ (yt−1, zt, ct) + bk , (7) where V is the set of all the target words. For the detailed description of the implementation, we refer the reader to the appendix of (Bahdanau et al., 2015). 2.2 Limited Vocabulary Issue and Conventional Solutions One of the main difficulties in training this neural machine translation model is the computational complexity involved in computing the target word probability (Eq. (6)). More specifically, we need to compute the dot product between the feature φ (yt−1, zt, ct) and the word vector wt as many times as there are words in a target vocabulary in order to compute the normalization constant (the denominator in Eq. (6)). This has to be done for, on average, 20–30 words per sentence, which easily becomes prohibitively expensive even with a mod</context>
<context citStr="Bahdanau et al., 2015" endWordPosition="3122" position="18323" startWordPosition="3119">th the dictionary, for each source sentence, we construct a target word set consisting of the K-most frequent words (according to the estimated unigram probability) and, using the dictionary, at most K' likely target words for each source word. K and K' may be chosen either to meet the computational requirement or to maximize the translation performance on the development set. We call a subset constructed in either of these ways a candidate list. 3.3 Source Words for Unknown Words In the experiments, we evaluate the proposed approach with the neural machine translation model called RNNsearch (Bahdanau et al., 2015) (see Sec. 2.1.1). In this model, as a part of decoding process, we obtain the alignments between the target words and source locations via the alignment model in Eq. (5). We can use this feature to infer the source word to which each target word was most aligned (indicated by the largest αt in Eq. (5)). This is especially useful when the model generated an [UNK] token. Once a translation is generated given a source sentence, each [UNK] may be replaced using a translation-specific technique based on the aligned source word. For instance, in the experiment, we try replacing each [UNK] token wit</context>
<context citStr="Bahdanau et al., 2015" endWordPosition="3426" position="20185" startWordPosition="3423">processed data can be found and downloaded from http://www-lium.univ-lemans.fr/ ˜schwenk/nnmt-shared-task/README. 5 English-French English-German Train Test Train Test 15k 93.5 90.8 88.5 83.8 30k 96.0 94.6 91.8 87.9 50k 97.3 96.3 93.7 90.4 500k 99.5 99.3 98.4 96.1 All 100.0 99.6 100.0 97.3 Table 1: Data coverage (in %) on target-side corpora for different vocabulary sizes. ”All” refers to all the tokens in the training set. To ensure fair comparison, the English—*French corpus, which comprises approximately 12 million sentences, is identical to the one used in (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Sutskever et al., 2014). As for English—*German, the corpus was preprocessed, in a manner similar to (Peitz et al., 2014; Li et al., 2014), in order to remove many poorly translated sentences. We evaluate the models on the WMT’14 test set (news-test 2014),3 while the concatenation of news-test-2012 and news-test-2013 is used for model selection (development set). Table 1 presents data coverage w.r.t. the vocabulary size, on the target side. Unless mentioned otherwise, all reported BLEU scores (Papineni et al., 2002) are computed with the multi-bleu.perl script4 on the cased tokenized transla</context>
<context citStr="Bahdanau et al., 2015" endWordPosition="3628" position="21523" startWordPosition="3625">et al., 2015), with 30k source and target words.5 Another RNNsearch model is trained for English—*German translation with 50k source and target words. For each language pair, we train another set of RNNsearch models with much larger vocabularies of 500k source and target words, using the proposed approach. We call these models RNNsearch-LV. We vary the size of the shortlist used during training (r in Sec. 3.1). We tried 3To compare with previous submissions, we use the filtered test sets. 4https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ generic/multi-bleu.perl 5The authors of (Bahdanau et al., 2015) gave us access to their trained models. We chose the best one on the validation set and resumed training. 15k and 30k for English—*French, and 15k and 50k for English—*German. We later report the results for the best performance on the development set, with models generally evaluated every twelve hours. The training speed is approximately the same as for RNNsearch. Using a 780 Ti or Titan Black GPU, we could process 100k mini-batches of 80 sentences in about 29 and 39 hours respectively for r = 15k and r = 50k. For both language pairs, we also trained new models, with r = 15k and r = 50k, by </context>
<context citStr="Bahdanau et al., 2015" endWordPosition="4053" position="23974" startWordPosition="4050">ensembles For each language pair, we began training four models from each of which two points corresponding to the best and secondbest performance on the development set were collected. We continued training from each point, while keeping the word embeddings fixed, until the best development performance was reached, and took the model at this point as a single model in an ensemble. This procedure resulted in a total of eight models from which we averaged the length-normalized log-probabilities. Since much of training had been shared, the composition of 6These experimental details differ from (Bahdanau et al., 2015). 6 RNNsearch RNNsearch-LV Google Phrase-based SMT Basic NMT 29.97 (26.58) 32.68 (28.76) 30.6* +Candidate List – 33.36 (29.32) – +UNK Replace 33.08 (29.08) 34.11 (29.98) 33.10 33.3∗ 37.03• +Reshuffle (T=50k) – 34.60 (30.53) – +Ensemble – 37.19 (31.98) 37.50 (a) English—*French RNNsearch RNNsearch-LV Phrase-based SMT Basic NMT 16.46 (17.13) 16.95 (17.85) +Candidate List – 17.46 (18.00) +UNK Replace 18.97 (19.16) 18.89 (19.03) 20.67° +Reshuffle – 19.40 (19.37) +Ensemble – 21.59 (21.06) (b) English—*German Table 2: The translation performances in BLEU obtained by different models on (a) English—*</context>
</contexts>
<marker>Bahdanau, Cho, Bengio, 2015</marker>
<rawString>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In ICLR’2015, arXiv:1409.0473.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fr´ed´eric Bastien</author>
<author>Pascal Lamblin</author>
<author>Razvan Pascanu</author>
<author>James Bergstra</author>
<author>Ian J Goodfellow</author>
<author>Arnaud Bergeron</author>
<author>Nicolas Bouchard</author>
<author>Yoshua Bengio</author>
</authors>
<title>Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS</title>
<date>2012</date>
<note>Workshop.</note>
<marker>Bastien, Lamblin, Pascanu, Bergstra, Goodfellow, Bergeron, Bouchard, Bengio, 2012</marker>
<rawString>Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian J. Goodfellow, Arnaud Bergeron, Nicolas Bouchard, and Yoshua Bengio. 2012. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Jean-S´ebastien S´en´ecal</author>
</authors>
<title>Adaptive importance sampling to accelerate training of a neural probabilistic language model.</title>
<date>2008</date>
<journal>IEEE Trans. Neural Networks,</journal>
<volume>19</volume>
<issue>4</issue>
<marker>Bengio, S´en´ecal, 2008</marker>
<rawString>Yoshua Bengio and Jean-S´ebastien S´en´ecal. 2008. Adaptive importance sampling to accelerate training of a neural probabilistic language model. IEEE Trans. Neural Networks, 19(4):713–722.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Bergstra</author>
<author>Olivier Breuleux</author>
<author>Fr´ed´eric Bastien</author>
<author>Pascal Lamblin</author>
<author>Razvan Pascanu</author>
<author>Guillaume Desjardins</author>
<author>Joseph Turian</author>
<author>David Warde-Farley</author>
<author>Yoshua Bengio</author>
</authors>
<title>Theano: a CPU and GPU math expression compiler.</title>
<date>2010</date>
<booktitle>In Proceedings of the Python for Scientific Computing Conference (SciPy),</booktitle>
<marker>Bergstra, Breuleux, Bastien, Lamblin, Pascanu, Desjardins, Turian, Warde-Farley, Bengio, 2010</marker>
<rawString>James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. 2010. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy), June. Oral Presentation. groundhog</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Buck</author>
<author>Kenneth Heafield</author>
<author>Bas van Ooyen</author>
</authors>
<title>N-gram counts and language models from the common crawl.</title>
<date>2014</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference,</booktitle>
<location>Reykjavik, Iceland,</location>
<marker>Buck, Heafield, van Ooyen, 2014</marker>
<rawString>Christian Buck, Kenneth Heafield, and Bas van Ooyen. 2014. N-gram counts and language models from the common crawl. In Proceedings of the Language Resources and Evaluation Conference, Reykjavik, Iceland, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyunghyun Cho</author>
<author>Bart van Merri¨enboer</author>
<author>Dzmitry Bahdanau</author>
<author>Yoshua Bengio</author>
</authors>
<title>On the properties of neural machine translation: Encoder– Decoder approaches.</title>
<date>2014</date>
<booktitle>In Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation,</booktitle>
<marker>Cho, van Merri¨enboer, Bahdanau, Bengio, 2014</marker>
<rawString>Kyunghyun Cho, Bart van Merri¨enboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014a. On the properties of neural machine translation: Encoder– Decoder approaches. In Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyunghyun Cho</author>
<author>Bart van Merrienboer</author>
</authors>
<title>Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014b. Learning phrase representations using RNN encoder-decoder for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP</booktitle>
<marker>Cho, van Merrienboer, 2014</marker>
<rawString>Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014b. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Kenneth Heafield</author>
</authors>
<title>Edinburgh’s phrase-based machine translation systems for WMT-14.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<pages>97--104</pages>
<location>Baltimore, MD, USA.</location>
<contexts>
<context citStr="Durrani et al., 2014" endWordPosition="4233" position="25162" startWordPosition="4230">different models on (a) English—*French and (b) English—*German translation tasks. RNNsearch is the model proposed in (Bahdanau et al., 2015), RNNsearch-LV is the RNNsearch trained with the approach proposed in this paper, and Google is the LSTM-based model proposed in (Sutskever et al., 2014). Unless mentioned otherwise, we report singlemodel RNNsearch-LV scores using T = 30k (English—*French) and T = 50k (English—*German). For the experiments we have run ourselves, we show the scores on the development set as well in the brackets. (*) (Sutskever et al., 2014), (o) (Luong et al., 2015), (•) (Durrani et al., 2014), (*) Standard Moses Setting (Cho et al., 2014b), (o) (Buck et al., 2014). such ensembles may be sub-optimal. This is supported by the fact that higher cross-model BLEU scores (Freitag et al., 2014) are observed for models that were partially trained together. 4.2 Translation Performance In Table 2, we present the results obtained by the trained models with very large target vocabularies, and alongside them, the previous results reported in (Sutskever et al., 2014), (Luong et al., 2015), (Buck et al., 2014) and (Durrani et al., 2014). Without translation-specific strategies, we can clearly see</context>
</contexts>
<marker>Durrani, Haddow, Koehn, Heafield, 2014</marker>
<rawString>Nadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Heafield. 2014. Edinburgh’s phrase-based machine translation systems for WMT-14. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 97–104. Association for Computational Linguistics Baltimore, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Victor Chahuneau</author>
<author>Noah A Smith</author>
</authors>
<title>A simple, fast, and effective reparameterization of IBM Model 2.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>644--648</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context citStr="Dyer et al., 2013" endWordPosition="3921" position="23168" startWordPosition="3918">lopment set is observed. This helped increase BLEU scores on the development set. We use beam search to generate a translation given a source. During beam search, we keep a set of 12 hypotheses and normalize probabilities by the length of the candidate sentences, as in (Cho et al., 2014a).6 The candidate list is chosen to maximize the performance on the development set, for K E 115k, 30k, 50k} and K' E 110, 201. As explained in Sec. 3.2, we test using a bilingual dictionary to accelerate decoding and to replace unknown words in translations. The bilingual dictionary is built using fast align (Dyer et al., 2013). We use the dictionary only if a word starts with a lowercase letter, and otherwise, we copy the source word directly. This led to better performance on the development sets. Note on ensembles For each language pair, we began training four models from each of which two points corresponding to the best and secondbest performance on the development set were collected. We continued training from each point, while keeping the word embeddings fixed, until the best development performance was reached, and took the model at this point as a single model in an ensemble. This procedure resulted in a to</context>
</contexts>
<marker>Dyer, Chahuneau, Smith, 2013</marker>
<rawString>Chris Dyer, Victor Chahuneau, and Noah A. Smith. 2013. A simple, fast, and effective reparameterization of IBM Model 2. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 644–648, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikel L Forcada</author>
<author>Ram´on P ˜Neco</author>
</authors>
<title>Recursive hetero-associative memories for translation.</title>
<date>1997</date>
<booktitle>Biological and Artificial Computation: From Neuroscience to Technology,</booktitle>
<volume>1240</volume>
<pages>453--462</pages>
<editor>In Jos´e Mira, Roberto Moreno-Diaz, and Joan Cabestany, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<marker>Forcada, ˜Neco, 1997</marker>
<rawString>Mikel L. Forcada and Ram´on P. ˜Neco. 1997. Recursive hetero-associative memories for translation. In Jos´e Mira, Roberto Moreno-Diaz, and Joan Cabestany, editors, Biological and Artificial Computation: From Neuroscience to Technology, volume 1240 of Lecture Notes in Computer Science, pages 453–462. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Freitag</author>
<author>Stephan Peitz</author>
<author>Joern Wuebker</author>
<author>Hermann Ney</author>
<author>Matthias Huck</author>
<author>Rico Sennrich</author>
<author>Nadir Durrani</author>
<author>Maria Nadejde</author>
<author>Philip Williams</author>
<author>Philipp Koehn</author>
</authors>
<title>Eu-bridge MT: Combined machine translation.</title>
<date>2014</date>
<booktitle>In Proc. of the Workshop on Statistical Machine Translation,</booktitle>
<pages>105--113</pages>
<contexts>
<context citStr="Freitag et al., 2014" endWordPosition="4267" position="25360" startWordPosition="4264"> proposed in this paper, and Google is the LSTM-based model proposed in (Sutskever et al., 2014). Unless mentioned otherwise, we report singlemodel RNNsearch-LV scores using T = 30k (English—*French) and T = 50k (English—*German). For the experiments we have run ourselves, we show the scores on the development set as well in the brackets. (*) (Sutskever et al., 2014), (o) (Luong et al., 2015), (•) (Durrani et al., 2014), (*) Standard Moses Setting (Cho et al., 2014b), (o) (Buck et al., 2014). such ensembles may be sub-optimal. This is supported by the fact that higher cross-model BLEU scores (Freitag et al., 2014) are observed for models that were partially trained together. 4.2 Translation Performance In Table 2, we present the results obtained by the trained models with very large target vocabularies, and alongside them, the previous results reported in (Sutskever et al., 2014), (Luong et al., 2015), (Buck et al., 2014) and (Durrani et al., 2014). Without translation-specific strategies, we can clearly see that the RNNsearch-LV outperforms the baseline RNNsearch. In the case of the English—*French task, RNNsearch-LV approached the performance level of the previous best single neural machine translati</context>
</contexts>
<marker>Freitag, Peitz, Wuebker, Ney, Huck, Sennrich, Durrani, Nadejde, Williams, Koehn, 2014</marker>
<rawString>Markus Freitag, Stephan Peitz, Joern Wuebker, Hermann Ney, Matthias Huck, Rico Sennrich, Nadir Durrani, Maria Nadejde, Philip Williams, Philipp Koehn, et al. 2014. Eu-bridge MT: Combined machine translation. In Proc. of the Workshop on Statistical Machine Translation, pages 105–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gutmann</author>
<author>A Hyvarinen</author>
</authors>
<title>Noisecontrastive estimation: A new estimation principle for unnormalized statistical models.</title>
<date>2010</date>
<booktitle>In Proceedings of The Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS’10).</booktitle>
<contexts>
<context citStr="Gutmann and Hyvarinen, 2010" endWordPosition="1678" position="9919" startWordPosition="1674">es many words that are not included in the shortlist (Cho et al., 2014a). This also affects the performance evaluation of the system which is often measured by BLEU. Second, the first issue becomes more problematic with languages that have a rich set of words such as German or other highly inflected languages. There are two model-specific approaches to this issue of large target vocabulary. The first approach is to stochastically approximate the target word probability. This has been proposed recently in (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013) based on noise-contrastive estimation (Gutmann and Hyvarinen, 2010). In the second approach, the target words are clustered into multiple classes, or hierarchical classes, and the target probability p(yt|y&lt;t, x) is factorized as a product of the class probability p(ct|y&lt;t, x) and the intraclass word probability p(yt|ct, y&lt;t, x). This reduces the number of required dot-products into the sum of the number of classes and the words in a class. These approaches mainly aim at reducing the computational complexity during training, but do not often result in speed-up when decoding a translation during test time.1 Other than these model-specific approaches, there exis</context>
</contexts>
<marker>Gutmann, Hyvarinen, 2010</marker>
<rawString>M. Gutmann and A. Hyvarinen. 2010. Noisecontrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of The Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent continuous translation models.</title>
<date>2013</date>
<booktitle>In Proceedings of the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1700--1709</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context citStr="Kalchbrenner and Blunsom, 2013" endWordPosition="247" position="1578" startWordPosition="244"> of the whole target vocabulary. The models trained by the proposed approach are empirically found to match, and in some cases outperform, the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use an ensemble of a few models with very large target vocabularies, we achieve performance comparable to the state of the art (measured by BLEU) on both the English→German and English→French translation tasks of WMT’14. 1 Introduction Neural machine translation (NMT) is a recently introduced approach to solving machine translation (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Sutskever et al., 2014). In neural machine translation, one builds a single neural network that reads a source sentence and generates its translation. The whole neural network is jointly trained to maximize the conditional probability of a correct translation given a source sentence, using the bilingual corpus. The NMT models have shown to perform as well as the most widely used conventional translation systems (Sutskever et al., 2014; Bahdanau et al., 2015). Neural machine translation has a number of advantages over the existing statistical machine translation system,</context>
<context citStr="Kalchbrenner and Blunsom, 2013" endWordPosition="899" position="5635" startWordPosition="896"> yet achieved by single NMT models on the WMT’14 English→French translation task. 2 Neural Machine Translation and Limited Vocabulary Problem In this section, we briefly describe an approach to neural machine translation proposed recently in (Bahdanau et al., 2015). Based on this description we explain the issue of limited vocabularies in neural machine translation. 2.1 Neural Machine Translation Neural machine translation is a recently proposed approach to machine translation, which uses a single neural network trained jointly to maximize the translation performance (Forcada and ˜Neco, 1997; Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014; Bahdanau et al., 2015). Neural machine translation is often implemented as the encoder–decoder network. The encoder reads the source sentence x = (x1, ... , xT) and encodes it into a sequence of hidden states h = (h1, ··· ,hT): ht = f (xt, ht−1) . (1) Then, the decoder, another recurrent neural network, generates a corresponding translation y = (y1, · · · , yT,) based on the encoded sequence of hidden states h: p(yt |y&lt;t, x) a exp {q (yt−1, zt, ct)} , (2) where zt = g (yt−1, zt−1, ct) , (3) ct = r (zt−1, h1, ... , hT) , (4) and y&lt;t = (y1, . . . , yt</context>
<context citStr="Kalchbrenner and Blunsom, 2013" endWordPosition="3422" position="20162" startWordPosition="3419">ommentary – Europarl v7 2The preprocessed data can be found and downloaded from http://www-lium.univ-lemans.fr/ ˜schwenk/nnmt-shared-task/README. 5 English-French English-German Train Test Train Test 15k 93.5 90.8 88.5 83.8 30k 96.0 94.6 91.8 87.9 50k 97.3 96.3 93.7 90.4 500k 99.5 99.3 98.4 96.1 All 100.0 99.6 100.0 97.3 Table 1: Data coverage (in %) on target-side corpora for different vocabulary sizes. ”All” refers to all the tokens in the training set. To ensure fair comparison, the English—*French corpus, which comprises approximately 12 million sentences, is identical to the one used in (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Sutskever et al., 2014). As for English—*German, the corpus was preprocessed, in a manner similar to (Peitz et al., 2014; Li et al., 2014), in order to remove many poorly translated sentences. We evaluate the models on the WMT’14 test set (news-test 2014),3 while the concatenation of news-test-2012 and news-test-2013 is used for model selection (development set). Table 1 presents data coverage w.r.t. the vocabulary size, on the target side. Unless mentioned otherwise, all reported BLEU scores (Papineni et al., 2002) are computed with the multi-bleu.perl script4 on the </context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models. In Proceedings of the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1700– 1709. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03,</booktitle>
<pages>48--54</pages>
<contexts>
<context citStr="Koehn et al., 2003" endWordPosition="349" position="2237" startWordPosition="346">., 2014). In neural machine translation, one builds a single neural network that reads a source sentence and generates its translation. The whole neural network is jointly trained to maximize the conditional probability of a correct translation given a source sentence, using the bilingual corpus. The NMT models have shown to perform as well as the most widely used conventional translation systems (Sutskever et al., 2014; Bahdanau et al., 2015). Neural machine translation has a number of advantages over the existing statistical machine translation system, specifically, the phrase-based system (Koehn et al., 2003). First, NMT requires a minimal set of domain knowledge. For instance, all of the models proposed in (Sutskever et al., 2014), (Bahdanau et al., 2015) or (Kalchbrenner and Blunsom, 2013) do not assume any linguistic property in both source and target sentences except that they are sequences of words. Second, the whole system is jointly trained to maximize the translation performance, unlike the existing phrase-based system which consists of many separately trained features whose weights are then tuned jointly. Lastly, the memory footprint of the NMT model is often much smaller than the existin</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Machine Translation.</title>
<date>2010</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context citStr="Koehn, 2010" endWordPosition="3260" position="19130" startWordPosition="3259"> to infer the source word to which each target word was most aligned (indicated by the largest αt in Eq. (5)). This is especially useful when the model generated an [UNK] token. Once a translation is generated given a source sentence, each [UNK] may be replaced using a translation-specific technique based on the aligned source word. For instance, in the experiment, we try replacing each [UNK] token with the aligned source word or its most likely translation determined by another word alignment model. Other techniques such as transliteration may also be used to further improve the performance (Koehn, 2010). 4 Experiments We evaluate the proposed approach in English—*French and English—*German translation tasks. We trained the neural machine translation models using only the bilingual, parallel corpora made available as a part of WMT’14. For each pair, the datasets we used are: • English—*French:2 – Common Crawl – News Commentary – Gigaword – Europarl v7 – UN • English—*German: – Common Crawl – News Commentary – Europarl v7 2The preprocessed data can be found and downloaded from http://www-lium.univ-lemans.fr/ ˜schwenk/nnmt-shared-task/README. 5 English-French English-German Train Test Train Tes</context>
</contexts>
<marker>Koehn, 2010</marker>
<rawString>Philipp Koehn. 2010. Statistical Machine Translation. Cambridge University Press, New York, NY, USA, 1st edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liangyou Li</author>
<author>Xiaofeng Wu</author>
<author>Santiago Cortes Vaillo</author>
<author>Jun Xie</author>
<author>Andy Way</author>
<author>Qun Liu</author>
</authors>
<title>on GermanEnglish translation task.</title>
<date>2014</date>
<booktitle>The DCUICTCAS MT system at WMT</booktitle>
<pages>136--141</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA,</location>
<contexts>
<context citStr="Li et al., 2014" endWordPosition="3450" position="20325" startWordPosition="3447"> Train Test Train Test 15k 93.5 90.8 88.5 83.8 30k 96.0 94.6 91.8 87.9 50k 97.3 96.3 93.7 90.4 500k 99.5 99.3 98.4 96.1 All 100.0 99.6 100.0 97.3 Table 1: Data coverage (in %) on target-side corpora for different vocabulary sizes. ”All” refers to all the tokens in the training set. To ensure fair comparison, the English—*French corpus, which comprises approximately 12 million sentences, is identical to the one used in (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Sutskever et al., 2014). As for English—*German, the corpus was preprocessed, in a manner similar to (Peitz et al., 2014; Li et al., 2014), in order to remove many poorly translated sentences. We evaluate the models on the WMT’14 test set (news-test 2014),3 while the concatenation of news-test-2012 and news-test-2013 is used for model selection (development set). Table 1 presents data coverage w.r.t. the vocabulary size, on the target side. Unless mentioned otherwise, all reported BLEU scores (Papineni et al., 2002) are computed with the multi-bleu.perl script4 on the cased tokenized translations. 4.1 Settings As a baseline for English—*French translation, we use the RNNsearch model proposed by (Bahdanau et al., 2015), with 30k </context>
</contexts>
<marker>Li, Wu, Vaillo, Xie, Way, Liu, 2014</marker>
<rawString>Liangyou Li, Xiaofeng Wu, Santiago Cortes Vaillo, Jun Xie, Andy Way, and Qun Liu. 2014. The DCUICTCAS MT system at WMT 2014 on GermanEnglish translation task. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 136–141, Baltimore, Maryland, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minh-Thang Luong</author>
<author>Ilya Sutskever</author>
<author>Quoc V Le</author>
<author>Oriol Vinyals</author>
<author>Wojciech Zaremba</author>
</authors>
<title>Addressing the rare word problem in neural machine translation.</title>
<date>2015</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context citStr="Luong et al., 2015" endWordPosition="1807" position="10740" startWordPosition="1804">y&lt;t, x) and the intraclass word probability p(yt|ct, y&lt;t, x). This reduces the number of required dot-products into the sum of the number of classes and the words in a class. These approaches mainly aim at reducing the computational complexity during training, but do not often result in speed-up when decoding a translation during test time.1 Other than these model-specific approaches, there exist translation-specific approaches. A translation-specific approach exploits the properties of the rare target words. For instance, Luong et al. proposed such an approach for neural machine translation (Luong et al., 2015). They replace rare words (the words that are not included in the shortlist) in both source and target sentences into corresponding (OOVn) tokens using the word alignment model. Once a source sentence is translated, each (OOVn) in the translation will be replaced based on the source word marked by the corresponding (OOVn). It is important to note that the modelspecific approaches and the translation-specific approaches are often complementary and can be used together to further improve the translation performance and reduce the computational complexity. 3 Approximate Learning Approach to Very </context>
<context citStr="Luong et al., 2015" endWordPosition="4228" position="25134" startWordPosition="4225">ances in BLEU obtained by different models on (a) English—*French and (b) English—*German translation tasks. RNNsearch is the model proposed in (Bahdanau et al., 2015), RNNsearch-LV is the RNNsearch trained with the approach proposed in this paper, and Google is the LSTM-based model proposed in (Sutskever et al., 2014). Unless mentioned otherwise, we report singlemodel RNNsearch-LV scores using T = 30k (English—*French) and T = 50k (English—*German). For the experiments we have run ourselves, we show the scores on the development set as well in the brackets. (*) (Sutskever et al., 2014), (o) (Luong et al., 2015), (•) (Durrani et al., 2014), (*) Standard Moses Setting (Cho et al., 2014b), (o) (Buck et al., 2014). such ensembles may be sub-optimal. This is supported by the fact that higher cross-model BLEU scores (Freitag et al., 2014) are observed for models that were partially trained together. 4.2 Translation Performance In Table 2, we present the results obtained by the trained models with very large target vocabularies, and alongside them, the previous results reported in (Sutskever et al., 2014), (Luong et al., 2015), (Buck et al., 2014) and (Durrani et al., 2014). Without translation-specific st</context>
<context citStr="Luong et al., 2015" endWordPosition="5295" position="31629" startWordPosition="5292">cted subset of the target vocabulary was used during decoding, this makes the proposed learning algorithm more practical. When measured by BLEU, our models showed translation performance comparable to the BLEU score 34.2 34.0 33.8 33.6 33.4 33.2 33.0 32.8 10 101 10 10 K' With UNK replacement Without UNK replacement 0.09 s 0.02 s 0.80 s 0.25 s 0.12 s 0.05 s 8 state-of-the-art translation systems on both the English→French task and English→German task. On the English→French task, a model trained with the proposed approach outperformed the best single neural machine translation (NMT) model from (Luong et al., 2015) by approximately 1 BLEU point. The performance of the ensemble of multiple models, despite its relatively less diverse composition, is approximately 0.3 BLEU points away from the best system (Luong et al., 2015). On the English→German task, the best performance of 21.59 BLEU by our model is higher than that of the previous state of the art (20.67) reported in (Buck et al., 2014). Finally, we release the source code used in our experiments to encourage progress in neural machine translation.8 Acknowledgments The authors would like to thank the developers of Theano (Bergstra et al., 2010; Basti</context>
</contexts>
<marker>Luong, Sutskever, Le, Vinyals, Zaremba, 2015</marker>
<rawString>Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Wojciech Zaremba. 2015. Addressing the rare word problem in neural machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In International Conference on Learning Representations: Workshops Track.</booktitle>
<contexts>
<context citStr="Mikolov et al., 2013" endWordPosition="1669" position="9851" startWordPosition="1666">grades heavily if the translation of a source sentence requires many words that are not included in the shortlist (Cho et al., 2014a). This also affects the performance evaluation of the system which is often measured by BLEU. Second, the first issue becomes more problematic with languages that have a rich set of words such as German or other highly inflected languages. There are two model-specific approaches to this issue of large target vocabulary. The first approach is to stochastically approximate the target word probability. This has been proposed recently in (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013) based on noise-contrastive estimation (Gutmann and Hyvarinen, 2010). In the second approach, the target words are clustered into multiple classes, or hierarchical classes, and the target probability p(yt|y&lt;t, x) is factorized as a product of the class probability p(ct|y&lt;t, x) and the intraclass word probability p(yt|ct, y&lt;t, x). This reduces the number of required dot-products into the sum of the number of classes and the words in a class. These approaches mainly aim at reducing the computational complexity during training, but do not often result in speed-up when decoding a translation durin</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. In International Conference on Learning Representations: Workshops Track.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Koray Kavukcuoglu</author>
</authors>
<title>Learning word embeddings efficiently with noise-contrastive estimation.</title>
<date>2013</date>
<booktitle>Advances in Neural Information Processing Systems 26,</booktitle>
<pages>2265--2273</pages>
<editor>In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors,</editor>
<publisher>Curran Associates, Inc.</publisher>
<contexts>
<context citStr="Mnih and Kavukcuoglu, 2013" endWordPosition="1665" position="9828" startWordPosition="1662"> performance of the model degrades heavily if the translation of a source sentence requires many words that are not included in the shortlist (Cho et al., 2014a). This also affects the performance evaluation of the system which is often measured by BLEU. Second, the first issue becomes more problematic with languages that have a rich set of words such as German or other highly inflected languages. There are two model-specific approaches to this issue of large target vocabulary. The first approach is to stochastically approximate the target word probability. This has been proposed recently in (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013) based on noise-contrastive estimation (Gutmann and Hyvarinen, 2010). In the second approach, the target words are clustered into multiple classes, or hierarchical classes, and the target probability p(yt|y&lt;t, x) is factorized as a product of the class probability p(ct|y&lt;t, x) and the intraclass word probability p(yt|ct, y&lt;t, x). This reduces the number of required dot-products into the sum of the number of classes and the words in a class. These approaches mainly aim at reducing the computational complexity during training, but do not often result in speed-up when decod</context>
</contexts>
<marker>Mnih, Kavukcuoglu, 2013</marker>
<rawString>Andriy Mnih and Koray Kavukcuoglu. 2013. Learning word embeddings efficiently with noise-contrastive estimation. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 2265–2273. Curran Associates, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context citStr="Papineni et al., 2002" endWordPosition="3507" position="20708" startWordPosition="3504">n sentences, is identical to the one used in (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Sutskever et al., 2014). As for English—*German, the corpus was preprocessed, in a manner similar to (Peitz et al., 2014; Li et al., 2014), in order to remove many poorly translated sentences. We evaluate the models on the WMT’14 test set (news-test 2014),3 while the concatenation of news-test-2012 and news-test-2013 is used for model selection (development set). Table 1 presents data coverage w.r.t. the vocabulary size, on the target side. Unless mentioned otherwise, all reported BLEU scores (Papineni et al., 2002) are computed with the multi-bleu.perl script4 on the cased tokenized translations. 4.1 Settings As a baseline for English—*French translation, we use the RNNsearch model proposed by (Bahdanau et al., 2015), with 30k source and target words.5 Another RNNsearch model is trained for English—*German translation with 50k source and target words. For each language pair, we train another set of RNNsearch models with much larger vocabularies of 500k source and target words, using the proposed approach. We call these models RNNsearch-LV. We vary the size of the shortlist used during training (r in Sec</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 311–318, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Peitz</author>
<author>Joern Wuebker</author>
<author>Markus Freitag</author>
<author>Hermann Ney</author>
</authors>
<title>The RWTH Aachen GermanEnglish machine translation system for WMT</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<pages>157--162</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA,</location>
<contexts>
<context citStr="Peitz et al., 2014" endWordPosition="3446" position="20307" startWordPosition="3443">rench English-German Train Test Train Test 15k 93.5 90.8 88.5 83.8 30k 96.0 94.6 91.8 87.9 50k 97.3 96.3 93.7 90.4 500k 99.5 99.3 98.4 96.1 All 100.0 99.6 100.0 97.3 Table 1: Data coverage (in %) on target-side corpora for different vocabulary sizes. ”All” refers to all the tokens in the training set. To ensure fair comparison, the English—*French corpus, which comprises approximately 12 million sentences, is identical to the one used in (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Sutskever et al., 2014). As for English—*German, the corpus was preprocessed, in a manner similar to (Peitz et al., 2014; Li et al., 2014), in order to remove many poorly translated sentences. We evaluate the models on the WMT’14 test set (news-test 2014),3 while the concatenation of news-test-2012 and news-test-2013 is used for model selection (development set). Table 1 presents data coverage w.r.t. the vocabulary size, on the target side. Unless mentioned otherwise, all reported BLEU scores (Papineni et al., 2002) are computed with the multi-bleu.perl script4 on the cased tokenized translations. 4.1 Settings As a baseline for English—*French translation, we use the RNNsearch model proposed by (Bahdanau et al.</context>
</contexts>
<marker>Peitz, Wuebker, Freitag, Ney, 2014</marker>
<rawString>Stephan Peitz, Joern Wuebker, Markus Freitag, and Hermann Ney. 2014. The RWTH Aachen GermanEnglish machine translation system for WMT 2014. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 157–162, Baltimore, Maryland, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc V Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In NIPS’2014.</booktitle>
<contexts>
<context citStr="Sutskever et al., 2014" endWordPosition="255" position="1626" startWordPosition="252">the proposed approach are empirically found to match, and in some cases outperform, the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use an ensemble of a few models with very large target vocabularies, we achieve performance comparable to the state of the art (measured by BLEU) on both the English→German and English→French translation tasks of WMT’14. 1 Introduction Neural machine translation (NMT) is a recently introduced approach to solving machine translation (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Sutskever et al., 2014). In neural machine translation, one builds a single neural network that reads a source sentence and generates its translation. The whole neural network is jointly trained to maximize the conditional probability of a correct translation given a source sentence, using the bilingual corpus. The NMT models have shown to perform as well as the most widely used conventional translation systems (Sutskever et al., 2014; Bahdanau et al., 2015). Neural machine translation has a number of advantages over the existing statistical machine translation system, specifically, the phrase-based system (Koehn et</context>
<context citStr="Sutskever et al., 2014" endWordPosition="550" position="3410" startWordPosition="547">e NMT model is often much smaller than the existing system which relies on maintaining large tables of phrase pairs. Despite these advantages and promising results, there is a major limitation in NMT compared to the existing phrase-based approach. That is, the number of target words must be limited. This is mainly because the complexity of training and using an NMT model increases as the number of target words increases. A usual practice is to construct a target vocabulary of the K most frequent words (a socalled shortlist), where K is often in the range of 30k (Bahdanau et al., 2015) to 80k (Sutskever et al., 2014). Any word not included in this vocabulary is mapped to a special token representing an unknown word [UNK]. This approach works well when there are only a few unknown words in the target sentence, but it has been observed 1 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1–10, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics that the translation performance degrades rapidly as the number of unknown words increases (Cho et al., 2014a; Bahdana</context>
<context citStr="Sutskever et al., 2014" endWordPosition="907" position="5678" startWordPosition="904">glish→French translation task. 2 Neural Machine Translation and Limited Vocabulary Problem In this section, we briefly describe an approach to neural machine translation proposed recently in (Bahdanau et al., 2015). Based on this description we explain the issue of limited vocabularies in neural machine translation. 2.1 Neural Machine Translation Neural machine translation is a recently proposed approach to machine translation, which uses a single neural network trained jointly to maximize the translation performance (Forcada and ˜Neco, 1997; Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014; Bahdanau et al., 2015). Neural machine translation is often implemented as the encoder–decoder network. The encoder reads the source sentence x = (x1, ... , xT) and encodes it into a sequence of hidden states h = (h1, ··· ,hT): ht = f (xt, ht−1) . (1) Then, the decoder, another recurrent neural network, generates a corresponding translation y = (y1, · · · , yT,) based on the encoded sequence of hidden states h: p(yt |y&lt;t, x) a exp {q (yt−1, zt, ct)} , (2) where zt = g (yt−1, zt−1, ct) , (3) ct = r (zt−1, h1, ... , hT) , (4) and y&lt;t = (y1, . . . , yt−1). The whole model is jointly trained to </context>
<context citStr="Sutskever et al., 2014" endWordPosition="1550" position="9111" startWordPosition="1547">e normalization constant (the denominator in Eq. (6)). This has to be done for, on average, 20–30 words per sentence, which easily becomes prohibitively expensive even with a moderate number of possible target words. Furthermore, the memory requirement grows linearly with respect to the number of target words. This has been a major hurdle for neural machine translation, compared to the existing non-parametric approaches such as phrase-based translation systems. Recently proposed neural machine translation models, hence, use a shortlist of 30k to 80k most frequent words (Bahdanau et al., 2015; Sutskever et al., 2014). This makes training more feasible, but comes with a number of problems. First of all, the performance of the model degrades heavily if the translation of a source sentence requires many words that are not included in the shortlist (Cho et al., 2014a). This also affects the performance evaluation of the system which is often measured by BLEU. Second, the first issue becomes more problematic with languages that have a rich set of words such as German or other highly inflected languages. There are two model-specific approaches to this issue of large target vocabulary. The first approach is to s</context>
<context citStr="Sutskever et al., 2014" endWordPosition="3430" position="20210" startWordPosition="3427">ound and downloaded from http://www-lium.univ-lemans.fr/ ˜schwenk/nnmt-shared-task/README. 5 English-French English-German Train Test Train Test 15k 93.5 90.8 88.5 83.8 30k 96.0 94.6 91.8 87.9 50k 97.3 96.3 93.7 90.4 500k 99.5 99.3 98.4 96.1 All 100.0 99.6 100.0 97.3 Table 1: Data coverage (in %) on target-side corpora for different vocabulary sizes. ”All” refers to all the tokens in the training set. To ensure fair comparison, the English—*French corpus, which comprises approximately 12 million sentences, is identical to the one used in (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Sutskever et al., 2014). As for English—*German, the corpus was preprocessed, in a manner similar to (Peitz et al., 2014; Li et al., 2014), in order to remove many poorly translated sentences. We evaluate the models on the WMT’14 test set (news-test 2014),3 while the concatenation of news-test-2012 and news-test-2013 is used for model selection (development set). Table 1 presents data coverage w.r.t. the vocabulary size, on the target side. Unless mentioned otherwise, all reported BLEU scores (Papineni et al., 2002) are computed with the multi-bleu.perl script4 on the cased tokenized translations. 4.1 Settings As a </context>
<context citStr="Sutskever et al., 2014" endWordPosition="4179" position="24835" startWordPosition="4176">37.19 (31.98) 37.50 (a) English—*French RNNsearch RNNsearch-LV Phrase-based SMT Basic NMT 16.46 (17.13) 16.95 (17.85) +Candidate List – 17.46 (18.00) +UNK Replace 18.97 (19.16) 18.89 (19.03) 20.67° +Reshuffle – 19.40 (19.37) +Ensemble – 21.59 (21.06) (b) English—*German Table 2: The translation performances in BLEU obtained by different models on (a) English—*French and (b) English—*German translation tasks. RNNsearch is the model proposed in (Bahdanau et al., 2015), RNNsearch-LV is the RNNsearch trained with the approach proposed in this paper, and Google is the LSTM-based model proposed in (Sutskever et al., 2014). Unless mentioned otherwise, we report singlemodel RNNsearch-LV scores using T = 30k (English—*French) and T = 50k (English—*German). For the experiments we have run ourselves, we show the scores on the development set as well in the brackets. (*) (Sutskever et al., 2014), (o) (Luong et al., 2015), (•) (Durrani et al., 2014), (*) Standard Moses Setting (Cho et al., 2014b), (o) (Buck et al., 2014). such ensembles may be sub-optimal. This is supported by the fact that higher cross-model BLEU scores (Freitag et al., 2014) are observed for models that were partially trained together. 4.2 Translat</context>
</contexts>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. In NIPS’2014.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>