<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant confidence="0.000000" no="0">
<title confidence="0.970631">
Learning to Translate with Multiple Objectives
</title>
<author confidence="0.966963">
Kevin Duh* Katsuhito Sudoh Xianchao Wu Hajime Tsukada Masaaki Nagata
</author>
<affiliation confidence="0.840538">
NTT Communication Science Laboratories
</affiliation>
<address confidence="0.967493">
2-4 Hikari-dai, Seika-cho, Kyoto 619-0237, JAPAN
</address>
<email confidence="0.999302">
kevinduh@is.naist.jp,lastname.firstname@lab.ntt.co.jp
</email>
<sectionHeader confidence="0.998602" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9987288125">We introduce an approach to optimize a machine translation (MT) system on multiple metrics simultaneously. Different metrics (e.g. BLEU, TER) focus on different aspects of translation quality; our multi-objective approach leverages these diverse aspects to improve overall quality. Our approach is based on the theory of Pareto Optimality. It is simple to implement on top of existing single-objective optimization methods (e.g. MERT, PRO) and outperforms ad hoc alternatives based on linear-combination of metrics. We also discuss the issue of metric tunability and show that our Pareto approach is more effective in incorporating new metrics from MT evaluation for MT optimization.</bodyText>
<sectionHeader confidence="0.99952" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.986428551020408">Weight optimization is an important step in building machine translation (MT) systems. Discriminative optimization methods such as MERT (Och, 2003), MIRA (Crammer et al., 2006), PRO (Hopkins and May, 2011), and Downhill-Simplex (Nelder and Mead, 1965) have been influential in improving MT systems in recent years. These methods are effective because they tune the system to maximize an automatic evaluation metric such as BLEU, which serve as surrogate objective for translation quality. However, we know that a single metric such as BLEU is not enough. Ideally, we want to tune towards an automatic metric that has perfect correlation with human judgments of translation quality. *Now at Nara Institute of Science &amp; Technology (NAIST) While many alternatives have been proposed, such a perfect evaluation metric remains elusive. As a result, many MT evaluation campaigns now report multiple evaluation metrics (Callison-Burch et al., 2011; Paul, 2010). Different evaluation metrics focus on different aspects of translation quality. For example, while BLEU (Papineni et al., 2002) focuses on word-based n-gram precision, METEOR (Lavie and Agarwal, 2007) allows for stem/synonym matching and incorporates recall. TER (Snover et al., 2006) allows arbitrary chunk movements, while permutation metrics like RIBES (Isozaki et al., 2010; Birch et al., 2010) measure deviation in word order. Syntax (Owczarzak et al., 2007) and semantics (Pado et al., 2009) also help. Arguably, all these metrics correspond to our intuitions on what is a good translation. The current approach of optimizing MT towards a single metric runs the risk of sacrificing other metrics. Can we really claim that a system is good if it has high BLEU, but very low METEOR? Similarly, is a high-METEOR low-BLEU system desirable? Our goal is to propose a multi-objective optimization method that avoids “overfitting to a single metric”. We want to build a MT system that does well with respect to many aspects of translation quality. In general, we cannot expect to improve multiple metrics jointly if there are some inherent tradeoffs. We therefore need to define the notion of Pareto Optimality (Pareto, 1906), which characterizes this tradeoff in a rigorous way and distinguishes the set of equally good solutions. We will describe Pareto Optimality in detail later, but roughly speaking, a hypothesis is pareto-optimal if there exist no other hypothesis better in all metrics.</bodyText>
<page confidence="0.82277">
1
</page>
<note confidence="0.9775995">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1–10,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.560700666666667">The contribution of this paper is two-fold:</bodyText>
<listItem confidence="0.9686626">• We introduce PMO (Pareto-based Multiobjective Optimization), a general approach for learning with multiple metrics. Existing singleobjective methods can be easily extended to multi-objective using PMO. • We show that PMO outperforms the alternative (single-objective optimization of linearlycombined metrics) in multi-objective space, and especially obtains stronger results for metrics that may be difficult to tune individually.</listItem>
<bodyText confidence="0.99868625">In the following, we first explain the theory of Pareto Optimality (Section 2), and then use it to build up our proposed PMO approach (Section 3). Experiments on NIST Chinese-English and PubMed English-Japanese translation using BLEU, TER, and RIBES are presented in Section 4. We conclude by discussing related work (Section 5) and opportunities/limitations (Section 6).</bodyText>
<sectionHeader confidence="0.902768" genericHeader="method">
2 Theory of Pareto Optimality
</sectionHeader>
<subsectionHeader confidence="0.929982">
2.1 Definitions and Concepts
</subsectionHeader>
<bodyText confidence="0.843566181818182">The idea of Pareto optimality comes originally from economics (Pareto, 1906), where the goal is to characterize situations when a change in allocation of goods does not make anybody worse off. Here, we will explain it in terms of MT: Let h E L be a hypothesis from an N-best list L. We have a total of K different metrics Mk(h) for evaluating the quality of h. Without loss of generality, we assume metric scores are bounded between 0 and 1, with 1 being perfect. Each hypothesis h can be mapped to a K-dimensional vector M(h) = [M1(h); M2(h); ...; MK(h)]. For example, suppose K = 2, M1(h) computes the BLEU score, and M2(h) gives the METEOR score of h. Figure 1 illustrates the set of vectors {M(h)} in a 10-best list. For two hypotheses h1, h2, we write M(h1) &gt; M(h2) if h1 is better than h2 in all metrics, and M(h1) &gt; M(h2) if h1 is better than or equal to h2 in all metrics. When M(h1) &gt; M(h2) and Mk(h1) &gt; Mk(h2) for at least one metric k, we say that h1 dominates h2 and write M(h1) &gt; M(h2).</bodyText>
<figure confidence="0.966713230769231">
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
metric1
</figure>
<figureCaption confidence="0.693931714285714">
Figure 1: Illustration of Pareto Frontier. Ten hypotheses
are plotted by their scores in two metrics. Hypotheses
indicated by a circle (o) are pareto-optimal, while those
indicated by a plus (+) are not. The line shows the convex
hull, which attains only a subset of pareto-optimal points.
The triangle (A) is a point that is weakly pareto-optimal
but not pareto-optimal.
</figureCaption>
<construct confidence="0.948865666666667">
Definition 1. Pareto Optimal: A hypothesis h* E
L is pareto-optimal iff there does not exist another
hypothesis h E L such that M(h) &gt; M(h*).
</construct>
<bodyText confidence="0.999794545454545">In Figure 1, the hypotheses indicated by circle (o) are pareto-optimal, while those with plus (+) are not. To visualize this, take for instance the paretooptimal point (0.4,0.7). There is no other point with either (metric1 &gt; 0.4 and metric2 &gt; 0.7), or (metric1 &gt; 0.4 and metric2 &gt; 0.7). On the other hand, the non-pareto point (0.6,0.4) is “dominated” by another point (0.7,0.6), because for metric1: 0.7 &gt; 0.6 and for metric2: 0.6 &gt; 0.4. There is another definition of optimality, which disregards ties and may be easier to visualize:</bodyText>
<construct confidence="0.779238">
Definition 2. Weakly Pareto Optimal: A hypothesis
h* E L is weakly pareto-optimal iff there is no other
hypothesis h E L such that M(h) &gt; M(h*).
</construct>
<bodyText confidence="0.9994963">Weakly pareto-optimal points are a superset of pareto-optimal points. A hypothesis is weakly pareto-optimal if there is no other hypothesis that improves all the metrics; a hypothesis is paretooptimal if there is no other hypothesis that improves at least one metric without detriment to other metrics. In Figure 1, point (0.1,0.8) is weakly paretooptimal but not pareto-optimal, because of the competing point (0.3,0.8). Here we focus on paretooptimality, but note our algorithms can be easily modified for weakly pareto-optimality.</bodyText>
<equation confidence="0.452676">
metric2
</equation>
<page confidence="0.953946">
2
</page>
<bodyText confidence="0.90792275">Finally, we can introduce the key concept used in our proposed PMO approach: Definition 3. Pareto Frontier: Given an N-best list L, the set of all pareto-optimal hypotheses h E L is called the Pareto Frontier. The Pareto Frontier has two desirable properties from the multi-objective optimization perspective:</bodyText>
<listItem confidence="0.982742">1. Hypotheses on the Frontier are equivalently good in the Pareto sense. 2. For each hypothesis not on the Frontier, there is always a better (pareto-optimal) hypothesis.</listItem>
<bodyText confidence="0.999696">This provides a principled approach to optimization: i.e. optimizing towards points on the Frontier and away from those that are not, and giving no preference to different pareto-optimal hypotheses.</bodyText>
<subsectionHeader confidence="0.999497">
2.2 Reduction to Linear Combination
</subsectionHeader>
<bodyText confidence="0.92742">Multi-objective problems can be formulated as:</bodyText>
<equation confidence="0.9847865">
arg max [M1(h); M2(h); ... ; Mk(h)] (1)
w
</equation>
<bodyText confidence="0.995908166666667">where h = Decode(w, f) Here, the MT system’s Decode function, parameterized by weight vector w, takes in a foreign sentence f and returns a translated hypothesis h. The argmax operates in vector space and our goal is to find w leading to hypotheses on the Pareto Frontier. In the study of Pareto Optimality, one central question is: To what extent can multi-objective problems be solved by single-objective methods? Equation 1 can be reduced to a single-objective problem by scalarizing the vector [M1(h); ... ; Mk(h)] with a linear combination:</bodyText>
<equation confidence="0.934582">
pkMk(h) (2)
</equation>
<bodyText confidence="0.989942">where h = Decode(w, f) Here, pk are positive real numbers indicating the relative importance of each metric (without loss of generality, assume Ek pk = 1). Are the solutions to Eq. 2 also solutions to Eq. 1 (i.e. pareto-optimal) and vice-versa? The theory says:</bodyText>
<construct confidence="0.983629166666667">
Theorem 1. Sufficient Condition: If w* is solution
to Eq. 2, then it is weakly pareto-optimal. Further,
if w* is unique, then it is pareto-optimal.
Theorem 2. No Necessary Condition: There may
exist solutions to Eq. 1 that cannot be achieved by
Eq. 2, irregardless of any setting of {pkI.
</construct>
<bodyText confidence="0.997520272727273">Theorem 1 is a positive result asserting that linear combination can give pareto-optimal solutions. However, Theorem 2 states the limits: in particular, Eq. 2 attains only pareto-optimal points that are on the convex hull. This is illustrated in Figure 1: imagine sweeping all values of p1 = [0, 1] and p2 = 1− p1 and recording the set of hypotheses that maximizes Ek pkMk(h). For 0.6 &lt; p1 &lt; 1 we get h = (0.9, 0.1), for p1 = 0.6 we get (0.7, 0.6), and for 0 &lt; p1 &lt; 0.6 we get (0.4, 0.8). At no setting of p1 do we attain h = (0.4, 0.7) which is also pareto-optimal but not on the convex hull.1 This may have ramifications for issues like metric tunability and local optima. To summarize, linearcombination is reasonable but has limitations. Our proposed approach will instead directly solve Eq. 1. Pareto Optimality and multi-objective optimization is a deep field with active inquiry in engineering, operations research, economics, etc. For the interested reader, we recommend the survey by Marler and Arora (2004) and books by (Sawaragi et al., 1985; Miettinen, 1998).</bodyText>
<sectionHeader confidence="0.998577" genericHeader="method">
3 Multi-objective Algorithms
</sectionHeader>
<subsectionHeader confidence="0.999496">
3.1 Computing the Pareto Frontier
</subsectionHeader>
<bodyText confidence="0.999858583333333">Our PMO approach will need to compute the Pareto Frontier for potentially large sets of points, so we first describe how this can be done efficiently. Given a set of N vectors {M(h)I from an N-best list L, our goal is extract the subset that are pareto-optimal. Here we present an algorithm based on iterative filtering, in our opinion the simplest algorithm to understand and implement. The strategy is to loop through the list L, keeping track of any dominant points. Given a dominant point, it is easy to filter out many points that are dominated by it. After successive rounds, any remaining points that are not fil-</bodyText>
<construct confidence="0.5452835">
1We note that scalarization by exponentiated-combination
Ek pkMk(h)9, for a suitable q &gt; 0, does satisfy necessary
conditions for pareto optimality. However the proper tuning of q
is not known a priori. See (Miettinen, 1998) for theorem proofs.
</construct>
<equation confidence="0.6785192">
K
E
k=1
arg max
w
</equation>
<page confidence="0.670287">
3
</page>
<figure confidence="0.823267333333333">
Algorithm 1 FindParetoFrontier
Input: {M(h)}, h ∈ L
Output: All pareto-optimal points of {M(h)}
</figure>
<listItem confidence="0.956527428571429">1: F = ∅ 2: while L is not empty do 3: h* = shift(L) 4: for each h in L do 5: if (M(h*) &gt; M(h)): remove h from L 6: else if (M(h) &gt; M(h*)): remove h from L; set h* = h 7: end for 8: Add h* to Frontier Set F 9: for each h in L do 10: if (M(h*) &gt; M(h)): remove h from L 11: end for 12: end while 13: Return F tered are necessarily pareto-optimal.</listItem>
<bodyText confidence="0.999844117647059">Algorithm 1 shows the pseudocode. In line 3, we take a point h* and check if it is dominating or dominated in the forloop (lines 4-8). At least one pareto-optimal point will be found by line 8. The second loop (lines 9-11) further filters the list for points that are dominated by h* but iterated before h* in the first for-loop. The outer while-loop stops exactly after P iterations, where P is the actual number of paretooptimal points in L. Each inner loop costs O(KN) so the total complexity is O(PKN). Since P ≤ N with the actual value depending on the probability distribution of {M(h)}, the worst-case run-time is O(KN2). For a survey of various Pareto algorithms, refer to (Godfrey et al., 2007). The algorithm we described here is borrowed from the database literature in what is known as skyline operators.2</bodyText>
<subsectionHeader confidence="0.986839">
3.2 PMO-PRO Algorithm
</subsectionHeader>
<bodyText confidence="0.999879537037037">We are now ready to present an algorithm for multiobjective optimization. As we will see, it can be seen as a generalization of the pairwise ranking optimization (PRO) of (Hopkins and May, 2011), so we call it PMO-PRO. PMO-PRO approach works by iteratively decoding-and-optimizing on the devset, sim2The inquisitive reader may wonder how is Pareto related to databases. The motivation is to incorporate preferences into relational queries(B¨orzs¨onyi et al., 2001). For K = 2 metrics, they also present an alternative faster O(N logN) algorithm by first topologically sorting along the 2 dimensions. All dominated points can be filtered by one-pass by comparing with the most-recent dominating point. ilar to many MT optimization methods. The main difference is that rather than trying to maximize a single metric, we maximize the number of pareto points, in order to expand the Pareto Frontier We will explain PMO-PRO in terms of the pseudo-code shown in Algorithm 2. For each sentence pair (f, e) in the devset, we first generate an N-best list L ≡ {h} using the current weight vector w (line 5). In line 6, we evaluate each hypothesis h with respect to the K metrics, giving a set of Kdimensional vectors {M(h)}. Lines 7-8 is the critical part: it gives a “label” to each hypothesis, based on whether it is in the Pareto Frontier. In particular, first we call FindParetoFrontier (Algorithm 1), which returns a set of pareto hypotheses; pareto-optimal hypotheses will get label 1 while non-optimal hypotheses will get label 0. This information is added to the training set T (line 8), which is then optimized by any conventional subroutine in line 10. We will follow PRO in using a pairwise classifier in line 10, which finds w* that separates hypotheses with labels 1 vs. 0. In essence, this is the trick we employ to directly optimize on the Pareto Frontier. If we had used BLEU scores rather than the {0, 1} labels in line 8, the entire PMO-PRO algorithm would revert to single-objective PRO. By definition, there is no single “best” result for multi-objective optimization, so we collect all weights and return the Pareto-optimal set. In line 13 we evaluate each weight w on K metrics across the entire corpus and call FindParetoFrontier in line 14.3 This choice highlights an interesting change of philosophy: While setting {N} in linearcombination forces the designer to make an a priori preference among metrics prior to optimization, the PMO strategy is to optimize first agnostically and a posteriori let the designer choose among a set of weights. Arguably it is easier to choose among solutions based on their evaluation scores rather than devising exact values for {N}.</bodyText>
<subsectionHeader confidence="0.937157">
3.3 Discussion
</subsectionHeader>
<bodyText confidence="0.964195">Variants: In practice we find that a slight modification of line 8 in Algorithm 2 leads to more sta-</bodyText>
<footnote confidence="0.595600666666667">
3Note this is the same FindParetoFrontier algorithm as used
in line 7. Both operate on sets of points in K-dimensional
space, induced from either weights {w} or hypotheses {h}.
</footnote>
<page confidence="0.976108">
4
</page>
<table confidence="0.427404666666667">
Algorithm 2 Proposed PMO-PRO algorithm
Input: Devset, max number of iterations I
Output: A set of (pareto-optimal) weight vectors
</table>
<listItem confidence="0.8318144">1: Initialize w. Let W = ∅. 2: for i = 1 to I do 3: Let T = ∅. 4: for each (f, e) in devset do 5: {h} =DecodeNbest(w,f) 6: {M(h)}=EvalMetricsOnSentence({h}, e) 7: {f} =FindParetoFrontier({M(h)}) 8: foreach h ∈ {h}: if h ∈ {f}, set l=1, else l=0; Add (l, h) to T 9: end for 10: w*=OptimizationSubroutine(T, w) 11: Add w* to W; Set w = w*. 12: end for 13: M(w) =EvalMetricsOnCorpus(w,devset) ∀w ∈ W 14: Return FindParetoFrontier({M(w)})</listItem>
<bodyText confidence="0.999214047619048">ble results for PMO-PRO: for non-pareto hypotheses h ∈/ {f}, we set label l = Ek Mk(h)/K instead of l= 0, so the method not only learns to discriminate pareto vs. non-pareto but also also learns to discriminate among competing non-pareto points. Also, like other MT works, in line 5 the N-best list is concatenated to N-best lists from previous iterations, so {h} is a set with i · N elements. General PMO Approach: The strategy we outlined in Section 3.2 can be easily applied to other MT optimization techniques. For example, by replacing the optimization subroutine (line 10, Algorithm 2) with a Powell search (Och, 2003), one can get PMO-MERT4. Alternatively, by using the largemargin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 4-9), one can get an online algorithm such PMO-MIRA. Virtually all MT optimization algorithms have a place where metric scores feedback into the optimization procedure; the idea of PMO is to replace these raw scores with labels derived from Pareto optimality.</bodyText>
<sectionHeader confidence="0.999969" genericHeader="evaluation and result">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.996299">
4.1 Evaluation Methodology
</subsectionHeader>
<bodyText confidence="0.997442">We experiment with two datasets: (1) The PubMed task is English-to-Japanese translation of scientific abstracts.</bodyText>
<footnote confidence="0.514278666666667">
4A difference with traditional MERT is the necessity of
sentence-BLEU (Liang et al., 2006) in line 6. We use sentence-
BLEU for optimization but corpus-BLEU for evaluation here.
</footnote>
<bodyText confidence="0.997018333333333">As metrics we use BLEU and RIBES (which demonstrated good human correlation in this language pair (Goto et al., 2011)).(2) The NIST task is Chinese-to-English translation with OpenMT08 training data and MT06 as devset. As metrics we use BLEU and NTER.</bodyText>
<listItem confidence="0.879075">• BLEU = BP × (IIprec)1/4. BP is brevity penality. prec,,, is precision of n-gram matches. • RIBES = (z + 1)/2 × prec1/4 1 , with Kendall’s z computed by measuring permutation between matching words in reference and hypothesis5.</listItem>
<listItem confidence="0.957439">• NTER=max(1−TER, 0), which normalizes Translation Edit Rate6 so that NTER=1 is best.</listItem>
<bodyText confidence="0.760038">We compare two multi-objective approaches:</bodyText>
<listItem confidence="0.999429">1. Linear-Combination of metrics (Eq. 2), optimized with PRO. We search a range of combination settings: (p1, p2) = {(0, 1), (0.3, 0.7), (0.5, 0.5), (0.7, 0.3), (1, 0)}.</listItem>
<bodyText confidence="0.6342485">Note (1, 0) reduces to standard single-metric optimization of e.g.BLEU.</bodyText>
<listItem confidence="0.713876">2. Proposed Pareto approach (PMO-PRO).</listItem>
<bodyText confidence="0.999766583333333">Evaluation of multi-objective problems can be tricky because there is no single figure-of-merit. We thus adopted the following methodology: We run both methods 5 times (i.e. using the 5 different (p1, p2) setting each time) and I = 20 iterations each. For each method, this generates 5x20=100 results, and we plot the Pareto Frontier of these points in a 2-dimensional metric space (e.g. see Figure 2). A method is deemed better if its final Pareto Frontier curve is strictly dominating the other. We report devset results here; testset trends are similar but not included due to space constraints.7</bodyText>
<footnote confidence="0.768944384615385">
5from www.kecl.ntt.co.jp/icl/lirg/ribes
6from www.umd.edu/˜snover/tercom
7An aside: For comparing optimization methods, we believe
devset comparison is preferable to testset since data mismatch
may confound results. If one worries about generalization, we
advocate to re-decode the devset with final weights and evaluate
its 1-best output (which is done here). This is preferable to sim-
ply reporting the achieved scores on devset N-best (as done in
some open-source scripts) since the learned weight may pick
out good hypotheses in the N-best but perform poorly when
re-decoding the same devset. The re-decode devset approach
avoids being overly optimistic while accurately measuring op-
timization performance.
</footnote>
<page confidence="0.980956">
5
</page>
<table confidence="0.997168333333333">
Train Devset #Feat Metrics
PubMed 0.2M 2k 14 BLEU, RIBES
NIST 7M 1.6k 8 BLEU, NTER
</table>
<tableCaption confidence="0.489993222222222">
Table 1: Task characteristics: #sentences in Train/Dev, #
of features, and metrics used. Our MT models are trained
with standard phrase-based Moses software (Koehn and
others, 2007), with IBM M4 alignments, 4gram SRILM,
lexical ordering for PubMed and distance ordering for the
NIST system. The decoder generates 50-best lists each
iteration. We use SVMRank (Joachims, 2006) as opti-
mization subroutine for PRO, which efficiently handle all
pairwise samples without the need for sampling.
</tableCaption>
<figure confidence="0.995163">
ribes
0.695
Linear Combination
Pareto (PMO−PRO)
0.69
0.685
0.68
0.675
0.67
0.665
0.2 0.21 0.22 0.23 0.24 0.25 0.26 0.27
bleu
</figure>
<subsectionHeader confidence="0.665501">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.88195075">Figures 2 and 3 show the results for PubMed and NIST, respectively. A method is better if its Pareto Frontier lies more towards the upper-right hand corner of the graph. Our observations are:</bodyText>
<listItem confidence="0.993604523809524">1. PMO-PRO generally outperforms LinearCombination with any setting of (p1,p2). The Pareto Frontier of PMO-PRO dominates that of Linear-Combination. This implies PMO is effective in optimizing towards Pareto hypotheses. 2. For both methods, trading-off between metrics is necessary. For example in PubMed, the designer would need to make a choice between picking the best weight according to BLEU (BLEU=.265,RIBES=.665) vs. another weight with higher RIBES but poorer BLEU, e.g. (.255,.675). Nevertheless, both the PMO and Linear-Combination with various (p1,p2) samples this joint-objective space broadly. 3. Interestingly, a multi-objective approach can sometimes outperform a single-objective optimizer in its own metric. In Figure 2, singleobjective PRO focusing on optimizing RIBES only achieves 0.68, but PMO-PRO using both BLEU and RIBES outperforms with 0.685.</listItem>
<bodyText confidence="0.999028">The third observation relates to the issue of metric tunability (Liu et al., 2011). We found that RIBES can be difficult to tune directly. It is an extremely non-smooth objective with many local optima–slight changes in word ordering causes large changes in RIBES. So the best way to improve RIBES is to iteration 2, 15) leading to a stronger result ultimately.</bodyText>
<figureCaption confidence="0.9953955">
Figure 2: PubMed Results. The curve represents the
Pareto Frontier of all results collected after multiple runs.
</figureCaption>
<figure confidence="0.648633">
bleu
</figure>
<figureCaption confidence="0.999749">
Figure 3: NIST Results
not to optimize it directly, but jointly with a more tunable metric BLEU. The learning curve in Figure 4 show that single-objective optimization of RIBES quickly falls into local optimum (at iteration 3) whereas PMO can zigzag and sacrifice RIBES in intermediate iterations (e.g.</figureCaption>
<bodyText confidence="0.999917636363636">The reason is the diversity of solutions provided by the Pareto Frontier. This finding suggests that multi-objective approaches may be preferred, especially when dealing with new metrics that may be difficult to tune.</bodyText>
<subsectionHeader confidence="0.999931">
4.3 Additional Analysis and Discussions
</subsectionHeader>
<bodyText confidence="0.999912">What is the training time? The Pareto approach does not add much overhead to PMO-PRO. While FindParetoFrontier scales quadratically by size of N-best list, Figure 5 shows that the runtime is trivial (0.3 seconds for 1000-best).</bodyText>
<figure confidence="0.977755944444445">
0.146 0.148 0.15 0.152 0.154 0.156 0.158 0.16 0.162 0
.164
nter
0.704
0.703
0.702
0.701
0.699
0.698
0.697
0.696
0.695
0.694
0.7
Linear Combination
Pareto (PMO−PRO)
6
iteration
</figure>
<figureCaption confidence="0.9996115">
Figure 4: Learning Curve on RIBES: comparing single-
objective optimization and PMO.
</figureCaption>
<bodyText confidence="0.999115">Table 2 shows the time usage breakdown in different iterations for PubMed. We see it is mostly dominated by decoding time (constant per iteration at 40 minutes on single 3.33GHz processor). At later iterations, Opt takes more time due to larger file I/O in SVMRank. Note Decode and Pareto can be “embarrasingly parallelized.”</bodyText>
<table confidence="0.9988966">
Iter Time Decode Pareto Opt Misc.
(line 5) (line 7) (line 10) (line 6,8)
1 47m 85% 1% 1% 13%
10 62m 67% 6% 8% 19%
20 91m 47% 15% 22% 16%
</table>
<tableCaption confidence="0.6605515">
Table 2: Training time usage in PMO-PRO (Algo 2).
How many Pareto points? The number of pareto
</tableCaption>
<figure confidence="0.546734">
Iterations
</figure>
<figureCaption confidence="0.999177">
Figure 6: Average number of Pareto points
hypotheses gives a rough indication of the diversity of hypotheses that can be exploited by PMO.</figureCaption>
<bodyText confidence="0.999603928571429">Figure 6 shows that this number increases gradually per iteration. This perhaps gives PMO-PRO more directions for optimizing around potential local optimal. Nevertheless, we note that tens of Pareto points is far few compared to the large size of N-best lists used at later iterations of PMO-PRO. This may explain why the differences between methods in Figure 3 are not more substantial. Theoretically, the number will eventually level off as it gets increasingly harder to generate new Pareto points in a crowded space (Bentley et al., 1978). Practical recommendation: We present the Pareto approach as a way to agnostically optimize multiple metrics jointly. However, in practice, one may have intuitions about metric tradeoffs even if one cannot specify {Pk}. For example, we might believe that approximately 1-point BLEU degradation is acceptable only if RIBES improves by at least 3-points. In this case, we recommend the following trick: Set up a multi-objective problem where one metric is BLEU and the other is 3/4BLEU+1/4RIBES. This encourages PMO to explore the joint metric space but avoid solutions that sacrifice too much BLEU, and should also outperform Linear Combination that searches only on the (3/4,1/4) direction.</bodyText>
<sectionHeader confidence="0.999935" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.9626375">Multi-objective optimization for MT is a relatively new area. Linear-combination of BLEU/TER is the most common technique (Zaidan, 2009), sometimes achieving good results in evaluation campaigns (Dyer et al., 2009).</bodyText>
<figure confidence="0.997997782608696">
0.69
0.68
0.67
0.66
0.65
0.64
0.63
0 2 4 6 8 10 12 14 16 18
Single−Objective RIBES
Pareto (PMO−PRO)
20
35
NIST
PubMed
30
25
20
15
10
5
0 2 4 6 8 10 12 14 16 18
ribes
Set size |L|
</figure>
<figureCaption confidence="0.975996">
Figure 5: Avg. runtime per sentence of FindPareto
</figureCaption>
<figure confidence="0.998559307692308">
Runtime (seconds)
0.35
0.25
0.15
0.05
0.3
0.2
0.1
0
0 100 200 300 400 500 600 700 800 900 1000
Algorithm 1
TopologicalSort (footnote 2)
Number of Pareto Points
</figure>
<page confidence="0.998517">
7
</page>
<bodyText confidence="0.999956682926829">As far as we known, the only work that directly proposes a multi-objective technique is (He and Way, 2009), which modifies MERT to optimize a single metric subject to the constraint that it does not degrade others. These approaches all require some setting of constraint strength or combination weights {pk}. Recent work in MT evaluation has examined combining metrics using machine learning for better correlation with human judgments (Liu and Gildea, 2007; Albrecht and Hwa, 2007; Gimnez and M`arquez, 2008) and may give insights for setting {pk}. We view our Pareto-based approach as orthogonal to these efforts. The tunability of metrics is a problem that is gaining recognition (Liu et al., 2011). If a good evaluation metric could not be used for tuning, it would be a pity. The Tunable Metrics task at WMT2011 concluded that BLEU is still the easiest to tune (Callison-Burch et al., 2011).(Mauser et al., 2008; Cer et al., 2010) report similar observations, in addition citing WER being difficult and BLEU-TER being amenable. One unsolved question is whether metric tunability is a problem inherent to the metric only, or depends also on the underlying optimization algorithm. Our positive results with PMO suggest that the choice of optimization algorithm can help. Multi-objective ideas are being explored in other NLP areas.(Spitkovsky et al., 2011) describe a technique that alternates between hard and soft EM objectives in order to achieve better local optimum in grammar induction.(Hall et al., 2011) investigates joint optimization of a supervised parsing objective and some extrinsic objectives based on downstream applications.(Agarwal et al., 2011) considers using multiple signals (of varying quality) from online users to train recommendation models.(Eisner and Daum´e III, 2011) trades off speed and accuracy of a parser with reinforcement learning. None of the techniques in NLP use Pareto concepts, however.</bodyText>
<sectionHeader confidence="0.997808" genericHeader="conclusion">
6 Opportunities and Limitations
</sectionHeader>
<bodyText confidence="0.999695354166667">We introduce a new approach (PMO) for training MT systems on multiple metrics. Leveraging the diverse perspectives of different evaluation metrics has the potential to improve overall quality. Based on Pareto Optimality, PMO is easy to implement and achieves better solutions compared to linearcombination baselines, for any setting of combination weights. Further we observe that multiobjective approaches can be helpful for optimizing difficult-to-tune metrics; this is beneficial for quickly introducing new metrics developed in MT evaluation into MT optimization, especially when good {pk} are not yet known. We conclude by drawing attention to some limitations and opportunities raised by this work: Limitations: (1) The performance of PMO is limited by the size of the Pareto set. Small N-best lists lead to sparsely-sampled Pareto Frontiers, and a much better approach would be to enlarge the hypothesis space using lattices (Macherey et al., 2008). How to compute Pareto points directly from lattices is an interesting open research question. (2) The binary distinction between pareto vs. non-pareto points ignores the fact that 2nd-place non-pareto points may also lead to good practical solutions. A better approach may be to adopt a graded definition of Pareto optimality as done in some multi-objective works (Deb et al., 2002). (3) A robust evaluation methodology that enables significance testing for multi-objective problems is sorely needed. This will make it possible to compare multi-objective methods on more than 2 metrics. We also need to follow up with human evaluation. Opportunities: (1) There is still much we do not understand about metric tunability; we can learn much by looking at joint metric-spaces and examining how new metrics correlate with established ones. (2) Pareto is just one approach among many in multi-objective optimization. A wealth of methods are available (Marler and Arora, 2004) and more experimentation in this space will definitely lead to new insights. (3) Finally, it would be interesting to explore other creative uses of multiple-objectives in MT beyond multiple metrics. For example: Can we learn to translate faster while sacrificing little on accuracy? Can we learn to jointly optimize cascaded systems, such as as speech translation or pivot translation? Life is full of multiple competing objectives.</bodyText>
<sectionHeader confidence="0.998813" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997241">We thank the reviewers for insightful feedback.</bodyText>
<page confidence="0.997366">
8
</page>
<sectionHeader confidence="0.990917" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998503990291263">
Deepak Agarwal, Bee-Chung Chen, Pradheep Elango,
and Xuanhui Wang. 2011. Click shaping to optimize
multiple objectives. In Proceedings of the 17th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, KDD ’11, pages 132–140,
New York, NY, USA. ACM.
J. Albrecht and R. Hwa. 2007. A re-examination of ma-
chine learning approaches for sentence-level mt evalu-
ation. In ACL.
J. L. Bentley, H. T. Kung, M. Schkolnick, and C. D.
Thompson. 1978. On the average number of max-
ima in a set of vectors and applications. Journal of the
Association for Computing Machinery (JACM), 25(4).
Alexandra Birch, Phil Blunsom, and Miles Osborne.
2010. Metrics for MT evaluation: Evaluating reorder-
ing. Machine Translation, 24(1).
S. B¨orzs¨onyi, D. Kossmann, and K. Stocker. 2001. The
skyline operator. In Proceedings of the 17th Interna-
tional Conference on Data Engineering (ICDE).
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 22–64, Edinburgh, Scotland, July. Associ-
ation for Computational Linguistics.
Daniel Cer, Christopher Manning, and Daniel Jurafsky.
2010. The best lexical metric for phrase-based statis-
tical MT system optimization. In NAACL HLT.
David Chiang, Wei Wang, and Kevin Knight. 2009.
11,001 new features for statistical machine translation.
In NAACL.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passiveag-
gressive algorithms. Journal of Machine Learning Re-
search, 7.
Kalyanmoy Deb, Amrit Pratap, Sammer Agarwal, and
T. Meyarivan. 2002. A fast and elitist multiobjective
genetic algorithm: NSGA-II. IEEE Transactions on
Evolutionary Computation, 6(2).
Chris Dyer, Hendra Setiawan, Yuval Marton, and Philip
Resnik. 2009. The university of maryland statistical
machine translation system for the fourth workshop on
machine translation. In Proc. of the Fourth Workshop
on Machine Translation.
Jason Eisner and Hal Daum´e III. 2011. Learning speed-
accuracy tradeoffs in nondeterministic inference algo-
rithms. In COST: NIPS 2011 Workshop on Computa-
tional Trade-offs in Statistical Learning.
Jes´us Gimnez and Llu´ıs M`arquez. 2008. Heterogeneous
automatic mt evaluation through non-parametric met-
ric combinations. In ICJNLP.
Parke Godfrey, Ryan Shipley, and Jarek Gyrz. 2007. Al-
gorithms and analyses for maximal vector computa-
tion. VLDB Journal, 16.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent ma-
chine translation task at the ntcir-9 workshop. In Pro-
ceedings of the NTCIR-9 Workshop Meeting.
Keith Hall, Ryan McDonald, Jason Katz-Brown, and
Michael Ringgaard. 2011. Training dependency
parsers by jointly optimizing multiple objectives.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1489–1499, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Yifan He and Andy Way. 2009. Improving the objec-
tive function in minimum error rate training. In MT
Summit.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of the 2011 Conference on Empir-
ical Methods in Natural Language Processing, pages
1352–1362, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
H. Isozaki, T. Hirao, K. Duh, K. Sudoh, and H. Tsukada.
2010. Automatic evaluation of translation quality for
distant language pairs. In EMNLP.
T. Joachims. 2006. Training linear SVMs in linear time.
In KDD.
P. Koehn et al. 2007. Moses: open source toolkit for
statistical machine translation. In ACL.
A. Lavie and A. Agarwal. 2007. METEOR: An auto-
matic metric for mt evaluation with high levels of cor-
relation with human judgments. In Workshop on Sta-
tistical Machine Translation.
P. Liang, A. Bouchard-Cote, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In ACL.
Ding Liu and Daniel Gildea. 2007. Source-language fea-
tures and maximum correlation training for machine
translation evaluation. In NAACL.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2011.
Better evaluation metrics lead to better machine trans-
lation. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation. In
EMNLP.
R. T. Marler and J. S. Arora. 2004. Survey of
multi-objective optimization methods for engineering.
Structural and Multidisciplinary Optimization, 26.
Arne Mauser, Saˇsa Hasan, and Hermann Ney. 2008.
Automatic evaluation measures for statistical machine
</reference>
<page confidence="0.944652">
9
</page>
<reference confidence="0.99971525">
translation system optimization. In International Con-
ference on Language Resources and Evaluation, Mar-
rakech, Morocco, May.
Kaisa Miettinen. 1998. Nonlinear Multiobjective Opti-
mization. Springer.
J.A. Nelder and R. Mead. 1965. The downhill simplex
method. Computer Journal, 7(308).
Franz Och. 2003. Minimum error rate training in statis-
tical machine translation. In ACL.
Karolina Owczarzak, Josef van Genabith, and Andy Way.
2007. Labelled dependencies in machine translation
evaluation. In Proceedings of the Second Workshop
on Statistical Machine Translation.
Sebastian Pado, Daniel Cer, Michel Galley, Dan Jurafsky,
and Christopher D. Manning. 2009. Measuring ma-
chine translation quality as semantic equivalence: A
metric based on entailment features. Machine Trans-
lation, 23(2-3).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In ACL.
Vilfredo Pareto. 1906. Manuale di Economica Politica,
(Translated into English by A.S. Schwier as Manual of
Political Economy, 1971). Societa Editrice Libraria,
Milan.
Michael Paul. 2010. Overview of the iwslt 2010 evalua-
tion campaign. In IWSLT.
Yoshikazu Sawaragi, Hirotaka Nakayama, and Tetsuzo
Tanino, editors. 1985. Theory of Multiobjective Opti-
mization. Academic Press.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Juraf-
sky. 2011. Lateen em: Unsupervised training with
multiple objectives, applied to dependency grammar
induction. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 1269–1280, Edinburgh, Scotland, UK., July. As-
sociation for Computational Linguistics.
Omar Zaidan. 2009. Z-MERT: A fully configurable open
source tool for minimum error rate training of machine
translation systems. In The Prague Bulletin of Mathe-
matical Linguistics.
</reference>
<page confidence="0.997773">
10
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant confidence="0.770707" no="0">
<title confidence="0.99996">Learning to Translate with Multiple Objectives</title>
<author confidence="0.994737">Katsuhito Sudoh Xianchao Wu Hajime Tsukada Masaaki</author>
<affiliation confidence="0.998515">NTT Communication Science</affiliation>
<address confidence="0.959688">2-4 Hikari-dai, Seika-cho, Kyoto 619-0237,</address>
<abstract confidence="0.988294411764706">We introduce an approach to optimize a machine translation (MT) system on multiple metrics (e.g. BLEU, TER) focus on different aspects of translation quality; our multi-objective approach leverages these diverse aspects to improve overall quality. Our approach is based on the theory of Pareto Optimality. It is simple to implement on top of existing single-objective optimization methods (e.g. MERT, PRO) and outperforms ad hoc alternatives based on linear-combination of metrics. We also discuss the issue of metric tunability and show that our Pareto approach is more effective in incorporating new metrics from MT evaluation for MT optimization.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Deepak Agarwal</author>
<author>Bee-Chung Chen</author>
<author>Pradheep Elango</author>
<author>Xuanhui Wang</author>
</authors>
<title>Click shaping to optimize multiple objectives.</title>
<date>2011</date>
<booktitle>In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’11,</booktitle>
<pages>132--140</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context citStr="Agarwal et al., 2011" endWordPosition="4539" position="27193" startWordPosition="4536">estion is whether metric tunability is a problem inherent to the metric only, or depends also on the underlying optimization algorithm. Our positive results with PMO suggest that the choice of optimization algorithm can help. Multi-objective ideas are being explored in other NLP areas. (Spitkovsky et al., 2011) describe a technique that alternates between hard and soft EM objectives in order to achieve better local optimum in grammar induction. (Hall et al., 2011) investigates joint optimization of a supervised parsing objective and some extrinsic objectives based on downstream applications. (Agarwal et al., 2011) considers using multiple signals (of varying quality) from online users to train recommendation models. (Eisner and Daum´e III, 2011) trades off speed and accuracy of a parser with reinforcement learning. None of the techniques in NLP use Pareto concepts, however. 6 Opportunities and Limitations We introduce a new approach (PMO) for training MT systems on multiple metrics. Leveraging the diverse perspectives of different evaluation metrics has the potential to improve overall quality. Based on Pareto Optimality, PMO is easy to implement and achieves better solutions compared to linearcombinat</context>
</contexts>
<marker>Agarwal, Chen, Elango, Wang, 2011</marker>
<rawString>Deepak Agarwal, Bee-Chung Chen, Pradheep Elango, and Xuanhui Wang. 2011. Click shaping to optimize multiple objectives. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’11, pages 132–140, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Albrecht</author>
<author>R Hwa</author>
</authors>
<title>A re-examination of machine learning approaches for sentence-level mt evaluation.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context citStr="Albrecht and Hwa, 2007" endWordPosition="4347" position="26004" startWordPosition="4344">r of Pareto Points 7 the most common technique (Zaidan, 2009), sometimes achieving good results in evaluation campaigns (Dyer et al., 2009). As far as we known, the only work that directly proposes a multi-objective technique is (He and Way, 2009), which modifies MERT to optimize a single metric subject to the constraint that it does not degrade others. These approaches all require some setting of constraint strength or combination weights {pk}. Recent work in MT evaluation has examined combining metrics using machine learning for better correlation with human judgments (Liu and Gildea, 2007; Albrecht and Hwa, 2007; Gimnez and M`arquez, 2008) and may give insights for setting {pk}. We view our Pareto-based approach as orthogonal to these efforts. The tunability of metrics is a problem that is gaining recognition (Liu et al., 2011). If a good evaluation metric could not be used for tuning, it would be a pity. The Tunable Metrics task at WMT2011 concluded that BLEU is still the easiest to tune (Callison-Burch et al., 2011). (Mauser et al., 2008; Cer et al., 2010) report similar observations, in addition citing WER being difficult and BLEU-TER being amenable. One unsolved question is whether metric tunabil</context>
</contexts>
<marker>Albrecht, Hwa, 2007</marker>
<rawString>J. Albrecht and R. Hwa. 2007. A re-examination of machine learning approaches for sentence-level mt evaluation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Bentley</author>
<author>H T Kung</author>
<author>M Schkolnick</author>
<author>C D Thompson</author>
</authors>
<title>On the average number of maxima in a set of vectors and applications.</title>
<date>1978</date>
<journal>Journal of the Association for Computing Machinery (JACM),</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context citStr="Bentley et al., 1978" endWordPosition="4043" position="24209" startWordPosition="4040"> rough indication of the diversity of hypotheses that can be exploited by PMO. Figure 6 shows that this number increases gradually per iteration. This perhaps gives PMO-PRO more directions for optimizing around potential local optimal. Nevertheless, we note that tens of Pareto points is far few compared to the large size of N-best lists used at later iterations of PMO-PRO. This may explain why the differences between methods in Figure 3 are not more substantial. Theoretically, the number will eventually level off as it gets increasingly harder to generate new Pareto points in a crowded space (Bentley et al., 1978). Practical recommendation: We present the Pareto approach as a way to agnostically optimize multiple metrics jointly. However, in practice, one may have intuitions about metric tradeoffs even if one cannot specify {Pk}. For example, we might believe that approximately 1-point BLEU degradation is acceptable only if RIBES improves by at least 3-points. In this case, we recommend the following trick: Set up a multi-objective problem where one metric is BLEU and the other is 3/4BLEU+1/4RIBES. This encourages PMO to explore the joint metric space but avoid solutions that sacrifice too much BLEU, a</context>
</contexts>
<marker>Bentley, Kung, Schkolnick, Thompson, 1978</marker>
<rawString>J. L. Bentley, H. T. Kung, M. Schkolnick, and C. D. Thompson. 1978. On the average number of maxima in a set of vectors and applications. Journal of the Association for Computing Machinery (JACM), 25(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Phil Blunsom</author>
<author>Miles Osborne</author>
</authors>
<title>Metrics for MT evaluation: Evaluating reordering.</title>
<date>2010</date>
<journal>Machine Translation,</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context citStr="Birch et al., 2010" endWordPosition="341" position="2319" startWordPosition="338">T) While many alternatives have been proposed, such a perfect evaluation metric remains elusive. As a result, many MT evaluation campaigns now report multiple evaluation metrics (Callison-Burch et al., 2011; Paul, 2010). Different evaluation metrics focus on different aspects of translation quality. For example, while BLEU (Papineni et al., 2002) focuses on word-based n-gram precision, METEOR (Lavie and Agarwal, 2007) allows for stem/synonym matching and incorporates recall. TER (Snover et al., 2006) allows arbitrary chunk movements, while permutation metrics like RIBES (Isozaki et al., 2010; Birch et al., 2010) measure deviation in word order. Syntax (Owczarzak et al., 2007) and semantics (Pado et al., 2009) also help. Arguably, all these metrics correspond to our intuitions on what is a good translation. The current approach of optimizing MT towards a single metric runs the risk of sacrificing other metrics. Can we really claim that a system is good if it has high BLEU, but very low METEOR? Similarly, is a high-METEOR low-BLEU system desirable? Our goal is to propose a multi-objective optimization method that avoids “overfitting to a single metric”. We want to build a MT system that does well with </context>
</contexts>
<marker>Birch, Blunsom, Osborne, 2010</marker>
<rawString>Alexandra Birch, Phil Blunsom, and Miles Osborne. 2010. Metrics for MT evaluation: Evaluating reordering. Machine Translation, 24(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B¨orzs¨onyi</author>
<author>D Kossmann</author>
<author>K Stocker</author>
</authors>
<title>The skyline operator.</title>
<date>2001</date>
<booktitle>In Proceedings of the 17th International Conference on Data Engineering (ICDE).</booktitle>
<marker>B¨orzs¨onyi, Kossmann, Stocker, 2001</marker>
<rawString>S. B¨orzs¨onyi, D. Kossmann, and K. Stocker. 2001. The skyline operator. In Proceedings of the 17th International Conference on Data Engineering (ICDE).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Omar Zaidan</author>
</authors>
<title>Findings of the 2011 workshop on statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>22--64</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<contexts>
<context citStr="Callison-Burch et al., 2011" endWordPosition="281" position="1906" startWordPosition="278">. These methods are effective because they tune the system to maximize an automatic evaluation metric such as BLEU, which serve as surrogate objective for translation quality. However, we know that a single metric such as BLEU is not enough. Ideally, we want to tune towards an automatic metric that has perfect correlation with human judgments of translation quality. *Now at Nara Institute of Science &amp; Technology (NAIST) While many alternatives have been proposed, such a perfect evaluation metric remains elusive. As a result, many MT evaluation campaigns now report multiple evaluation metrics (Callison-Burch et al., 2011; Paul, 2010). Different evaluation metrics focus on different aspects of translation quality. For example, while BLEU (Papineni et al., 2002) focuses on word-based n-gram precision, METEOR (Lavie and Agarwal, 2007) allows for stem/synonym matching and incorporates recall. TER (Snover et al., 2006) allows arbitrary chunk movements, while permutation metrics like RIBES (Isozaki et al., 2010; Birch et al., 2010) measure deviation in word order. Syntax (Owczarzak et al., 2007) and semantics (Pado et al., 2009) also help. Arguably, all these metrics correspond to our intuitions on what is a good t</context>
<context citStr="Callison-Burch et al., 2011" endWordPosition="4420" position="26418" startWordPosition="4417">int strength or combination weights {pk}. Recent work in MT evaluation has examined combining metrics using machine learning for better correlation with human judgments (Liu and Gildea, 2007; Albrecht and Hwa, 2007; Gimnez and M`arquez, 2008) and may give insights for setting {pk}. We view our Pareto-based approach as orthogonal to these efforts. The tunability of metrics is a problem that is gaining recognition (Liu et al., 2011). If a good evaluation metric could not be used for tuning, it would be a pity. The Tunable Metrics task at WMT2011 concluded that BLEU is still the easiest to tune (Callison-Burch et al., 2011). (Mauser et al., 2008; Cer et al., 2010) report similar observations, in addition citing WER being difficult and BLEU-TER being amenable. One unsolved question is whether metric tunability is a problem inherent to the metric only, or depends also on the underlying optimization algorithm. Our positive results with PMO suggest that the choice of optimization algorithm can help. Multi-objective ideas are being explored in other NLP areas. (Spitkovsky et al., 2011) describe a technique that alternates between hard and soft EM objectives in order to achieve better local optimum in grammar inductio</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Zaidan, 2011</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar Zaidan. 2011. Findings of the 2011 workshop on statistical machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 22–64, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Cer</author>
<author>Christopher Manning</author>
<author>Daniel Jurafsky</author>
</authors>
<title>The best lexical metric for phrase-based statistical MT system optimization.</title>
<date>2010</date>
<booktitle>In NAACL HLT.</booktitle>
<contexts>
<context citStr="Cer et al., 2010" endWordPosition="4428" position="26459" startWordPosition="4425">k in MT evaluation has examined combining metrics using machine learning for better correlation with human judgments (Liu and Gildea, 2007; Albrecht and Hwa, 2007; Gimnez and M`arquez, 2008) and may give insights for setting {pk}. We view our Pareto-based approach as orthogonal to these efforts. The tunability of metrics is a problem that is gaining recognition (Liu et al., 2011). If a good evaluation metric could not be used for tuning, it would be a pity. The Tunable Metrics task at WMT2011 concluded that BLEU is still the easiest to tune (Callison-Burch et al., 2011). (Mauser et al., 2008; Cer et al., 2010) report similar observations, in addition citing WER being difficult and BLEU-TER being amenable. One unsolved question is whether metric tunability is a problem inherent to the metric only, or depends also on the underlying optimization algorithm. Our positive results with PMO suggest that the choice of optimization algorithm can help. Multi-objective ideas are being explored in other NLP areas. (Spitkovsky et al., 2011) describe a technique that alternates between hard and soft EM objectives in order to achieve better local optimum in grammar induction. (Hall et al., 2011) investigates joint</context>
</contexts>
<marker>Cer, Manning, Jurafsky, 2010</marker>
<rawString>Daniel Cer, Christopher Manning, and Daniel Jurafsky. 2010. The best lexical metric for phrase-based statistical MT system optimization. In NAACL HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Wei Wang</author>
<author>Kevin Knight</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context citStr="Chiang et al., 2009" endWordPosition="2861" position="16822" startWordPosition="2858">0, so the method not only learns to discriminate pareto vs. non-pareto but also also learns to discriminate among competing non-pareto points. Also, like other MT works, in line 5 the N-best list is concatenated to N-best lists from previous iterations, so {h} is a set with i · N elements. General PMO Approach: The strategy we outlined in Section 3.2 can be easily applied to other MT optimization techniques. For example, by replacing the optimization subroutine (line 10, Algorithm 2) with a Powell search (Och, 2003), one can get PMO-MERT4. Alternatively, by using the largemargin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 4-9), one can get an online algorithm such PMO-MIRA. Virtually all MT optimization algorithms have a place where metric scores feedback into the optimization procedure; the idea of PMO is to replace these raw scores with labels derived from Pareto optimality. 4 Experiments 4.1 Evaluation Methodology We experiment with two datasets: (1) The PubMed task is English-to-Japanese translation of scientific 4A difference with traditional MERT is the necessity of sentence-BLEU (Liang et al., 2006) in line 6. We use sentenceBLEU for optimization but corpus-BL</context>
</contexts>
<marker>Chiang, Wang, Knight, 2009</marker>
<rawString>David Chiang, Wei Wang, and Kevin Knight. 2009. 11,001 new features for statistical machine translation. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai ShalevShwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passiveaggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>7</volume>
<contexts>
<context citStr="Crammer et al., 2006" endWordPosition="160" position="1142" startWordPosition="157">to improve overall quality. Our approach is based on the theory of Pareto Optimality. It is simple to implement on top of existing single-objective optimization methods (e.g. MERT, PRO) and outperforms ad hoc alternatives based on linear-combination of metrics. We also discuss the issue of metric tunability and show that our Pareto approach is more effective in incorporating new metrics from MT evaluation for MT optimization. 1 Introduction Weight optimization is an important step in building machine translation (MT) systems. Discriminative optimization methods such as MERT (Och, 2003), MIRA (Crammer et al., 2006), PRO (Hopkins and May, 2011), and Downhill-Simplex (Nelder and Mead, 1965) have been influential in improving MT systems in recent years. These methods are effective because they tune the system to maximize an automatic evaluation metric such as BLEU, which serve as surrogate objective for translation quality. However, we know that a single metric such as BLEU is not enough. Ideally, we want to tune towards an automatic metric that has perfect correlation with human judgments of translation quality. *Now at Nara Institute of Science &amp; Technology (NAIST) While many alternatives have been propo</context>
</contexts>
<marker>Crammer, Dekel, Keshet, ShalevShwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai ShalevShwartz, and Yoram Singer. 2006. Online passiveaggressive algorithms. Journal of Machine Learning Research, 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kalyanmoy Deb</author>
<author>Amrit Pratap</author>
<author>Sammer Agarwal</author>
<author>T Meyarivan</author>
</authors>
<title>A fast and elitist multiobjective genetic algorithm: NSGA-II.</title>
<date>2002</date>
<journal>IEEE Transactions on Evolutionary Computation,</journal>
<volume>6</volume>
<issue>2</issue>
<contexts>
<context citStr="Deb et al., 2002" endWordPosition="4792" position="28830" startWordPosition="4789">The performance of PMO is limited by the size of the Pareto set. Small N-best lists lead to sparsely-sampled Pareto Frontiers, and a much better approach would be to enlarge the hypothesis space using lattices (Macherey et al., 2008). How to compute Pareto points directly from lattices is an interesting open research question. (2) The binary distinction between pareto vs. non-pareto points ignores the fact that 2nd-place non-pareto points may also lead to good practical solutions. A better approach may be to adopt a graded definition of Pareto optimality as done in some multi-objective works (Deb et al., 2002). (3) A robust evaluation methodology that enables significance testing for multi-objective problems is sorely needed. This will make it possible to compare multi-objective methods on more than 2 metrics. We also need to follow up with human evaluation. Opportunities: (1) There is still much we do not understand about metric tunability; we can learn much by looking at joint metric-spaces and examining how new metrics correlate with established ones. (2) Pareto is just one approach among many in multi-objective optimization. A wealth of methods are available (Marler and Arora, 2004) and more ex</context>
</contexts>
<marker>Deb, Pratap, Agarwal, Meyarivan, 2002</marker>
<rawString>Kalyanmoy Deb, Amrit Pratap, Sammer Agarwal, and T. Meyarivan. 2002. A fast and elitist multiobjective genetic algorithm: NSGA-II. IEEE Transactions on Evolutionary Computation, 6(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Hendra Setiawan</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>The university of maryland statistical machine translation system for the fourth workshop on machine translation.</title>
<date>2009</date>
<booktitle>In Proc. of the Fourth Workshop on Machine Translation.</booktitle>
<contexts>
<context citStr="Dyer et al., 2009" endWordPosition="4271" position="25521" startWordPosition="4268">Related Work Multi-objective optimization for MT is a relatively new area. Linear-combination of BLEU/TER is 0.69 0.68 0.67 0.66 0.65 0.64 0.63 0 2 4 6 8 10 12 14 16 18 Single−Objective RIBES Pareto (PMO−PRO) 20 35 NIST PubMed 30 25 20 15 10 5 0 2 4 6 8 10 12 14 16 18 ribes Set size |L| Figure 5: Avg. runtime per sentence of FindPareto Runtime (seconds) 0.35 0.25 0.15 0.05 0.3 0.2 0.1 0 0 100 200 300 400 500 600 700 800 900 1000 Algorithm 1 TopologicalSort (footnote 2) Number of Pareto Points 7 the most common technique (Zaidan, 2009), sometimes achieving good results in evaluation campaigns (Dyer et al., 2009). As far as we known, the only work that directly proposes a multi-objective technique is (He and Way, 2009), which modifies MERT to optimize a single metric subject to the constraint that it does not degrade others. These approaches all require some setting of constraint strength or combination weights {pk}. Recent work in MT evaluation has examined combining metrics using machine learning for better correlation with human judgments (Liu and Gildea, 2007; Albrecht and Hwa, 2007; Gimnez and M`arquez, 2008) and may give insights for setting {pk}. We view our Pareto-based approach as orthogonal </context>
</contexts>
<marker>Dyer, Setiawan, Marton, Resnik, 2009</marker>
<rawString>Chris Dyer, Hendra Setiawan, Yuval Marton, and Philip Resnik. 2009. The university of maryland statistical machine translation system for the fourth workshop on machine translation. In Proc. of the Fourth Workshop on Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Hal Daum´e</author>
</authors>
<title>Learning speedaccuracy tradeoffs in nondeterministic inference algorithms.</title>
<date>2011</date>
<booktitle>In COST: NIPS 2011 Workshop on Computational Trade-offs in Statistical Learning.</booktitle>
<marker>Eisner, Daum´e, 2011</marker>
<rawString>Jason Eisner and Hal Daum´e III. 2011. Learning speedaccuracy tradeoffs in nondeterministic inference algorithms. In COST: NIPS 2011 Workshop on Computational Trade-offs in Statistical Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gimnez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Heterogeneous automatic mt evaluation through non-parametric metric combinations.</title>
<date>2008</date>
<booktitle>In ICJNLP.</booktitle>
<marker>Gimnez, M`arquez, 2008</marker>
<rawString>Jes´us Gimnez and Llu´ıs M`arquez. 2008. Heterogeneous automatic mt evaluation through non-parametric metric combinations. In ICJNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Parke Godfrey</author>
<author>Ryan Shipley</author>
<author>Jarek Gyrz</author>
</authors>
<title>Algorithms and analyses for maximal vector computation.</title>
<date>2007</date>
<journal>VLDB Journal,</journal>
<volume>16</volume>
<contexts>
<context citStr="Godfrey et al., 2007" endWordPosition="2101" position="12420" startWordPosition="2098"> dominating or dominated in the forloop (lines 4-8). At least one pareto-optimal point will be found by line 8. The second loop (lines 9-11) further filters the list for points that are dominated by h* but iterated before h* in the first for-loop. The outer while-loop stops exactly after P iterations, where P is the actual number of paretooptimal points in L. Each inner loop costs O(KN) so the total complexity is O(PKN). Since P ≤ N with the actual value depending on the probability distribution of {M(h)}, the worst-case run-time is O(KN2). For a survey of various Pareto algorithms, refer to (Godfrey et al., 2007). The algorithm we described here is borrowed from the database literature in what is known as skyline operators.2 3.2 PMO-PRO Algorithm We are now ready to present an algorithm for multiobjective optimization. As we will see, it can be seen as a generalization of the pairwise ranking optimization (PRO) of (Hopkins and May, 2011), so we call it PMO-PRO. PMO-PRO approach works by iteratively decoding-and-optimizing on the devset, sim2The inquisitive reader may wonder how is Pareto related to databases. The motivation is to incorporate preferences into relational queries(B¨orzs¨onyi et al., 2001</context>
</contexts>
<marker>Godfrey, Shipley, Gyrz, 2007</marker>
<rawString>Parke Godfrey, Ryan Shipley, and Jarek Gyrz. 2007. Algorithms and analyses for maximal vector computation. VLDB Journal, 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isao Goto</author>
<author>Bin Lu</author>
<author>Ka Po Chow</author>
<author>Eiichiro Sumita</author>
<author>Benjamin K Tsou</author>
</authors>
<title>Overview of the patent machine translation task at the ntcir-9 workshop.</title>
<date>2011</date>
<booktitle>In Proceedings of the NTCIR-9 Workshop Meeting.</booktitle>
<contexts>
<context citStr="Goto et al., 2011" endWordPosition="2980" position="17574" startWordPosition="2977">thms have a place where metric scores feedback into the optimization procedure; the idea of PMO is to replace these raw scores with labels derived from Pareto optimality. 4 Experiments 4.1 Evaluation Methodology We experiment with two datasets: (1) The PubMed task is English-to-Japanese translation of scientific 4A difference with traditional MERT is the necessity of sentence-BLEU (Liang et al., 2006) in line 6. We use sentenceBLEU for optimization but corpus-BLEU for evaluation here. abstracts. As metrics we use BLEU and RIBES (which demonstrated good human correlation in this language pair (Goto et al., 2011)). (2) The NIST task is Chinese-to-English translation with OpenMT08 training data and MT06 as devset. As metrics we use BLEU and NTER. • BLEU = BP × (IIprec)1/4. BP is brevity penality. prec,,, is precision of n-gram matches. • RIBES = (z + 1)/2 × prec1/4 1 , with Kendall’s z computed by measuring permutation between matching words in reference and hypothesis5. • NTER=max(1−TER, 0), which normalizes Translation Edit Rate6 so that NTER=1 is best. We compare two multi-objective approaches: 1. Linear-Combination of metrics (Eq. 2), optimized with PRO. We search a range of combination settings: (</context>
</contexts>
<marker>Goto, Lu, Chow, Sumita, Tsou, 2011</marker>
<rawString>Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and Benjamin K. Tsou. 2011. Overview of the patent machine translation task at the ntcir-9 workshop. In Proceedings of the NTCIR-9 Workshop Meeting.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Hall</author>
<author>Ryan McDonald</author>
<author>Jason Katz-Brown</author>
<author>Michael Ringgaard</author>
</authors>
<title>Training dependency parsers by jointly optimizing multiple objectives.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1489--1499</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context citStr="Hall et al., 2011" endWordPosition="4519" position="27040" startWordPosition="4516">auser et al., 2008; Cer et al., 2010) report similar observations, in addition citing WER being difficult and BLEU-TER being amenable. One unsolved question is whether metric tunability is a problem inherent to the metric only, or depends also on the underlying optimization algorithm. Our positive results with PMO suggest that the choice of optimization algorithm can help. Multi-objective ideas are being explored in other NLP areas. (Spitkovsky et al., 2011) describe a technique that alternates between hard and soft EM objectives in order to achieve better local optimum in grammar induction. (Hall et al., 2011) investigates joint optimization of a supervised parsing objective and some extrinsic objectives based on downstream applications. (Agarwal et al., 2011) considers using multiple signals (of varying quality) from online users to train recommendation models. (Eisner and Daum´e III, 2011) trades off speed and accuracy of a parser with reinforcement learning. None of the techniques in NLP use Pareto concepts, however. 6 Opportunities and Limitations We introduce a new approach (PMO) for training MT systems on multiple metrics. Leveraging the diverse perspectives of different evaluation metrics ha</context>
</contexts>
<marker>Hall, McDonald, Katz-Brown, Ringgaard, 2011</marker>
<rawString>Keith Hall, Ryan McDonald, Jason Katz-Brown, and Michael Ringgaard. 2011. Training dependency parsers by jointly optimizing multiple objectives. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1489–1499, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yifan He</author>
<author>Andy Way</author>
</authors>
<title>Improving the objective function in minimum error rate training.</title>
<date>2009</date>
<booktitle>In MT Summit.</booktitle>
<contexts>
<context citStr="He and Way, 2009" endWordPosition="4290" position="25629" startWordPosition="4287">0.69 0.68 0.67 0.66 0.65 0.64 0.63 0 2 4 6 8 10 12 14 16 18 Single−Objective RIBES Pareto (PMO−PRO) 20 35 NIST PubMed 30 25 20 15 10 5 0 2 4 6 8 10 12 14 16 18 ribes Set size |L| Figure 5: Avg. runtime per sentence of FindPareto Runtime (seconds) 0.35 0.25 0.15 0.05 0.3 0.2 0.1 0 0 100 200 300 400 500 600 700 800 900 1000 Algorithm 1 TopologicalSort (footnote 2) Number of Pareto Points 7 the most common technique (Zaidan, 2009), sometimes achieving good results in evaluation campaigns (Dyer et al., 2009). As far as we known, the only work that directly proposes a multi-objective technique is (He and Way, 2009), which modifies MERT to optimize a single metric subject to the constraint that it does not degrade others. These approaches all require some setting of constraint strength or combination weights {pk}. Recent work in MT evaluation has examined combining metrics using machine learning for better correlation with human judgments (Liu and Gildea, 2007; Albrecht and Hwa, 2007; Gimnez and M`arquez, 2008) and may give insights for setting {pk}. We view our Pareto-based approach as orthogonal to these efforts. The tunability of metrics is a problem that is gaining recognition (Liu et al., 2011). If </context>
</contexts>
<marker>He, Way, 2009</marker>
<rawString>Yifan He and Andy Way. 2009. Improving the objective function in minimum error rate training. In MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1352--1362</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context citStr="Hopkins and May, 2011" endWordPosition="166" position="1171" startWordPosition="162">Our approach is based on the theory of Pareto Optimality. It is simple to implement on top of existing single-objective optimization methods (e.g. MERT, PRO) and outperforms ad hoc alternatives based on linear-combination of metrics. We also discuss the issue of metric tunability and show that our Pareto approach is more effective in incorporating new metrics from MT evaluation for MT optimization. 1 Introduction Weight optimization is an important step in building machine translation (MT) systems. Discriminative optimization methods such as MERT (Och, 2003), MIRA (Crammer et al., 2006), PRO (Hopkins and May, 2011), and Downhill-Simplex (Nelder and Mead, 1965) have been influential in improving MT systems in recent years. These methods are effective because they tune the system to maximize an automatic evaluation metric such as BLEU, which serve as surrogate objective for translation quality. However, we know that a single metric such as BLEU is not enough. Ideally, we want to tune towards an automatic metric that has perfect correlation with human judgments of translation quality. *Now at Nara Institute of Science &amp; Technology (NAIST) While many alternatives have been proposed, such a perfect evaluatio</context>
<context citStr="Hopkins and May, 2011" endWordPosition="2158" position="12751" startWordPosition="2155">r of paretooptimal points in L. Each inner loop costs O(KN) so the total complexity is O(PKN). Since P ≤ N with the actual value depending on the probability distribution of {M(h)}, the worst-case run-time is O(KN2). For a survey of various Pareto algorithms, refer to (Godfrey et al., 2007). The algorithm we described here is borrowed from the database literature in what is known as skyline operators.2 3.2 PMO-PRO Algorithm We are now ready to present an algorithm for multiobjective optimization. As we will see, it can be seen as a generalization of the pairwise ranking optimization (PRO) of (Hopkins and May, 2011), so we call it PMO-PRO. PMO-PRO approach works by iteratively decoding-and-optimizing on the devset, sim2The inquisitive reader may wonder how is Pareto related to databases. The motivation is to incorporate preferences into relational queries(B¨orzs¨onyi et al., 2001). For K = 2 metrics, they also present an alternative faster O(N logN) algorithm by first topologically sorting along the 2 dimensions. All dominated points can be filtered by one-pass by comparing with the most-recent dominating point. ilar to many MT optimization methods. The main difference is that rather than trying to maxim</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1352–1362, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Isozaki</author>
<author>T Hirao</author>
<author>K Duh</author>
<author>K Sudoh</author>
<author>H Tsukada</author>
</authors>
<title>Automatic evaluation of translation quality for distant language pairs.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context citStr="Isozaki et al., 2010" endWordPosition="337" position="2298" startWordPosition="334">nce &amp; Technology (NAIST) While many alternatives have been proposed, such a perfect evaluation metric remains elusive. As a result, many MT evaluation campaigns now report multiple evaluation metrics (Callison-Burch et al., 2011; Paul, 2010). Different evaluation metrics focus on different aspects of translation quality. For example, while BLEU (Papineni et al., 2002) focuses on word-based n-gram precision, METEOR (Lavie and Agarwal, 2007) allows for stem/synonym matching and incorporates recall. TER (Snover et al., 2006) allows arbitrary chunk movements, while permutation metrics like RIBES (Isozaki et al., 2010; Birch et al., 2010) measure deviation in word order. Syntax (Owczarzak et al., 2007) and semantics (Pado et al., 2009) also help. Arguably, all these metrics correspond to our intuitions on what is a good translation. The current approach of optimizing MT towards a single metric runs the risk of sacrificing other metrics. Can we really claim that a system is good if it has high BLEU, but very low METEOR? Similarly, is a high-METEOR low-BLEU system desirable? Our goal is to propose a multi-objective optimization method that avoids “overfitting to a single metric”. We want to build a MT system</context>
</contexts>
<marker>Isozaki, Hirao, Duh, Sudoh, Tsukada, 2010</marker>
<rawString>H. Isozaki, T. Hirao, K. Duh, K. Sudoh, and H. Tsukada. 2010. Automatic evaluation of translation quality for distant language pairs. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Training linear SVMs in linear time.</title>
<date>2006</date>
<booktitle>In KDD.</booktitle>
<contexts>
<context citStr="Joachims, 2006" endWordPosition="3379" position="20119" startWordPosition="3378"> poorly when re-decoding the same devset. The re-decode devset approach avoids being overly optimistic while accurately measuring optimization performance. 5 Train Devset #Feat Metrics PubMed 0.2M 2k 14 BLEU, RIBES NIST 7M 1.6k 8 BLEU, NTER Table 1: Task characteristics: #sentences in Train/Dev, # of features, and metrics used. Our MT models are trained with standard phrase-based Moses software (Koehn and others, 2007), with IBM M4 alignments, 4gram SRILM, lexical ordering for PubMed and distance ordering for the NIST system. The decoder generates 50-best lists each iteration. We use SVMRank (Joachims, 2006) as optimization subroutine for PRO, which efficiently handle all pairwise samples without the need for sampling. ribes 0.695 Linear Combination Pareto (PMO−PRO) 0.69 0.685 0.68 0.675 0.67 0.665 0.2 0.21 0.22 0.23 0.24 0.25 0.26 0.27 bleu 4.2 Results Figures 2 and 3 show the results for PubMed and NIST, respectively. A method is better if its Pareto Frontier lies more towards the upper-right hand corner of the graph. Our observations are: 1. PMO-PRO generally outperforms LinearCombination with any setting of (p1,p2). The Pareto Frontier of PMO-PRO dominates that of Linear-Combination. This imp</context>
</contexts>
<marker>Joachims, 2006</marker>
<rawString>T. Joachims. 2006. Training linear SVMs in linear time. In KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<marker>Koehn, 2007</marker>
<rawString>P. Koehn et al. 2007. Moses: open source toolkit for statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavie</author>
<author>A Agarwal</author>
</authors>
<title>METEOR: An automatic metric for mt evaluation with high levels of correlation with human judgments.</title>
<date>2007</date>
<booktitle>In Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context citStr="Lavie and Agarwal, 2007" endWordPosition="312" position="2121" startWordPosition="309"> as BLEU is not enough. Ideally, we want to tune towards an automatic metric that has perfect correlation with human judgments of translation quality. *Now at Nara Institute of Science &amp; Technology (NAIST) While many alternatives have been proposed, such a perfect evaluation metric remains elusive. As a result, many MT evaluation campaigns now report multiple evaluation metrics (Callison-Burch et al., 2011; Paul, 2010). Different evaluation metrics focus on different aspects of translation quality. For example, while BLEU (Papineni et al., 2002) focuses on word-based n-gram precision, METEOR (Lavie and Agarwal, 2007) allows for stem/synonym matching and incorporates recall. TER (Snover et al., 2006) allows arbitrary chunk movements, while permutation metrics like RIBES (Isozaki et al., 2010; Birch et al., 2010) measure deviation in word order. Syntax (Owczarzak et al., 2007) and semantics (Pado et al., 2009) also help. Arguably, all these metrics correspond to our intuitions on what is a good translation. The current approach of optimizing MT towards a single metric runs the risk of sacrificing other metrics. Can we really claim that a system is good if it has high BLEU, but very low METEOR? Similarly, is</context>
</contexts>
<marker>Lavie, Agarwal, 2007</marker>
<rawString>A. Lavie and A. Agarwal. 2007. METEOR: An automatic metric for mt evaluation with high levels of correlation with human judgments. In Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>A Bouchard-Cote</author>
<author>D Klein</author>
<author>B Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context citStr="Liang et al., 2006" endWordPosition="2945" position="17360" startWordPosition="2942">MERT4. Alternatively, by using the largemargin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 4-9), one can get an online algorithm such PMO-MIRA. Virtually all MT optimization algorithms have a place where metric scores feedback into the optimization procedure; the idea of PMO is to replace these raw scores with labels derived from Pareto optimality. 4 Experiments 4.1 Evaluation Methodology We experiment with two datasets: (1) The PubMed task is English-to-Japanese translation of scientific 4A difference with traditional MERT is the necessity of sentence-BLEU (Liang et al., 2006) in line 6. We use sentenceBLEU for optimization but corpus-BLEU for evaluation here. abstracts. As metrics we use BLEU and RIBES (which demonstrated good human correlation in this language pair (Goto et al., 2011)). (2) The NIST task is Chinese-to-English translation with OpenMT08 training data and MT06 as devset. As metrics we use BLEU and NTER. • BLEU = BP × (IIprec)1/4. BP is brevity penality. prec,,, is precision of n-gram matches. • RIBES = (z + 1)/2 × prec1/4 1 , with Kendall’s z computed by measuring permutation between matching words in reference and hypothesis5. • NTER=max(1−TER, 0),</context>
</contexts>
<marker>Liang, Bouchard-Cote, Klein, Taskar, 2006</marker>
<rawString>P. Liang, A. Bouchard-Cote, D. Klein, and B. Taskar. 2006. An end-to-end discriminative approach to machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Source-language features and maximum correlation training for machine translation evaluation.</title>
<date>2007</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context citStr="Liu and Gildea, 2007" endWordPosition="4343" position="25980" startWordPosition="4340">ort (footnote 2) Number of Pareto Points 7 the most common technique (Zaidan, 2009), sometimes achieving good results in evaluation campaigns (Dyer et al., 2009). As far as we known, the only work that directly proposes a multi-objective technique is (He and Way, 2009), which modifies MERT to optimize a single metric subject to the constraint that it does not degrade others. These approaches all require some setting of constraint strength or combination weights {pk}. Recent work in MT evaluation has examined combining metrics using machine learning for better correlation with human judgments (Liu and Gildea, 2007; Albrecht and Hwa, 2007; Gimnez and M`arquez, 2008) and may give insights for setting {pk}. We view our Pareto-based approach as orthogonal to these efforts. The tunability of metrics is a problem that is gaining recognition (Liu et al., 2011). If a good evaluation metric could not be used for tuning, it would be a pity. The Tunable Metrics task at WMT2011 concluded that BLEU is still the easiest to tune (Callison-Burch et al., 2011). (Mauser et al., 2008; Cer et al., 2010) report similar observations, in addition citing WER being difficult and BLEU-TER being amenable. One unsolved question i</context>
</contexts>
<marker>Liu, Gildea, 2007</marker>
<rawString>Ding Liu and Daniel Gildea. 2007. Source-language features and maximum correlation training for machine translation evaluation. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Liu</author>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Better evaluation metrics lead to better machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context citStr="Liu et al., 2011" endWordPosition="3593" position="21513" startWordPosition="3590">ake a choice between picking the best weight according to BLEU (BLEU=.265,RIBES=.665) vs. another weight with higher RIBES but poorer BLEU, e.g. (.255,.675). Nevertheless, both the PMO and Linear-Combination with various (p1,p2) samples this joint-objective space broadly. 3. Interestingly, a multi-objective approach can sometimes outperform a single-objective optimizer in its own metric. In Figure 2, singleobjective PRO focusing on optimizing RIBES only achieves 0.68, but PMO-PRO using both BLEU and RIBES outperforms with 0.685. The third observation relates to the issue of metric tunability (Liu et al., 2011). We found that RIBES can be difficult to tune directly. It is an extremely non-smooth objective with many local optima–slight changes in word ordering causes large changes in RIBES. So the best way to improve RIBES is to Figure 2: PubMed Results. The curve represents the Pareto Frontier of all results collected after multiple runs. bleu Figure 3: NIST Results not to optimize it directly, but jointly with a more tunable metric BLEU. The learning curve in Figure 4 show that single-objective optimization of RIBES quickly falls into local optimum (at iteration 3) whereas PMO can zigzag and sacrif</context>
<context citStr="Liu et al., 2011" endWordPosition="4384" position="26224" startWordPosition="4381"> is (He and Way, 2009), which modifies MERT to optimize a single metric subject to the constraint that it does not degrade others. These approaches all require some setting of constraint strength or combination weights {pk}. Recent work in MT evaluation has examined combining metrics using machine learning for better correlation with human judgments (Liu and Gildea, 2007; Albrecht and Hwa, 2007; Gimnez and M`arquez, 2008) and may give insights for setting {pk}. We view our Pareto-based approach as orthogonal to these efforts. The tunability of metrics is a problem that is gaining recognition (Liu et al., 2011). If a good evaluation metric could not be used for tuning, it would be a pity. The Tunable Metrics task at WMT2011 concluded that BLEU is still the easiest to tune (Callison-Burch et al., 2011). (Mauser et al., 2008; Cer et al., 2010) report similar observations, in addition citing WER being difficult and BLEU-TER being amenable. One unsolved question is whether metric tunability is a problem inherent to the metric only, or depends also on the underlying optimization algorithm. Our positive results with PMO suggest that the choice of optimization algorithm can help. Multi-objective ideas are </context>
</contexts>
<marker>Liu, Dahlmeier, Ng, 2011</marker>
<rawString>Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2011. Better evaluation metrics lead to better machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Macherey</author>
<author>Franz Och</author>
<author>Ignacio Thayer</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Lattice-based minimum error rate training for statistical machine translation.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context citStr="Macherey et al., 2008" endWordPosition="4732" position="28446" startWordPosition="4729">of combination weights. Further we observe that multiobjective approaches can be helpful for optimizing difficult-to-tune metrics; this is beneficial for quickly introducing new metrics developed in MT evaluation into MT optimization, especially when good {pk} are not yet known. We conclude by drawing attention to some limitations and opportunities raised by this work: Limitations: (1) The performance of PMO is limited by the size of the Pareto set. Small N-best lists lead to sparsely-sampled Pareto Frontiers, and a much better approach would be to enlarge the hypothesis space using lattices (Macherey et al., 2008). How to compute Pareto points directly from lattices is an interesting open research question. (2) The binary distinction between pareto vs. non-pareto points ignores the fact that 2nd-place non-pareto points may also lead to good practical solutions. A better approach may be to adopt a graded definition of Pareto optimality as done in some multi-objective works (Deb et al., 2002). (3) A robust evaluation methodology that enables significance testing for multi-objective problems is sorely needed. This will make it possible to compare multi-objective methods on more than 2 metrics. We also nee</context>
</contexts>
<marker>Macherey, Och, Thayer, Uszkoreit, 2008</marker>
<rawString>Wolfgang Macherey, Franz Och, Ignacio Thayer, and Jakob Uszkoreit. 2008. Lattice-based minimum error rate training for statistical machine translation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R T Marler</author>
<author>J S Arora</author>
</authors>
<title>Survey of multi-objective optimization methods for engineering.</title>
<date>2004</date>
<journal>Structural and Multidisciplinary Optimization,</journal>
<volume>26</volume>
<contexts>
<context citStr="Marler and Arora (2004)" endWordPosition="1705" position="10273" startWordPosition="1701"> we get h = (0.9, 0.1), for p1 = 0.6 we get (0.7, 0.6), and for 0 &lt; p1 &lt; 0.6 we get (0.4, 0.8). At no setting of p1 do we attain h = (0.4, 0.7) which is also pareto-optimal but not on the convex hull.1 This may have ramifications for issues like metric tunability and local optima. To summarize, linearcombination is reasonable but has limitations. Our proposed approach will instead directly solve Eq. 1. Pareto Optimality and multi-objective optimization is a deep field with active inquiry in engineering, operations research, economics, etc. For the interested reader, we recommend the survey by Marler and Arora (2004) and books by (Sawaragi et al., 1985; Miettinen, 1998). 3 Multi-objective Algorithms 3.1 Computing the Pareto Frontier Our PMO approach will need to compute the Pareto Frontier for potentially large sets of points, so we first describe how this can be done efficiently. Given a set of N vectors {M(h)I from an N-best list L, our goal is extract the subset that are pareto-optimal. Here we present an algorithm based on iterative filtering, in our opinion the simplest algorithm to understand and implement. The strategy is to loop through the list L, keeping track of any dominant points. Given a dom</context>
</contexts>
<marker>Marler, Arora, 2004</marker>
<rawString>R. T. Marler and J. S. Arora. 2004. Survey of multi-objective optimization methods for engineering. Structural and Multidisciplinary Optimization, 26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arne Mauser</author>
<author>Saˇsa Hasan</author>
<author>Hermann Ney</author>
</authors>
<title>Automatic evaluation measures for statistical machine translation system optimization.</title>
<date>2008</date>
<booktitle>In International Conference on Language Resources and Evaluation,</booktitle>
<location>Marrakech, Morocco,</location>
<contexts>
<context citStr="Mauser et al., 2008" endWordPosition="4424" position="26440" startWordPosition="4421">ghts {pk}. Recent work in MT evaluation has examined combining metrics using machine learning for better correlation with human judgments (Liu and Gildea, 2007; Albrecht and Hwa, 2007; Gimnez and M`arquez, 2008) and may give insights for setting {pk}. We view our Pareto-based approach as orthogonal to these efforts. The tunability of metrics is a problem that is gaining recognition (Liu et al., 2011). If a good evaluation metric could not be used for tuning, it would be a pity. The Tunable Metrics task at WMT2011 concluded that BLEU is still the easiest to tune (Callison-Burch et al., 2011). (Mauser et al., 2008; Cer et al., 2010) report similar observations, in addition citing WER being difficult and BLEU-TER being amenable. One unsolved question is whether metric tunability is a problem inherent to the metric only, or depends also on the underlying optimization algorithm. Our positive results with PMO suggest that the choice of optimization algorithm can help. Multi-objective ideas are being explored in other NLP areas. (Spitkovsky et al., 2011) describe a technique that alternates between hard and soft EM objectives in order to achieve better local optimum in grammar induction. (Hall et al., 2011)</context>
</contexts>
<marker>Mauser, Hasan, Ney, 2008</marker>
<rawString>Arne Mauser, Saˇsa Hasan, and Hermann Ney. 2008. Automatic evaluation measures for statistical machine translation system optimization. In International Conference on Language Resources and Evaluation, Marrakech, Morocco, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kaisa Miettinen</author>
</authors>
<title>Nonlinear Multiobjective Optimization.</title>
<date>1998</date>
<publisher>Springer.</publisher>
<contexts>
<context citStr="Miettinen, 1998" endWordPosition="1714" position="10327" startWordPosition="1713">or 0 &lt; p1 &lt; 0.6 we get (0.4, 0.8). At no setting of p1 do we attain h = (0.4, 0.7) which is also pareto-optimal but not on the convex hull.1 This may have ramifications for issues like metric tunability and local optima. To summarize, linearcombination is reasonable but has limitations. Our proposed approach will instead directly solve Eq. 1. Pareto Optimality and multi-objective optimization is a deep field with active inquiry in engineering, operations research, economics, etc. For the interested reader, we recommend the survey by Marler and Arora (2004) and books by (Sawaragi et al., 1985; Miettinen, 1998). 3 Multi-objective Algorithms 3.1 Computing the Pareto Frontier Our PMO approach will need to compute the Pareto Frontier for potentially large sets of points, so we first describe how this can be done efficiently. Given a set of N vectors {M(h)I from an N-best list L, our goal is extract the subset that are pareto-optimal. Here we present an algorithm based on iterative filtering, in our opinion the simplest algorithm to understand and implement. The strategy is to loop through the list L, keeping track of any dominant points. Given a dominant point, it is easy to filter out many points that</context>
</contexts>
<marker>Miettinen, 1998</marker>
<rawString>Kaisa Miettinen. 1998. Nonlinear Multiobjective Optimization. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Nelder</author>
<author>R Mead</author>
</authors>
<title>The downhill simplex method.</title>
<date>1965</date>
<journal>Computer Journal,</journal>
<volume>7</volume>
<issue>308</issue>
<contexts>
<context citStr="Nelder and Mead, 1965" endWordPosition="172" position="1217" startWordPosition="169">Optimality. It is simple to implement on top of existing single-objective optimization methods (e.g. MERT, PRO) and outperforms ad hoc alternatives based on linear-combination of metrics. We also discuss the issue of metric tunability and show that our Pareto approach is more effective in incorporating new metrics from MT evaluation for MT optimization. 1 Introduction Weight optimization is an important step in building machine translation (MT) systems. Discriminative optimization methods such as MERT (Och, 2003), MIRA (Crammer et al., 2006), PRO (Hopkins and May, 2011), and Downhill-Simplex (Nelder and Mead, 1965) have been influential in improving MT systems in recent years. These methods are effective because they tune the system to maximize an automatic evaluation metric such as BLEU, which serve as surrogate objective for translation quality. However, we know that a single metric such as BLEU is not enough. Ideally, we want to tune towards an automatic metric that has perfect correlation with human judgments of translation quality. *Now at Nara Institute of Science &amp; Technology (NAIST) While many alternatives have been proposed, such a perfect evaluation metric remains elusive. As a result, many MT</context>
</contexts>
<marker>Nelder, Mead, 1965</marker>
<rawString>J.A. Nelder and R. Mead. 1965. The downhill simplex method. Computer Journal, 7(308).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context citStr="Och, 2003" endWordPosition="155" position="1113" startWordPosition="154">e diverse aspects to improve overall quality. Our approach is based on the theory of Pareto Optimality. It is simple to implement on top of existing single-objective optimization methods (e.g. MERT, PRO) and outperforms ad hoc alternatives based on linear-combination of metrics. We also discuss the issue of metric tunability and show that our Pareto approach is more effective in incorporating new metrics from MT evaluation for MT optimization. 1 Introduction Weight optimization is an important step in building machine translation (MT) systems. Discriminative optimization methods such as MERT (Och, 2003), MIRA (Crammer et al., 2006), PRO (Hopkins and May, 2011), and Downhill-Simplex (Nelder and Mead, 1965) have been influential in improving MT systems in recent years. These methods are effective because they tune the system to maximize an automatic evaluation metric such as BLEU, which serve as surrogate objective for translation quality. However, we know that a single metric such as BLEU is not enough. Ideally, we want to tune towards an automatic metric that has perfect correlation with human judgments of translation quality. *Now at Nara Institute of Science &amp; Technology (NAIST) While many</context>
<context citStr="Och, 2003" endWordPosition="2845" position="16723" startWordPosition="2844">r PMO-PRO: for non-pareto hypotheses h ∈/ {f}, we set label l = Ek Mk(h)/K instead of l= 0, so the method not only learns to discriminate pareto vs. non-pareto but also also learns to discriminate among competing non-pareto points. Also, like other MT works, in line 5 the N-best list is concatenated to N-best lists from previous iterations, so {h} is a set with i · N elements. General PMO Approach: The strategy we outlined in Section 3.2 can be easily applied to other MT optimization techniques. For example, by replacing the optimization subroutine (line 10, Algorithm 2) with a Powell search (Och, 2003), one can get PMO-MERT4. Alternatively, by using the largemargin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 4-9), one can get an online algorithm such PMO-MIRA. Virtually all MT optimization algorithms have a place where metric scores feedback into the optimization procedure; the idea of PMO is to replace these raw scores with labels derived from Pareto optimality. 4 Experiments 4.1 Evaluation Methodology We experiment with two datasets: (1) The PubMed task is English-to-Japanese translation of scientific 4A difference with traditional MERT is the necessity </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Och. 2003. Minimum error rate training in statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Labelled dependencies in machine translation evaluation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation.</booktitle>
<marker>Owczarzak, van Genabith, Way, 2007</marker>
<rawString>Karolina Owczarzak, Josef van Genabith, and Andy Way. 2007. Labelled dependencies in machine translation evaluation. In Proceedings of the Second Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pado</author>
<author>Daniel Cer</author>
<author>Michel Galley</author>
<author>Dan Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Measuring machine translation quality as semantic equivalence: A metric based on entailment features.</title>
<date>2009</date>
<journal>Machine Translation,</journal>
<pages>23--2</pages>
<contexts>
<context citStr="Pado et al., 2009" endWordPosition="358" position="2418" startWordPosition="355">a result, many MT evaluation campaigns now report multiple evaluation metrics (Callison-Burch et al., 2011; Paul, 2010). Different evaluation metrics focus on different aspects of translation quality. For example, while BLEU (Papineni et al., 2002) focuses on word-based n-gram precision, METEOR (Lavie and Agarwal, 2007) allows for stem/synonym matching and incorporates recall. TER (Snover et al., 2006) allows arbitrary chunk movements, while permutation metrics like RIBES (Isozaki et al., 2010; Birch et al., 2010) measure deviation in word order. Syntax (Owczarzak et al., 2007) and semantics (Pado et al., 2009) also help. Arguably, all these metrics correspond to our intuitions on what is a good translation. The current approach of optimizing MT towards a single metric runs the risk of sacrificing other metrics. Can we really claim that a system is good if it has high BLEU, but very low METEOR? Similarly, is a high-METEOR low-BLEU system desirable? Our goal is to propose a multi-objective optimization method that avoids “overfitting to a single metric”. We want to build a MT system that does well with respect to many aspects of translation quality. In general, we cannot expect to improve multiple me</context>
</contexts>
<marker>Pado, Cer, Galley, Jurafsky, Manning, 2009</marker>
<rawString>Sebastian Pado, Daniel Cer, Michel Galley, Dan Jurafsky, and Christopher D. Manning. 2009. Measuring machine translation quality as semantic equivalence: A metric based on entailment features. Machine Translation, 23(2-3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context citStr="Papineni et al., 2002" endWordPosition="302" position="2048" startWordPosition="299">ive for translation quality. However, we know that a single metric such as BLEU is not enough. Ideally, we want to tune towards an automatic metric that has perfect correlation with human judgments of translation quality. *Now at Nara Institute of Science &amp; Technology (NAIST) While many alternatives have been proposed, such a perfect evaluation metric remains elusive. As a result, many MT evaluation campaigns now report multiple evaluation metrics (Callison-Burch et al., 2011; Paul, 2010). Different evaluation metrics focus on different aspects of translation quality. For example, while BLEU (Papineni et al., 2002) focuses on word-based n-gram precision, METEOR (Lavie and Agarwal, 2007) allows for stem/synonym matching and incorporates recall. TER (Snover et al., 2006) allows arbitrary chunk movements, while permutation metrics like RIBES (Isozaki et al., 2010; Birch et al., 2010) measure deviation in word order. Syntax (Owczarzak et al., 2007) and semantics (Pado et al., 2009) also help. Arguably, all these metrics correspond to our intuitions on what is a good translation. The current approach of optimizing MT towards a single metric runs the risk of sacrificing other metrics. Can we really claim that</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vilfredo Pareto</author>
</authors>
<title>Manuale di Economica Politica, (Translated into English by A.S.</title>
<date>1906</date>
<booktitle>Schwier as Manual of Political Economy,</booktitle>
<location>Milan.</location>
<contexts>
<context citStr="Pareto, 1906" endWordPosition="486" position="3144" startWordPosition="485">pproach of optimizing MT towards a single metric runs the risk of sacrificing other metrics. Can we really claim that a system is good if it has high BLEU, but very low METEOR? Similarly, is a high-METEOR low-BLEU system desirable? Our goal is to propose a multi-objective optimization method that avoids “overfitting to a single metric”. We want to build a MT system that does well with respect to many aspects of translation quality. In general, we cannot expect to improve multiple metrics jointly if there are some inherent tradeoffs. We therefore need to define the notion of Pareto Optimality (Pareto, 1906), which characterizes this tradeoff in a rigorous way and distinguishes the set of equally good solutions. We will describe Pareto Optimality in detail later, but roughly speaking, a 1 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1–10, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics hypothesis is pareto-optimal if there exist no other hypothesis better in all metrics. The contribution of this paper is two-fold: • We introduce PMO (Pareto-based Multiobjective Optimization), a general approach for learnin</context>
<context citStr="Pareto, 1906" endWordPosition="699" position="4591" startWordPosition="698">ve space, and especially obtains stronger results for metrics that may be difficult to tune individually. In the following, we first explain the theory of Pareto Optimality (Section 2), and then use it to build up our proposed PMO approach (Section 3). Experiments on NIST Chinese-English and PubMed English-Japanese translation using BLEU, TER, and RIBES are presented in Section 4. We conclude by discussing related work (Section 5) and opportunities/limitations (Section 6). 2 Theory of Pareto Optimality 2.1 Definitions and Concepts The idea of Pareto optimality comes originally from economics (Pareto, 1906), where the goal is to characterize situations when a change in allocation of goods does not make anybody worse off. Here, we will explain it in terms of MT: Let h E L be a hypothesis from an N-best list L. We have a total of K different metrics Mk(h) for evaluating the quality of h. Without loss of generality, we assume metric scores are bounded between 0 and 1, with 1 being perfect. Each hypothesis h can be mapped to a K-dimensional vector M(h) = [M1(h); M2(h); ...; MK(h)]. For example, suppose K = 2, M1(h) computes the BLEU score, and M2(h) gives the METEOR score of h. Figure 1 illustrates </context>
</contexts>
<marker>Pareto, 1906</marker>
<rawString>Vilfredo Pareto. 1906. Manuale di Economica Politica, (Translated into English by A.S. Schwier as Manual of Political Economy, 1971). Societa Editrice Libraria, Milan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
</authors>
<title>Overview of the iwslt 2010 evaluation campaign.</title>
<date>2010</date>
<booktitle>In IWSLT.</booktitle>
<contexts>
<context citStr="Paul, 2010" endWordPosition="283" position="1919" startWordPosition="282"> because they tune the system to maximize an automatic evaluation metric such as BLEU, which serve as surrogate objective for translation quality. However, we know that a single metric such as BLEU is not enough. Ideally, we want to tune towards an automatic metric that has perfect correlation with human judgments of translation quality. *Now at Nara Institute of Science &amp; Technology (NAIST) While many alternatives have been proposed, such a perfect evaluation metric remains elusive. As a result, many MT evaluation campaigns now report multiple evaluation metrics (Callison-Burch et al., 2011; Paul, 2010). Different evaluation metrics focus on different aspects of translation quality. For example, while BLEU (Papineni et al., 2002) focuses on word-based n-gram precision, METEOR (Lavie and Agarwal, 2007) allows for stem/synonym matching and incorporates recall. TER (Snover et al., 2006) allows arbitrary chunk movements, while permutation metrics like RIBES (Isozaki et al., 2010; Birch et al., 2010) measure deviation in word order. Syntax (Owczarzak et al., 2007) and semantics (Pado et al., 2009) also help. Arguably, all these metrics correspond to our intuitions on what is a good translation. T</context>
</contexts>
<marker>Paul, 2010</marker>
<rawString>Michael Paul. 2010. Overview of the iwslt 2010 evaluation campaign. In IWSLT.</rawString>
</citation>
<citation valid="true">
<title>Theory of Multiobjective Optimization.</title>
<date>1985</date>
<editor>Yoshikazu Sawaragi, Hirotaka Nakayama, and Tetsuzo Tanino, editors.</editor>
<publisher>Academic Press.</publisher>
<marker>1985</marker>
<rawString>Yoshikazu Sawaragi, Hirotaka Nakayama, and Tetsuzo Tanino, editors. 1985. Theory of Multiobjective Optimization. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>L Micciulla</author>
<author>J Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In AMTA.</booktitle>
<contexts>
<context citStr="Snover et al., 2006" endWordPosition="324" position="2205" startWordPosition="321">rfect correlation with human judgments of translation quality. *Now at Nara Institute of Science &amp; Technology (NAIST) While many alternatives have been proposed, such a perfect evaluation metric remains elusive. As a result, many MT evaluation campaigns now report multiple evaluation metrics (Callison-Burch et al., 2011; Paul, 2010). Different evaluation metrics focus on different aspects of translation quality. For example, while BLEU (Papineni et al., 2002) focuses on word-based n-gram precision, METEOR (Lavie and Agarwal, 2007) allows for stem/synonym matching and incorporates recall. TER (Snover et al., 2006) allows arbitrary chunk movements, while permutation metrics like RIBES (Isozaki et al., 2010; Birch et al., 2010) measure deviation in word order. Syntax (Owczarzak et al., 2007) and semantics (Pado et al., 2009) also help. Arguably, all these metrics correspond to our intuitions on what is a good translation. The current approach of optimizing MT towards a single metric runs the risk of sacrificing other metrics. Can we really claim that a system is good if it has high BLEU, but very low METEOR? Similarly, is a high-METEOR low-BLEU system desirable? Our goal is to propose a multi-objective o</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. 2006. A study of translation edit rate with targeted human annotation. In AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Lateen em: Unsupervised training with multiple objectives, applied to dependency grammar induction.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1269--1280</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context citStr="Spitkovsky et al., 2011" endWordPosition="4492" position="26884" startWordPosition="4489">not be used for tuning, it would be a pity. The Tunable Metrics task at WMT2011 concluded that BLEU is still the easiest to tune (Callison-Burch et al., 2011). (Mauser et al., 2008; Cer et al., 2010) report similar observations, in addition citing WER being difficult and BLEU-TER being amenable. One unsolved question is whether metric tunability is a problem inherent to the metric only, or depends also on the underlying optimization algorithm. Our positive results with PMO suggest that the choice of optimization algorithm can help. Multi-objective ideas are being explored in other NLP areas. (Spitkovsky et al., 2011) describe a technique that alternates between hard and soft EM objectives in order to achieve better local optimum in grammar induction. (Hall et al., 2011) investigates joint optimization of a supervised parsing objective and some extrinsic objectives based on downstream applications. (Agarwal et al., 2011) considers using multiple signals (of varying quality) from online users to train recommendation models. (Eisner and Daum´e III, 2011) trades off speed and accuracy of a parser with reinforcement learning. None of the techniques in NLP use Pareto concepts, however. 6 Opportunities and Limit</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2011</marker>
<rawString>Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. 2011. Lateen em: Unsupervised training with multiple objectives, applied to dependency grammar induction. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1269–1280, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar Zaidan</author>
</authors>
<title>Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems.</title>
<date>2009</date>
<booktitle>In The Prague Bulletin of Mathematical Linguistics.</booktitle>
<contexts>
<context citStr="Zaidan, 2009" endWordPosition="4258" position="25443" startWordPosition="4257">form Linear Combination that searches only on the (3/4,1/4) direction. 5 Related Work Multi-objective optimization for MT is a relatively new area. Linear-combination of BLEU/TER is 0.69 0.68 0.67 0.66 0.65 0.64 0.63 0 2 4 6 8 10 12 14 16 18 Single−Objective RIBES Pareto (PMO−PRO) 20 35 NIST PubMed 30 25 20 15 10 5 0 2 4 6 8 10 12 14 16 18 ribes Set size |L| Figure 5: Avg. runtime per sentence of FindPareto Runtime (seconds) 0.35 0.25 0.15 0.05 0.3 0.2 0.1 0 0 100 200 300 400 500 600 700 800 900 1000 Algorithm 1 TopologicalSort (footnote 2) Number of Pareto Points 7 the most common technique (Zaidan, 2009), sometimes achieving good results in evaluation campaigns (Dyer et al., 2009). As far as we known, the only work that directly proposes a multi-objective technique is (He and Way, 2009), which modifies MERT to optimize a single metric subject to the constraint that it does not degrade others. These approaches all require some setting of constraint strength or combination weights {pk}. Recent work in MT evaluation has examined combining metrics using machine learning for better correlation with human judgments (Liu and Gildea, 2007; Albrecht and Hwa, 2007; Gimnez and M`arquez, 2008) and may gi</context>
</contexts>
<marker>Zaidan, 2009</marker>
<rawString>Omar Zaidan. 2009. Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems. In The Prague Bulletin of Mathematical Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>