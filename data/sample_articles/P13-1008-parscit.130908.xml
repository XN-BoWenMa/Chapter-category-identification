<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant confidence="0.000000" no="0">
<title confidence="0.981182">
Joint Event Extraction via Structured Prediction with Global Features
</title>
<author confidence="0.999528">
Qi Li Heng Ji Liang Huang
</author>
<affiliation confidence="0.993806">
Departments of Computer Science and Linguistics
The Graduate Center and Queens College
City University of New York
</affiliation>
<address confidence="0.980566">
New York, NY 10016, USA
</address>
<email confidence="0.997081">
{liqiearth, hengjicuny, liang.huang.sh}@gmail.com
</email>
<sectionHeader confidence="0.993848" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999948217391304">Traditional approaches to the task of ACE event extraction usually rely on sequential pipelines with multiple stages, which suffer from error propagation since event triggers and arguments are predicted in isolation by independent local classifiers. By contrast, we propose a joint framework based on structured prediction which extracts triggers and arguments together so that the local predictions can be mutually improved. In addition, we propose to incorporate global features which explicitly capture the dependencies of multiple triggers and arguments. Experimental results show that our joint approach with local features outperforms the pipelined baseline, and adding global features further improves the performance significantly. Our approach advances state-ofthe-art sentence-level event extraction, and even outperforms previous argument labeling methods which use external knowledge from other sentences and documents.</bodyText>
<sectionHeader confidence="0.999131" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997614375">Event extraction is an important and challenging task in Information Extraction (IE), which aims to discover event triggers with specific types and their arguments. Most state-of-the-art approaches (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011) use sequential pipelines as building blocks, which break down the whole task into separate subtasks, such as trigger identification/classification and argument identification/classification. As a common drawback of the staged architecture, errors in upstream component are often compounded and propagated to the downstream classifiers. The downstream components, however, cannot impact earlier decisions. For example, consider the following sentences with an ambiguous word “fired”:</bodyText>
<listItem confidence="0.980665333333333">(1) In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel. (2) He has fired his air defense chief.</listItem>
<bodyText confidence="0.999929647058824">In sentence (1), “fired” is a trigger of type Attack. Because of the ambiguity, a local classifier may miss it or mislabel it as a trigger of End-Position. However, knowing that “tank” is very likely to be an Instrument argument of Attack events, the correct event subtype assignment of “fired” is obviously Attack. Likewise, in sentence (2), “air defense chief” is a job title, hence the argument classifier is likely to label it as an Entity argument for End-Position trigger. In addition, the local classifiers are incapable of capturing inter-dependencies among multiple event triggers and arguments. Consider sentence (1) again. Figure 1 depicts the corresponding event triggers and arguments. The dependency between “fired” and “died” cannot be captured by the local classifiers, which may fail to attach “cameraman” to “fired” as a Target argument. By using global features, we can propagate the Victim argument of the Die event to the Target argument of the Attack event. As another example, knowing that an Attack event usually only has one Attacker argument, we could penalize assignments in which one trigger has more than one Attacker. Such global features cannot be easily exploited by a local classifier. Therefore, we take a fresh look at this problem and formulate it, for the first time, as a structured learning problem. We propose a novel joint event extraction algorithm to predict the triggers and arguments simultaneously, and use the structured perceptron (Collins, 2002) to train the joint model. This way we can capture the dependencies between triggers and argument as well as explore arbitrary global features over multiple local predictions.</bodyText>
<page confidence="0.988881">
73
</page>
<note confidence="0.9416905">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 73–82,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figure confidence="0.672625">
Place
In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel.
Die Attack
</figure>
<figureCaption confidence="0.99393">
Figure 1: Event mentions of example (1). There are two event mentions that share three arguments,
namely the Die event mention triggered by “died”, and the Attack event mention triggered by “fired”.
</figureCaption>
<figure confidence="0.997693">
Target
Instrument
Target
Place
Victim
Instrument
</figure>
<bodyText confidence="0.9908907">However, different from easier tasks such as part-of-speech tagging or noun phrase chunking where efficient dynamic programming decoding is feasible, here exact joint inference is intractable. Therefore we employ beam search in decoding, and train the model using the early-update perceptron variant tailored for beam search (Collins and Roark, 2004; Huang et al., 2012). We make the following contributions:</bodyText>
<listItem confidence="0.901240090909091">1. Different from traditional pipeline approach, we present a novel framework for sentencelevel event extraction, which predicts triggers and their arguments jointly (Section 3). 2. We develop a rich set of features for event extraction which yield promising performance even with the traditional pipeline (Section 3.4.1). In this paper we refer to them as local features. 3. We introduce various global features to exploit dependencies among multiple triggers and arguments (Section 3.4.2).</listItem>
<bodyText confidence="0.998553428571428">Experiments show that our approach outperforms the pipelined approach with the same set of local features, and significantly advances the state-of-the-art with the addition of global features which brings a notable further improvement (Section 4).</bodyText>
<sectionHeader confidence="0.964744" genericHeader="method">
2 Event Extraction Task
</sectionHeader>
<bodyText confidence="0.988765833333333">In this paper we focus on the event extraction task defined in Automatic Content Extraction (ACE) evaluation.1 The task defines 8 event types and 33 subtypes such as Attack, End-Position etc. We introduce the terminology of the ACE event extraction that we used in this paper:</bodyText>
<footnote confidence="0.980477">
1http://projects.ldc.upenn.edu/ace/
</footnote>
<listItem confidence="0.999182818181818">• Event mention: an occurrence of an event with a particular type and subtype. • Event trigger: the word most clearly expresses the event mention. • Event argument: an entity mention, temporal expression or value (e.g. Job-Title) that serves as a participant or attribute with a specific role in an event mention. • Event mention: an instance that includes one event trigger and some arguments that appear within the same sentence.</listItem>
<bodyText confidence="0.99932925">Given an English text document, an event extraction system should predict event triggers with specific subtypes and their arguments from each sentence. Figure 1 depicts the event triggers and their arguments of sentence (1) in Section 1. The outcome of the entire sentence can be considered a graph in which each argument role is represented as a typed edge from a trigger to its argument. In this work, we assume that argument candidates such as entities are part of the input to the event extraction, and can be from either gold standard or IE system output.</bodyText>
<sectionHeader confidence="0.986423" genericHeader="method">
3 Joint Framework for Event Extraction
</sectionHeader>
<bodyText confidence="0.999932333333333">Based on the hypothesis that facts are interdependent, we propose to use structured perceptron with inexact search to jointly extract triggers and arguments that co-occur in the same sentence. In this section, we will describe the training and decoding algorithms for this model.</bodyText>
<subsectionHeader confidence="0.99495">
3.1 Structured perceptron with beam search
</subsectionHeader>
<bodyText confidence="0.999635333333333">Structured perceptron is an extension to the standard linear perceptron for structured prediction, which was proposed in (Collins, 2002). Given a sentence instance x E X, which in our case is a sentence with argument candidates, the structured perceptron involves the following decoding problem which finds the best configuration z E Y according to the current model w:</bodyText>
<page confidence="0.997549">
74
</page>
<equation confidence="0.9980555">
z = argmax w · f(x, y') (1)
y'EY(x)
</equation>
<bodyText confidence="0.999926555555555">where f(x, y') represents the feature vector for instance x along with configuration y'. The perceptron learns the model w in an online fashion. Let D = {(x(j), y(j))}n j=1 be the set of training instances (with j indexing the current training instance). In each iteration, the algorithm finds the best configuration z for x under the current model (Eq. 1). If z is incorrect, the weights are updated as follows:</bodyText>
<equation confidence="0.997594">
w = w + f(x, y) − f(x, z) (2)
</equation>
<bodyText confidence="0.979876794871795">The key step of the training and test is the decoding procedure, which aims to search for the best configuration under the current parameters. In simpler tasks such as part-of-speech tagging and noun phrase chunking, efficient dynamic programming algorithms can be employed to perform exact inference. Unfortunately, it is intractable to perform the exact search in our framework because: (1) by jointly modeling the trigger labeling and argument labeling, the search space becomes much more complex. (2) we propose to make use of arbitrary global features, which makes it infeasible to perform exact inference efficiently. To address this problem, we apply beam-search along with early-update strategy to perform inexact decoding. Collins and Roark (2004) proposed the early-update idea, and Huang et al. (2012) later proved its convergence and formalized a general framework which includes it as a special case. Figure 2 describes the skeleton of perceptron training algorithm with beam search. In each step of the beam search, if the prefix of oracle assignment y falls out from the beam, then the top result in the beam is returned for early update. One could also use the standard-update for inference, however, with highly inexact search the standardupdate generally does not work very well because of “invalid updates”, i.e., updates that do not fix a violation (Huang et al., 2012). In Section 4.5 we will show that the standard perceptron introduces many invalid updates especially with smaller beam sizes, also observed by Huang et al. (2012). To reduce overfitting, we used averaged parameters after training to decode test instances in our experiments. The resulting model is called averaged perceptron (Collins, 2002). Input: Training set D = {(x(j), y(j))}ni=1, maximum iteration number T Output: Model parameters w</bodyText>
<listItem confidence="0.883606333333333">1 Initialization: Set w = 0; 2 for t ← 1...T do 3 foreach (x, y) E D do</listItem>
<figure confidence="0.918182666666667">
4 z ← beamSearch (x, y, w)
5 if z =� y then
6 w ← w + f(x, y[1:|z|]) − f(x, z)
</figure>
<figureCaption confidence="0.997846333333333">
Figure 2: Perceptron training with beam-
search (Huang et al., 2012). Here y[1:i] de-
notes the prefix of y that has length i, e.g.,
y[1:3] = (y1, y2, y3).</figureCaption>
<subsectionHeader confidence="0.999104">
3.2 Label sets
</subsectionHeader>
<bodyText confidence="0.999948714285715">Here we introduce the label sets for trigger and argument in the model. We use G U {+} to denote the trigger label alphabet, where G represents the 33 event subtypes, and + indicates that the token is not a trigger. Similarly, R U {+} denotes the argument label sets, where R is the set of possible argument roles, and + means that the argument candidate is not an argument for the current trigger. It is worth to note that the set R of each particular event subtype is subject to the entity type constraints defined in the official ACE annotation guideline2. For example, the Attacker argument for an Attack event can only be one of PER, ORG and GPE (Geo-political Entity).</bodyText>
<subsectionHeader confidence="0.998158">
3.3 Decoding
</subsectionHeader>
<bodyText confidence="0.9999275">Let x = ((x1, x2, ..., xs), £) denote the sentence instance, where xi represents the i-th token in the sentence and £ = {ek}mk=1 is the set of argument candidates. We use to denote the corresponding gold standard structure, where ti represents the trigger assignment for the token xi, and ai,k represents the argument role label for the edge between xi and argument candidate ek.</bodyText>
<equation confidence="0.730434">
y = (t1, a1,1, ... , a1,m, ... , ts, as,1, ... , as,m)
</equation>
<footnote confidence="0.9471005">
2http://projects.ldc.upenn.edu/ace/docs/English-Events-
Guidelines v5.4.3.pdf
</footnote>
<page confidence="0.941443">
75
</page>
<equation confidence="0.946419">
g(1) g(2) h(2,1) h(3, 2)
y = (t1, a1,1, a1,2, t2, a2,1, a2,2, t3, a3,1, a3,2)
\ Y J
X2
</equation>
<figureCaption confidence="0.998295">
Figure 3: Example notation with s = 3, m = 2.
</figureCaption>
<bodyText confidence="0.517059">For simplicity, throughout this paper we use yg(i) and yh(i,k) to represent ti and ai,k, respectively. Figure 3 demonstrates the notation with s = 3 and m = 2. The variables for the toy sentence “Jobs founded Apple” are as follows:</bodyText>
<figure confidence="0.995171035087719">
y = (⊥, ⊥, ⊥, Start Org, Agent, Org � 1. ,⊥,⊥,⊥)
v v �
t2
args for founded
x2
x = h(Jobs, founded, Apple),
6
� �� �
{JobsPER, AppleORG}i
Input: Instance x =
..., xs),
and
the oracle output y if for training.
K: Beam size.
trigger label alphabet.
R
argument label alphabet.
Output: 1-best prediction z for x
1 Set beam
[E] /*empty configuration*/
2 for i
do
3 buf
4 if
then
5 return
/*for early-update*/
6 for ek
do /*search for arguments*/
7 buf
R}
h(x1,x2,
Ei
L∪{⊥}:
∪{⊥}:
B←
←1...s
←{z' ◦ l  |z' ∈ B,l ∈ L ∪ {⊥}}B←K-best(buf )
y[1:g(i)]∈6B
B[0]
∈E
←∅8
∈B
←
∪
◦⊥}
=6⊥
←
∪
◦
|
∈
B←K-best(buf )
y[1:h(i,k)]∈6B
B[0]
B[0]
argument candidates
</figure>
<bodyText confidence="0.821912857142857">to label the edges between each argument candidate and the trigger. After labeling each argument candidate, we again score each partial assignment an E d select the K-best results to the beam.</bodyText>
<subsectionHeader confidence="0.436806">
ment can
</subsectionHeader>
<bodyText confidence="0.76606075">didates may be affected by earlier argument assignments. The overall time complexity for decoding is O(K · s · m).</bodyText>
<figure confidence="0.965633736842105">
for z'
do
9 buf
buf
{z'
10 if z'g(i)
then /*xi is a trigger*/
11 buf
buf
{z'
r
r
12
13 if
then
14 return
/*for early-update*/
15 return
stance f in f is afunction f : X
</figure>
<bodyText confidence="0.968586285714286">][8, which maps x and y to a feature value. Local features are only related to predictions on individual trigger or argument. In the case of unigram tagging for trigger labeling, each local feature takes the form of i, where i denotes the index of the current token, and yg(i) is its trigger label. In practice, it is convenient to define the local feature function as an ×Y→ f(x, yg(i)), indicator function, for example:</bodyText>
<listItem confidence="0.6581786">76 Figure 4: Decoding algorithm for event extraction. means appending label l to the end of z. Duri 1 if</listItem>
<equation confidence="0.977948">
=Attack an
�
ys(i)
d xi = “fire”
f1(x, i, yg(i)) =
</equation>
<bodyText confidence="0.9960187">The global features, by contrast, involve longer range of the output structure. Formally, each global feature function takes the form of f(x, i, k, y), where i and k denote the indices of the current token and argument candidate in decoding, respectively. The following indicator function is a simple example of global features: Figure 4 describes the beam-search procedure with early-update for event extraction. During each step with token i, there are two sub-steps:</bodyText>
<listItem confidence="0.965857692307692">• Trigger labeling We enumerate all possible trigger labels for the current token. The linear model defined in Eq. (1) is used to score each partial configuration. Then the K-best partial configurations are selected to the beam, assuming the beam size is K. • Argument labeling After the trigger labeling step, we traverse all configurations in the beam. Once a trigger label for xi is found in the beam, the decoder searches through the After the second step, the rank of different trigger assignments can be changed because of the argument edges. Likewise, the decision on later argu-</listItem>
<subsectionHeader confidence="0.535506">
3.4 Features
</subsectionHeader>
<bodyText confidence="0.987189846153846">In this framework, we define two types of features, namely local features and global features. We first introduce the definition of local and global features in this paper, and then describe the implementation details later. Recall that in the linear model defined in Eq. (1), y) denotes the features extracted from the input instan f(x, ce x along z◦l ng test, lines 4-5 &amp; 13-14 are omitted. with configuration y. In general, each feature inversions of q(yg(i), yh(i,k)):
q0(yg(i), yh(i,k)) = { yh(i,k) if yh(i,k) is Place,
q1(yg(i), yh(i,k)) = Time or None
yg(i) ◦ yh(i,k) otherwise
�
1 if yh(i,k) #None
0 otherwise</bodyText>
<equation confidence="0.9682044">
1 if ys(i) =Attack and
y has only one “Attacker”
0 otherwise
0 otherwise
f101(x, i,k, y) =
</equation>
<table confidence="0.874597666666667">
⎧
⎨⎪
⎪⎩
Category Type Feature Description
Trigger Lexical 1. unigrams/bigrams of the current and context words within the window of size 2
2. unigrams/bigrams of part-of-speech tags of the current and context words within the
window of size 2
3. lemma and synonyms of the current token
4. base form of the current token extracted from Nomlex (Macleod et al., 1998)
5. Brown clusters that are learned from ACE English corpus (Brown et al., 1992; Miller et
al., 2004; Sun et al., 2011). We used the clusters with prefixes of length 13, 16 and 20 for
each token.
Syntactic 6. dependent and governor words of the current token
7. dependency types associated the current token
8. whether the current token is a modifier of job title
9. whether the current token is a non-referential pronoun
Entity 10. unigrams/bigrams normalized by entity types
Information 11. dependency features normalized by entity types
12. nearest entity type and string in the sentence/clause
Argument Basic 1. context words of the entity mention
2. trigger word and subtype
3. entity type, subtype and entity role if it is a geo-political entity mention
4. entity mention head, and head of any other name mention from co-reference chain
5. lexical distance between the argument candidate and the trigger
6. the relative position between the argument candidate and the trigger: {before, after,
overlap, or separated by punctuation}
7. whether it is the nearest argument candidate with the same type
8. whether it is the only mention of the same entity type in the sentence
Syntactic 9. dependency path between the argument candidate and the trigger
10. path from the argument candidate and the trigger in constituent parse tree
11. length of the path between the argument candidate and the trigger in dependency graph
12. common root node and its depth of the argument candidate and parse tree
13. whether the argument candidate and the trigger appear in the same clause
</table>
<tableCaption confidence="0.987782">
Table 1: Local features.
</tableCaption>
<subsectionHeader confidence="0.656546">
3.4.1 Local features
</subsectionHeader>
<bodyText confidence="0.997103142857143">In general there are two kinds of local features: Trigger features The local feature function for trigger labeling can be factorized as f(x, i, yg(i)) = p(x, i) ◦ q(yg(i)), where p(x, i) is a predicate about the input, which we call text feature, and q(yg(i)) is a predicate on the trigger label. In practice, we define two versions of q(yg(i)):</bodyText>
<equation confidence="0.991665">
q0(yg(i)) = yg(i) (event subtype)
q1(yg(i)) = event type of yg(i)
</equation>
<bodyText confidence="0.998834">q1(yg(i)) is a backoff version of the standard unigram feature. Some text features for the same event type may share a certain distributional similarity regardless of the subtypes. For example, if the nearest entity mention is “Company”, the current token is likely to be Personnel no matter whether it is End-Postion or Start-Position. Argument features Similarly, the local feature function for argument labeling can be represented as f(x, i, k, yg(i), yh(i,k)) = p(x, i, k) ◦ q(yg(i), yh(i,k)), where yh(i,k) denotes the argument assignment for the edge between trigger word i and argument candidate ek. We define two It is notable that Place and Time arguments are applicable and behave similarly to all event subtypes. Therefore features for these arguments are not conjuncted with trigger labels. q1(yh(i,k)) can be considered as a backoff version of q0(yh(i,k)), which does not discriminate different argument roles but only focuses on argument identification. Table 1 summarizes the text features about the input for trigger and argument labeling. In our experiments, we used the Stanford parser (De Marneffe et al., 2006) to create dependency parses.</bodyText>
<subsubsectionHeader confidence="0.778256">
3.4.2 Global features
</subsubsectionHeader>
<bodyText confidence="0.977091">Table 2 summarizes the 8 types of global features we developed in this work. They can be roughly divided into the following two categories:</bodyText>
<page confidence="0.996875">
77
</page>
<table confidence="0.374847454545455">
Category Feature Description
Trigger 1. bigram of trigger types occur in the same sentence or the same clause
2. binary feature indicating whether synonyms in the same sentence have the same trigger label
3. context and dependency paths between two triggers conjuncted with their types
Argument 4. context and dependency features about two argument candidates which share the same role within the
same event mention
5. features about one argument candidate which plays as arguments in two event mentions in the same
sentence
6. features about two arguments of an event mention which are overlapping
7. the number of arguments with each role type of an event mention conjuncted with the event subtype
8. the pairs of time arguments within an event mention conjuncted with the event subtype
</table>
<tableCaption confidence="0.991487">
Table 2: Global features.
</tableCaption>
<figure confidence="0.999550636363636">
(transport)
conj and
(women) (children)
(a)
(cameramen)
advcl
(died) (fired)
(b)
(resigned)
[ [ ]]
(c)
</figure>
<figureCaption confidence="0.999761">
Figure 5: Illustration of global features (4-6) in Table 2.
</figureCaption>
<bodyText confidence="0.862616259259259">Event Probability Attack Die Transport Injure Meet Table 3: Top 5 event subtypes that co-occur with Attack event in the same sentence. Trigger global feature This type of feature captures the dependencies between two triggers within the same sentence. For instance: feature (1) captures the co-occurrence of trigger types. This kind of feature is motivated by the fact that two event mentions in the same sentence tend to be semantically coherent. As an example, from Table 3 we can see that Attack event often co-occur with Die event in the same sentence, but rarely co-occur with Start-Position event. Feature (2) encourages synonyms or identical tokens to have the same label. Feature (3) exploits the lexical and syntactic relation between two triggers. A simple example is whether an Attack trigger and a Die trigger are linked by the dependency relation conj and. Argument global feature This type of feature is defined over multiple arguments for the same or different triggers. Consider the following sentence:</bodyText>
<listItem confidence="0.68849">(3) Trains running to southern Sudan were used to transport abducted women and children.</listItem>
<bodyText confidence="0.991167571428571">The Transport event mention “transport” has two Artifact arguments, “women” and “children”. The dependency edge conj and between “women” and “children” indicates that they should play the same role in the event mention. The triangle structure in Figure 5(a) is an example of feature (4) for the above example. This feature encourages entities that are linked by dependency relation conj and to play the same role Artifact in any Transport event. Similarly, Figure 5(b) depicts an example of feature (5) for sentence (1) in Section 1. In this example, an entity mention is Victim argument to Die event and Target argument to Attack event, and the two event triggers are connected by the typed dependency advcl. Here advcl means that the word “fired” is an adverbial clause modier of “died”. Figure 5(c) shows an example of feature (6) for the following sentence: (4) Barry Diller resigned as co-chief executive of Vivendi Universal Entertainment. The job title “co-chief executive of Vivendi Universal Entertainment” overlaps with the Organization mention “Vivendi Universal Entertainment”. The feature in the triangle shape can be considered as a soft constraint such that if a JobTitle mention is a Position argument to an EndPosition trigger, then the Organization mention which appears at the end of it should be labeled as Entity argument for the same trigger.</bodyText>
<figure confidence="0.9925548">
0.34
0.14
0.08
0.04
0.02
</figure>
<page confidence="0.994275">
78
</page>
<bodyText confidence="0.999961777777778">Feature (7-8) are based on the statistics about different arguments for the same trigger. For instance, in many cases, a trigger can only have one Place argument. If a partial configuration mistakenly classifies more than one entity mention as Place arguments for the same trigger, then it will be penalized.</bodyText>
<sectionHeader confidence="0.999882" genericHeader="evaluation and result">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999973">
4.1 Data set and evaluation metric
</subsectionHeader>
<bodyText confidence="0.99982775">We utilized the ACE 2005 corpus as our testbed. For comparison, we used the same test set with 40 newswire articles (672 sentences) as in (Ji and Grishman, 2008; Liao and Grishman, 2010) for the experiments, and randomly selected 30 other documents (863 sentences) from different genres as the development set. The rest 529 documents (14, 840 sentences) are used for training. Following previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011), we use the following criteria to determine the correctness of an predicted event mention:</bodyText>
<listItem confidence="0.850253111111111">• A trigger is correct if its event subtype and offsets match those of a reference trigger. • An argument is correctly identified if its event subtype and offsets match those of any of the reference argument mentions. • An argument is correctly identified and classified if its event subtype, offsets and argument role match those of any of the reference argument mentions.</listItem>
<bodyText confidence="0.998676">Finally we use Precision (P), Recall (R) and Fmeasure (F1) to evaluate the overall performance.</bodyText>
<subsectionHeader confidence="0.99885">
4.2 Baseline system
</subsectionHeader>
<bodyText confidence="0.9999539">Chen and Ng (2012) have proven that performing identification and classification in one step is better than two steps. To compare our proposed method with the previous pipelined approaches, we implemented two Maximum Entropy (MaxEnt) classifiers for trigger labeling and argument labeling respectively. To make a fair comparison, the feature sets in the baseline are identical to the local text features we developed in our framework (see Figure 1).</bodyText>
<subsectionHeader confidence="0.999833">
4.3 Training curves
</subsectionHeader>
<bodyText confidence="0.999068333333333">We use the harmonic mean of the trigger’s F1 measure and argument’s F1 measure to measure the performance on the development set.</bodyText>
<figure confidence="0.997774727272727">
0.60
0.58
0.56
0.54
0.52
0.50
0.48
local+global
local
0.441 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
# of training iteration
</figure>
<figureCaption confidence="0.999926">
Figure 6: Training curves on dev set.
</figureCaption>
<bodyText confidence="0.992242625">Figure 6 shows the training curves of the averaged perceptron with respect to the performance on the development set when the beam size is 4. As we can see both curves converge around iteration 20 and the global features improve the overall performance, compared to its counterpart with only local features. Therefore we set the number of iterations as 20 in the remaining experiments.</bodyText>
<subsectionHeader confidence="0.999902">
4.4 Impact of beam size
</subsectionHeader>
<bodyText confidence="0.9999965">The beam size is an important hyper parameter in both training and test. Larger beam size will increase the computational cost while smaller beam size may reduce the performance. Table 4 shows the performance on the development set with several different beam sizes. When beam size = 4, the algorithm achieved the highest performance on the development set with trigger F1 = 67.9, argument F1 = 51.5, and harmonic mean = 58.6. When the size is increased to 32, the accuracy was not improved. Based on this observation, we chose beam size as 4 for the remaining experiments.</bodyText>
<subsectionHeader confidence="0.979394">
4.5 Early-update vs. standard-update
</subsectionHeader>
<bodyText confidence="0.999429">Huang et al. (2012) define “invalid update” to be an update that does not fix a violation (and instead reinforces the error), and show that it strongly (anti-)correlates with search quality and learning quality. Figure 7 depicts the percentage of invalid updates in standard-update with and without global features, respectively. With global features, there are numerous invalid updates when the</bodyText>
<figure confidence="0.8607485">
Harmonic mean
0.46
</figure>
<page confidence="0.95674">
79
</page>
<table confidence="0.995868666666667">
Beam size 1 2 4 8 16 32
Training time (sec) 993 2,034 3,982 8,036 15,878 33,026
Harmonic mean 57.6 57.7 58.6 58.0 57.8 57.8
</table>
<tableCaption confidence="0.999766">
Table 4: Comparison of training time and accuracy on the dev set.
</tableCaption>
<figureCaption confidence="0.987576">
Figure 7: Percentage of the so-called “invalid up-
dates” (Huang et al., 2012) in standard perceptron.
</figureCaption>
<table confidence="0.998184">
Strategy F1 on Dev F1 on Test
Trigger Arg Trigger Arg
Standard (b = 1) 68.3 47.4 64.4 49.8
Early (b = 1) 68.9 49.5 65.2 52.1
Standard (b = 4) 68.4 50.5 67.1 51.4
Early (b = 4) 67.9 51.5 67.5 52.7
</table>
<tableCaption confidence="0.996967">
Table 5: Comparison between the performance
</tableCaption>
<bodyText confidence="0.97354975">(%) of standard-update and early-update with global features. Here b stands for beam size. beam size is small. The ratio decreases monotonically as beam size increases. The model with only local features made much smaller numbers of invalid updates, which suggests that the use of global features makes the search problem much harder. This observation justify the application of early-update in this work. To further investigate the difference between early-update and standardupdate, we tested the performance of both strategies, which is summarized in Table 5. As we can see the performance of standard-update is generally worse than early-update. When the beam size is increased (b = 4), the gap becomes smaller as the ratio of invalid updates is reduced.</bodyText>
<subsectionHeader confidence="0.998023">
4.6 Overall performance
</subsectionHeader>
<bodyText confidence="0.999970605263158">Table 6 shows the overall performance on the blind test set. In addition to our baseline, we compare against the sentence-level system reported in Hong et al. (2011), which, to the best of our knowledge, is the best-reported system in the literature based on gold standard argument candidates. The proposed joint framework with local features achieves comparable performance for triggers and outperforms the staged baseline especially on arguments. By adding global features, the overall performance is further improved significantly. Compared to the staged baseline, it gains 1.6% improvement on trigger’s F-measure and 8.8% improvement on argument’s F-measure. Remarkably, compared to the cross-entity approach reported in (Hong et al., 2011), which attained 68.3% F1 for triggers and 48.3% for arguments, our approach with global features achieves even better performance on argument labeling although we only used sentencelevel information. We also tested the performance with argument candidates automatically extracted by a highperforming name tagger (Li et al., 2012b) and an IE system (Grishman et al., 2005). The results are summarized in Table 7. The joint approach with global features significantly outperforms the baseline and the model with only local features. We also show that it outperforms the sentencelevel baseline reported in (Ji and Grishman, 2008; Liao and Grishman, 2010), both of which attained 59.7% F1 for triggers and 36.6% for arguments. Our approach aims to tackle the problem of sentence-level event extraction, thereby only used intra-sentential evidence. Nevertheless, the performance of our approach is still comparable with the best-reported methods based on cross-document and cross-event inference (Ji and Grishman, 2008; Liao and Grishman, 2010).</bodyText>
<sectionHeader confidence="0.999971" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.998807777777778">Most recent studies about ACE event extraction rely on staged pipeline which consists of separate local classifiers for trigger labeling and argument labeling (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012a; Chen and Ng, 2012). To the best of our knowledge, our work is the first attempt to jointly model these two ACE event subtasks.</bodyText>
<figure confidence="0.996135428571429">
% of invalid updates
0.40
0.30
0.20
0.10
0.001 2 4 8 16 32
beam size
0.45
0.35
0.25
0.15
0.05
local+global
local
</figure>
<page confidence="0.961206">
80
</page>
<table confidence="0.998361">
Methods Trigger Trigger Identification Argument Argument Role (%)
Identification (%) + classification (%) Identification (%)
P R F1 P R F1 P R F1 P R F1
Sentence-level in Hong et al. (2011) N/A 67.6 53.5 59.7 46.5 37.15 41.3 41.0 32.8 36.5
Staged MaxEnt classifiers 76.2 60.5 67.4 74.5 59.1 65.9 74.1 37.4 49.7 65.4 33.1 43.9
Joint w/ local features 77.4 62.3 69.0 73.7 59.3 65.7 69.7 39.6 50.5 64.1 36.5 46.5
Joint w/ local + global features 76.9 65.0 70.4 73.7 62.3 67.5 69.8 47.9 56.8 64.7 44.4 52.7
Cross-entity in Hong et al. (2011)† N/A 72.9 64.3 68.3 53.4 52.9 53.1 51.6 45.5 48.3
</table>
<tableCaption confidence="0.975844">
Table 6: Overall performance with gold-standard entities, timex, and values. †beyond sentence level.
</tableCaption>
<table confidence="0.999915125">
Methods Trigger F1 Arg F1
Ji and Grishman (2008) 67.3 42.6
cross-doc Inference
Ji and Grishman (2008) 59.7 36.6
sentence-level
MaxEnt classifiers 64.7 (↓1.2) 33.7 (↓10.2)
Joint w/ local 63.7 (↓2.0) 35.8 (↓10.7)
Joint w/ local + global 65.6 (↓1.9) 41.8 (↓10.9)
</table>
<tableCaption confidence="0.99334">
Table 7: Overall performance (%) with predicted
entities, timex, and values.</tableCaption>
<bodyText confidence="0.970722882352941">↓ indicates the performance drop from experiments with gold-standard argument candidates (see Table 6). For the Message Understanding Conference (MUC) and FAS Program for Monitoring Emerging Diseases (ProMED) event extraction tasks, Patwardhan and Riloff (2009) proposed a probabilistic framework to extract event role fillers conditioned on the sentential event occurrence. Besides having different task definitions, the key difference from our approach is that their role filler recognizer and sentential event recognizer are trained independently but combined in the test stage. Our experiments, however, have demonstrated that it is more advantageous to do both training and testing with joint inference. There has been some previous work on joint modeling for biomedical events (Riedel and McCallum, 2011a; Riedel et al., 2009; McClosky et al., 2011; Riedel and McCallum, 2011b). (McClosky et al., 2011) is most closely related to our approach. They casted the problem of biomedical event extraction as a dependency parsing problem. The key assumption that event structure can be considered as trees is incompatible with ACE event extraction. In addition, they used a separate classifier to predict the event triggers before applying the parser, while we extract the triggers and argument jointly. Finally, the features in the parser are edge-factorized. To exploit global features, they applied a MaxEnt-based global re-ranker. In comparison, our approach is a unified framework based on beam search, which allows us to exploit arbitrary global features efficiently.</bodyText>
<sectionHeader confidence="0.999072" genericHeader="conclusion">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999992">We presented a joint framework for ACE event extraction based on structured perceptron with inexact search. As opposed to traditional pipelined approaches, we re-defined the task as a structured prediction problem. The experiments proved that the perceptron with local features outperforms the staged baseline and the global features further improve the performance significantly, surpassing the current state-of-the-art by a large margin. As shown in Table 7, the overall performance drops substantially when using predicted argument candidates. To improve the accuracy of endto-end IE system, we plan to develop a complete joint framework to recognize entities together with event mentions for future work. Also we are interested in applying this framework to other IE tasks such as relation extraction.</bodyText>
<sectionHeader confidence="0.998381" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.958184875">This work was supported by the U.S. Army Research Laboratory under Cooperative Agreement No. W911NF-09-2-0053 (NS-CTA), U.S. NSF CAREER Award under Grant IIS-0953149, U.S. NSF EAGER Award under Grant No. IIS1144111, U.S. DARPA Award No. FA8750-13-20041 in the “Deep Exploration and Filtering of Text” (DEFT) Program, a CUNY Junior Faculty Award, and Queens College equipment funds. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.</bodyText>
<page confidence="0.99805">
81
</page>
<sectionHeader confidence="0.99588" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99987595049505">
David Ahn. 2006. The stages of event extraction.
In Proceedings of the Workshop on Annotating and
Reasoning about Time and Events, pages 1–8.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467–479.
Zheng Chen and Heng Ji. 2009. Language specific
issue and feature exploration in chinese event ex-
traction. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Companion Volume: Short Pa-
pers, pages 209–212.
Chen Chen and Vincent Ng. 2012. Joint modeling for
chinese event extraction with rich linguistic features.
In COLING, pages 529–544.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, page 111.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing-Volume 10, pages 1–8.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, volume 6, pages 449–454.
Ralph Grishman, David Westbrook, and Adam Meyers.
2005. Nyu’s english ace 2005 system description.
In Proceedings of ACE 2005 Evaluation Workshop.
Washington.
Yu Hong, Jianfeng Zhang, Bin Ma, Jian-Min Yao,
Guodong Zhou, and Qiaoming Zhu. 2011. Using
cross-entity inference to improve event extraction.
In Proceedings of ACL, pages 1127–1136.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
142–151.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Pro-
ceedings of ACL, pages 254–262.
Peifeng Li, Guodong Zhou, Qiaoming Zhu, and Li-
bin Hou. 2012a. Employing compositional seman-
tics and discourse consistency in chinese event ex-
traction. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 1006–1016.
Qi Li, Haibo Li, Heng Ji, Wen Wang, Jing Zheng, and
Fei Huang. 2012b. Joint bilingual name tagging for
parallel corpora. In Proceedings of the 21st ACM
international conference on Information and knowl-
edge management, pages 1727–1731.
Shasha Liao and Ralph Grishman. 2010. Using doc-
ument level cross-event inference to improve event
extraction. In Proceedings of ACL, pages 789–797.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. Nomlex: A
lexicon of nominalizations. In Proceedings of EU-
RALEX, volume 98, pages 187–193.
David McClosky, Mihai Surdeanu, and Christopher D.
Manning. 2011. Event extraction as dependency
parsing. In Proceedings of ACL, pages 1626–1635.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrim-
inative training. In Proceedings of HLT-NAACL,
volume 4, pages 337–342.
Siddharth Patwardhan and Ellen Riloff. 2009. A uni-
fied model of phrasal and sentential evidence for in-
formation extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 1-Volume 1, pages 151–
160.
Sebastian Riedel and Andrew McCallum. 2011a. Fast
and robust joint models for biomedical event extrac-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 1–
12.
Sebastian Riedel and Andrew McCallum. 2011b. Ro-
bust biomedical event extraction with dual decom-
position and minimal domain adaptation. In Pro-
ceedings of the BioNLP Shared Task 2011 Work-
shop, pages 46–50.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun’ichi Tsujii. 2009. A markov logic approach
to bio-molecular event extraction. In Proceedings
of the Workshop on Current Trends in Biomedical
Natural Language Processing: Shared Task, pages
41–49.
Ang Sun, Ralph Grishman, and Satoshi Sekine. 2011.
Semi-supervised relation extraction with large-scale
word clustering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
521–529.
</reference>
<page confidence="0.999128">
82
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant confidence="0.842988" no="0">
<title confidence="0.999254">Joint Event Extraction via Structured Prediction with Global Features</title>
<author confidence="0.999192">Qi Li Heng Ji Liang Huang</author>
<affiliation confidence="0.961417333333333">Departments of Computer Science and The Graduate Center and Queens City University of New</affiliation>
<address confidence="0.986265">New York, NY 10016,</address>
<abstract confidence="0.997516708333333">Traditional approaches to the task of ACE event extraction usually rely on sequential pipelines with multiple stages, which suffer from error propagation since event triggers and arguments are predicted in isolation by independent local classifiers. By contrast, we propose a joint framework based on structured prediction which extracts triggers and arguments together so that the local predictions can be mutually improved. In addition, we propose to incorporate global features which explicitly capture the dependencies of multiple triggers and arguments. Experimental results show that our joint approach with local features outperforms the pipelined baseline, and adding global features further improves the performance significantly. Our approach advances state-ofthe-art sentence-level event extraction, and even outperforms previous argument labeling methods which use external knowledge from other sentences and documents.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Ahn</author>
</authors>
<title>The stages of event extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Annotating and Reasoning about Time and Events,</booktitle>
<pages>1--8</pages>
<contexts>
<context citStr="Ahn, 2006" endWordPosition="4915" position="29336" startWordPosition="4914">shman, 2010), both of which attained 59.7% F1 for triggers and 36.6% for arguments. Our approach aims to tackle the problem of sentence-level event extraction, thereby only used intra-sentential evidence. Nevertheless, the performance of our approach is still comparable with the best-reported methods based on cross-document and cross-event inference (Ji and Grishman, 2008; Liao and Grishman, 2010). 5 Related Work Most recent studies about ACE event extraction rely on staged pipeline which consists of separate local classifiers for trigger labeling and argument labeling (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012a; Chen and Ng, 2012). To the best of our knowledge, our work is the first attempt to jointly model these two ACE event subtasks. % of invalid updates 0.40 0.30 0.20 0.10 0.001 2 4 8 16 32 beam size 0.45 0.35 0.25 0.15 0.05 local+global local 80 Methods Trigger Trigger Identification Argument Argument Role (%) Identification (%) + classification (%) Identification (%) P R F1 P R F1 P R F1 P R F1 Sentence-level in Hong et al. (2011) N/A 67.6 53.5 59.7 46.5 37.15 41.3 41.0 32.8 36.5 Staged MaxE</context>
</contexts>
<marker>Ahn, 2006</marker>
<rawString>David Ahn. 2006. The stages of event extraction. In Proceedings of the Workshop on Annotating and Reasoning about Time and Events, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V Desouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context citStr="Brown et al., 1992" endWordPosition="2617" position="15473" startWordPosition="2614">st, lines 4-5 &amp; 13-14 are omitted. with configuration y. In general, each feature in1 if ys(i) =Attack and y has only one “Attacker” 0 otherwise 0 otherwise f101(x, i,k, y) = ⎧ ⎨⎪ ⎪⎩ Category Type Feature Description Trigger Lexical 1. unigrams/bigrams of the current and context words within the window of size 2 2. unigrams/bigrams of part-of-speech tags of the current and context words within the window of size 2 3. lemma and synonyms of the current token 4. base form of the current token extracted from Nomlex (Macleod et al., 1998) 5. Brown clusters that are learned from ACE English corpus (Brown et al., 1992; Miller et al., 2004; Sun et al., 2011). We used the clusters with prefixes of length 13, 16 and 20 for each token. Syntactic 6. dependent and governor words of the current token 7. dependency types associated the current token 8. whether the current token is a modifier of job title 9. whether the current token is a non-referential pronoun Entity 10. unigrams/bigrams normalized by entity types Information 11. dependency features normalized by entity types 12. nearest entity type and string in the sentence/clause Argument Basic 1. context words of the entity mention 2. trigger word and subtype</context>
</contexts>
<marker>Brown, Desouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based n-gram models of natural language. Computational linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng Chen</author>
<author>Heng Ji</author>
</authors>
<title>Language specific issue and feature exploration in chinese event extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers,</booktitle>
<pages>209--212</pages>
<contexts>
<context citStr="Chen and Ji, 2009" endWordPosition="4923" position="29378" startWordPosition="4920">ed 59.7% F1 for triggers and 36.6% for arguments. Our approach aims to tackle the problem of sentence-level event extraction, thereby only used intra-sentential evidence. Nevertheless, the performance of our approach is still comparable with the best-reported methods based on cross-document and cross-event inference (Ji and Grishman, 2008; Liao and Grishman, 2010). 5 Related Work Most recent studies about ACE event extraction rely on staged pipeline which consists of separate local classifiers for trigger labeling and argument labeling (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012a; Chen and Ng, 2012). To the best of our knowledge, our work is the first attempt to jointly model these two ACE event subtasks. % of invalid updates 0.40 0.30 0.20 0.10 0.001 2 4 8 16 32 beam size 0.45 0.35 0.25 0.15 0.05 local+global local 80 Methods Trigger Trigger Identification Argument Argument Role (%) Identification (%) + classification (%) Identification (%) P R F1 P R F1 P R F1 P R F1 Sentence-level in Hong et al. (2011) N/A 67.6 53.5 59.7 46.5 37.15 41.3 41.0 32.8 36.5 Staged MaxEnt classifiers 76.2 60.5 67.4 74.5 59.1 65</context>
</contexts>
<marker>Chen, Ji, 2009</marker>
<rawString>Zheng Chen and Heng Ji. 2009. Language specific issue and feature exploration in chinese event extraction. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, pages 209–212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Chen</author>
<author>Vincent Ng</author>
</authors>
<title>Joint modeling for chinese event extraction with rich linguistic features.</title>
<date>2012</date>
<booktitle>In COLING,</booktitle>
<pages>529--544</pages>
<contexts>
<context citStr="Chen and Ng (2012)" endWordPosition="4008" position="23834" startWordPosition="4005">Hong et al., 2011), we use the following criteria to determine the correctness of an predicted event mention: • A trigger is correct if its event subtype and offsets match those of a reference trigger. • An argument is correctly identified if its event subtype and offsets match those of any of the reference argument mentions. • An argument is correctly identified and classified if its event subtype, offsets and argument role match those of any of the reference argument mentions. Finally we use Precision (P), Recall (R) and Fmeasure (F1) to evaluate the overall performance. 4.2 Baseline system Chen and Ng (2012) have proven that performing identification and classification in one step is better than two steps. To compare our proposed method with the previous pipelined approaches, we implemented two Maximum Entropy (MaxEnt) classifiers for trigger labeling and argument labeling respectively. To make a fair comparison, the feature sets in the baseline are identical to the local text features we developed in our framework (see Figure 1). 4.3 Training curves We use the harmonic mean of the trigger’s F1 measure and argument’s F1 measure to measure the performance on the development set. 0.60 0.58 0.56 0.5</context>
<context citStr="Chen and Ng, 2012" endWordPosition="4940" position="29460" startWordPosition="4937"> problem of sentence-level event extraction, thereby only used intra-sentential evidence. Nevertheless, the performance of our approach is still comparable with the best-reported methods based on cross-document and cross-event inference (Ji and Grishman, 2008; Liao and Grishman, 2010). 5 Related Work Most recent studies about ACE event extraction rely on staged pipeline which consists of separate local classifiers for trigger labeling and argument labeling (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012a; Chen and Ng, 2012). To the best of our knowledge, our work is the first attempt to jointly model these two ACE event subtasks. % of invalid updates 0.40 0.30 0.20 0.10 0.001 2 4 8 16 32 beam size 0.45 0.35 0.25 0.15 0.05 local+global local 80 Methods Trigger Trigger Identification Argument Argument Role (%) Identification (%) + classification (%) Identification (%) P R F1 P R F1 P R F1 P R F1 Sentence-level in Hong et al. (2011) N/A 67.6 53.5 59.7 46.5 37.15 41.3 41.0 32.8 36.5 Staged MaxEnt classifiers 76.2 60.5 67.4 74.5 59.1 65.9 74.1 37.4 49.7 65.4 33.1 43.9 Joint w/ local features 77.4 62.3 69.0 73.7 59.3 </context>
</contexts>
<marker>Chen, Ng, 2012</marker>
<rawString>Chen Chen and Vincent Ng. 2012. Joint modeling for chinese event extraction with rich linguistic features. In COLING, pages 529–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>111</pages>
<contexts>
<context citStr="Collins and Roark, 2004" endWordPosition="717" position="4668" startWordPosition="714">le (1). There are two event mentions that share three arguments, namely the Die event mention triggered by “died”, and the Attack event mention triggered by “fired”. Target Instrument Target Place Victim Instrument arbitrary global features over multiple local predictions. However, different from easier tasks such as part-of-speech tagging or noun phrase chunking where efficient dynamic programming decoding is feasible, here exact joint inference is intractable. Therefore we employ beam search in decoding, and train the model using the early-update perceptron variant tailored for beam search (Collins and Roark, 2004; Huang et al., 2012). We make the following contributions: 1. Different from traditional pipeline approach, we present a novel framework for sentencelevel event extraction, which predicts triggers and their arguments jointly (Section 3). 2. We develop a rich set of features for event extraction which yield promising performance even with the traditional pipeline (Section 3.4.1). In this paper we refer to them as local features. 3. We introduce various global features to exploit dependencies among multiple triggers and arguments (Section 3.4.2). Experiments show that our approach outperforms t</context>
<context citStr="Collins and Roark (2004)" endWordPosition="1395" position="8768" startWordPosition="1392">arameters. In simpler tasks such as part-of-speech tagging and noun phrase chunking, efficient dynamic programming algorithms can be employed to perform exact inference. Unfortunately, it is intractable to perform the exact search in our framework because: (1) by jointly modeling the trigger labeling and argument labeling, the search space becomes much more complex. (2) we propose to make use of arbitrary global features, which makes it infeasible to perform exact inference efficiently. To address this problem, we apply beam-search along with early-update strategy to perform inexact decoding. Collins and Roark (2004) proposed the early-update idea, and Huang et al. (2012) later proved its convergence and formalized a general framework which includes it as a special case. Figure 2 describes the skeleton of perceptron training algorithm with beam search. In each step of the beam search, if the prefix of oracle assignment y falls out from the beam, then the top result in the beam is returned for early update. One could also use the standard-update for inference, however, with highly inexact search the standardupdate generally does not work very well because of “invalid updates”, i.e., updates that do not fix</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10,</booktitle>
<pages>1--8</pages>
<contexts>
<context citStr="Collins, 2002" endWordPosition="558" position="3609" startWordPosition="557">, we can propagate the Victim argument of the Die event to the Target argument of the Attack event. As another example, knowing that an Attack event usually only has one Attacker argument, we could penalize assignments in which one trigger has more than one Attacker. Such global features cannot be easily exploited by a local classifier. Therefore, we take a fresh look at this problem and formulate it, for the first time, as a structured learning problem. We propose a novel joint event extraction algorithm to predict the triggers and arguments simultaneously, and use the structured perceptron (Collins, 2002) to train the joint model. This way we can capture the dependencies between triggers and argument as well as explore 73 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 73–82, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Place In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel. Die Attack Figure 1: Event mentions of example (1). There are two event mentions that share three arguments, namely the Die event mention triggered by “died”, and the Attack event mention triggered by “fired”</context>
<context citStr="Collins, 2002" endWordPosition="1140" position="7296" startWordPosition="1139">s entities are part of the input to the event extraction, and can be from either gold standard or IE system output. 3 Joint Framework for Event Extraction Based on the hypothesis that facts are interdependent, we propose to use structured perceptron with inexact search to jointly extract triggers and arguments that co-occur in the same sentence. In this section, we will describe the training and decoding algorithms for this model. 3.1 Structured perceptron with beam search Structured perceptron is an extension to the standard linear perceptron for structured prediction, which was proposed in (Collins, 2002). Given a sentence instance x E X, which in our case is a sentence with argument candidates, the structured perceptron involves the following decoding prob74 lem which finds the best configuration z E Y according to the current model w: z = argmax w · f(x, y') (1) y'EY(x) where f(x, y') represents the feature vector for instance x along with configuration y'. The perceptron learns the model w in an online fashion. Let D = {(x(j), y(j))}n j=1 be the set of training instances (with j indexing the current training instance). In each iteration, the algorithm finds the best configuration z for x un</context>
<context citStr="Collins, 2002" endWordPosition="1558" position="9742" startWordPosition="1557">he beam is returned for early update. One could also use the standard-update for inference, however, with highly inexact search the standardupdate generally does not work very well because of “invalid updates”, i.e., updates that do not fix a violation (Huang et al., 2012). In Section 4.5 we will show that the standard perceptron introduces many invalid updates especially with smaller beam sizes, also observed by Huang et al. (2012). To reduce overfitting, we used averaged parameters after training to decode test instances in our experiments. The resulting model is called averaged perceptron (Collins, 2002). Input: Training set D = {(x(j), y(j))}ni=1, maximum iteration number T Output: Model parameters w 1 Initialization: Set w = 0; 2 for t ← 1...T do 3 foreach (x, y) E D do 4 z ← beamSearch (x, y, w) 5 if z =� y then 6 w ← w + f(x, y[1:|z|]) − f(x, z) Figure 2: Perceptron training with beamsearch (Huang et al., 2012). Here y[1:i] denotes the prefix of y that has length i, e.g., y[1:3] = (y1, y2, y3). 3.2 Label sets Here we introduce the label sets for trigger and argument in the model. We use G U {+} to denote the trigger label alphabet, where G represents the 33 event subtypes, and + indicates</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<volume>6</volume>
<pages>449--454</pages>
<marker>De Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine De Marneffe, Bill MacCartney, and Christopher D Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC, volume 6, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>David Westbrook</author>
<author>Adam Meyers</author>
</authors>
<title>Nyu’s english ace 2005 system description.</title>
<date>2005</date>
<booktitle>In Proceedings of ACE 2005 Evaluation Workshop.</booktitle>
<location>Washington.</location>
<contexts>
<context citStr="Grishman et al., 2005" endWordPosition="4781" position="28459" startWordPosition="4778">mance is further improved significantly. Compared to the staged baseline, it gains 1.6% improvement on trigger’s F-measure and 8.8% improvement on argument’s F-measure. Remarkably, compared to the cross-entity approach reported in (Hong et al., 2011), which attained 68.3% F1 for triggers and 48.3% for arguments, our approach with global features achieves even better performance on argument labeling although we only used sentencelevel information. We also tested the performance with argument candidates automatically extracted by a highperforming name tagger (Li et al., 2012b) and an IE system (Grishman et al., 2005). The results are summarized in Table 7. The joint approach with global features significantly outperforms the baseline and the model with only local features. We also show that it outperforms the sentencelevel baseline reported in (Ji and Grishman, 2008; Liao and Grishman, 2010), both of which attained 59.7% F1 for triggers and 36.6% for arguments. Our approach aims to tackle the problem of sentence-level event extraction, thereby only used intra-sentential evidence. Nevertheless, the performance of our approach is still comparable with the best-reported methods based on cross-document and cr</context>
</contexts>
<marker>Grishman, Westbrook, Meyers, 2005</marker>
<rawString>Ralph Grishman, David Westbrook, and Adam Meyers. 2005. Nyu’s english ace 2005 system description. In Proceedings of ACE 2005 Evaluation Workshop. Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu Hong</author>
<author>Jianfeng Zhang</author>
<author>Bin Ma</author>
<author>Jian-Min Yao</author>
<author>Guodong Zhou</author>
<author>Qiaoming Zhu</author>
</authors>
<title>Using cross-entity inference to improve event extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1127--1136</pages>
<contexts>
<context citStr="Hong et al., 2011" endWordPosition="221" position="1506" startWordPosition="218">joint approach with local features outperforms the pipelined baseline, and adding global features further improves the performance significantly. Our approach advances state-ofthe-art sentence-level event extraction, and even outperforms previous argument labeling methods which use external knowledge from other sentences and documents. 1 Introduction Event extraction is an important and challenging task in Information Extraction (IE), which aims to discover event triggers with specific types and their arguments. Most state-of-the-art approaches (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011) use sequential pipelines as building blocks, which break down the whole task into separate subtasks, such as trigger identification/classification and argument identification/classification. As a common drawback of the staged architecture, errors in upstream component are often compounded and propagated to the downstream classifiers. The downstream components, however, cannot impact earlier decisions. For example, consider the following sentences with an ambiguous word “fired”: (1) In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel. (2) He has fired his air defens</context>
<context citStr="Hong et al., 2011" endWordPosition="3906" position="23234" startWordPosition="3903">ies more than one entity mention as Place arguments for the same trigger, then it will be penalized. 4 Experiments 4.1 Data set and evaluation metric We utilized the ACE 2005 corpus as our testbed. For comparison, we used the same test set with 40 newswire articles (672 sentences) as in (Ji and Grishman, 2008; Liao and Grishman, 2010) for the experiments, and randomly selected 30 other documents (863 sentences) from different genres as the development set. The rest 529 documents (14, 840 sentences) are used for training. Following previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011), we use the following criteria to determine the correctness of an predicted event mention: • A trigger is correct if its event subtype and offsets match those of a reference trigger. • An argument is correctly identified if its event subtype and offsets match those of any of the reference argument mentions. • An argument is correctly identified and classified if its event subtype, offsets and argument role match those of any of the reference argument mentions. Finally we use Precision (P), Recall (R) and Fmeasure (F1) to evaluate the overall performance. 4.2 Baseline system Chen and Ng (2012)</context>
<context citStr="Hong et al. (2011)" endWordPosition="4640" position="27508" startWordPosition="4637">der. This observation justify the application of early-update in this work. To further investigate the difference between early-update and standardupdate, we tested the performance of both strategies, which is summarized in Table 5. As we can see the performance of standard-update is generally worse than early-update. When the beam size is increased (b = 4), the gap becomes smaller as the ratio of invalid updates is reduced. 4.6 Overall performance Table 6 shows the overall performance on the blind test set. In addition to our baseline, we compare against the sentence-level system reported in Hong et al. (2011), which, to the best of our knowledge, is the best-reported system in the literature based on gold standard argument candidates. The proposed joint framework with local features achieves comparable performance for triggers and outperforms the staged baseline especially on arguments. By adding global features, the overall performance is further improved significantly. Compared to the staged baseline, it gains 1.6% improvement on trigger’s F-measure and 8.8% improvement on argument’s F-measure. Remarkably, compared to the cross-entity approach reported in (Hong et al., 2011), which attained 68.3</context>
<context citStr="Hong et al., 2011" endWordPosition="4932" position="29422" startWordPosition="4929">ents. Our approach aims to tackle the problem of sentence-level event extraction, thereby only used intra-sentential evidence. Nevertheless, the performance of our approach is still comparable with the best-reported methods based on cross-document and cross-event inference (Ji and Grishman, 2008; Liao and Grishman, 2010). 5 Related Work Most recent studies about ACE event extraction rely on staged pipeline which consists of separate local classifiers for trigger labeling and argument labeling (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012a; Chen and Ng, 2012). To the best of our knowledge, our work is the first attempt to jointly model these two ACE event subtasks. % of invalid updates 0.40 0.30 0.20 0.10 0.001 2 4 8 16 32 beam size 0.45 0.35 0.25 0.15 0.05 local+global local 80 Methods Trigger Trigger Identification Argument Argument Role (%) Identification (%) + classification (%) Identification (%) P R F1 P R F1 P R F1 P R F1 Sentence-level in Hong et al. (2011) N/A 67.6 53.5 59.7 46.5 37.15 41.3 41.0 32.8 36.5 Staged MaxEnt classifiers 76.2 60.5 67.4 74.5 59.1 65.9 74.1 37.4 49.7 65.4 33.1 43.9 Joint w/ lo</context>
</contexts>
<marker>Hong, Zhang, Ma, Yao, Zhou, Zhu, 2011</marker>
<rawString>Yu Hong, Jianfeng Zhang, Bin Ma, Jian-Min Yao, Guodong Zhou, and Qiaoming Zhu. 2011. Using cross-entity inference to improve event extraction. In Proceedings of ACL, pages 1127–1136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Suphan Fayong</author>
<author>Yang Guo</author>
</authors>
<title>Structured perceptron with inexact search.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>142--151</pages>
<contexts>
<context citStr="Huang et al., 2012" endWordPosition="721" position="4689" startWordPosition="718">nt mentions that share three arguments, namely the Die event mention triggered by “died”, and the Attack event mention triggered by “fired”. Target Instrument Target Place Victim Instrument arbitrary global features over multiple local predictions. However, different from easier tasks such as part-of-speech tagging or noun phrase chunking where efficient dynamic programming decoding is feasible, here exact joint inference is intractable. Therefore we employ beam search in decoding, and train the model using the early-update perceptron variant tailored for beam search (Collins and Roark, 2004; Huang et al., 2012). We make the following contributions: 1. Different from traditional pipeline approach, we present a novel framework for sentencelevel event extraction, which predicts triggers and their arguments jointly (Section 3). 2. We develop a rich set of features for event extraction which yield promising performance even with the traditional pipeline (Section 3.4.1). In this paper we refer to them as local features. 3. We introduce various global features to exploit dependencies among multiple triggers and arguments (Section 3.4.2). Experiments show that our approach outperforms the pipelined approach</context>
<context citStr="Huang et al. (2012)" endWordPosition="1404" position="8824" startWordPosition="1401">d noun phrase chunking, efficient dynamic programming algorithms can be employed to perform exact inference. Unfortunately, it is intractable to perform the exact search in our framework because: (1) by jointly modeling the trigger labeling and argument labeling, the search space becomes much more complex. (2) we propose to make use of arbitrary global features, which makes it infeasible to perform exact inference efficiently. To address this problem, we apply beam-search along with early-update strategy to perform inexact decoding. Collins and Roark (2004) proposed the early-update idea, and Huang et al. (2012) later proved its convergence and formalized a general framework which includes it as a special case. Figure 2 describes the skeleton of perceptron training algorithm with beam search. In each step of the beam search, if the prefix of oracle assignment y falls out from the beam, then the top result in the beam is returned for early update. One could also use the standard-update for inference, however, with highly inexact search the standardupdate generally does not work very well because of “invalid updates”, i.e., updates that do not fix a violation (Huang et al., 2012). In Section 4.5 we wil</context>
<context citStr="Huang et al., 2012" endWordPosition="1626" position="10059" startWordPosition="1623">standard perceptron introduces many invalid updates especially with smaller beam sizes, also observed by Huang et al. (2012). To reduce overfitting, we used averaged parameters after training to decode test instances in our experiments. The resulting model is called averaged perceptron (Collins, 2002). Input: Training set D = {(x(j), y(j))}ni=1, maximum iteration number T Output: Model parameters w 1 Initialization: Set w = 0; 2 for t ← 1...T do 3 foreach (x, y) E D do 4 z ← beamSearch (x, y, w) 5 if z =� y then 6 w ← w + f(x, y[1:|z|]) − f(x, z) Figure 2: Perceptron training with beamsearch (Huang et al., 2012). Here y[1:i] denotes the prefix of y that has length i, e.g., y[1:3] = (y1, y2, y3). 3.2 Label sets Here we introduce the label sets for trigger and argument in the model. We use G U {+} to denote the trigger label alphabet, where G represents the 33 event subtypes, and + indicates that the token is not a trigger. Similarly, R U {+} denotes the argument label sets, where R is the set of possible argument roles, and + means that the argument candidate is not an argument for the current trigger. It is worth to note that the set R of each particular event subtype is subject to the entity type co</context>
<context citStr="Huang et al. (2012)" endWordPosition="4321" position="25630" startWordPosition="4318">ze is an important hyper parameter in both training and test. Larger beam size will increase the computational cost while smaller beam size may reduce the performance. Table 4 shows the performance on the development set with several different beam sizes. When beam size = 4, the algorithm achieved the highest performance on the development set with trigger F1 = 67.9, argument F1 = 51.5, and harmonic mean = 58.6. When the size is increased to 32, the accuracy was not improved. Based on this observation, we chose beam size as 4 for the remaining experiments. 4.5 Early-update vs. standard-update Huang et al. (2012) define “invalid update” to be an update that does not fix a violation (and instead reinforces the error), and show that it strongly (anti-)correlates with search quality and learning quality. Figure 7 depicts the percentage of invalid updates in standard-update with and without global features, respectively. With global features, there are numerous invalid updates when the Harmonic mean 0.46 79 Beam size 1 2 4 8 16 32 Training time (sec) 993 2,034 3,982 8,036 15,878 33,026 Harmonic mean 57.6 57.7 58.6 58.0 57.8 57.8 Table 4: Comparison of training time and accuracy on the dev set. Figure 7: P</context>
</contexts>
<marker>Huang, Fayong, Guo, 2012</marker>
<rawString>Liang Huang, Suphan Fayong, and Yang Guo. 2012. Structured perceptron with inexact search. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 142–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
</authors>
<title>Refining event extraction through cross-document inference.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>254--262</pages>
<contexts>
<context citStr="Ji and Grishman, 2008" endWordPosition="212" position="1461" startWordPosition="209">d arguments. Experimental results show that our joint approach with local features outperforms the pipelined baseline, and adding global features further improves the performance significantly. Our approach advances state-ofthe-art sentence-level event extraction, and even outperforms previous argument labeling methods which use external knowledge from other sentences and documents. 1 Introduction Event extraction is an important and challenging task in Information Extraction (IE), which aims to discover event triggers with specific types and their arguments. Most state-of-the-art approaches (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011) use sequential pipelines as building blocks, which break down the whole task into separate subtasks, such as trigger identification/classification and argument identification/classification. As a common drawback of the staged architecture, errors in upstream component are often compounded and propagated to the downstream classifiers. The downstream components, however, cannot impact earlier decisions. For example, consider the following sentences with an ambiguous word “fired”: (1) In Baghdad, a cameraman died when an American tank fired on the Pal</context>
<context citStr="Ji and Grishman, 2008" endWordPosition="3857" position="22926" startWordPosition="3853">0.04 0.02 78 which appears at the end of it should be labeled as Entity argument for the same trigger. Feature (7-8) are based on the statistics about different arguments for the same trigger. For instance, in many cases, a trigger can only have one Place argument. If a partial configuration mistakenly classifies more than one entity mention as Place arguments for the same trigger, then it will be penalized. 4 Experiments 4.1 Data set and evaluation metric We utilized the ACE 2005 corpus as our testbed. For comparison, we used the same test set with 40 newswire articles (672 sentences) as in (Ji and Grishman, 2008; Liao and Grishman, 2010) for the experiments, and randomly selected 30 other documents (863 sentences) from different genres as the development set. The rest 529 documents (14, 840 sentences) are used for training. Following previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011), we use the following criteria to determine the correctness of an predicted event mention: • A trigger is correct if its event subtype and offsets match those of a reference trigger. • An argument is correctly identified if its event subtype and offsets match those of any of the reference a</context>
<context citStr="Ji and Grishman, 2008" endWordPosition="4821" position="28713" startWordPosition="4818">ch attained 68.3% F1 for triggers and 48.3% for arguments, our approach with global features achieves even better performance on argument labeling although we only used sentencelevel information. We also tested the performance with argument candidates automatically extracted by a highperforming name tagger (Li et al., 2012b) and an IE system (Grishman et al., 2005). The results are summarized in Table 7. The joint approach with global features significantly outperforms the baseline and the model with only local features. We also show that it outperforms the sentencelevel baseline reported in (Ji and Grishman, 2008; Liao and Grishman, 2010), both of which attained 59.7% F1 for triggers and 36.6% for arguments. Our approach aims to tackle the problem of sentence-level event extraction, thereby only used intra-sentential evidence. Nevertheless, the performance of our approach is still comparable with the best-reported methods based on cross-document and cross-event inference (Ji and Grishman, 2008; Liao and Grishman, 2010). 5 Related Work Most recent studies about ACE event extraction rely on staged pipeline which consists of separate local classifiers for trigger labeling and argument labeling (Grishman </context>
<context citStr="Ji and Grishman (2008)" endWordPosition="5115" position="30422" startWordPosition="5112">ation (%) P R F1 P R F1 P R F1 P R F1 Sentence-level in Hong et al. (2011) N/A 67.6 53.5 59.7 46.5 37.15 41.3 41.0 32.8 36.5 Staged MaxEnt classifiers 76.2 60.5 67.4 74.5 59.1 65.9 74.1 37.4 49.7 65.4 33.1 43.9 Joint w/ local features 77.4 62.3 69.0 73.7 59.3 65.7 69.7 39.6 50.5 64.1 36.5 46.5 Joint w/ local + global features 76.9 65.0 70.4 73.7 62.3 67.5 69.8 47.9 56.8 64.7 44.4 52.7 Cross-entity in Hong et al. (2011)† N/A 72.9 64.3 68.3 53.4 52.9 53.1 51.6 45.5 48.3 Table 6: Overall performance with gold-standard entities, timex, and values. †beyond sentence level. Methods Trigger F1 Arg F1 Ji and Grishman (2008) 67.3 42.6 cross-doc Inference Ji and Grishman (2008) 59.7 36.6 sentence-level MaxEnt classifiers 64.7 (↓1.2) 33.7 (↓10.2) Joint w/ local 63.7 (↓2.0) 35.8 (↓10.7) Joint w/ local + global 65.6 (↓1.9) 41.8 (↓10.9) Table 7: Overall performance (%) with predicted entities, timex, and values. ↓ indicates the performance drop from experiments with gold-standard argument candidates (see Table 6). For the Message Understanding Conference (MUC) and FAS Program for Monitoring Emerging Diseases (ProMED) event extraction tasks, Patwardhan and Riloff (2009) proposed a probabilistic framework to extract eve</context>
</contexts>
<marker>Ji, Grishman, 2008</marker>
<rawString>Heng Ji and Ralph Grishman. 2008. Refining event extraction through cross-document inference. In Proceedings of ACL, pages 254–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peifeng Li</author>
<author>Guodong Zhou</author>
<author>Qiaoming Zhu</author>
<author>Libin Hou</author>
</authors>
<title>Employing compositional semantics and discourse consistency in chinese event extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1006--1016</pages>
<contexts>
<context citStr="Li et al., 2012" endWordPosition="4773" position="28416" startWordPosition="4770"> global features, the overall performance is further improved significantly. Compared to the staged baseline, it gains 1.6% improvement on trigger’s F-measure and 8.8% improvement on argument’s F-measure. Remarkably, compared to the cross-entity approach reported in (Hong et al., 2011), which attained 68.3% F1 for triggers and 48.3% for arguments, our approach with global features achieves even better performance on argument labeling although we only used sentencelevel information. We also tested the performance with argument candidates automatically extracted by a highperforming name tagger (Li et al., 2012b) and an IE system (Grishman et al., 2005). The results are summarized in Table 7. The joint approach with global features significantly outperforms the baseline and the model with only local features. We also show that it outperforms the sentencelevel baseline reported in (Ji and Grishman, 2008; Liao and Grishman, 2010), both of which attained 59.7% F1 for triggers and 36.6% for arguments. Our approach aims to tackle the problem of sentence-level event extraction, thereby only used intra-sentential evidence. Nevertheless, the performance of our approach is still comparable with the best-repo</context>
</contexts>
<marker>Li, Zhou, Zhu, Hou, 2012</marker>
<rawString>Peifeng Li, Guodong Zhou, Qiaoming Zhu, and Libin Hou. 2012a. Employing compositional semantics and discourse consistency in chinese event extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1006–1016.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Li</author>
<author>Haibo Li</author>
<author>Heng Ji</author>
<author>Wen Wang</author>
<author>Jing Zheng</author>
<author>Fei Huang</author>
</authors>
<title>Joint bilingual name tagging for parallel corpora.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st ACM international conference on Information and knowledge management,</booktitle>
<pages>1727--1731</pages>
<contexts>
<context citStr="Li et al., 2012" endWordPosition="4773" position="28416" startWordPosition="4770"> global features, the overall performance is further improved significantly. Compared to the staged baseline, it gains 1.6% improvement on trigger’s F-measure and 8.8% improvement on argument’s F-measure. Remarkably, compared to the cross-entity approach reported in (Hong et al., 2011), which attained 68.3% F1 for triggers and 48.3% for arguments, our approach with global features achieves even better performance on argument labeling although we only used sentencelevel information. We also tested the performance with argument candidates automatically extracted by a highperforming name tagger (Li et al., 2012b) and an IE system (Grishman et al., 2005). The results are summarized in Table 7. The joint approach with global features significantly outperforms the baseline and the model with only local features. We also show that it outperforms the sentencelevel baseline reported in (Ji and Grishman, 2008; Liao and Grishman, 2010), both of which attained 59.7% F1 for triggers and 36.6% for arguments. Our approach aims to tackle the problem of sentence-level event extraction, thereby only used intra-sentential evidence. Nevertheless, the performance of our approach is still comparable with the best-repo</context>
</contexts>
<marker>Li, Li, Ji, Wang, Zheng, Huang, 2012</marker>
<rawString>Qi Li, Haibo Li, Heng Ji, Wen Wang, Jing Zheng, and Fei Huang. 2012b. Joint bilingual name tagging for parallel corpora. In Proceedings of the 21st ACM international conference on Information and knowledge management, pages 1727–1731.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shasha Liao</author>
<author>Ralph Grishman</author>
</authors>
<title>Using document level cross-event inference to improve event extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>789--797</pages>
<contexts>
<context citStr="Liao and Grishman, 2010" endWordPosition="217" position="1486" startWordPosition="213">al results show that our joint approach with local features outperforms the pipelined baseline, and adding global features further improves the performance significantly. Our approach advances state-ofthe-art sentence-level event extraction, and even outperforms previous argument labeling methods which use external knowledge from other sentences and documents. 1 Introduction Event extraction is an important and challenging task in Information Extraction (IE), which aims to discover event triggers with specific types and their arguments. Most state-of-the-art approaches (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011) use sequential pipelines as building blocks, which break down the whole task into separate subtasks, such as trigger identification/classification and argument identification/classification. As a common drawback of the staged architecture, errors in upstream component are often compounded and propagated to the downstream classifiers. The downstream components, however, cannot impact earlier decisions. For example, consider the following sentences with an ambiguous word “fired”: (1) In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel. (2) He has </context>
<context citStr="Liao and Grishman, 2010" endWordPosition="3861" position="22952" startWordPosition="3858">ars at the end of it should be labeled as Entity argument for the same trigger. Feature (7-8) are based on the statistics about different arguments for the same trigger. For instance, in many cases, a trigger can only have one Place argument. If a partial configuration mistakenly classifies more than one entity mention as Place arguments for the same trigger, then it will be penalized. 4 Experiments 4.1 Data set and evaluation metric We utilized the ACE 2005 corpus as our testbed. For comparison, we used the same test set with 40 newswire articles (672 sentences) as in (Ji and Grishman, 2008; Liao and Grishman, 2010) for the experiments, and randomly selected 30 other documents (863 sentences) from different genres as the development set. The rest 529 documents (14, 840 sentences) are used for training. Following previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011), we use the following criteria to determine the correctness of an predicted event mention: • A trigger is correct if its event subtype and offsets match those of a reference trigger. • An argument is correctly identified if its event subtype and offsets match those of any of the reference argument mentions. • An arg</context>
<context citStr="Liao and Grishman, 2010" endWordPosition="4825" position="28739" startWordPosition="4822">r triggers and 48.3% for arguments, our approach with global features achieves even better performance on argument labeling although we only used sentencelevel information. We also tested the performance with argument candidates automatically extracted by a highperforming name tagger (Li et al., 2012b) and an IE system (Grishman et al., 2005). The results are summarized in Table 7. The joint approach with global features significantly outperforms the baseline and the model with only local features. We also show that it outperforms the sentencelevel baseline reported in (Ji and Grishman, 2008; Liao and Grishman, 2010), both of which attained 59.7% F1 for triggers and 36.6% for arguments. Our approach aims to tackle the problem of sentence-level event extraction, thereby only used intra-sentential evidence. Nevertheless, the performance of our approach is still comparable with the best-reported methods based on cross-document and cross-event inference (Ji and Grishman, 2008; Liao and Grishman, 2010). 5 Related Work Most recent studies about ACE event extraction rely on staged pipeline which consists of separate local classifiers for trigger labeling and argument labeling (Grishman et al., 2005; Ahn, 2006; J</context>
</contexts>
<marker>Liao, Grishman, 2010</marker>
<rawString>Shasha Liao and Ralph Grishman. 2010. Using document level cross-event inference to improve event extraction. In Proceedings of ACL, pages 789–797.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine Macleod</author>
<author>Ralph Grishman</author>
<author>Adam Meyers</author>
<author>Leslie Barrett</author>
<author>Ruth Reeves</author>
</authors>
<title>Nomlex: A lexicon of nominalizations.</title>
<date>1998</date>
<booktitle>In Proceedings of EURALEX,</booktitle>
<volume>98</volume>
<pages>187--193</pages>
<contexts>
<context citStr="Macleod et al., 1998" endWordPosition="2603" position="15394" startWordPosition="2600"> y) denotes the features extracted from the input instan f(x, ce x along z◦l ng test, lines 4-5 &amp; 13-14 are omitted. with configuration y. In general, each feature in1 if ys(i) =Attack and y has only one “Attacker” 0 otherwise 0 otherwise f101(x, i,k, y) = ⎧ ⎨⎪ ⎪⎩ Category Type Feature Description Trigger Lexical 1. unigrams/bigrams of the current and context words within the window of size 2 2. unigrams/bigrams of part-of-speech tags of the current and context words within the window of size 2 3. lemma and synonyms of the current token 4. base form of the current token extracted from Nomlex (Macleod et al., 1998) 5. Brown clusters that are learned from ACE English corpus (Brown et al., 1992; Miller et al., 2004; Sun et al., 2011). We used the clusters with prefixes of length 13, 16 and 20 for each token. Syntactic 6. dependent and governor words of the current token 7. dependency types associated the current token 8. whether the current token is a modifier of job title 9. whether the current token is a non-referential pronoun Entity 10. unigrams/bigrams normalized by entity types Information 11. dependency features normalized by entity types 12. nearest entity type and string in the sentence/clause Ar</context>
</contexts>
<marker>Macleod, Grishman, Meyers, Barrett, Reeves, 1998</marker>
<rawString>Catherine Macleod, Ralph Grishman, Adam Meyers, Leslie Barrett, and Ruth Reeves. 1998. Nomlex: A lexicon of nominalizations. In Proceedings of EURALEX, volume 98, pages 187–193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Mihai Surdeanu</author>
<author>Christopher D Manning</author>
</authors>
<title>Event extraction as dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1626--1635</pages>
<contexts>
<context citStr="McClosky et al., 2011" endWordPosition="5289" position="31565" startWordPosition="5286">Patwardhan and Riloff (2009) proposed a probabilistic framework to extract event role fillers conditioned on the sentential event occurrence. Besides having different task definitions, the key difference from our approach is that their role filler recognizer and sentential event recognizer are trained independently but combined in the test stage. Our experiments, however, have demonstrated that it is more advantageous to do both training and testing with joint inference. There has been some previous work on joint modeling for biomedical events (Riedel and McCallum, 2011a; Riedel et al., 2009; McClosky et al., 2011; Riedel and McCallum, 2011b). (McClosky et al., 2011) is most closely related to our approach. They casted the problem of biomedical event extraction as a dependency parsing problem. The key assumption that event structure can be considered as trees is incompatible with ACE event extraction. In addition, they used a separate classifier to predict the event triggers before applying the parser, while we extract the triggers and argument jointly. Finally, the features in the parser are edge-factorized. To exploit global features, they applied a MaxEnt-based global re-ranker. In comparison, our a</context>
</contexts>
<marker>McClosky, Surdeanu, Manning, 2011</marker>
<rawString>David McClosky, Mihai Surdeanu, and Christopher D. Manning. 2011. Event extraction as dependency parsing. In Proceedings of ACL, pages 1626–1635.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Jethran Guinness</author>
<author>Alex Zamanian</author>
</authors>
<title>Name tagging with word clusters and discriminative training.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<volume>4</volume>
<pages>337--342</pages>
<contexts>
<context citStr="Miller et al., 2004" endWordPosition="2621" position="15494" startWordPosition="2618">4 are omitted. with configuration y. In general, each feature in1 if ys(i) =Attack and y has only one “Attacker” 0 otherwise 0 otherwise f101(x, i,k, y) = ⎧ ⎨⎪ ⎪⎩ Category Type Feature Description Trigger Lexical 1. unigrams/bigrams of the current and context words within the window of size 2 2. unigrams/bigrams of part-of-speech tags of the current and context words within the window of size 2 3. lemma and synonyms of the current token 4. base form of the current token extracted from Nomlex (Macleod et al., 1998) 5. Brown clusters that are learned from ACE English corpus (Brown et al., 1992; Miller et al., 2004; Sun et al., 2011). We used the clusters with prefixes of length 13, 16 and 20 for each token. Syntactic 6. dependent and governor words of the current token 7. dependency types associated the current token 8. whether the current token is a modifier of job title 9. whether the current token is a non-referential pronoun Entity 10. unigrams/bigrams normalized by entity types Information 11. dependency features normalized by entity types 12. nearest entity type and string in the sentence/clause Argument Basic 1. context words of the entity mention 2. trigger word and subtype 3. entity type, subt</context>
</contexts>
<marker>Miller, Guinness, Zamanian, 2004</marker>
<rawString>Scott Miller, Jethran Guinness, and Alex Zamanian. 2004. Name tagging with word clusters and discriminative training. In Proceedings of HLT-NAACL, volume 4, pages 337–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Ellen Riloff</author>
</authors>
<title>A unified model of phrasal and sentential evidence for information extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>1</volume>
<pages>151--160</pages>
<contexts>
<context citStr="Patwardhan and Riloff (2009)" endWordPosition="5196" position="30972" startWordPosition="5193">lues. †beyond sentence level. Methods Trigger F1 Arg F1 Ji and Grishman (2008) 67.3 42.6 cross-doc Inference Ji and Grishman (2008) 59.7 36.6 sentence-level MaxEnt classifiers 64.7 (↓1.2) 33.7 (↓10.2) Joint w/ local 63.7 (↓2.0) 35.8 (↓10.7) Joint w/ local + global 65.6 (↓1.9) 41.8 (↓10.9) Table 7: Overall performance (%) with predicted entities, timex, and values. ↓ indicates the performance drop from experiments with gold-standard argument candidates (see Table 6). For the Message Understanding Conference (MUC) and FAS Program for Monitoring Emerging Diseases (ProMED) event extraction tasks, Patwardhan and Riloff (2009) proposed a probabilistic framework to extract event role fillers conditioned on the sentential event occurrence. Besides having different task definitions, the key difference from our approach is that their role filler recognizer and sentential event recognizer are trained independently but combined in the test stage. Our experiments, however, have demonstrated that it is more advantageous to do both training and testing with joint inference. There has been some previous work on joint modeling for biomedical events (Riedel and McCallum, 2011a; Riedel et al., 2009; McClosky et al., 2011; Riede</context>
</contexts>
<marker>Patwardhan, Riloff, 2009</marker>
<rawString>Siddharth Patwardhan and Ellen Riloff. 2009. A unified model of phrasal and sentential evidence for information extraction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1, pages 151– 160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Fast and robust joint models for biomedical event extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--12</pages>
<contexts>
<context citStr="Riedel and McCallum, 2011" endWordPosition="5281" position="31520" startWordPosition="5277">erging Diseases (ProMED) event extraction tasks, Patwardhan and Riloff (2009) proposed a probabilistic framework to extract event role fillers conditioned on the sentential event occurrence. Besides having different task definitions, the key difference from our approach is that their role filler recognizer and sentential event recognizer are trained independently but combined in the test stage. Our experiments, however, have demonstrated that it is more advantageous to do both training and testing with joint inference. There has been some previous work on joint modeling for biomedical events (Riedel and McCallum, 2011a; Riedel et al., 2009; McClosky et al., 2011; Riedel and McCallum, 2011b). (McClosky et al., 2011) is most closely related to our approach. They casted the problem of biomedical event extraction as a dependency parsing problem. The key assumption that event structure can be considered as trees is incompatible with ACE event extraction. In addition, they used a separate classifier to predict the event triggers before applying the parser, while we extract the triggers and argument jointly. Finally, the features in the parser are edge-factorized. To exploit global features, they applied a MaxEnt</context>
</contexts>
<marker>Riedel, McCallum, 2011</marker>
<rawString>Sebastian Riedel and Andrew McCallum. 2011a. Fast and robust joint models for biomedical event extraction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1– 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Robust biomedical event extraction with dual decomposition and minimal domain adaptation.</title>
<date>2011</date>
<booktitle>In Proceedings of the BioNLP Shared Task 2011 Workshop,</booktitle>
<pages>46--50</pages>
<contexts>
<context citStr="Riedel and McCallum, 2011" endWordPosition="5281" position="31520" startWordPosition="5277">erging Diseases (ProMED) event extraction tasks, Patwardhan and Riloff (2009) proposed a probabilistic framework to extract event role fillers conditioned on the sentential event occurrence. Besides having different task definitions, the key difference from our approach is that their role filler recognizer and sentential event recognizer are trained independently but combined in the test stage. Our experiments, however, have demonstrated that it is more advantageous to do both training and testing with joint inference. There has been some previous work on joint modeling for biomedical events (Riedel and McCallum, 2011a; Riedel et al., 2009; McClosky et al., 2011; Riedel and McCallum, 2011b). (McClosky et al., 2011) is most closely related to our approach. They casted the problem of biomedical event extraction as a dependency parsing problem. The key assumption that event structure can be considered as trees is incompatible with ACE event extraction. In addition, they used a separate classifier to predict the event triggers before applying the parser, while we extract the triggers and argument jointly. Finally, the features in the parser are edge-factorized. To exploit global features, they applied a MaxEnt</context>
</contexts>
<marker>Riedel, McCallum, 2011</marker>
<rawString>Sebastian Riedel and Andrew McCallum. 2011b. Robust biomedical event extraction with dual decomposition and minimal domain adaptation. In Proceedings of the BioNLP Shared Task 2011 Workshop, pages 46–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Hong-Woo Chun</author>
<author>Toshihisa Takagi</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A markov logic approach to bio-molecular event extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing: Shared Task,</booktitle>
<pages>41--49</pages>
<contexts>
<context citStr="Riedel et al., 2009" endWordPosition="5285" position="31542" startWordPosition="5282">nt extraction tasks, Patwardhan and Riloff (2009) proposed a probabilistic framework to extract event role fillers conditioned on the sentential event occurrence. Besides having different task definitions, the key difference from our approach is that their role filler recognizer and sentential event recognizer are trained independently but combined in the test stage. Our experiments, however, have demonstrated that it is more advantageous to do both training and testing with joint inference. There has been some previous work on joint modeling for biomedical events (Riedel and McCallum, 2011a; Riedel et al., 2009; McClosky et al., 2011; Riedel and McCallum, 2011b). (McClosky et al., 2011) is most closely related to our approach. They casted the problem of biomedical event extraction as a dependency parsing problem. The key assumption that event structure can be considered as trees is incompatible with ACE event extraction. In addition, they used a separate classifier to predict the event triggers before applying the parser, while we extract the triggers and argument jointly. Finally, the features in the parser are edge-factorized. To exploit global features, they applied a MaxEnt-based global re-ranke</context>
</contexts>
<marker>Riedel, Chun, Takagi, Tsujii, 2009</marker>
<rawString>Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi, and Jun’ichi Tsujii. 2009. A markov logic approach to bio-molecular event extraction. In Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing: Shared Task, pages 41–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ang Sun</author>
<author>Ralph Grishman</author>
<author>Satoshi Sekine</author>
</authors>
<title>Semi-supervised relation extraction with large-scale word clustering.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>521--529</pages>
<contexts>
<context citStr="Sun et al., 2011" endWordPosition="2625" position="15513" startWordPosition="2622">onfiguration y. In general, each feature in1 if ys(i) =Attack and y has only one “Attacker” 0 otherwise 0 otherwise f101(x, i,k, y) = ⎧ ⎨⎪ ⎪⎩ Category Type Feature Description Trigger Lexical 1. unigrams/bigrams of the current and context words within the window of size 2 2. unigrams/bigrams of part-of-speech tags of the current and context words within the window of size 2 3. lemma and synonyms of the current token 4. base form of the current token extracted from Nomlex (Macleod et al., 1998) 5. Brown clusters that are learned from ACE English corpus (Brown et al., 1992; Miller et al., 2004; Sun et al., 2011). We used the clusters with prefixes of length 13, 16 and 20 for each token. Syntactic 6. dependent and governor words of the current token 7. dependency types associated the current token 8. whether the current token is a modifier of job title 9. whether the current token is a non-referential pronoun Entity 10. unigrams/bigrams normalized by entity types Information 11. dependency features normalized by entity types 12. nearest entity type and string in the sentence/clause Argument Basic 1. context words of the entity mention 2. trigger word and subtype 3. entity type, subtype and entity role</context>
</contexts>
<marker>Sun, Grishman, Sekine, 2011</marker>
<rawString>Ang Sun, Ralph Grishman, and Satoshi Sekine. 2011. Semi-supervised relation extraction with large-scale word clustering. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 521–529.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>