<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant confidence="0.000002" no="0">
<title confidence="0.999059">
Integrating Translation Memory into Phrase-Based
Machine Translation during Decoding
</title>
<author confidence="0.993618">
Kun Wang† Chengqing Zong† Keh-Yih Su‡
</author>
<affiliation confidence="0.966431">
†National Laboratory of Pattern Recognition, Institute of Automation,
Chinese Academy of Sciences, Beijing, China
‡Behavior Design Corporation, Taiwan
</affiliation>
<email confidence="0.990585">
†{kunwang, cqzong}@nlpr.ia.ac.cn, ‡kysu@bdc.com.tw
</email>
<sectionHeader confidence="0.99565" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998055">Since statistical machine translation (SMT) and translation memory (TM) complement each other in matched and unmatched regions, integrated models are proposed in this paper to incorporate TM information into phrase-based SMT. Unlike previous multi-stage pipeline approaches, which directly merge TM result into the final output, the proposed models refer to the corresponding TM information associated with each phrase at SMT decoding. On a Chinese–English TM database, our experiments show that the proposed integrated Model-III is significantly better than either the SMT or the TM systems when the fuzzy match score is above 0.4. Furthermore, integrated Model-III achieves overall 3.48 BLEU points improvement and 2.62 TER points reduction in comparison with the pure SMT system. Besides, the proposed models also outperform previous approaches significantly.</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999876851851852">Statistical machine translation (SMT), especially the phrase-based model (Koehn et al., 2003), has developed very fast in the last decade. For certain language pairs and special applications, SMT output has reached an acceptable level, especially in the domains where abundant parallel corpora are available (He et al., 2010). However, SMT is rarely applied to professional translation because its output quality is still far from satisfactory. Especially, there is no guarantee that a SMT system can produce translations in a consistent manner (Ma et al., 2011). In contrast, translation memory (TM), which uses the most similar translation sentence (usually above a certain fuzzy match threshold) in the database as the reference for post-editing, has been widely adopted in professional translation field for many years (Lagoudaki, 2006). TM is very useful for repetitive material such as updated product manuals, and can give high quality and consistent translations when the similarity of fuzzy match is high. Therefore, professional translators trust TM much more than SMT. However, high-similarity fuzzy matches are available unless the material is very repetitive. In general, for those matched segments1, TM provides more reliable results than SMT does. One reason is that the results of TM have been revised by human according to the global context, but SMT only utilizes local context. However, for those unmatched segments, SMT is more reliable. Since TM and SMT complement each other in those matched and unmatched segments, the output quality is expected to be raised significantly if they can be combined to supplement each other. In recent years, some previous works have incorporated TM matched segments into SMT in a pipelined manner (Koehn and Senellart, 2010; Zhechev and van Genabith, 2010; He et al., 2011; Ma et al., 2011). All these pipeline approaches translate the sentence in two stages. They first determine whether the extracted TM sentence pair should be adopted or not. Most of them use fuzzy match score as the threshold, but He et al. (2011) and Ma et al. (2011) use a classifier to make the judgment. Afterwards, they merge the relevant translations of matched segments into the source sentence, and then force the SMT system to only translate those unmatched segments at decoding. There are three obvious drawbacks for the above pipeline approaches. Firstly, all of them determine whether those matched segments</bodyText>
<footnote confidence="0.661694">
1 We mean “sub-sentential segments” in this work.
</footnote>
<page confidence="0.98745">
11
</page>
<note confidence="0.9594585">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 11–21,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999004">
Figure 1: Phrase Mapping Example
should be adopted or not at sentence level.</figureCaption>
<bodyText confidence="0.999950120689655">That is, they are either all adopted or all abandoned regardless of their individual quality. Secondly, as several TM target phrases might be available for one given TM source phrase due to insertions, the incorrect selection made in the merging stage cannot be remedied in the following translation stage. For example, there are six possible corresponding TM target phrases for the given TM source phrase “关联4 的5 对象6” (as shown in Figure 1) such as “object2 that3 is4 associated5”, and “an1 object2 that3 is4 associated5 with6”, etc. And it is hard to tell which one should be adopted in the merging stage. Thirdly, the pipeline approach does not utilize the SMT probabilistic information in deciding whether a matched TM phrase should be adopted or not, and which target phrase should be selected when we have multiple candidates. Therefore, the possible improvements resulted from those pipeline approaches are quite limited. On the other hand, instead of directly merging TM matched phrases into the source sentence, some approaches (Biçici and Dymetman, 2008; Simard and Isabelle, 2009) simply add the longest matched pairs into SMT phrase table, and then associate them with a fixed large probability value to favor the corresponding TM target phrase at SMT decoding. However, since only one aligned target phrase will be added for each matched source phrase, they share most drawbacks with the pipeline approaches mentioned above and merely achieve similar performance. To avoid the drawbacks of the pipeline approach (mainly due to making a hard decision before decoding), we propose several integrated models to completely make use of TM information during decoding. For each TM source phrase, we keep all its possible corresponding target phrases (instead of keeping only one of them). The integrated models then consider all corresponding TM target phrases and SMT preference during decoding. Therefore, the proposed integrated models combine SMT and TM at a deep level (versus the surface level at which TM result is directly plugged in under previous pipeline approaches). On a Chinese–English computer technical documents TM database, our experiments have shown that the proposed Model-III improves the translation quality significantly over either the pure phrase-based SMT or the TM systems when the fuzzy match score is above 0.4. Compared with the pure SMT system, the proposed integrated Model-III achieves 3.48 BLEU points improvement and 2.62 TER points reduction overall. Furthermore, the proposed models significantly outperform previous pipeline approaches.</bodyText>
<sectionHeader confidence="0.950691" genericHeader="method">
2 Problem Formulation
</sectionHeader>
<bodyText confidence="0.989581782608695">Compared with the standard phrase-based machine translation model, the translation problem is reformulated as follows (only based on the best TM, however, it is similar for multiple TM sentences): Where is the given source sentence to be translated, is the corresponding target sentence and is the final translation; are the associated information of the best TM sentence-pair; and denote the corresponding TM sentence pair; denotes its associated fuzzy match score (from 0.0 to 1.0); is the editing operations between and ; and denotes the word alignment between and Afterwards, for any given source phrase , we can find its corresponding TM source phrase and all possible TM target phrases (each of them is denoted by ) with the help of corresponding editing operations and word alignment . As mentioned above, we can have six different possible TM target phrases for the TM source phrase “关联 4 的 5 对象 6”. This</bodyText>
<equation confidence="0.97041">
gets0 an1 object2 that3 is4 associated5
with6 the7 annotation8 label9 .10
Source
TM Source
TM Target
V*_0 A1 AX2 43 X714 YJK5 *06 3&amp;quot;M7 a8
V*_0 41 X712 4.13 YJK4 *05 3&amp;quot;M6 a7
.
</equation>
<bodyText confidence="0.999010714285714">Let and denote the k-th associated source phrase and target phrase, respectively. Also, and denote the associated source phrase sequence and the target phrase sequence, respectively (total phrases without insertion). Then the above formula (1) can be decomposed as below:</bodyText>
<page confidence="0.997518">
12
</page>
<bodyText confidence="0.995382666666667">is because there are insertions around the directly aligned TM target phrase. In the above Equation (2), we first segment the given source sentence into various phrases, and then translate the sentence based on those source phrases. Also, is replaced by , as they are actually the same segmentation sequence. Assume that the segmentation probability is a uniform distribution, with the corresponding TM source and target phrases obtained above, this problem can be further simplified as follows:</bodyText>
<equation confidence="0.653985">
(3)
</equation>
<bodyText confidence="0.998558454545455">Where is the corresponding TM phrase matching status for , which is a vector consisting of various indicators (e.g., Target Phrase Content Matching Status, etc., to be defined later), and reflects the quality of the given candidate; is the linking status vector of (the aligned source phrase of within ), and indicates the matching and linking status in the source side (which is closely related to the status in the target side); also, indicates the corresponding TM fuzzy match interval specified later. In the second line of Equation (3), we convert the fuzzy match score into its corresponding interval , and incorporate all possible combinations of TM target phrases. Afterwards, we select the best one in the third line. Last, in the fourth line, we introduce the source matching status and the target linking status (detailed features would be defined later). Since we might have several possible TM target phrases useful information from the best TM sentence pair to guide SMT decoding.</bodyText>
<sectionHeader confidence="0.994688" genericHeader="method">
3 Proposed Models
</sectionHeader>
<bodyText confidence="0.9973905">Three integrated models are proposed to incorporate different features as follows:</bodyText>
<subsectionHeader confidence="0.999436">
3.1 Model-I
</subsectionHeader>
<bodyText confidence="0.971317354166667">In this simplest model, we only consider Target Phrase Content Matching Status (TCM) for . For , we consider four different features at the same time: Source Phrase Content Matching Status (SCM), Number of Linking Neighbors (NLN), Source Phrase Length (SPL), and Sentence End Punctuation Indicator (SEP). Those features will be defined below. is then specified as: All features incorporated in this model are specified as follows: TM Fuzzy Match Interval (z): The fuzzy match score (FMS) between source sentence and TM source sentence indicates the reliability of the given TM sentence, and is defined as (Sikes, 2007): Where is the word-based Levenshtein Distance (Levenshtein, 1966) between and . We equally divide FMS into ten fuzzy match intervals such as: [0.9, 1.0), [0.8, 0.9) etc., and the index specifies the corresponding interval. For example, since the fuzzy match score between and in Figure 1 is 0.667, then . Target Phrase Content Matching Status (TCM): It indicates the content matching status between and , and reflects the quality of . Because is nearly perfect when FMS is high, if the similarity between and is high, it implies that the given is possibly a good candidate. It is a member of {Same, High, Low, NA (Not-Applicable)}, and is specified as: , ; (b) else if , ; ; Here is null means that either there is no corresponding TM source phrase or there is no corresponding TM target phrase , the one with the maximum score will be adopted during decoding. The first factor in the above formula (3) is just the typical phrase-based SMT model, and the second factor (to be specified in the Section 3) is the information derived from the TM sentence pair. Therefore, we can still keep the original phrase-based SMT model and only pay attention to how to extract</bodyText>
<figure confidence="0.98650725">
(1) If is not null:
(a) if
(c) else, ;
(2) If is null,
</figure>
<page confidence="0.826829">
13
</page>
<equation confidence="0.8303774">
5
的 6 对象 7” and is “object that is associated”.
If is “object2 that3 is4 associated5”,
; if is “an1 object2 that3
is4 associated5”,
</equation>
<bodyText confidence="0.979049142857143">Source Phrase Content Matching Status (SCM): Which indicates the content matching status between and , and it affects the matching status of and greatly. The more similar is to , the more similar is to . It is a member of {Same, High, Low, NA} and is defined as:</bodyText>
<listItem confidence="0.440123">(1) If is not null: (a) if</listItem>
<bodyText confidence="0.997383829268292">Here is null means that there is no corresponding TM source phrase for the given source phrase . Take the source phrase “关联 5 的 6 对象 7” in Figure 1 for an example, since its corresponding is “关联 4 的 5 对象 6”, then . Number of Linking Neighbors (NLN): Usually, the context of a source phrase would affect its target translation. The more similar the context are, the more likely that the translations are the same. Therefore, this NLN feature reflects the number of matched neighbors (words) and it is a vector of &lt;x, y&gt;. Where “x” denotes the number of matched source neighbors; and “y” denotes how many those neighbors are also linked to target words (not null), which also affects the TM target phrase selection. This feature is a member of {&lt;x, y&gt;: &lt;2, 2&gt;, &lt;2, 1&gt;, &lt;2, 0&gt;, &lt;1, 1&gt;, &lt;1, 0&gt;, &lt;0, 0&gt;}. For the source phrase “关联 5 的 6 对象 7” in Figure 1, the corresponding TM source phrase is “关联 4 的 5 对象 6” . As only their right neighbors “。8” and “。7” are matched, and “。7” is aligned with “.10”, NLN will be &lt;1, 1&gt;. Source Phrase Length (SPL): Usually the longer the source phrase is, the more reliable the TM target phrase is. For example, the corresponding for the source phrase with 5 words would be more reliable than that with only one word. This feature denotes the number of words included in , and is a member of {1, 2, 3, 4, ≥5}. For the case “关联 5 的 6 对象 7”, SPL will be 3. Sentence End Punctuation Indicator (SEP): Which indicates whether the current phrase is a punctuation at the end of the sentence, and is a member of {Yes, No}. For example, the SEP for “关联 5 的 6 对象 7” will be “No”. It is introduced because the SCM and TCM for a sentence-end-punctuation are always “Same” regardless of other features. Therefore, it is used to distinguish this special case from other cases.</bodyText>
<subsectionHeader confidence="0.999741">
3.2 Model-II
</subsectionHeader>
<bodyText confidence="0.999046366666667">As Model-I ignores the relationship among various possible TM target phrases, we add two features TM Candidate Set Status (CSS) and Longest TM Candidate Indicator (LTC) to incorporate this relationship among them. Since CSS is redundant after LTC is known, we thus ignore it for evaluating TCM probability in the following derivation: The two new features CSS and LTC adopted in Model-II are defined as follows: TM Candidate Set Status (CSS): Which restricts the possible status of , and is a member of {Single, Left-Ext, Right-Ext, Both-Ext, NA}. Where “Single” means that there is only one candidate for the given source phrase ; “Left-Ext” means that there are multiple candidates, and all the candidates are generated by extending only the left boundary; “Right-Ext” means that there are multiple candidates, and all the candidates are generated by only extending to the right; “Both-Ext” means that there are multiple candidates, and the candidates are generated by extending to both sides; “NA” means that is null. For “关联 4 的 5 对象 6” in Figure 1, the linked TM target phrase is “object2 that3 is4 associated5”, and there are 5 other candidates by extending to both sides. Therefore, .</bodyText>
<subsectionHeader confidence="0.477651">
Longest TM Candidate Indicator (LTC):
</subsectionHeader>
<bodyText confidence="0.999202666666667">Which indicates whether the given is the longest candidate or not, and is a member of {Original, Left-Longest, Right-Longest, BothLongest, Medium, NA}. Where “Original” means that the given is the one without extension; “Left-Longest” means that the given is only extended to the left and is the longest one; “Right-Longest” means that the given is only extended to the right and is the longest one; “Both-Longest” means that the given is extended to both sides and is the longest one; “Medium” means that the given has been extended but not the longest one; “NA” means that is null.</bodyText>
<figure confidence="0.978107166666667">
(c) else, ;
(2) If is null,
;
(b) else if ,
;
aligned with . In the example of
</figure>
<figureCaption confidence="0.982274">
Figure 1, assume that the given is “关联
</figureCaption>
<equation confidence="0.6485255">
.
, ;
</equation>
<page confidence="0.977314">
14
</page>
<bodyText confidence="0.825133384615385">For “object2 that3 is4 associated5” in Figure 1, ; for “an1 object2 that3 is4 associated5”, for the longest “an1 object2 that3 is4 associated5 with6 the7”, .</bodyText>
<subsectionHeader confidence="0.99754">
3.3 Model-III
</subsectionHeader>
<bodyText confidence="0.999991666666667">The abovementioned integrated models ignore the reordering information implied by TM. Therefore, we add a new feature Target Phrase</bodyText>
<subsubsectionHeader confidence="0.892656">
Adjacent Candidate Relative Position
</subsubsectionHeader>
<bodyText confidence="0.982072384615385">Matching Status (CPM) into Model-II and Model-III is given as: We assume that CPM is independent with SPL and SEP, because the length of source phrase would not affect reordering too much and SEP is used to distinguish the sentence end punctuation with other phrases. The new feature CPM adopted in Model-III is defined as: Target Phrase Adjacent Candidate Relative Position Matching Status (CPM): Which indicates the matching status between the relative position of and the relative position of</bodyText>
<listItem confidence="0.865371277777778">. It checks if are positioned in the same order with , and reflects the quality of ordering the given target candidate . It is a member of {Adjacent-Same, Adjacent-Substitute, Linked-Interleaved, Linked-Cross, LinkedReversed, Skip-Forward, Skip-Cross, SkipReversed, NA}. Recall that is always right adjacent to , then various cases are defined as follows: (1) If both and are not null: (a) If is on the right of and they are also adjacent to each other: i. If the right boundary words of and are the same, and the left boundary words of and are the same, ; ii. Otherwise, ; (b) If is on the right of but they are not adjacent to each other, ; (c) If is not on the right of : i. If there are cross parts between and , ; ii. Otherwise, ; (2) If is null but is not null, then find the first which is not null ( starts from 2)2: (a) If is on the right of ; (b) If is not on the right of : i. If there are cross parts between and , ; ii. Otherwise, .</listItem>
<bodyText confidence="0.987781214285714">. In Figure 1, assume that , and are “gets an”, “object that is associated with” and “gets0 an1”, respectively. For “object2 that3 is4 associated5”, because is on the right of and they are adjacent pair, and both boundary words (“an” and “an1”; “object” and “object2”) are matched, ; for “an1 object2 that3 is4 associated5”, because there are cross parts “an1” between and , . On the other hand, assume that , and are “gets”, “object that is associated with” and “gets0”, respectively. For “an1 object2 that3 is4 associated5”, because and are adjacent pair, but the left boundary words of and (“object” and “an1”) are not matched, ; for “object2 that3 is4 associated5”, because is on the right of but they are not adjacent pair, therefore, . One more example, assume that , and are “the annotation label”, “object that is associated with” and “the7 annotation8 label9”, respectively. For “an1 object2 that3 is4 associated5”, because is on the left of , and there are no cross parts, .</bodyText>
<footnote confidence="0.47267">
2 It can be identified by simply memorizing the index of
nearest non-null during search.
</footnote>
<figure confidence="0.788641">
;
,
(3) If is null,
</figure>
<page confidence="0.918642">
15
</page>
<sectionHeader confidence="0.993102" genericHeader="evaluation and result">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.99529">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.997685055555556">Our TM database consists of computer domain Chinese-English translation sentence-pairs, which contains about 267k sentence-pairs. The average length of Chinese sentences is 13.85 words and that of English sentences is 13.86 words. We randomly selected a development set and a test set, and then the remaining sentence pairs are for training set. The detailed corpus statistics are shown in Table 1. Furthermore, development set and test set are divided into various intervals according to their best fuzzy match scores. Corpus statistics for each interval in the test set are shown in Table 2. For the phrase-based SMT system, we adopted the Moses toolkit (Koehn et al., 2007). The system configurations are as follows: GIZA++ (Och and Ney, 2003) is used to obtain the bidirectional word alignments. Afterwards, “intersection” 3 refinement (Koehn et al., 2003) is adopted to extract phrase-pairs. We use the SRI Language Model toolkit (Stolcke, 2002) to train a 5-gram model with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) on the target-side (English) training corpus. All the feature weights and the weight for each probability factor (3 factors for Model-III) are tuned on the development set with minimumerror-rate training (MERT) (Och, 2003). The maximum phrase length is set to 7 in our experiments. In this work, the translation performance is measured with case-insensitive BLEU-4 score (Papineni et al., 2002) and TER score (Snover et al., 2006). Statistical significance test is conducted with re-sampling (1,000 times) approach (Koehn, 2004) in 95% confidence level.</bodyText>
<subsectionHeader confidence="0.968227">
4.2 Cross-Fold Translation
</subsectionHeader>
<bodyText confidence="0.999953">To estimate the probabilities of proposed models, the corresponding phrase segmentations for bilingual sentences are required. As we want to check what actually happened during decoding in the real situation, cross-fold translation is used to obtain the corresponding phrase segmentations. We first extract 95% of the bilingual sentences as a new training corpus to train a SMT system. Afterwards, we generate the corresponding phrase segmentations for the remaining 5% bi-</bodyText>
<footnote confidence="0.634380666666667">
3 “grow-diag-final” and “grow-diag-final-and” are also test-
ed. However, “intersection” is the best option in our exper-
iments, especially for those high fuzzy match intervals.
</footnote>
<table confidence="0.999618166666667">
Train Develop Test
#Sentences 261,906 2,569 2,576
#Chn. Words 3,623,516 38,585 38,648
#Chn. VOC. 43,112 3,287 3,460
#Eng. Words 3,627,028 38,329 38,510
#Eng. VOC. 44,221 3,993 4,046
</table>
<tableCaption confidence="0.961597">
Table 1: Corpus Statistics
</tableCaption>
<table confidence="0.9998408">
Intervals #Sentences #Words W/S
[0.9, 1.0) 269 4,468 16.6
[0.8, 0.9) 362 5,004 13.8
[0.7, 0.8) 290 4,046 14.0
[0.6, 0.7) 379 4,998 13.2
[0.5, 0.6) 472 6,073 12.9
[0.4, 0.5) 401 5,921 14.8
[0.3, 0.4) 305 5,499 18.0
(0.0, 0.3) 98 2,639 26.9
(0.0, 1.0) 2,576 38,648 15.0
</table>
<tableCaption confidence="0.997143">
Table 2: Corpus Statistics for Test-Set
lingual sentences with Forced Decoding (Li et al., 2000; Zollmann et al., 2008; Auli et al., 2009; Wisniewski et al., 2010), which searches the best phrase segmentation for the specified output.</tableCaption>
<bodyText confidence="0.99927105">Having repeated the above steps 20 times4, we obtain the corresponding phrase segmentations for the SMT training data (which will then be used to train the integrated models). Due to OOV words and insertion words, not all given source sentences can generate the desired results through forced decoding. Fortunately, in our work, 71.7% of the training bilingual sentences can generate the corresponding target results. The remaining 28.3% of the sentence pairs are thus not adopted for generating training samples. Furthermore, more than 90% obtained source phrases are observed to be less than 5 words, which explains why five different quantization levels are adopted for Source Phrase Length (SPL) in section 3.1.</bodyText>
<subsectionHeader confidence="0.99927">
4.3 Translation Results
</subsectionHeader>
<bodyText confidence="0.99957525">After obtaining all the training samples via crossfold translation, we use Factored Language Model toolkit (Kirchhoff et al., 2007) to estimate the probabilities of integrated models with Witten-Bell smoothing (Bell et al., 1990; Witten et al., 1991) and Back-off method. Afterwards, we incorporate the TM information for each phrase at decoding. All experiments are conducted using the Moses phrase-based decoder (Koehn et al., 2007).</bodyText>
<footnote confidence="0.981459666666667">
4 This training process only took about 10 hours on our
Ubuntu server (Intel 4-core Xeon 3.47GHz, 132 GB of
RAM).
</footnote>
<page confidence="0.994973">
16
</page>
<table confidence="0.9996602">
Intervals TM SMT Model-I Model-II Model-III Koehn-10 Ma-11 Ma-11-U
[0.9, 1.0) 81.31 81.38 85.44 * 86.47 *# 89.41 *# 82.79 77.72 82.78
[0.8, 0.9) 73.25 76.16 79.97 * 80.89 * 84.04 *# 79.74 * 73.00 77.66
[0.7, 0.8) 63.62 67.71 71.65 * 72.39 * 74.73 *# 71.02 * 66.54 69.78
[0.6, 0.7) 43.64 54.56 54.88 # 55.88 *# 57.53 *# 53.06 54.00 56.37
[0.5, 0.6) 27.37 46.32 47.32 *# 47.45 *# 47.54 *# 39.31 46.06 47.73
[0.4, 0.5) 15.43 37.18 37.25 # 37.60 # 38.18 *# 28.99 36.23 37.93
[0.3, 0.4) 8.24 29.27 29.52 # 29.38 # 29.15 # 23.58 29.40 30.20
(0.0, 0.3) 4.13 26.38 25.61 # 25.32 # 25.57 # 18.56 26.30 26.92
(0.0, 1.0) 40.17 53.03 54.57 *# 55.10 *# 56.51 *# 50.31 51.98 54.32
</table>
<tableCaption confidence="0.9759345">
Table 3: Translation Results (BLEU%). Scores marked by “*” are significantly better (p &lt; 0.05) than both TM
and SMT systems, and those marked by “#” are significantly better (p &lt; 0.05) than Koehn-10.
</tableCaption>
<table confidence="0.999848">
Intervals TM SMT Model-I Model-II Model-III Koehn-10 Ma-11 Ma-11-U
[0.9, 1.0) 9.79 13.01 9.22 # 8.52 *# 6.77 *# 13.01 18.80 11.90
[0.8, 0.9) 16.21 16.07 13.12 *# 12.74 *# 10.75 *# 15.27 20.60 14.74
[0.7, 0.8) 27.79 22.80 19.10 *# 18.58 *# 17.11 *# 21.85 25.33 21.11
[0.6, 0.7) 46.40 33.38 32.63 # 32.27 *# 29.96 *# 35.93 35.24 31.76
[0.5, 0.6) 62.59 39.56 38.24 *# 38.77 *# 38.74 *# 47.37 40.24 38.01
[0.4, 0.5) 73.93 47.19 47.03 # 46.34 *# 46.00 *# 56.84 48.74 46.10
[0.3, 0.4) 79.86 55.71 55.38 # 55.44 # 55.87 # 64.55 55.93 54.15
(0.0, 0.3) 85.31 61.76 62.38 # 63.66 # 63.51 # 73.30 63.00 60.67
(0.0, 1.0) 50.51 35.88 34.34 *# 34.18 *# 33.26 *# 40.75 38.10 34.49
</table>
<tableCaption confidence="0.935285">
Table 4: Translation Results (TER%). Scores marked by “*” are significantly better (p &lt; 0.05) than both TM and
SMT systems, and those marked by “#” are significantly better (p &lt; 0.05) than Koehn-10.
</tableCaption>
<bodyText confidence="0.999140739130435">Table 3 and 4 give the translation results of TM, SMT, and three integrated models in the test set. In the tables, the best translation results (either in BLEU or TER) at each interval have been marked in bold. Scores marked by “*” are significantly better (p &lt; 0.05) than both the TM and the SMT systems. It can be seen that TM significantly exceeds SMT at the interval [0.9, 1.0) in TER score, which illustrates why professional translators prefer TM rather than SMT as their assistant tool. Compared with TM and SMT, Model-I is significantly better than the SMT system in either BLEU or TER when the fuzzy match score is above 0.7; Model-II significantly outperforms both the TM and the SMT systems in either BLEU or TER when the fuzzy match score is above 0.5; Model-III significantly exceeds both the TM and the SMT systems in either BLEU or TER when the fuzzy match score is above 0.4. All these improvements show that our integrated models have combined the strength of both TM and SMT. However, the improvements from integrated models get less when the fuzzy match score decreases. For example, Model-III outperforms SMT 8.03 BLEU points at interval [0.9, 1.0), while the advantage is only 2.97 BLEU points at interval [0.6, 0.7). This is because lower fuzzy match score means that there are more unmatched parts between and ; the output of TM is thus less reliable. Across all intervals (the last row in the table), Model-III not only achieves the best BLEU score (56.51), but also gets the best TER score (33.26). If intervals are evaluated separately, when the fuzzy match score is above 0.4, Model-III outperforms both Model-II and Model-I in either BLEU or TER. Model-II also exceeds Model-I in either BLEU or TER. The only exception is at interval [0.5, 0.6), in which Model-I achieves the best TER score. This might be due to that the optimization criterion for MERT is BLEU rather than TER in our work.</bodyText>
<subsectionHeader confidence="0.999892">
4.4 Comparison with Previous Work
</subsectionHeader>
<bodyText confidence="0.99873275">In order to compare our proposed models with previous work, we re-implement two XMLMarkup approaches: (Koehn and Senellart, 2010) and (Ma et al, 2011), which are denoted as Koehn-10 and Ma-11, respectively. They are selected because they report superior performances in the literature. A brief description of them is as follows:</bodyText>
<page confidence="0.996081">
17
</page>
<figure confidence="0.807037714285714">
if you do you disable this policy setting , internet explorer does not check the internet for new
Koehn-10 versions of the browser , so does not prompt users to install them . [Insert two spurious target
words]
if you disable this policy setting , internet explorer does not prompt users to install internet for
Ma-11 new versions of the browser . [Miss 7 target words: 9~12, 20~21, 28; Has one wrong permuta-
tion]
if you disable this policy setting , internet explorer does not prompt users to install new ver-
Model-I sions of the browser , so does not check the internet . [Miss 2 target words: 14, 28; Has one
wrong permutation]
if you disable this policy setting , internet explorer does not prompt users to install new ver-
Model-II sions of the browser , so does not check the internet . [Miss 2 target words: 14, 28; Has one
wrong permutation]
Model-III if you disable this policy setting , internet explorer does not check the internet for new versions
of the browser , so does not prompt users to install them . [Exactly the same as the reference]
</figure>
<figureCaption confidence="0.911035">
Figure 2: A Translation Example at Interval [0.9, 1.0] (with FMS=0.920)
</figureCaption>
<figure confidence="0.950691">
if0 you1 disable2 this3 policy4 setting5 ,6 internet7 explorer8 does9 not10 check11 the12 internet13
Reference for14 new15 versions16 of17 the18 browser19 ,20 so21 does22 not23 prompt24 users25 to26 install27
them28 .29
TM 如果0 不 1 配置 2 此 3 策略 4 设置 5 ,6 internet7 explorer8 不 9 搜索 10 internet11 查找 12 浏览
Source 器 13 的 14 新 15 版本 16 ,17 因此 18 不 19 会 20 提示 21 用户 22 安装 23 。24
TM
Target
TM
Alignment
if0 you1 do2 not3 configure4 this5 policy6 setting7 ,8 internet9 explorer10 does11 not12 check13 the14
internet15 for16 new17 versions18 of19 the20 browser21 ,22 so23 does24 not25 prompt26 users27 to28
install29 them30 .31
0-0 1-3 2-4 3-5 4-6 5-7 6-8 7-9 8-10 9-11 11-15 13-21 14-19 15-17 16-18 17-22 18-23 19-24
21-26 22-27 23-29 24-31
if you disable this policy setting , internet explorer does not prompt users to install internet for
SMT new versions of the browser . [Miss 7 target words: 9~12, 20~21, 28; Has one wrong permuta-
tion]
如果0
Source 的 13 新 14 版本 15 ,16 因此 17 不 18 会 19 提示 20 用户 21 安装22 。23
禁用 1 此 2 策略 3 设置 4 ,5 internet6 explorer7 不 8 搜索 9 internet10 查找 11 浏览器 12
</figure>
<bodyText confidence="0.999672333333334">Koehn et al. (2010) first find out the unmatched parts between the given source sentence and TM source sentence. Afterwards, for each unmatched phrase in the TM source sentence, they replace its corresponding translation in the TM target sentence by the corresponding source phrase in the input sentence, and then mark the substitution part. After replacing the corresponding translations of all unmatched source phrases in the TM target sentence, an XML input sentence (with mixed TM target phrases and marked input source phrases) is thus obtained. The SMT decoder then only translates the unmatched/marked source phrases and gets the desired results. Therefore, the inserted parts in the TM target sentence are automatically included. They use fuzzy match score to determine whether the current sentence should be marked or not; and their experiments show that this method is only effective when the fuzzy match score is above 0.8. Ma et al. (2011) think fuzzy match score is not reliable and use a discriminative learning method to decide whether the current sentence should be marked or not. Another difference between Ma11 and Koehn-10 is how the XML input is constructed. In constructing the XML input sentence, Ma-11 replaces each matched source phrase in the given source sentence with the corresponding TM target phrase. Therefore, the inserted parts in the TM target sentence are not included. In Ma’s another paper (He et al., 2011), more linguistic features for discriminative learning are also added. In our work, we only re-implement the XMLMarkup method used in (He et al., 2011; Ma et al, 2011), but do not implement the discriminative learning method. This is because the features adopted in their discriminative learning are complicated and difficult to re-implement. However, the proposed Model-III even outperforms the upper bound of their methods, which will be discussed later. Table 3 and 4 give the translation results of Koehn-10 and Ma-11 (without the discriminator). Scores marked by “#” are significantly better (p &lt; 0.05) than Koehn-10. Besides, the upper bound of (Ma et al, 2011) is also given in the tables, which is denoted as Ma-11-U. We calculate this upper bound according to the method described in (Ma et al., 2011).</bodyText>
<page confidence="0.997973">
18
</page>
<bodyText confidence="0.999930181818182">Since He et al., (2011) only add more linguistic features to the discriminative learning method, the upper bound of (He et al., 2011) is still the same with (Ma et al., 2011); therefore, Ma-11-U applies for both cases. It is observed that Model-III significantly exceeds Koehn-10 at all intervals. More importantly, the proposed models achieve much better TER score than the TM system does at interval [0.9, 1.0), but Koehn-10 does not even exceed the TM system at this interval. Furthermore, Model-III is much better than Ma-11-U at most intervals. Therefore, it can be concluded that the proposed models outperform the pipeline approaches significantly. Figure 2 gives an example at interval [0.9, 1.0), which shows the difference among different system outputs. It can be seen that “you do” is redundant for Koehn-10, because they are insertions and thus are kept in the XML input. However, SMT system still inserts another “you”, regardless of “you do” has already existed. This problem does not occur at Ma-11, but it misses some words and adopts one wrong permutation. Besides, Model-I selects more right words than SMT does but still puts them in wrong positions due to ignoring TM reordering information. In this example, Model-II obtains the same results with Model-I because it also lacks reordering information. Last, since Model-III considers both TM content and TM position information, it gives a perfect translation.</bodyText>
<sectionHeader confidence="0.990525" genericHeader="conclusion">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99924596">Unlike the previous pipeline approaches, which directly merge TM phrases into the final translation result, we integrate TM information of each source phrase into the phrase-based SMT at decoding. In addition, all possible TM target phrases are kept and the proposed models select the best one during decoding via referring SMT information. Besides, the integrated model considers the probability information of both SMT and TM factors. The experiments show that the proposed Model-III outperforms both the TM and the SMT systems significantly (p &lt; 0.05) in either BLEU or TER when fuzzy match score is above 0.4. Compared with the pure SMT system, Model-III achieves overall 3.48 BLEU points improvement and 2.62 TER points reduction on a Chinese– English TM database. Furthermore, Model-III significantly exceeds all previous pipeline approaches. Similar improvements are also observed on the Hansards parts of LDC2004T08 (not shown in this paper due to space limitation). Since no language-dependent feature is adopted, the proposed approaches can be easily adapted for other language pairs. Moreover, following the approaches of Koehn-10 and Ma-11 (to give a fair comparison), training data for SMT and TM are the same in the current experiments. However, the TM is expected to play an even more important role when the SMT training-set differs from the TM database, as additional phrase-pairs that are unseen in the SMT phrase table can be extracted from TM (which can then be dynamically added into the SMT phrase table at decoding time). Our another study has shown that the integrated model would be even more effective when the TM database and the SMT training data-set are from different corpora in the same domain (not shown in this paper). In addition, more source phrases can be matched if a set of high-FMS sentences, instead of only the sentence with the highest FMS, can be extracted and referred at the same time. And it could further raise the performance. Last, some related approaches (Smith and Clark, 2009; Phillips, 2011) combine SMT and example-based machine translation (EBMT) (Nagao, 1984). It would be also interesting to compare our integrated approach with that of theirs.</bodyText>
<sectionHeader confidence="0.997283" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999006230769231">The research work has been funded by the HiTech Research and Development Program (“863” Program) of China under Grant No. 2011AA01A207, 2012AA011101, and 2012AA011102 and also supported by the Key Project of Knowledge Innovation Program of Chinese Academy of Sciences under Grant No.KGZD-EW-501. The authors would like to thank the anonymous reviewers for their insightful comments and suggestions. Our sincere thanks are also extended to Dr. Yanjun Ma and Dr. Yifan He for their valuable discussions during this study.</bodyText>
<sectionHeader confidence="0.998997" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9899004">
Michael Auli, Adam Lopez, Hieu Hoang and Philipp
Koehn, 2009. A systematic analysis of translation
model search spaces. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pag-
es 224–232.
</reference>
<page confidence="0.993763">
19
</page>
<reference confidence="0.998945844036697">
Timothy C. Bell, J.G. Cleary and Ian H. Witten, 1990.
Text compression: Prentice Hall, Englewood Cliffs,
NJ.
Ergun Biçici and Marc Dymetman. 2008. Dynamic
translation memory: using statistical machine trans-
lation to improve translation memory fuzzy match-
es. In Proceedings of the 9th International Confer-
ence on Intelligent Text Processing and Computa-
tional Linguistics (CICLing 2008), pages 454–465.
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Harvard
University Center for Research in Computing
Technology.
Yifan He, Yanjun Ma, Josef van Genabith and Andy
Way, 2010. Bridging SMT and TM with transla-
tion recommendation. In Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 622–630.
Yifan He, Yanjun Ma, Andy Way and Josef van
Genabith. 2011. Rich linguistic features for transla-
tion memory-inspired consistent translation. In
Proceedings of the Thirteenth Machine Translation
Summit, pages 456–463.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In
Proceedings of the IEEE International Conference
on Acoustics, Speech and Signal Processing, pages
181–184.
Katrin Kirchhoff, Jeff A. Bilmes and Kevin Duh.
2007. Factored language models tutorial. Technical
report, Department of Electrical Engineering, Uni-
versity of Washington, Seattle, Washington, USA.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages
388–395, Barcelona, Spain.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer and Ondřej Bojar. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of the ACL 2007 Demo
and Poster Sessions, pages 177–180.
Philipp Koehn, Franz Josef Och and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
pages 48–54.
Philipp Koehn and Jean Senellart. 2010. Convergence
of translation memory and statistical machine
translation. In AMTA Workshop on MT Research
and the Translation Industry, pages 21–31.
Elina Lagoudaki. 2006. Translation memories survey
2006: Users’ perceptions around tm use. In Pro-
ceedings of the ASLIB International Conference
Translating and the Computer 28, pages 1–29.
Qi Li, Biing-Hwang Juang, Qiru Zhou, and Chin-Hui
Lee. 2000. Automatic verbal information verifica-
tion for user authentication. IEEE transactions on
speech and audio processing, Vol. 8, No. 5, pages
1063–6676.
Vladimir Iosifovich Levenshtein. 1966. Binary codes
capable of correcting deletions, insertions, and re-
versals. Soviet Physics Doklady, 10 (8). pages 707–
710.
Yanjun Ma, Yifan He, Andy Way and Josef van
Genabith. 2011. Consistent translation using dis-
criminative learning: a translation memory-inspired
approach. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1239–1248, Portland, Oregon.
Makoto Nagao, 1984. A framework of a mechanical
translation between Japanese and English by anal-
ogy principle. In: Banerji, Alick Elithorn and Ran-
an (ed). Artifiical and Human Intelligence: Edited
Review Papers Presented at the International
NATO Symposium on Artificial and Human Intelli-
gence. North-Holland, Amsterdam, 173–180.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the Association for
Computational Linguistics, pages 160–167.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment
models. Computational Linguistics, 29 (1). pages
19–51.
Kishore Papineni, Salim Roukos, Todd Ward and
Wei-Jing Zhu. 2002. BLEU: a method for automat-
ic evaluation of machine translation. In Proceed-
ings of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 311–
318.
Aaron B. Phillips, 2011. Cunei: open-source machine
translation with relevance-based models of each
translation instance. Machine Translation, 25 (2).
pages 166-177.
Richard Sikes. 2007, Fuzzy matching in theory and
practice. Multilingual, 18(6):39–43.
Michel Simard and Pierre Isabelle. 2009. Phrase-
based machine translation in a computer-assisted
translation environment. In Proceedings of the
Twelfth Machine Translation Summit (MT Summit
XII), pages 120–127.
James Smith and Stephen Clark. 2009. EBMT for
SMT: a new EBMT-SMT hybrid. In Proceedings
of the 3rd International Workshop on Example-
</reference>
<page confidence="0.904598">
20
</page>
<reference confidence="0.998658857142857">
Based Machine Translation (EBMT'09), pages 3–
10, Dublin, Ireland.
Matthew Snover, Bonnie Dorr, Richard Schwartz,
Linnea Micciulla and John Makhoul. 2006. A
study of translation edit rate with targeted human
annotation. In Proceedings of Association for Ma-
chine Translation in the Americas (AMTA-2006),
pages 223–231.
Andreas Stolcke. 2002. SRILM-an extensible lan-
guage modeling toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing, pages 311–318.
Guillaume Wisniewski, Alexandre Allauzen and
François Yvon, 2010. Assessing phrase-based
translation models with oracle decoding. In Pro-
ceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
933–943.
Ian H. Witten and Timothy C. Bell. 1991. The zero-
frequency problem: estimating the probabilities of
novel events in adaptive test compression. IEEE
Transactions on Information Theory, 37(4): 1085–
1094, July.
Ventsislav Zhechev and Josef van Genabith. 2010.
Seeding statistical machine translation with transla-
tion memory output through tree-based structural
alignment. In Proceedings of the 4th Workshop on
Syntax and Structure in Statistical Translation,
pages 43–51.
Andreas Zollmann, Ashish Venugopal, Franz Josef
Och and Jay Ponte, 2008. A systematic comparison
of phrase-based, hierarchical and syntax-
augmented statistical MT. In Proceedings of the
22nd International Conference on Computational
Linguistics (Coling 2008), pages 1145–1152.
</reference>
<page confidence="0.999411">
21
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant confidence="0.478077" no="0">
<title confidence="0.993817">Integrating Translation Memory into Machine Translation during Decoding</title>
<author confidence="0.7143">Chengqing Keh-Yih</author>
<affiliation confidence="0.9817145">Laboratory of Pattern Recognition, Institute of Chinese Academy of Sciences, Beijing,</affiliation>
<address confidence="0.697107">Design Corporation, Taiwan</address>
<email confidence="0.977185">cqzong}@nlpr.ia.ac.cn,</email>
<abstract confidence="0.99889080952381">Since statistical machine translation (SMT) and translation memory (TM) complement each other in matched and unmatched regions, integrated models are proposed in this paper to incorporate TM information into phrase-based SMT. Unlike previous multi-stage pipeline approaches, which directly merge TM result into the final output, the proposed models refer to the corresponding TM information associated with each phrase at SMT decoding. On a TM database, our experiments show that the proposed integrated Model-III is significantly better than either the SMT or the TM systems when the fuzzy match score is above 0.4. Furthermore, integrated Model-III achieves overall 3.48 BLEU points improvement and 2.62 TER points reduction in comparison with the pure SMT system. Besides, the proposed models also outperform previous approaches significantly.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Adam Lopez</author>
<author>Hieu Hoang</author>
<author>Philipp Koehn</author>
</authors>
<title>A systematic analysis of translation model search spaces.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>224--232</pages>
<contexts>
<context citStr="Auli et al., 2009" endWordPosition="3594" position="21374" startWordPosition="3591">s. Train Develop Test #Sentences 261,906 2,569 2,576 #Chn. Words 3,623,516 38,585 38,648 #Chn. VOC. 43,112 3,287 3,460 #Eng. Words 3,627,028 38,329 38,510 #Eng. VOC. 44,221 3,993 4,046 Table 1: Corpus Statistics Intervals #Sentences #Words W/S [0.9, 1.0) 269 4,468 16.6 [0.8, 0.9) 362 5,004 13.8 [0.7, 0.8) 290 4,046 14.0 [0.6, 0.7) 379 4,998 13.2 [0.5, 0.6) 472 6,073 12.9 [0.4, 0.5) 401 5,921 14.8 [0.3, 0.4) 305 5,499 18.0 (0.0, 0.3) 98 2,639 26.9 (0.0, 1.0) 2,576 38,648 15.0 Table 2: Corpus Statistics for Test-Set lingual sentences with Forced Decoding (Li et al., 2000; Zollmann et al., 2008; Auli et al., 2009; Wisniewski et al., 2010), which searches the best phrase segmentation for the specified output. Having repeated the above steps 20 times4, we obtain the corresponding phrase segmentations for the SMT training data (which will then be used to train the integrated models). Due to OOV words and insertion words, not all given source sentences can generate the desired results through forced decoding. Fortunately, in our work, 71.7% of the training bilingual sentences can generate the corresponding target results. The remaining 28.3% of the sentence pairs are thus not adopted for generating traini</context>
</contexts>
<marker>Auli, Lopez, Hoang, Koehn, 2009</marker>
<rawString>Michael Auli, Adam Lopez, Hieu Hoang and Philipp Koehn, 2009. A systematic analysis of translation model search spaces. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 224–232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy C Bell</author>
<author>J G Cleary</author>
<author>Ian H Witten</author>
</authors>
<title>Text compression:</title>
<date>1990</date>
<publisher>Prentice Hall,</publisher>
<location>Englewood Cliffs, NJ.</location>
<contexts>
<context citStr="Bell et al., 1990" endWordPosition="3760" position="22440" startWordPosition="3757">bilingual sentences can generate the corresponding target results. The remaining 28.3% of the sentence pairs are thus not adopted for generating training samples. Furthermore, more than 90% obtained source phrases are observed to be less than 5 words, which explains why five different quantization levels are adopted for Source Phrase Length (SPL) in section 3.1. 4.3 Translation Results After obtaining all the training samples via crossfold translation, we use Factored Language Model toolkit (Kirchhoff et al., 2007) to estimate the probabilities of integrated models with Witten-Bell smoothing (Bell et al., 1990; Witten et al., 1991) and Back-off method. Afterwards, we incorporate the TM information for each phrase at decoding. All experiments are 4 This training process only took about 10 hours on our Ubuntu server (Intel 4-core Xeon 3.47GHz, 132 GB of RAM). 16 Intervals TM SMT Model-I Model-II Model-III Koehn-10 Ma-11 Ma-11-U [0.9, 1.0) 81.31 81.38 85.44 * 86.47 *# 89.41 *# 82.79 77.72 82.78 [0.8, 0.9) 73.25 76.16 79.97 * 80.89 * 84.04 *# 79.74 * 73.00 77.66 [0.7, 0.8) 63.62 67.71 71.65 * 72.39 * 74.73 *# 71.02 * 66.54 69.78 [0.6, 0.7) 43.64 54.56 54.88 # 55.88 *# 57.53 *# 53.06 54.00 56.37 [0.5, 0</context>
</contexts>
<marker>Bell, Cleary, Witten, 1990</marker>
<rawString>Timothy C. Bell, J.G. Cleary and Ian H. Witten, 1990. Text compression: Prentice Hall, Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Biçici</author>
<author>Marc Dymetman</author>
</authors>
<title>Dynamic translation memory: using statistical machine translation to improve translation memory fuzzy matches.</title>
<date>2008</date>
<booktitle>In Proceedings of the 9th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing</booktitle>
<pages>454--465</pages>
<contexts>
<context citStr="Biçici and Dymetman, 2008" endWordPosition="780" position="5037" startWordPosition="777"> 1) such as “object2 that3 is4 associated5”, and “an1 object2 that3 is4 associated5 with6”, etc. And it is hard to tell which one should be adopted in the merging stage. Thirdly, the pipeline approach does not utilize the SMT probabilistic information in deciding whether a matched TM phrase should be adopted or not, and which target phrase should be selected when we have multiple candidates. Therefore, the possible improvements resulted from those pipeline approaches are quite limited. On the other hand, instead of directly merging TM matched phrases into the source sentence, some approaches (Biçici and Dymetman, 2008; Simard and Isabelle, 2009) simply add the longest matched pairs into SMT phrase table, and then associate them with a fixed large probability value to favor the corresponding TM target phrase at SMT decoding. However, since only one aligned target phrase will be added for each matched source phrase, they share most drawbacks with the pipeline approaches mentioned above and merely achieve similar performance. To avoid the drawbacks of the pipeline approach (mainly due to making a hard decision before decoding), we propose several integrated models to completely make use of TM information duri</context>
</contexts>
<marker>Biçici, Dymetman, 2008</marker>
<rawString>Ergun Biçici and Marc Dymetman. 2008. Dynamic translation memory: using statistical machine translation to improve translation memory fuzzy matches. In Proceedings of the 9th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing 2008), pages 454–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Harvard University Center for Research in Computing Technology.</institution>
<contexts>
<context citStr="Chen and Goodman, 1998" endWordPosition="3306" position="19531" startWordPosition="3303">est set are divided into various intervals according to their best fuzzy match scores. Corpus statistics for each interval in the test set are shown in Table 2. For the phrase-based SMT system, we adopted the Moses toolkit (Koehn et al., 2007). The system configurations are as follows: GIZA++ (Och and Ney, 2003) is used to obtain the bidirectional word alignments. Afterwards, “intersection” 3 refinement (Koehn et al., 2003) is adopted to extract phrase-pairs. We use the SRI Language Model toolkit (Stolcke, 2002) to train a 5-gram model with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) on the target-side (English) training corpus. All the feature weights and the weight for each probability factor (3 factors for Model-III) are tuned on the development set with minimumerror-rate training (MERT) (Och, 2003). The maximum phrase length is set to 7 in our experiments. In this work, the translation performance is measured with case-insensitive BLEU-4 score (Papineni et al., 2002) and TER score (Snover et al., 2006). Statistical significance test is conducted with re-sampling (1,000 times) approach (Koehn, 2004) in 95% confidence level. 4.2 Cross-Fold Translation To estimate the pr</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Harvard University Center for Research in Computing Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yifan He</author>
<author>Yanjun Ma</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Bridging SMT and TM with translation recommendation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>622--630</pages>
<marker>He, Ma, van Genabith, Way, 2010</marker>
<rawString>Yifan He, Yanjun Ma, Josef van Genabith and Andy Way, 2010. Bridging SMT and TM with translation recommendation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 622–630.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yifan He</author>
<author>Yanjun Ma</author>
<author>Andy Way</author>
<author>Josef van Genabith</author>
</authors>
<title>Rich linguistic features for translation memory-inspired consistent translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Thirteenth Machine Translation Summit,</booktitle>
<pages>456--463</pages>
<marker>He, Ma, Way, van Genabith, 2011</marker>
<rawString>Yifan He, Yanjun Ma, Andy Way and Josef van Genabith. 2011. Rich linguistic features for translation memory-inspired consistent translation. In Proceedings of the Thirteenth Machine Translation Summit, pages 456–463.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>181--184</pages>
<contexts>
<context citStr="Kneser and Ney, 1995" endWordPosition="3302" position="19506" startWordPosition="3299"> development set and test set are divided into various intervals according to their best fuzzy match scores. Corpus statistics for each interval in the test set are shown in Table 2. For the phrase-based SMT system, we adopted the Moses toolkit (Koehn et al., 2007). The system configurations are as follows: GIZA++ (Och and Ney, 2003) is used to obtain the bidirectional word alignments. Afterwards, “intersection” 3 refinement (Koehn et al., 2003) is adopted to extract phrase-pairs. We use the SRI Language Model toolkit (Stolcke, 2002) to train a 5-gram model with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) on the target-side (English) training corpus. All the feature weights and the weight for each probability factor (3 factors for Model-III) are tuned on the development set with minimumerror-rate training (MERT) (Och, 2003). The maximum phrase length is set to 7 in our experiments. In this work, the translation performance is measured with case-insensitive BLEU-4 score (Papineni et al., 2002) and TER score (Snover et al., 2006). Statistical significance test is conducted with re-sampling (1,000 times) approach (Koehn, 2004) in 95% confidence level. 4.2 Cross-Fold Trans</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Kirchhoff</author>
<author>Jeff A Bilmes</author>
<author>Kevin Duh</author>
</authors>
<title>Factored language models tutorial.</title>
<date>2007</date>
<tech>Technical report,</tech>
<institution>Department of Electrical Engineering, University of Washington,</institution>
<location>Seattle, Washington, USA.</location>
<contexts>
<context citStr="Kirchhoff et al., 2007" endWordPosition="3745" position="22343" startWordPosition="3742"> generate the desired results through forced decoding. Fortunately, in our work, 71.7% of the training bilingual sentences can generate the corresponding target results. The remaining 28.3% of the sentence pairs are thus not adopted for generating training samples. Furthermore, more than 90% obtained source phrases are observed to be less than 5 words, which explains why five different quantization levels are adopted for Source Phrase Length (SPL) in section 3.1. 4.3 Translation Results After obtaining all the training samples via crossfold translation, we use Factored Language Model toolkit (Kirchhoff et al., 2007) to estimate the probabilities of integrated models with Witten-Bell smoothing (Bell et al., 1990; Witten et al., 1991) and Back-off method. Afterwards, we incorporate the TM information for each phrase at decoding. All experiments are 4 This training process only took about 10 hours on our Ubuntu server (Intel 4-core Xeon 3.47GHz, 132 GB of RAM). 16 Intervals TM SMT Model-I Model-II Model-III Koehn-10 Ma-11 Ma-11-U [0.9, 1.0) 81.31 81.38 85.44 * 86.47 *# 89.41 *# 82.79 77.72 82.78 [0.8, 0.9) 73.25 76.16 79.97 * 80.89 * 84.04 *# 79.74 * 73.00 77.66 [0.7, 0.8) 63.62 67.71 71.65 * 72.39 * 74.73 </context>
</contexts>
<marker>Kirchhoff, Bilmes, Duh, 2007</marker>
<rawString>Katrin Kirchhoff, Jeff A. Bilmes and Kevin Duh. 2007. Factored language models tutorial. Technical report, Department of Electrical Engineering, University of Washington, Seattle, Washington, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>388--395</pages>
<location>Barcelona,</location>
<contexts>
<context citStr="Koehn, 2004" endWordPosition="3388" position="20060" startWordPosition="3387">l with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) on the target-side (English) training corpus. All the feature weights and the weight for each probability factor (3 factors for Model-III) are tuned on the development set with minimumerror-rate training (MERT) (Och, 2003). The maximum phrase length is set to 7 in our experiments. In this work, the translation performance is measured with case-insensitive BLEU-4 score (Papineni et al., 2002) and TER score (Snover et al., 2006). Statistical significance test is conducted with re-sampling (1,000 times) approach (Koehn, 2004) in 95% confidence level. 4.2 Cross-Fold Translation To estimate the probabilities of proposed models, the corresponding phrase segmentations for bilingual sentences are required. As we want to check what actually happened during decoding in the real situation, cross-fold translation is used to obtain the corresponding phrase segmentations. We first extract 95% of the bilingual sentences as a new training corpus to train a SMT system. Afterwards, we generate the corresponding phrase segmentations for the remaining 5% bi3 “grow-diag-final” and “grow-diag-final-and” are also tested. However, “in</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 388–395, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer and Ondřej Bojar.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL</booktitle>
<pages>177--180</pages>
<contexts>
<context citStr="Koehn et al., 2007" endWordPosition="3246" position="19151" startWordPosition="3243">sh translation sentence-pairs, which contains about 267k sentence-pairs. The average length of Chinese sentences is 13.85 words and that of English sentences is 13.86 words. We randomly selected a development set and a test set, and then the remaining sentence pairs are for training set. The detailed corpus statistics are shown in Table 1. Furthermore, development set and test set are divided into various intervals according to their best fuzzy match scores. Corpus statistics for each interval in the test set are shown in Table 2. For the phrase-based SMT system, we adopted the Moses toolkit (Koehn et al., 2007). The system configurations are as follows: GIZA++ (Och and Ney, 2003) is used to obtain the bidirectional word alignments. Afterwards, “intersection” 3 refinement (Koehn et al., 2003) is adopted to extract phrase-pairs. We use the SRI Language Model toolkit (Stolcke, 2002) to train a 5-gram model with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) on the target-side (English) training corpus. All the feature weights and the weight for each probability factor (3 factors for Model-III) are tuned on the development set with minimumerror-rate training (MERT) (Och, 20</context>
<context citStr="Koehn et al., 2007" endWordPosition="4135" position="24495" startWordPosition="4132"># 35.93 35.24 31.76 [0.5, 0.6) 62.59 39.56 38.24 *# 38.77 *# 38.74 *# 47.37 40.24 38.01 [0.4, 0.5) 73.93 47.19 47.03 # 46.34 *# 46.00 *# 56.84 48.74 46.10 [0.3, 0.4) 79.86 55.71 55.38 # 55.44 # 55.87 # 64.55 55.93 54.15 (0.0, 0.3) 85.31 61.76 62.38 # 63.66 # 63.51 # 73.30 63.00 60.67 (0.0, 1.0) 50.51 35.88 34.34 *# 34.18 *# 33.26 *# 40.75 38.10 34.49 Table 4: Translation Results (TER%). Scores marked by “*” are significantly better (p &lt; 0.05) than both TM and SMT systems, and those marked by “#” are significantly better (p &lt; 0.05) than Koehn-10. conducted using the Moses phrase-based decoder (Koehn et al., 2007). Table 3 and 4 give the translation results of TM, SMT, and three integrated models in the test set. In the tables, the best translation results (either in BLEU or TER) at each interval have been marked in bold. Scores marked by “*” are significantly better (p &lt; 0.05) than both the TM and the SMT systems. It can be seen that TM significantly exceeds SMT at the interval [0.9, 1.0) in TER score, which illustrates why professional translators prefer TM rather than SMT as their assistant tool. Compared with TM and SMT, Model-I is significantly better than the SMT system in either BLEU or TER when</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer and Ondřej Bojar. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>48--54</pages>
<contexts>
<context citStr="Koehn et al., 2003" endWordPosition="177" position="1305" startWordPosition="174">er to the corresponding TM information associated with each phrase at SMT decoding. On a Chinese–English TM database, our experiments show that the proposed integrated Model-III is significantly better than either the SMT or the TM systems when the fuzzy match score is above 0.4. Furthermore, integrated Model-III achieves overall 3.48 BLEU points improvement and 2.62 TER points reduction in comparison with the pure SMT system. Besides, the proposed models also outperform previous approaches significantly. 1 Introduction Statistical machine translation (SMT), especially the phrase-based model (Koehn et al., 2003), has developed very fast in the last decade. For certain language pairs and special applications, SMT output has reached an acceptable level, especially in the domains where abundant parallel corpora are available (He et al., 2010). However, SMT is rarely applied to professional translation because its output quality is still far from satisfactory. Especially, there is no guarantee that a SMT system can produce translations in a consistent manner (Ma et al., 2011). In contrast, translation memory (TM), which uses the most similar translation sentence (usually above a certain fuzzy match thres</context>
<context citStr="Koehn et al., 2003" endWordPosition="3274" position="19335" startWordPosition="3271">y selected a development set and a test set, and then the remaining sentence pairs are for training set. The detailed corpus statistics are shown in Table 1. Furthermore, development set and test set are divided into various intervals according to their best fuzzy match scores. Corpus statistics for each interval in the test set are shown in Table 2. For the phrase-based SMT system, we adopted the Moses toolkit (Koehn et al., 2007). The system configurations are as follows: GIZA++ (Och and Ney, 2003) is used to obtain the bidirectional word alignments. Afterwards, “intersection” 3 refinement (Koehn et al., 2003) is adopted to extract phrase-pairs. We use the SRI Language Model toolkit (Stolcke, 2002) to train a 5-gram model with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) on the target-side (English) training corpus. All the feature weights and the weight for each probability factor (3 factors for Model-III) are tuned on the development set with minimumerror-rate training (MERT) (Och, 2003). The maximum phrase length is set to 7 in our experiments. In this work, the translation performance is measured with case-insensitive BLEU-4 score (Papineni et al., 2002) and TER </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Jean Senellart</author>
</authors>
<title>Convergence of translation memory and statistical machine translation.</title>
<date>2010</date>
<booktitle>In AMTA Workshop on MT Research and the Translation Industry,</booktitle>
<pages>21--31</pages>
<contexts>
<context citStr="Koehn and Senellart, 2010" endWordPosition="447" position="2990" startWordPosition="444">is very repetitive. In general, for those matched segments1, TM provides more reliable results than SMT does. One reason is that the results of TM have been revised by human according to the global context, but SMT only utilizes local context. However, for those unmatched segments, SMT is more reliable. Since TM and SMT complement each other in those matched and unmatched segments, the output quality is expected to be raised significantly if they can be combined to supplement each other. In recent years, some previous works have incorporated TM matched segments into SMT in a pipelined manner (Koehn and Senellart, 2010; Zhechev and van Genabith, 2010; He et al., 2011; Ma et al., 2011). All these pipeline approaches translate the sentence in two stages. They first determine whether the extracted TM sentence pair should be adopted or not. Most of them use fuzzy match score as the threshold, but He et al. (2011) and Ma et al. (2011) use a classifier to make the judgment. Afterwards, they merge the relevant translations of matched segments into the source sentence, and then force the SMT system to only translate those unmatched segments at decoding. There are three obvious drawbacks for the above pipeline appro</context>
<context citStr="Koehn and Senellart, 2010" endWordPosition="4500" position="26579" startWordPosition="4497"> best BLEU score (56.51), but also gets the best TER score (33.26). If intervals are evaluated separately, when the fuzzy match score is above 0.4, Model-III outperforms both Model-II and Model-I in either BLEU or TER. Model-II also exceeds Model-I in either BLEU or TER. The only exception is at interval [0.5, 0.6), in which Model-I achieves the best TER score. This might be due to that the optimization criterion for MERT is BLEU rather than TER in our work. 4.4 Comparison with Previous Work In order to compare our proposed models with previous work, we re-implement two XMLMarkup approaches: (Koehn and Senellart, 2010) and (Ma et al, 2011), which are denoted as Koehn-10 and Ma-11, respectively. They are selected because they report superior performances in the literature. A brief description of them is as follows: 17 if you do you disable this policy setting , internet explorer does not check the internet for new Koehn-10 versions of the browser , so does not prompt users to install them . [Insert two spurious target words] if you disable this policy setting , internet explorer does not prompt users to install internet for Ma-11 new versions of the browser . [Miss 7 target words: 9~12, 20~21, 28; Has one wr</context>
</contexts>
<marker>Koehn, Senellart, 2010</marker>
<rawString>Philipp Koehn and Jean Senellart. 2010. Convergence of translation memory and statistical machine translation. In AMTA Workshop on MT Research and the Translation Industry, pages 21–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elina Lagoudaki</author>
</authors>
<title>Translation memories survey 2006: Users’ perceptions around tm use.</title>
<date>2006</date>
<booktitle>In Proceedings of the ASLIB International Conference Translating and the Computer 28,</booktitle>
<pages>1--29</pages>
<contexts>
<context citStr="Lagoudaki, 2006" endWordPosition="295" position="2052" startWordPosition="294">able level, especially in the domains where abundant parallel corpora are available (He et al., 2010). However, SMT is rarely applied to professional translation because its output quality is still far from satisfactory. Especially, there is no guarantee that a SMT system can produce translations in a consistent manner (Ma et al., 2011). In contrast, translation memory (TM), which uses the most similar translation sentence (usually above a certain fuzzy match threshold) in the database as the reference for post-editing, has been widely adopted in professional translation field for many years (Lagoudaki, 2006). TM is very useful for repetitive material such as updated product manuals, and can give high quality and consistent translations when the similarity of fuzzy match is high. Therefore, professional translators trust TM much more than SMT. However, high-similarity fuzzy matches are available unless the material is very repetitive. In general, for those matched segments1, TM provides more reliable results than SMT does. One reason is that the results of TM have been revised by human according to the global context, but SMT only utilizes local context. However, for those unmatched segments, SMT </context>
</contexts>
<marker>Lagoudaki, 2006</marker>
<rawString>Elina Lagoudaki. 2006. Translation memories survey 2006: Users’ perceptions around tm use. In Proceedings of the ASLIB International Conference Translating and the Computer 28, pages 1–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Li</author>
<author>Biing-Hwang Juang</author>
<author>Qiru Zhou</author>
<author>Chin-Hui Lee</author>
</authors>
<title>Automatic verbal information verification for user authentication.</title>
<date>2000</date>
<booktitle>IEEE transactions on speech and audio processing,</booktitle>
<volume>8</volume>
<pages>1063--6676</pages>
<contexts>
<context citStr="Li et al., 2000" endWordPosition="3586" position="21332" startWordPosition="3583">ally for those high fuzzy match intervals. Train Develop Test #Sentences 261,906 2,569 2,576 #Chn. Words 3,623,516 38,585 38,648 #Chn. VOC. 43,112 3,287 3,460 #Eng. Words 3,627,028 38,329 38,510 #Eng. VOC. 44,221 3,993 4,046 Table 1: Corpus Statistics Intervals #Sentences #Words W/S [0.9, 1.0) 269 4,468 16.6 [0.8, 0.9) 362 5,004 13.8 [0.7, 0.8) 290 4,046 14.0 [0.6, 0.7) 379 4,998 13.2 [0.5, 0.6) 472 6,073 12.9 [0.4, 0.5) 401 5,921 14.8 [0.3, 0.4) 305 5,499 18.0 (0.0, 0.3) 98 2,639 26.9 (0.0, 1.0) 2,576 38,648 15.0 Table 2: Corpus Statistics for Test-Set lingual sentences with Forced Decoding (Li et al., 2000; Zollmann et al., 2008; Auli et al., 2009; Wisniewski et al., 2010), which searches the best phrase segmentation for the specified output. Having repeated the above steps 20 times4, we obtain the corresponding phrase segmentations for the SMT training data (which will then be used to train the integrated models). Due to OOV words and insertion words, not all given source sentences can generate the desired results through forced decoding. Fortunately, in our work, 71.7% of the training bilingual sentences can generate the corresponding target results. The remaining 28.3% of the sentence pairs </context>
</contexts>
<marker>Li, Juang, Zhou, Lee, 2000</marker>
<rawString>Qi Li, Biing-Hwang Juang, Qiru Zhou, and Chin-Hui Lee. 2000. Automatic verbal information verification for user authentication. IEEE transactions on speech and audio processing, Vol. 8, No. 5, pages 1063–6676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Iosifovich Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions, and reversals.</title>
<date>1966</date>
<journal>Soviet Physics Doklady,</journal>
<volume>10</volume>
<issue>8</issue>
<pages>707--710</pages>
<contexts>
<context citStr="Levenshtein, 1966" endWordPosition="1624" position="10233" startWordPosition="1623">ching Status (TCM) for . For , we consider four different features at the same time: Source Phrase Content Matching Status (SCM), Number of Linking Neighbors (NLN), Source Phrase Length (SPL), and Sentence End Punctuation Indicator (SEP). Those features will be defined below. is then specified as: All features incorporated in this model are specified as follows: TM Fuzzy Match Interval (z): The fuzzy match score (FMS) between source sentence and TM source sentence indicates the reliability of the given TM sentence, and is defined as (Sikes, 2007): Where is the word-based Levenshtein Distance (Levenshtein, 1966) between and . We equally divide FMS into ten fuzzy match intervals such as: [0.9, 1.0), [0.8, 0.9) etc., and the index specifies the corresponding interval. For example, since the fuzzy match score between and in Figure 1 is 0.667, then . Target Phrase Content Matching Status (TCM): It indicates the content matching status between and , and reflects the quality of . Because is nearly perfect when FMS is high, if the similarity between and is high, it implies that the given is possibly a good candidate. It is a member of {Same, High, Low, NA (Not-Applicable)}, and is specified as: , ; (b) else</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Vladimir Iosifovich Levenshtein. 1966. Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics Doklady, 10 (8). pages 707– 710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanjun Ma</author>
<author>Yifan He</author>
<author>Andy Way</author>
<author>Josef van Genabith</author>
</authors>
<title>Consistent translation using discriminative learning: a translation memory-inspired approach.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1239--1248</pages>
<location>Portland, Oregon.</location>
<marker>Ma, He, Way, van Genabith, 2011</marker>
<rawString>Yanjun Ma, Yifan He, Andy Way and Josef van Genabith. 2011. Consistent translation using discriminative learning: a translation memory-inspired approach. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1239–1248, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Nagao</author>
</authors>
<title>A framework of a mechanical translation between Japanese and English by analogy principle.</title>
<date>1984</date>
<booktitle>In: Banerji, Alick Elithorn and Ranan (ed). Artifiical and Human Intelligence: Edited Review Papers Presented at the International NATO Symposium on Artificial and Human Intelligence.</booktitle>
<pages>173--180</pages>
<publisher>North-Holland,</publisher>
<location>Amsterdam,</location>
<contexts>
<context citStr="Nagao, 1984" endWordPosition="5908" position="34819" startWordPosition="5907">o the SMT phrase table at decoding time). Our another study has shown that the integrated model would be even more effective when the TM database and the SMT training data-set are from different corpora in the same domain (not shown in this paper). In addition, more source phrases can be matched if a set of high-FMS sentences, instead of only the sentence with the highest FMS, can be extracted and referred at the same time. And it could further raise the performance. Last, some related approaches (Smith and Clark, 2009; Phillips, 2011) combine SMT and example-based machine translation (EBMT) (Nagao, 1984). It would be also interesting to compare our integrated approach with that of theirs. Acknowledgments The research work has been funded by the HiTech Research and Development Program (“863” Program) of China under Grant No. 2011AA01A207, 2012AA011101, and 2012AA011102 and also supported by the Key Project of Knowledge Innovation Program of Chinese Academy of Sciences under Grant No.KGZD-EW-501. The authors would like to thank the anonymous reviewers for their insightful comments and suggestions. Our sincere thanks are also extended to Dr. Yanjun Ma and Dr. Yifan He for their valuable discussi</context>
</contexts>
<marker>Nagao, 1984</marker>
<rawString>Makoto Nagao, 1984. A framework of a mechanical translation between Japanese and English by analogy principle. In: Banerji, Alick Elithorn and Ranan (ed). Artifiical and Human Intelligence: Edited Review Papers Presented at the International NATO Symposium on Artificial and Human Intelligence. North-Holland, Amsterdam, 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<contexts>
<context citStr="Och, 2003" endWordPosition="3340" position="19754" startWordPosition="3339">, 2007). The system configurations are as follows: GIZA++ (Och and Ney, 2003) is used to obtain the bidirectional word alignments. Afterwards, “intersection” 3 refinement (Koehn et al., 2003) is adopted to extract phrase-pairs. We use the SRI Language Model toolkit (Stolcke, 2002) to train a 5-gram model with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) on the target-side (English) training corpus. All the feature weights and the weight for each probability factor (3 factors for Model-III) are tuned on the development set with minimumerror-rate training (MERT) (Och, 2003). The maximum phrase length is set to 7 in our experiments. In this work, the translation performance is measured with case-insensitive BLEU-4 score (Papineni et al., 2002) and TER score (Snover et al., 2006). Statistical significance test is conducted with re-sampling (1,000 times) approach (Koehn, 2004) in 95% confidence level. 4.2 Cross-Fold Translation To estimate the probabilities of proposed models, the corresponding phrase segmentations for bilingual sentences are required. As we want to check what actually happened during decoding in the real situation, cross-fold translation is used t</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<pages>pages</pages>
<contexts>
<context citStr="Och and Ney, 2003" endWordPosition="3258" position="19221" startWordPosition="3255">. The average length of Chinese sentences is 13.85 words and that of English sentences is 13.86 words. We randomly selected a development set and a test set, and then the remaining sentence pairs are for training set. The detailed corpus statistics are shown in Table 1. Furthermore, development set and test set are divided into various intervals according to their best fuzzy match scores. Corpus statistics for each interval in the test set are shown in Table 2. For the phrase-based SMT system, we adopted the Moses toolkit (Koehn et al., 2007). The system configurations are as follows: GIZA++ (Och and Ney, 2003) is used to obtain the bidirectional word alignments. Afterwards, “intersection” 3 refinement (Koehn et al., 2003) is adopted to extract phrase-pairs. We use the SRI Language Model toolkit (Stolcke, 2002) to train a 5-gram model with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) on the target-side (English) training corpus. All the feature weights and the weight for each probability factor (3 factors for Model-III) are tuned on the development set with minimumerror-rate training (MERT) (Och, 2003). The maximum phrase length is set to 7 in our experiments. In this</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29 (1). pages 19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<contexts>
<context citStr="Papineni et al., 2002" endWordPosition="3368" position="19926" startWordPosition="3365">refinement (Koehn et al., 2003) is adopted to extract phrase-pairs. We use the SRI Language Model toolkit (Stolcke, 2002) to train a 5-gram model with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) on the target-side (English) training corpus. All the feature weights and the weight for each probability factor (3 factors for Model-III) are tuned on the development set with minimumerror-rate training (MERT) (Och, 2003). The maximum phrase length is set to 7 in our experiments. In this work, the translation performance is measured with case-insensitive BLEU-4 score (Papineni et al., 2002) and TER score (Snover et al., 2006). Statistical significance test is conducted with re-sampling (1,000 times) approach (Koehn, 2004) in 95% confidence level. 4.2 Cross-Fold Translation To estimate the probabilities of proposed models, the corresponding phrase segmentations for bilingual sentences are required. As we want to check what actually happened during decoding in the real situation, cross-fold translation is used to obtain the corresponding phrase segmentations. We first extract 95% of the bilingual sentences as a new training corpus to train a SMT system. Afterwards, we generate the</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311– 318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron B Phillips</author>
</authors>
<title>Cunei: open-source machine translation with relevance-based models of each translation instance.</title>
<date>2011</date>
<journal>Machine Translation,</journal>
<volume>25</volume>
<issue>2</issue>
<pages>166--177</pages>
<contexts>
<context citStr="Phillips, 2011" endWordPosition="5899" position="34748" startWordPosition="5898">se table can be extracted from TM (which can then be dynamically added into the SMT phrase table at decoding time). Our another study has shown that the integrated model would be even more effective when the TM database and the SMT training data-set are from different corpora in the same domain (not shown in this paper). In addition, more source phrases can be matched if a set of high-FMS sentences, instead of only the sentence with the highest FMS, can be extracted and referred at the same time. And it could further raise the performance. Last, some related approaches (Smith and Clark, 2009; Phillips, 2011) combine SMT and example-based machine translation (EBMT) (Nagao, 1984). It would be also interesting to compare our integrated approach with that of theirs. Acknowledgments The research work has been funded by the HiTech Research and Development Program (“863” Program) of China under Grant No. 2011AA01A207, 2012AA011101, and 2012AA011102 and also supported by the Key Project of Knowledge Innovation Program of Chinese Academy of Sciences under Grant No.KGZD-EW-501. The authors would like to thank the anonymous reviewers for their insightful comments and suggestions. Our sincere thanks are also</context>
</contexts>
<marker>Phillips, 2011</marker>
<rawString>Aaron B. Phillips, 2011. Cunei: open-source machine translation with relevance-based models of each translation instance. Machine Translation, 25 (2). pages 166-177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sikes</author>
</authors>
<title>Fuzzy matching in theory and practice.</title>
<date>2007</date>
<journal>Multilingual,</journal>
<volume>18</volume>
<issue>6</issue>
<contexts>
<context citStr="Sikes, 2007" endWordPosition="1616" position="10167" startWordPosition="1615">s simplest model, we only consider Target Phrase Content Matching Status (TCM) for . For , we consider four different features at the same time: Source Phrase Content Matching Status (SCM), Number of Linking Neighbors (NLN), Source Phrase Length (SPL), and Sentence End Punctuation Indicator (SEP). Those features will be defined below. is then specified as: All features incorporated in this model are specified as follows: TM Fuzzy Match Interval (z): The fuzzy match score (FMS) between source sentence and TM source sentence indicates the reliability of the given TM sentence, and is defined as (Sikes, 2007): Where is the word-based Levenshtein Distance (Levenshtein, 1966) between and . We equally divide FMS into ten fuzzy match intervals such as: [0.9, 1.0), [0.8, 0.9) etc., and the index specifies the corresponding interval. For example, since the fuzzy match score between and in Figure 1 is 0.667, then . Target Phrase Content Matching Status (TCM): It indicates the content matching status between and , and reflects the quality of . Because is nearly perfect when FMS is high, if the similarity between and is high, it implies that the given is possibly a good candidate. It is a member of {Same, </context>
</contexts>
<marker>Sikes, 2007</marker>
<rawString>Richard Sikes. 2007, Fuzzy matching in theory and practice. Multilingual, 18(6):39–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Simard</author>
<author>Pierre Isabelle</author>
</authors>
<title>Phrasebased machine translation in a computer-assisted translation environment.</title>
<date>2009</date>
<booktitle>In Proceedings of the Twelfth Machine Translation Summit (MT Summit XII),</booktitle>
<pages>120--127</pages>
<contexts>
<context citStr="Simard and Isabelle, 2009" endWordPosition="784" position="5065" startWordPosition="781">is4 associated5”, and “an1 object2 that3 is4 associated5 with6”, etc. And it is hard to tell which one should be adopted in the merging stage. Thirdly, the pipeline approach does not utilize the SMT probabilistic information in deciding whether a matched TM phrase should be adopted or not, and which target phrase should be selected when we have multiple candidates. Therefore, the possible improvements resulted from those pipeline approaches are quite limited. On the other hand, instead of directly merging TM matched phrases into the source sentence, some approaches (Biçici and Dymetman, 2008; Simard and Isabelle, 2009) simply add the longest matched pairs into SMT phrase table, and then associate them with a fixed large probability value to favor the corresponding TM target phrase at SMT decoding. However, since only one aligned target phrase will be added for each matched source phrase, they share most drawbacks with the pipeline approaches mentioned above and merely achieve similar performance. To avoid the drawbacks of the pipeline approach (mainly due to making a hard decision before decoding), we propose several integrated models to completely make use of TM information during decoding. For each TM sou</context>
</contexts>
<marker>Simard, Isabelle, 2009</marker>
<rawString>Michel Simard and Pierre Isabelle. 2009. Phrasebased machine translation in a computer-assisted translation environment. In Proceedings of the Twelfth Machine Translation Summit (MT Summit XII), pages 120–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Smith</author>
<author>Stephen Clark</author>
</authors>
<title>EBMT for SMT: a new EBMT-SMT hybrid.</title>
<date>2009</date>
<booktitle>In Proceedings of the 3rd International Workshop on ExampleBased Machine Translation (EBMT'09),</booktitle>
<pages>3--10</pages>
<location>Dublin, Ireland.</location>
<contexts>
<context citStr="Smith and Clark, 2009" endWordPosition="5897" position="34731" startWordPosition="5894"> unseen in the SMT phrase table can be extracted from TM (which can then be dynamically added into the SMT phrase table at decoding time). Our another study has shown that the integrated model would be even more effective when the TM database and the SMT training data-set are from different corpora in the same domain (not shown in this paper). In addition, more source phrases can be matched if a set of high-FMS sentences, instead of only the sentence with the highest FMS, can be extracted and referred at the same time. And it could further raise the performance. Last, some related approaches (Smith and Clark, 2009; Phillips, 2011) combine SMT and example-based machine translation (EBMT) (Nagao, 1984). It would be also interesting to compare our integrated approach with that of theirs. Acknowledgments The research work has been funded by the HiTech Research and Development Program (“863” Program) of China under Grant No. 2011AA01A207, 2012AA011101, and 2012AA011102 and also supported by the Key Project of Knowledge Innovation Program of Chinese Academy of Sciences under Grant No.KGZD-EW-501. The authors would like to thank the anonymous reviewers for their insightful comments and suggestions. Our sincer</context>
</contexts>
<marker>Smith, Clark, 2009</marker>
<rawString>James Smith and Stephen Clark. 2009. EBMT for SMT: a new EBMT-SMT hybrid. In Proceedings of the 3rd International Workshop on ExampleBased Machine Translation (EBMT'09), pages 3– 10, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of Association for Machine Translation in the Americas (AMTA-2006),</booktitle>
<pages>223--231</pages>
<contexts>
<context citStr="Snover et al., 2006" endWordPosition="3375" position="19962" startWordPosition="3372">pted to extract phrase-pairs. We use the SRI Language Model toolkit (Stolcke, 2002) to train a 5-gram model with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) on the target-side (English) training corpus. All the feature weights and the weight for each probability factor (3 factors for Model-III) are tuned on the development set with minimumerror-rate training (MERT) (Och, 2003). The maximum phrase length is set to 7 in our experiments. In this work, the translation performance is measured with case-insensitive BLEU-4 score (Papineni et al., 2002) and TER score (Snover et al., 2006). Statistical significance test is conducted with re-sampling (1,000 times) approach (Koehn, 2004) in 95% confidence level. 4.2 Cross-Fold Translation To estimate the probabilities of proposed models, the corresponding phrase segmentations for bilingual sentences are required. As we want to check what actually happened during decoding in the real situation, cross-fold translation is used to obtain the corresponding phrase segmentations. We first extract 95% of the bilingual sentences as a new training corpus to train a SMT system. Afterwards, we generate the corresponding phrase segmentations </context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of Association for Machine Translation in the Americas (AMTA-2006), pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<pages>311--318</pages>
<contexts>
<context citStr="Stolcke, 2002" endWordPosition="3289" position="19425" startWordPosition="3288">ning set. The detailed corpus statistics are shown in Table 1. Furthermore, development set and test set are divided into various intervals according to their best fuzzy match scores. Corpus statistics for each interval in the test set are shown in Table 2. For the phrase-based SMT system, we adopted the Moses toolkit (Koehn et al., 2007). The system configurations are as follows: GIZA++ (Och and Ney, 2003) is used to obtain the bidirectional word alignments. Afterwards, “intersection” 3 refinement (Koehn et al., 2003) is adopted to extract phrase-pairs. We use the SRI Language Model toolkit (Stolcke, 2002) to train a 5-gram model with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) on the target-side (English) training corpus. All the feature weights and the weight for each probability factor (3 factors for Model-III) are tuned on the development set with minimumerror-rate training (MERT) (Och, 2003). The maximum phrase length is set to 7 in our experiments. In this work, the translation performance is measured with case-insensitive BLEU-4 score (Papineni et al., 2002) and TER score (Snover et al., 2006). Statistical significance test is conducted with re-sampling (</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM-an extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guillaume Wisniewski</author>
</authors>
<title>Alexandre Allauzen and François Yvon,</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>933--943</pages>
<marker>Wisniewski, 2010</marker>
<rawString>Guillaume Wisniewski, Alexandre Allauzen and François Yvon, 2010. Assessing phrase-based translation models with oracle decoding. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 933–943.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Timothy C Bell</author>
</authors>
<title>The zerofrequency problem: estimating the probabilities of novel events in adaptive test compression.</title>
<date>1991</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>37</volume>
<issue>4</issue>
<pages>1094</pages>
<marker>Witten, Bell, 1991</marker>
<rawString>Ian H. Witten and Timothy C. Bell. 1991. The zerofrequency problem: estimating the probabilities of novel events in adaptive test compression. IEEE Transactions on Information Theory, 37(4): 1085– 1094, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ventsislav Zhechev</author>
<author>Josef van Genabith</author>
</authors>
<title>Seeding statistical machine translation with translation memory output through tree-based structural alignment.</title>
<date>2010</date>
<booktitle>In Proceedings of the 4th Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<pages>43--51</pages>
<marker>Zhechev, van Genabith, 2010</marker>
<rawString>Ventsislav Zhechev and Josef van Genabith. 2010. Seeding statistical machine translation with translation memory output through tree-based structural alignment. In Proceedings of the 4th Workshop on Syntax and Structure in Statistical Translation, pages 43–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
<author>Franz Josef Och</author>
<author>Jay Ponte</author>
</authors>
<title>A systematic comparison of phrase-based, hierarchical and syntaxaugmented statistical MT.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>1145--1152</pages>
<contexts>
<context citStr="Zollmann et al., 2008" endWordPosition="3590" position="21355" startWordPosition="3587">gh fuzzy match intervals. Train Develop Test #Sentences 261,906 2,569 2,576 #Chn. Words 3,623,516 38,585 38,648 #Chn. VOC. 43,112 3,287 3,460 #Eng. Words 3,627,028 38,329 38,510 #Eng. VOC. 44,221 3,993 4,046 Table 1: Corpus Statistics Intervals #Sentences #Words W/S [0.9, 1.0) 269 4,468 16.6 [0.8, 0.9) 362 5,004 13.8 [0.7, 0.8) 290 4,046 14.0 [0.6, 0.7) 379 4,998 13.2 [0.5, 0.6) 472 6,073 12.9 [0.4, 0.5) 401 5,921 14.8 [0.3, 0.4) 305 5,499 18.0 (0.0, 0.3) 98 2,639 26.9 (0.0, 1.0) 2,576 38,648 15.0 Table 2: Corpus Statistics for Test-Set lingual sentences with Forced Decoding (Li et al., 2000; Zollmann et al., 2008; Auli et al., 2009; Wisniewski et al., 2010), which searches the best phrase segmentation for the specified output. Having repeated the above steps 20 times4, we obtain the corresponding phrase segmentations for the SMT training data (which will then be used to train the integrated models). Due to OOV words and insertion words, not all given source sentences can generate the desired results through forced decoding. Fortunately, in our work, 71.7% of the training bilingual sentences can generate the corresponding target results. The remaining 28.3% of the sentence pairs are thus not adopted fo</context>
</contexts>
<marker>Zollmann, Venugopal, Och, Ponte, 2008</marker>
<rawString>Andreas Zollmann, Ashish Venugopal, Franz Josef Och and Jay Ponte, 2008. A systematic comparison of phrase-based, hierarchical and syntaxaugmented statistical MT. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1145–1152.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>