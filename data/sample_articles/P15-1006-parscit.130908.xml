<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant confidence="0.000028" no="0">
<title confidence="0.892226">
Text to 3D Scene Generation with Rich Lexical Grounding
</title>
<author confidence="0.868239">
Angel Chang, Will Monroe; Manolis Savva,
Christopher Potts and Christopher D. Manning
</author>
<affiliation confidence="0.785258">
Stanford University, Stanford, CA 94305
</affiliation>
<email confidence="0.9645455">
{angelx,wmonroe4,msavva}@cs.stanford.edu,
{cgpotts,manning}@stanford.edu
</email>
<sectionHeader confidence="0.997378" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999959363636364">The ability to map descriptions of scenes to 3D geometric representations has many applications in areas such as art, education, and robotics. However, prior work on the text to 3D scene generation task has used manually specified object categories and language that identifies them. We introduce a dataset of 3D scenes annotated with natural language descriptions and learn from this data how to ground textual descriptions to physical objects. Our method successfully grounds a variety of lexical terms to concrete referents, and we show quantitatively that our method improves 3D scene generation over previous work using purely rule-based methods. We evaluate the fidelity and plausibility of 3D scenes generated with our grounding approach through human judgments. To ease evaluation on this task, we also introduce an automated metric that strongly correlates with human judgments.</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.949093888888889">We examine the task of text to 3D scene generation. The ability to map descriptions of scenes to 3D geometric representations has a wide variety of applications; many creative industries use 3D scenes. Robotics applications need to interpret commands referring to real-world environments, and the ability to visualize scenarios given highlevel descriptions is of great practical use in educational tools. Unfortunately, 3D scene design user interfaces are prohibitively complex for novice users. Prior work has shown the task remains challenging and time intensive for non-experts, even with simplified interfaces (Savva et al., 2014). *The first two authors are listed in alphabetical order. {...a multicolored table in the middle of the room..., ...four red and white chairs and a colorful table, ...}</bodyText>
<figureCaption confidence="0.723038">
Figure 1: We learn how to ground references such
as “L-shaped room” to 3D models in a paired corpus of 3D scenes and natural language descriptions.</figureCaption>
<bodyText confidence="0.98021703125">Sentence fragments in bold were identified as high-weighted references to the shown objects. Language offers a convenient way for designers to express their creative goals. Systems that can interpret natural descriptions to build a visual representation allow non-experts to visually express their thoughts with language, as was demonstrated by WordsEye, a pioneering work in text to 3D scene generation (Coyne and Sproat, 2001). WordsEye and other prior work in this area (Seversky and Yin, 2006; Chang et al., 2014) used manually chosen mappings between language and objects in scenes. To our knowledge, we present the first 3D scene generation approach that learns from data how to map textual terms to objects. First, we collect a dataset of 3D scenes along with textual descriptions by people, which we contribute to the community. We then train a classifier on a scene discrimination task and extract high-weight features that ground lexical terms to 3D models. We integrate our learned lexical groundings with a rule-based scene generation approach, and we show through a humanjudgment evaluation that the combination outperforms both approaches in isolation. Finally, we introduce a scene similarity metric that correlates with human judgments. {...L-shaped room with walls that have 2 tones of gray..., A dark room with a pool table...}</bodyText>
<page confidence="0.98884">
53
</page>
<note confidence="0.989575333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 53–62,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figureCaption confidence="0.996943">
Figure 2: Illustration of the text to 3D scene generation pipeline. The input is text describing a scene
</figureCaption>
<bodyText confidence="0.594484888888889">(left), which we parse into an abstract scene template representation capturing objects and relations (middle). The scene template is then used to generate a concrete 3D scene visualizing the input description (right). The 3D scene is constructed by retrieving and arranging appropriate 3D models. Generation Parsing &amp;quot;There is a desk and there is a notepad on the desk. There is a pen next to the notepad.&amp;quot;</bodyText>
<figure confidence="0.955174611111111">
o1
desk
o2
notepad
on(o1,o2)
o0 – category:room, modelId:420
o1 – category:desk, modelId:132
o2 – category:notepad, modelId:343
o3 – category:pen, modelId:144
on(o1,o3)
next_to(o3,o2)
o3
pen
Input Text Scene Template
3D Scene
o0
room
on(o0,o1)
</figure>
<sectionHeader confidence="0.978738" genericHeader="method">
2 Task Description
</sectionHeader>
<bodyText confidence="0.999943183673469">In the text to 3D scene generation task, the input is a natural language description, and the output is a 3D representation of a plausible scene that fits the description and can be viewed and rendered from multiple perspectives. More precisely, given an utterance x as input, the output is a scene y: an arrangement of 3D models representing objects at specified positions and orientations in space. In this paper, we focus on the subproblem of lexical grounding of textual terms to 3D model referents (i.e., choosing 3D models that represent objects referred to by terms in the input utterance x). We employ an intermediate scene template representation parsed from the input text to capture the physical objects present in a scene and constraints between them. This representation is then used to generate a 3D scene (Figure 2). A naive approach to scene generation might use keyword search to retrieve 3D models. However, such an approach is unlikely to generalize well in that it fails to capture important object attributes and spatial relations. In order for the generated scene to accurately reflect the input description, a deep understanding of language describing environments is necessary. Many challenging subproblems need to be tackled: physical object mention detection, estimation of object attributes such as size, extraction of spatial constraints, and placement of objects at appropriate relative positions and orientations. The subproblem of lexical grounding to 3D models has a larged impact on the quality of generated scenes, as later stages of scene generation rely on having a correctly chosen set of objects to arrange. Another challenge is that much common knowledge about the physical properties of objects and the structure of environments is rarely mentioned in natural language (e.g., that most tables are supported on the floor and in an upright orientation). Unfortunately, common 3D representations of objects and scenes used in computer graphics specify only geometry and appearance, and rarely include such information. Prior work in text to 3D scene generation focused on collecting manual annotations of object properties and relations (Rouhizadeh et al., 2011; Coyne et al., 2012), which are used to drive rule-based generation systems. Regrettably, the task of scene generation has not yet benefited from recent related work in NLP.</bodyText>
<sectionHeader confidence="0.999971" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.994425352941177">There is much prior work in image retrieval given textual queries; a recent overview is provided by Siddiquie et al. (2011). The image retrieval task bears some similarity to our task insofar as 3D scene retrieval is an approach that can approximate 3D scene generation. However, there are fundamental differences between 2D images and 3D scenes. Generation in image space has predominantly focused on composition of simple 2D clip art elements, as exemplified recently by Zitnick et al. (2013). The task of composing 3D scenes presents a much higherdimensional search space of scene configurations where finding plausible and desirable configurations is difficult. Unlike prior work in clip art generation which uses a small pre-specified set of objects, we ground to a large database of objects that can occur in various indoor environments: 12490
3D models from roughly 270 categories.</bodyText>
<page confidence="0.998794">
54
</page>
<bodyText confidence="0.997763">There is a bed and there is a There is a chair and a table. There is a table and there are four chairs. There nightstand next to the bed. are four plates and there are four sandwiches.</bodyText>
<listItem confidence="0.99890756">• There is a bed with three pillows and a bedside table next to it. • The room appears to be a bedroom. A blue bed and white nightstand are pushed against the furthest wall. A window is on the left side. • A dark bedroom with a queen bed with blue comforter and three pillows. There is a night stand. One wall is decorated with a large design and another wall has three large windows. • There is a chair and a circular table in the middle of a floral print room. • a corner widow room with a a table and chair sitting to the east side. • There's a dresser in the corner of the room, and a yellow table with a brown wooden chair. • dinning room with four plates, four chairs, and four sandwiches • dark room with two small windows. A rectangular table seating four is in the middle of the room with plates set. There is a set of two gray double doors on another wall. • i see a rectangular table in the center of the room. There are 4 chairs around the table and 4 plates on the table</listItem>
<figureCaption confidence="0.81016625">
Figure 3: Scenes created by participants from seed description sentences (top). Additional descriptions
provided by other participants from the created scene (bottom). Our dataset contains around 19 scenes
per seed sentence, for a total of 1129 scenes. Scenes exhibit variation in the specific objects chosen and
their placement. Each scene is described by 3 or 4 other people, for a total of 4358 descriptions.
</figureCaption>
<subsectionHeader confidence="0.999735">
3.1 Text to Scene Systems
</subsectionHeader>
<bodyText confidence="0.999982133333333">Pioneering work on the SHRDLU system (Winograd, 1972) demonstrated linguistic manipulation of objects in 3D scenes. However, the discourse domain was restricted to a micro-world with simple geometric shapes to simplify parsing and grounding of natural language input. More recently, prototype text to 3D scene generation systems have been built for broader domains, most notably the WordsEye system (Coyne and Sproat, 2001) and later work by Seversky and Yin (2006). Chang et al. (2014) showed it is possible to learn spatial priors for objects and relations directly from 3D scene data. These systems use manually defined mappings between language and their representation of the physical world. This prevents generalization to more complex object descriptions, variations in word choice and spelling, and other languages. It also forces users to use unnatural language to express their intent (e.g., the table is two feet to the south of the window). We propose reducing reliance on manual lexicons by learning to map descriptions to objects from a corpus of 3D scenes and associated textual descriptions. While we find that lexical knowledge alone is not sufficient to generate high-quality scenes, a learned approach to lexical grounding can be used in combination with a rule-based system for handling compositional knowledge, resulting in better scenes than either component alone.</bodyText>
<subsectionHeader confidence="0.986417">
3.2 Related Tasks
</subsectionHeader>
<bodyText confidence="0.999982387096774">Prior work has generated sentences that describe 2D images (Farhadi et al., 2010; Kulkarni et al., 2011; Karpathy et al., 2014) and referring expressions for specific objects in images (FitzGerald et al., 2013; Kazemzadeh et al., 2014). However, generating scenes is currently out of reach for purely image-based approaches. 3D scene representations serve as an intermediate level of structure between raw image pixels and simpler microcosms (e.g., grid and block worlds). This level of structure is amenable to the generation task but still realistic enough to present a variety of challenges associated with natural scenes. A related line of work focuses on grounding referring expressions to referents in 3D worlds with simple colored geometric shapes (Gorniak and Roy, 2004; Gorniak and Roy, 2005). More recent work grounds text to object attributes such as color and shape in images (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013). Golland et al. (2010) ground spatial relationship language in 3D scenes (e.g., to the left of, behind) by learning from pairwise object relations provided by crowdworkers. In contrast, we ground general descriptions to a wide variety of possible objects. The objects in our scenes represent a broader space of possible referents than the first two lines of work. Unlike the latter work, our descriptions are provided as unrestricted free-form text, rather than filling in specific templates of object references and fixed spatial relationships.</bodyText>
<page confidence="0.997283">
55
</page>
<sectionHeader confidence="0.998848" genericHeader="method">
4 Dataset
</sectionHeader>
<bodyText confidence="0.999987875">We introduce a new dataset of 1128 scenes and 4284 free-form natural language descriptions of these scenes.1 To create this training set, we used a simple online scene design interface that allows users to assemble scenes using available 3D models of common household objects (each model is annotated with a category label and has a unique ID). We used a set of 60 seed sentences describing simple configurations of interior scenes as prompts and asked workers on the Amazon Mechanical Turk crowdsourcing platform to create scenes corresponding to these seed descriptions. To obtain more varied descriptions for each scene, we asked other workers to describe each scene. Figure 3 shows examples of seed description sentences, 3D scenes created by people given those descriptions, and new descriptions provided by others viewing the created scenes. We manually examined a random subset of the descriptions (approximately 10%) to eliminate spam and unacceptably poor descriptions. When we identified an unacceptable description, we also examined all other descriptions by the same worker, as most poor descriptions came from a small number of workers. From our sample, we estimate that less than 3% of descriptions were spam or unacceptably incoherent. To reflect natural use, we retained minor typographical and grammatical errors. Despite the small set of seed sentences, the Turker-provided scenes exhibit much variety in the specific objects used and their placements within the scene. Over 600 distinct 3D models appear in at least one scene, and more than 40% of nonroom objects are rotated from their default orientation, despite the fact that this requires an extra manipulation in the scene-building interface. The descriptions collected for these scenes are similarly diverse and usually differ substantially in length and content from the seed sentences.2</bodyText>
<sectionHeader confidence="0.994511" genericHeader="method">
5 Model
</sectionHeader>
<bodyText confidence="0.999156">To create a model for generating scene templates from text, we train a classifier to learn lexical groundings.</bodyText>
<footnote confidence="0.823518333333333">
1Available at http://nlp.stanford.edu/data/
text2scene.shtml.
2Mean 26.2 words, SD 17.4; versus mean 16.6, SD 7.2
for the seed sentences. If one considers seed sentences to be
the “reference,” the macro-averaged BLEU score (Papineni
et al., 2002) of the Turker descriptions is 12.0.
</footnote>
<bodyText confidence="0.9999445">We then combine our learned lexical groundings with a rule-based scene generation model. The learned groundings allow us to select better models, while the rule-based model offers simple compositionality for handling coreference and relationships between objects.</bodyText>
<subsectionHeader confidence="0.997862">
5.1 Learning lexical groundings
</subsectionHeader>
<bodyText confidence="0.999991633333333">To learn lexical mappings from examples, we train a classifier on a related grounding task and extract the weights of lexical features for use in scene generation. This classifier learns from a “discrimination” version of our scene dataset, in which the scene in each scene–description pair is hidden among four other distractor scenes sampled uniformly at random. The training objective is to maximize the L2-regularized log likelihood of this scene discrimination dataset under a one-vs.all logistic regression model, using each true scene and each distractor scene as one example (with true/distractor as the output label). The learned model uses binary-valued features indicating the co-occurrence of a unigram or bigram and an object category or model ID. For example, features extracted from the scene-description pair shown in Figure 2 would include the tuples (desk, modelTd:132) and (the notepad, category:notepad). To evaluate our learned model’s performance at discriminating scenes, independently of its use in scene generation, we split our scene and description corpus (augmented with distractor scenes) randomly into train, development, and test portions 70%-15%-15% by scene. Using only model ID features, the classifier achieves a discrimination accuracy of 0.715 on the test set; adding features that use object categories as well as model IDs improves accuracy to 0.833.</bodyText>
<subsectionHeader confidence="0.746836">
5.2 Rule-based Model
</subsectionHeader>
<bodyText confidence="0.999426">We use the rule-based parsing component described in Chang et al. (2014). This system incorporates knowledge that is important for scene generation and not addressed by our learned model (e.g., spatial relationships and coreference). In Section 5.3, we describe how we use our learned model to augment this model. This rule-based approach is a three-stage process using established NLP systems: 1) The input text is split into multiple sentences and parsed using the Stanford CoreNLP pipeline (Manning et red cup round yellow table green room black top tan love seat black bed open window al., 2014).</bodyText>
<page confidence="0.993886">
56
</page>
<figureCaption confidence="0.98605">
Figure 4: Some examples extracted from the top 20 highest-weight features in our learned model: lexical
terms from the descriptions in our scene corpus are grounded to 3D models within the scene corpus.
</figureCaption>
<bodyText confidence="0.983273533333333">Head words of noun phrases are identified as candidate object categories, filtered using WordNet (Miller, 1995) to only include physical objects.2) References to the same object are collapsed using the Stanford coreference system.3) Properties are attached to each object by extracting other adjectives and nouns in the noun phrase. These properties are later used to query the 3D model database. We use the same model database as Chang et al.(2014) and also extract spatial relations between objects using the same set of dependency patterns.</bodyText>
<subsectionHeader confidence="0.874633">
5.3 Combined Model
</subsectionHeader>
<bodyText confidence="0.995046181818182">The rule-based parsing model is limited in its ability to choose appropriate 3D models. We integrate our learned lexical groundings with this model to build an improved scene generation system. Identifying object categories Using the rulebased model, we extract all noun phrases as potential objects. For each noun phrase p, we extract features fϕil and compute the score of a category c being described by the noun phrase as the sum of the feature weights from the learned model in Section 5.1: is the weight for associating feature ϕi with category c. From categories with a score higher than Tc = 0.5, we select the best-scoring category as the representative for the noun phrase. If no category’s score exceeds Tc, we use the head of the noun phrase for the object category. 3D model selection For each object mention detected in the description, we use the feature weights from the learned model to select a specific object to add to the scene. After using dependency rules to extract spatial relationships and descriptive terms associated with the object, we compute the score of a 3D modelmgiven the category c and a set of descriptive terms d using a similar sum of feature weights. As the rule-based system may not accurately identify the correct set of terms d, we augment the score with a sum of feature weights over the entire input description x: For the results shown here, d = 0.75 and x = 0.25. We select the best-scoring 3D model with positive score. If no model has positive score, we assume the object mention was spurious and omit the object.</bodyText>
<table confidence="0.999577857142857">
text category text category
chair Chair round RoundTable
lamp Lamp laptop Laptop
couch Couch fruit Bowl
vase Vase roundtable RoundTable
sofa Couch laptop Computer
bed Bed bookshelf Bookcase
</table>
<tableCaption confidence="0.999268">
Table 1: Top groundings of lexical terms in our
</tableCaption>
<sectionHeader confidence="0.995631" genericHeader="method">
6 Learned lexical groundings
</sectionHeader>
<bodyText confidence="0.995046181818182">By extracting high-weight features from our learned model, we can visualize specific models to which lexical terms are grounded (see Figure 4). These features correspond to high frequency text– 3D model pairs within the scene corpus. Table 1 shows some of the top learned lexical groundings to model database categories. We are able to recover many simple identity mappings without using lexical similarity features, and we capture several lexical variants (e.g., sofa for Couch). A few erroneous mappings reflect common cooccurrences; for example, fruit is mapped to Bowl due to fruit typically being observed in bowls in our dataset.</bodyText>
<page confidence="0.99226">
57
</page>
<figure confidence="0.9675204">
Description
Seed sentence:
There is a desk and a computer.
MTurk sentences:
A round table is in the center of the
</figure>
<figureCaption confidence="0.838701666666667">
room with four chairs around the
table. There is a double window facing
west. A door is on the east side of the
room.
In between the doors and the window,
there is a black couch with red
cushions, two white pillows, and one
black pillow. In front of the couch,
there is a wooden coffee table with a
glass top and two newspapers. Next
to the table, facing the couch, is a
wooden folding chair.
random learned rule combo
Figure 5: Qualitative comparison of generated scenes for three input descriptions (one Seed and two
MTurk), using the four different methods: random, learned, rule, combo.
</figureCaption>
<sectionHeader confidence="0.996901" genericHeader="evaluation and result">
7 Experimental Results
</sectionHeader>
<bodyText confidence="0.999967851851852">We conduct a human judgment experiment to compare the quality of generated scenes using the approaches we presented and baseline methods. To evaluate whether lexical grounding improves scene generation, we need a method to arrange the chosen models into 3D scenes. Since 3D scene layout is not a focus of our work, we use an approach based on prior work in 3D scene synthesis and text to scene generation (Fisher et al., 2012; Chang et al., 2014), simplified by using sampling rather than a hill climbing strategy. Conditions We compare five conditions: {random, learned, rule, combo, human}. The random condition represents a baseline which synthesizes a scene with randomly-selected models, while human represents scenes created by people. The learned condition takes our learned lexical groundings, picks the four3 most likely objects, and generates a scene based on them. The rule and combo conditions use scenes generated by the rule-based approach and the combined model, respectively. Descriptions We consider two sets of input descriptions: {Seeds, MTurk}. The Seeds descriptions are 50 of the initial seed sentences from which workers were asked to create scenes. These seed sentences were simple (e.g., There is a desk and a chair, There is a plate on a table) and did not have modifiers describing the objects.</bodyText>
<footnote confidence="0.7493395">
3The average number of objects in a scene in our human-
built dataset was 3.9.
</footnote>
<bodyText confidence="0.998585666666667">The MTurk descriptions are much more descriptive and exhibit a wider variety in language (including misspellings and ungrammatical constructs). Our hypothesis was that the rule-based system would perform well on the simple Seeds descriptions, but it would be insufficient for handling the complexities of the more varied MTurk descriptions. For these more natural descriptions, we expected our combination model to perform better. Our experimental results confirm this hypothesis.</bodyText>
<subsectionHeader confidence="0.991678">
7.1 Qualitative Evaluation
</subsectionHeader>
<bodyText confidence="0.99991875">Figure 5 shows a qualitative comparison of 3D scenes generated from example input descriptions using each of the four methods. In the top row, the rule-based approach selects a CPU chassis for computer, while combo and learned select a more iconic monitor. In the bottom row, the rule-based approach selects two newspapers and places them on the floor, while the combined approach correctly selects a coffee table with two newspapers on it. The learned model is limited to four objects and does not have a notion of object identity, so it often duplicates objects.</bodyText>
<subsectionHeader confidence="0.996581">
7.2 Human Evaluation
</subsectionHeader>
<bodyText confidence="0.999876666666667">We performed an experiment in which people rated the degree to which scenes match the textual descriptions from which they were generated.</bodyText>
<page confidence="0.996492">
58
</page>
<figure confidence="0.711886166666667">
method Seeds MTurk
random 2.03 (1.88–2.18) 1.68 (1.57–1.79)
learned 3.51 (3.23–3.77) 2.61 (2.40–2.84)
rule 5.44 (5.26–5.61) 3.15 (2.91–3.40)
combo 5.23 (4.96–5.44) 3.73 (3.48–3.95)
human 6.06 (5.90 – 6.19) 5.87 (5.74–6.00)
</figure>
<figureCaption confidence="0.820293">
Table 2: Average scene-description match ratings
across sentence types and methods (95% C.I.).
Figure 6: Screenshot of the UI for rating scene-
description match.
</figureCaption>
<bodyText confidence="0.999862322033898">cance of our results.4 We treat the participant and the specific scene as random effects of varying intercept, and the method condition as the fixed effect. Such ratings are a natural way to evaluate how well our approach can generate scenes from text: in practical use, a person would provide an input description and then judge the suitability of the resulting scenes. For the MTurk descriptions, we randomly sampled 100 descriptions from the development split of our dataset. Procedure During the experiment, each participant was shown 30 pairs of scene descriptions and generated 3D scenes drawn randomly from all five conditions. All participants provided 30 responses each for a total of 5040 scene-description ratings. Participants were asked to rate how well the generated scene matched the input description on a 7point Likert scale, with 1 indicating a poor match and 7 a very good one (see Figure 6). In a separate task with the same experimental procedure, we asked other participants to rate the overall plausibility of each generated scene without a reference description. This plausibility rating measures whether a method can generate plausible scenes irrespective of the degree to which the input description is matched. We used Amazon Mechanical Turk to recruit 168 participants for rating the match of scenes to descriptions and 63 participants for rating scene plausibility. Design The experiment followed a withinsubjects factorial design. The dependent measure was the Likert rating. Since per-participant and per-scene variance on the rating is not accounted for by a standard ANOVA, we use a mixed effects model which can account for both fixed effects and random effects to determine the statistical signifiResults There was a significant effect of the method condition on the scene-description match rating: x2(4, N = 5040) = 1378.2,p &lt; 0.001. Table 2 summarizes the average scene-description match ratings and 95% confidence intervals for all sentence type–condition pairs. All pairwise differences between ratings were significant under Wilcoxon rank-sum tests with the BonferroniHolm correction (p &lt; 0.05). The scene plausibility ratings, which were obtained independent of descriptions, indicated that the only significant difference in plausibility was between scenes created by people (human) and all the other conditions. We see that for the simple seed sentences both the rule-based and combined model approach the quality of human-created scenes. However, all methods have significantly lower ratings for the more complex MTurk sentences. In this more challenging scenario, the combined model is closest to the manually created scenes and significantly outperforms both rule-based and learned models in isolation.</bodyText>
<subsectionHeader confidence="0.451018">
7.3 Error Analysis
</subsectionHeader>
<bodyText confidence="0.629037222222222">Figure 7 shows some common error cases in our system. The top left scene was generated with the rule-based method, the top right with the learned method, and the bottom two with the combined approach. At the top left, there is an erroneous selection of concrete object category (wood logs) for the four wood chairs reference in the input description, due to an incorrect head identification. At top right, the learned model identifies the presence of brown desk and lamp but erroneously picks two desks and two lamps (since we always pick the top four objects).</bodyText>
<footnote confidence="0.863176">
4We used the lme4 R package and optimized fit with
maximum log-likelihood (Baayen et al., 2008). We report
significance results using the likelihood-ratio (LR) test.
</footnote>
<page confidence="0.993596">
59
</page>
<table confidence="0.989741666666667">
ASTS(s, z) = mAax
E(n,n′)EA S(n, n′)
J(A) + JAI .
method Human ASTS
random 1.68 0.08
learned 2.61 0.23
rule 3.15 0.32
combo 3.73 0.44
score over all such alignments:
</table>
<tableCaption confidence="0.972702">
Table 3: Average human ratings (out of 7) and
aligned scene template similarity scores.
</tableCaption>
<figureCaption confidence="0.979896">
Figure 7: Common scene generation errors. From
top left clockwise: Wood table and four wood chairs in the center of the room; There is a black and brown desk with a table lamp and flowers; There is a white desk, a black chair, and a lamp in the corner of the room; There in the middle is a table, on the table is a cup.</figureCaption>
<bodyText confidence="0.976263625">The scene on the bottom right does not obey the expressed spatial constraints (in the corner of the room) since our system does not understand the grounding of room corner and that the top right side is not a good fit due to the door. In the bottom left, incorrect coreference resolution results in two tables for There in the middle is a table, on the table is a cup.</bodyText>
<subsectionHeader confidence="0.99935">
7.4 Scene Similarity Metric
</subsectionHeader>
<bodyText confidence="0.999942666666667">We introduce an automated metric for scoring scenes given a scene template representation, the aligned scene template similarity (ASTS). Given a one-to-one alignment A between the nodes of a scene template and the objects in a scene, let the alignment penalty J(A) be the sum of the number of unaligned nodes in the scene template and the number of unaligned objects in the scene. For the aligned nodes, we compute a similarity score S per node pair (n, n′) where S(n, n′) = 1 if the model ID matches, S(n, n′) = 0.5 if only the category matches and 0 otherwise. We define the ASTS of a scene with respect to a scene template to be the maximum alignment With this definition, we compare average ASTS scores for each method against average human ratings (Table 3). We test the correlation of the ASTS metric against human ratings using Pearson’s r and Kendall’s rank correlation coefficient rT. We find that ASTS and human ratings are strongly correlated (r = 0.70, rT = 0.49, p &lt; 0.001). This suggests ASTS scores could be used to train and algorithmically evaluate scene generation systems that map descriptions to scene templates.</bodyText>
<sectionHeader confidence="0.9996" genericHeader="conclusion">
8 Future Work
</sectionHeader>
<bodyText confidence="0.999965565217391">Many error cases in our generated scenes resulted from not interpreting spatial relations. An obvious improvement would be to expand our learned lexical grounding approach to include spatial relations. This would help with spatial language that is not handled by the rule-based system’s dependency patterns (e.g., around, between, on the east side). One approach would be to add spatial constraints to the definition of our scene similarity score and use this improved metric in training a semantic parser to generate scene templates. To choose objects, our current system uses information obtained from language–object cooccurrences and sparse manually-annotated category labels; another promising avenue for achieving better lexical grounding is to propagate category labels using geometric and image features to learn the categories of unlabeled objects. Novel categories can also be extracted from Turker descriptions. These new labels could be used to improve the annotations in our 3D model database, enabling a wider range of object types to be used in scene generation.</bodyText>
<page confidence="0.994649">
60
</page>
<bodyText confidence="0.999963666666667">Our approach learns object references without using lexical similarity features or a manuallyassembled lexicon. Thus, we expect that our method for lexical grounding can facilitate development of text-to-scene systems in other languages. However, additional data collection and experiments are necessary to confirm this and identify challenges specific to other languages. The necessity of handling omitted information suggests that a model incorporating a more sophisticated theory of pragmatic inference could be beneficial. Another important problem not addressed here is the role of context and discourse in interpreting scene descriptions. For example, several of our collected descriptions include language imagining embodied presence in the scene (e.g., The wooden table is to your right, if you’re entering the room from the doors).</bodyText>
<sectionHeader confidence="0.998479" genericHeader="conclusion">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999948285714286">Prior work in 3D scene generation relies on purely rule-based methods to map object references to concrete 3D objects. We introduce a dataset of 3D scenes annotated with natural language descriptions which we believe will be of great interest to the research community. Using this corpus of scenes and descriptions, we present an approach that learns from data how to ground textual descriptions to objects. To evaluate how our grounding approach impacts generated scene quality, we collect human judgments of generated scenes. In addition, we present a metric for automatically comparing generated scene templates to scenes, and we show that it correlates strongly with human judgments. We demonstrate that rich lexical grounding can be learned directly from an unaligned corpus of 3D scenes and natural language descriptions, and that our model can successfully ground lexical terms to concrete referents, improving scene generation over baselines adapted from prior work.</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999490625">We thank Katherine Breeden for valuable feedback. The authors gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract no. FA875013-2-0040, the National Science Foundation under grant no. IIS 1159679, the Department of the Navy, Office of Naval Research, under grant no. N00014-10-1-0109, and the Stanford Graduate Fellowship fund. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation, the Office of Naval Research, DARPA, AFRL, or the US government.</bodyText>
<sectionHeader confidence="0.998796" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999123431818182">
R.H. Baayen, D.J. Davidson, and D.M. Bates. 2008.
Mixed-effects modeling with crossed random effects
for subjects and items. Journal of Memory and Lan-
guage, 59(4):390–412.
Angel X. Chang, Manolis Savva, and Christopher D.
Manning. 2014. Learning spatial knowledge for
text to 3D scene generation. In Proceedings of
Empirical Methods in Natural Language Processing
(EMNLP).
Bob Coyne and Richard Sproat. 2001. WordsEye: an
automatic text-to-scene conversion system. In Pro-
ceedings of the 28th Annual Conference on Com-
puter Graphics and Interactive Techniques.
Bob Coyne, Alexander Klapheke, Masoud
Rouhizadeh, Richard Sproat, and Daniel Bauer.
2012. Annotation tools and knowledge represen-
tation for a text-to-scene system. Proceedings of
COLING 2012: Technical Papers.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every pic-
ture tells a story: Generating sentences from images.
In Computer Vision–ECCV 2010.
Matthew Fisher, Daniel Ritchie, Manolis Savva,
Thomas Funkhouser, and Pat Hanrahan. 2012.
Example-based synthesis of 3D object arrange-
ments. ACM Transactions on Graphics (TOG),
31(6):135.
Nicholas FitzGerald, Yoav Artzi, and Luke Zettle-
moyer. 2013. Learning distributions over logical
forms for referring expression generation. In Pro-
ceedings of Empirical Methods in Natural Language
Processing (EMNLP).
Dave Golland, Percy Liang, and Dan Klein. 2010.
A game-theoretic approach to generating spatial de-
scriptions. In Proceedings of Empirical Methods in
Natural Language Processing (EMNLP).
Peter Gorniak and Deb Roy. 2004. Grounded semantic
composition for visual scenes. Journal of Artificial
Intelligence Research (JAIR), 21(1):429–470.
Peter Gorniak and Deb Roy. 2005. Probabilistic
grounding of situated speech using plan recognition
and reference resolution. In Proceedings of the 7th
International Conference on Multimodal Interfaces.
</reference>
<page confidence="0.983344">
61
</page>
<reference confidence="0.999692712121212">
Andrej Karpathy, Armand Joulin, and Li Fei-Fei. 2014.
Deep fragment embeddings for bidirectional image
sentence mapping. In Advances in Neural Informa-
tion Processing Systems.
Sahar Kazemzadeh, Vicente Ordonez, Mark Matten,
and Tamara L. Berg. 2014. ReferItGame: Refer-
ring to objects in photographs of natural scenes. In
Proceedings of Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Jayant Krishnamurthy and Thomas Kollar. 2013.
Jointly learning to parse and perceive: Connecting
natural language to the physical world. Transac-
tions of the Association for Computational Linguis-
tics, 1:193–206.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. Baby talk: Understanding and gener-
ating simple image descriptions. In IEEE Confer-
ence on Computer Vision and Pattern Recognition
(CVPR).
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics: System Demonstrations.
Cynthia Matuszek, Nicholas FitzGerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012. A joint
model of language and perception for grounded at-
tribute learning. In International Conference on Ma-
chine Learning (ICML).
George A. Miller. 1995. WordNet: A lexical
database for English. Communications of the ACM,
38(11):39–41.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics.
Masoud Rouhizadeh, Margit Bowler, Richard Sproat,
and Bob Coyne. 2011. Collecting semantic data by
Mechanical Turk for the lexical knowledge resource
of a text-to-picture generating system. In Proceed-
ings of the Ninth International Conference on Com-
putational Semantics.
Manolis Savva, Angel X. Chang, Gilbert Bernstein,
Christopher D. Manning, and Pat Hanrahan. 2014.
On being the right scale: Sizing large collections of
3D models. In SIGGRAPH Asia 2014 Workshop
on Indoor Scene Understanding: Where Graphics
meets Vision.
Lee M. Seversky and Lijun Yin. 2006. Real-time au-
tomatic 3D scene generation from natural language
voice and text descriptions. In Proceedings of the
14th Annual ACM International Conference on Mul-
timedia.
Behjat Siddiquie, Rog´erio Schmidt Feris, and Larry S.
Davis. 2011. Image ranking and retrieval based
on multi-attribute queries. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR).
Terry Winograd. 1972. Understanding natural lan-
guage. Cognitive Psychology, 3(1):1–191.
C. Lawrence Zitnick, Devi Parikh, and Lucy Vander-
wende. 2013. Learning the visual interpretation
of sentences. In IEEE International Conference on
Computer Vision (ICCV).
</reference>
<page confidence="0.999183">
62
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant confidence="0.544564" no="0">
<title confidence="0.73921">Text to 3D Scene Generation with Rich Lexical Grounding</title>
<author confidence="0.663009">D Potts</author>
<affiliation confidence="0.574242">Stanford University, Stanford, CA</affiliation>
<abstract confidence="0.999801652173913">The ability to map descriptions of scenes to 3D geometric representations has many applications in areas such as art, education, and robotics. However, prior work the to 3D scene generation has used manually specified object categories and language that identifies them. We introduce a dataset of 3D scenes annotated with natural language descriptions and learn from this data how to ground textual descriptions to physical objects. Our method successfully grounds a variety of lexical terms to concrete referents, and we show quantitatively that our method improves 3D scene generation over previous work using purely rule-based methods. We evaluate the fidelity and plausibility of 3D scenes generated with our grounding approach through human judgments. To ease evaluation on this task, we also introduce an automated metric that strongly correlates with human judgments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R H Baayen</author>
<author>D J Davidson</author>
<author>D M Bates</author>
</authors>
<title>Mixed-effects modeling with crossed random effects for subjects and items.</title>
<date>2008</date>
<journal>Journal of Memory and Language,</journal>
<volume>59</volume>
<issue>4</issue>
<contexts>
<context citStr="Baayen et al., 2008" endWordPosition="4442" position="27323" startWordPosition="4439">antly outperforms both rule-based and learned models in isolation. 7.3 Error Analysis Figure 7 shows some common error cases in our system. The top left scene was generated with the rule-based method, the top right with the learned method, and the bottom two with the combined approach. At the top left, there is an erroneous selection of concrete object category (wood logs) for the four wood chairs reference in the input description, due to an incorrect head identification. At top right, the learned model identifies the 4We used the lme4 R package and optimized fit with maximum log-likelihood (Baayen et al., 2008). We report significance results using the likelihood-ratio (LR) test. 59 ASTS(s, z) = mAax E(n,n′)EA S(n, n′) J(A) + JAI . method Human ASTS random 1.68 0.08 learned 2.61 0.23 rule 3.15 0.32 combo 3.73 0.44 score over all such alignments: Table 3: Average human ratings (out of 7) and aligned scene template similarity scores. Figure 7: Common scene generation errors. From top left clockwise: Wood table and four wood chairs in the center of the room; There is a black and brown desk with a table lamp and flowers; There is a white desk, a black chair, and a lamp in the corner of the room; There i</context>
</contexts>
<marker>Baayen, Davidson, Bates, 2008</marker>
<rawString>R.H. Baayen, D.J. Davidson, and D.M. Bates. 2008. Mixed-effects modeling with crossed random effects for subjects and items. Journal of Memory and Language, 59(4):390–412.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angel X Chang</author>
<author>Manolis Savva</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning spatial knowledge for text to 3D scene generation.</title>
<date>2014</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context citStr="Chang et al., 2014" endWordPosition="407" position="2636" startWordPosition="404">ch as “L-shaped room” to 3D models in a paired corpus of 3D scenes and natural language descriptions. Sentence fragments in bold were identified as high-weighted references to the shown objects. Language offers a convenient way for designers to express their creative goals. Systems that can interpret natural descriptions to build a visual representation allow non-experts to visually express their thoughts with language, as was demonstrated by WordsEye, a pioneering work in text to 3D scene generation (Coyne and Sproat, 2001). WordsEye and other prior work in this area (Seversky and Yin, 2006; Chang et al., 2014) used manually chosen mappings between language and objects in scenes. To our knowledge, we present the first 3D scene generation approach that learns from data how to map textual terms to objects. First, we collect a dataset of 3D scenes along with textual descriptions by people, which we contribute to the community. We then train a classifier on a scene discrimination task and extract high-weight features that ground lexical terms to 3D models. We integrate our learned lexical groundings with a rule-based scene generation approach, and we show through a humanjudgment evaluation that the comb</context>
<context citStr="Chang et al. (2014)" endWordPosition="1617" position="9875" startWordPosition="1614">sen and their placement. Each scene is described by 3 or 4 other people, for a total of 4358 descriptions. 3.1 Text to Scene Systems Pioneering work on the SHRDLU system (Winograd, 1972) demonstrated linguistic manipulation of objects in 3D scenes. However, the discourse domain was restricted to a micro-world with simple geometric shapes to simplify parsing and grounding of natural language input. More recently, prototype text to 3D scene generation systems have been built for broader domains, most notably the WordsEye system (Coyne and Sproat, 2001) and later work by Seversky and Yin (2006). Chang et al. (2014) showed it is possible to learn spatial priors for objects and relations directly from 3D scene data. These systems use manually defined mappings between language and their representation of the physical world. This prevents generalization to more complex object descriptions, variations in word choice and spelling, and other languages. It also forces users to use unnatural language to express their intent (e.g., the table is two feet to the south of the window). We propose reducing reliance on manual lexicons by learning to map descriptions to objects from a corpus of 3D scenes and associated </context>
<context citStr="Chang et al. (2014)" endWordPosition="2634" position="16346" startWordPosition="2631">les (desk, modelTd:132) and (the notepad, category:notepad). To evaluate our learned model’s performance at discriminating scenes, independently of its use in scene generation, we split our scene and description corpus (augmented with distractor scenes) randomly into train, development, and test portions 70%-15%-15% by scene. Using only model ID features, the classifier achieves a discrimination accuracy of 0.715 on the test set; adding features that use object categories as well as model IDs improves accuracy to 0.833. 5.2 Rule-based Model We use the rule-based parsing component described in Chang et al. (2014). This system incorporates knowledge that is important for scene generation and not addressed by our learned model (e.g., spatial relationships and coreference). In Section 5.3, we describe how we use our learned model to augment this model. This rule-based approach is a three-stage process using established NLP systems: 1) The input text is split into multiple sentences and parsed using the Stanford CoreNLP pipeline (Manning et 56 Figure 4: Some examples extracted from the top 20 highest-weight features in our learned model: lexical terms from the descriptions in our scene corpus are grounded</context>
<context citStr="Chang et al., 2014" endWordPosition="3506" position="21446" startWordPosition="3503">comparison of generated scenes for three input descriptions (one Seed and two MTurk), using the four different methods: random, learned, rule, combo. 7 Experimental Results We conduct a human judgment experiment to compare the quality of generated scenes using the approaches we presented and baseline methods. To evaluate whether lexical grounding improves scene generation, we need a method to arrange the chosen models into 3D scenes. Since 3D scene layout is not a focus of our work, we use an approach based on prior work in 3D scene synthesis and text to scene generation (Fisher et al., 2012; Chang et al., 2014), simplified by using sampling rather than a hill climbing strategy. Conditions We compare five conditions: {random, learned, rule, combo, human}. The random condition represents a baseline which synthesizes a scene with randomly-selected models, while human represents scenes created by people. The learned condition takes our learned lexical groundings, picks the four3 most likely objects, and generates a scene based on them. The rule and combo conditions use scenes generated by the rule-based approach and the combined model, respectively. Descriptions We consider two sets of input description</context>
</contexts>
<marker>Chang, Savva, Manning, 2014</marker>
<rawString>Angel X. Chang, Manolis Savva, and Christopher D. Manning. 2014. Learning spatial knowledge for text to 3D scene generation. In Proceedings of Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coyne</author>
<author>Richard Sproat</author>
</authors>
<title>WordsEye: an automatic text-to-scene conversion system.</title>
<date>2001</date>
<booktitle>In Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques.</booktitle>
<contexts>
<context citStr="Coyne and Sproat, 2001" endWordPosition="391" position="2547" startWordPosition="388">ed and white chairs and a colorful table, ...} Figure 1: We learn how to ground references such as “L-shaped room” to 3D models in a paired corpus of 3D scenes and natural language descriptions. Sentence fragments in bold were identified as high-weighted references to the shown objects. Language offers a convenient way for designers to express their creative goals. Systems that can interpret natural descriptions to build a visual representation allow non-experts to visually express their thoughts with language, as was demonstrated by WordsEye, a pioneering work in text to 3D scene generation (Coyne and Sproat, 2001). WordsEye and other prior work in this area (Seversky and Yin, 2006; Chang et al., 2014) used manually chosen mappings between language and objects in scenes. To our knowledge, we present the first 3D scene generation approach that learns from data how to map textual terms to objects. First, we collect a dataset of 3D scenes along with textual descriptions by people, which we contribute to the community. We then train a classifier on a scene discrimination task and extract high-weight features that ground lexical terms to 3D models. We integrate our learned lexical groundings with a rule-base</context>
<context citStr="Coyne and Sproat, 2001" endWordPosition="1605" position="9812" startWordPosition="1602">f 1129 scenes. Scenes exhibit variation in the specific objects chosen and their placement. Each scene is described by 3 or 4 other people, for a total of 4358 descriptions. 3.1 Text to Scene Systems Pioneering work on the SHRDLU system (Winograd, 1972) demonstrated linguistic manipulation of objects in 3D scenes. However, the discourse domain was restricted to a micro-world with simple geometric shapes to simplify parsing and grounding of natural language input. More recently, prototype text to 3D scene generation systems have been built for broader domains, most notably the WordsEye system (Coyne and Sproat, 2001) and later work by Seversky and Yin (2006). Chang et al. (2014) showed it is possible to learn spatial priors for objects and relations directly from 3D scene data. These systems use manually defined mappings between language and their representation of the physical world. This prevents generalization to more complex object descriptions, variations in word choice and spelling, and other languages. It also forces users to use unnatural language to express their intent (e.g., the table is two feet to the south of the window). We propose reducing reliance on manual lexicons by learning to map des</context>
</contexts>
<marker>Coyne, Sproat, 2001</marker>
<rawString>Bob Coyne and Richard Sproat. 2001. WordsEye: an automatic text-to-scene conversion system. In Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coyne</author>
<author>Alexander Klapheke</author>
<author>Masoud Rouhizadeh</author>
<author>Richard Sproat</author>
<author>Daniel Bauer</author>
</authors>
<title>Annotation tools and knowledge representation for a text-to-scene system.</title>
<date>2012</date>
<booktitle>Proceedings of COLING 2012: Technical Papers.</booktitle>
<contexts>
<context citStr="Coyne et al., 2012" endWordPosition="1058" position="6721" startWordPosition="1055">a correctly chosen set of objects to arrange. Another challenge is that much common knowledge about the physical properties of objects and the structure of environments is rarely mentioned in natural language (e.g., that most tables are supported on the floor and in an upright orientation). Unfortunately, common 3D representations of objects and scenes used in computer graphics specify only geometry and appearance, and rarely include such information. Prior work in text to 3D scene generation focused on collecting manual annotations of object properties and relations (Rouhizadeh et al., 2011; Coyne et al., 2012), which are used to drive rule-based generation systems. Regrettably, the task of scene generation has not yet benefited from recent related work in NLP. 3 Related Work There is much prior work in image retrieval given textual queries; a recent overview is provided by Siddiquie et al. (2011). The image retrieval task bears some similarity to our task insofar as 3D scene retrieval is an approach that can approximate 3D scene generation. However, there are fundamental differences between 2D images and 3D scenes. Generation in image space has predominantly focused on composition of simple 2D clip</context>
</contexts>
<marker>Coyne, Klapheke, Rouhizadeh, Sproat, Bauer, 2012</marker>
<rawString>Bob Coyne, Alexander Klapheke, Masoud Rouhizadeh, Richard Sproat, and Daniel Bauer. 2012. Annotation tools and knowledge representation for a text-to-scene system. Proceedings of COLING 2012: Technical Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Farhadi</author>
<author>Mohsen Hejrati</author>
<author>Mohammad Amin Sadeghi</author>
<author>Peter Young</author>
<author>Cyrus Rashtchian</author>
<author>Julia Hockenmaier</author>
<author>David Forsyth</author>
</authors>
<title>Every picture tells a story: Generating sentences from images.</title>
<date>2010</date>
<booktitle>In Computer Vision–ECCV</booktitle>
<contexts>
<context citStr="Farhadi et al., 2010" endWordPosition="1775" position="10875" startWordPosition="1772">uage to express their intent (e.g., the table is two feet to the south of the window). We propose reducing reliance on manual lexicons by learning to map descriptions to objects from a corpus of 3D scenes and associated textual descriptions. While we find that lexical knowledge alone is not sufficient to generate high-quality scenes, a learned approach to lexical grounding can be used in combination with a rule-based system for handling compositional knowledge, resulting in better scenes than either component alone. 3.2 Related Tasks Prior work has generated sentences that describe 2D images (Farhadi et al., 2010; Kulkarni et al., 2011; Karpathy et al., 2014) and referring expressions for specific objects in images (FitzGerald et al., 2013; Kazemzadeh et al., 2014). However, generating scenes is currently out of reach for purely image-based approaches. 3D scene representations serve as an intermediate level of structure between raw image pixels and simpler microcosms (e.g., grid and block worlds). This level of structure is amenable to the generation task but still realistic enough to present a variety of challenges associated with natural scenes. A related line of work focuses on grounding referring </context>
</contexts>
<marker>Farhadi, Hejrati, Sadeghi, Young, Rashtchian, Hockenmaier, Forsyth, 2010</marker>
<rawString>Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, and David Forsyth. 2010. Every picture tells a story: Generating sentences from images. In Computer Vision–ECCV 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Fisher</author>
<author>Daniel Ritchie</author>
<author>Manolis Savva</author>
<author>Thomas Funkhouser</author>
<author>Pat Hanrahan</author>
</authors>
<title>Example-based synthesis of 3D object arrangements.</title>
<date>2012</date>
<journal>ACM Transactions on Graphics (TOG),</journal>
<volume>31</volume>
<issue>6</issue>
<contexts>
<context citStr="Fisher et al., 2012" endWordPosition="3502" position="21425" startWordPosition="3499">igure 5: Qualitative comparison of generated scenes for three input descriptions (one Seed and two MTurk), using the four different methods: random, learned, rule, combo. 7 Experimental Results We conduct a human judgment experiment to compare the quality of generated scenes using the approaches we presented and baseline methods. To evaluate whether lexical grounding improves scene generation, we need a method to arrange the chosen models into 3D scenes. Since 3D scene layout is not a focus of our work, we use an approach based on prior work in 3D scene synthesis and text to scene generation (Fisher et al., 2012; Chang et al., 2014), simplified by using sampling rather than a hill climbing strategy. Conditions We compare five conditions: {random, learned, rule, combo, human}. The random condition represents a baseline which synthesizes a scene with randomly-selected models, while human represents scenes created by people. The learned condition takes our learned lexical groundings, picks the four3 most likely objects, and generates a scene based on them. The rule and combo conditions use scenes generated by the rule-based approach and the combined model, respectively. Descriptions We consider two sets</context>
</contexts>
<marker>Fisher, Ritchie, Savva, Funkhouser, Hanrahan, 2012</marker>
<rawString>Matthew Fisher, Daniel Ritchie, Manolis Savva, Thomas Funkhouser, and Pat Hanrahan. 2012. Example-based synthesis of 3D object arrangements. ACM Transactions on Graphics (TOG), 31(6):135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas FitzGerald</author>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Learning distributions over logical forms for referring expression generation.</title>
<date>2013</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context citStr="FitzGerald et al., 2013" endWordPosition="1796" position="11004" startWordPosition="1793">lexicons by learning to map descriptions to objects from a corpus of 3D scenes and associated textual descriptions. While we find that lexical knowledge alone is not sufficient to generate high-quality scenes, a learned approach to lexical grounding can be used in combination with a rule-based system for handling compositional knowledge, resulting in better scenes than either component alone. 3.2 Related Tasks Prior work has generated sentences that describe 2D images (Farhadi et al., 2010; Kulkarni et al., 2011; Karpathy et al., 2014) and referring expressions for specific objects in images (FitzGerald et al., 2013; Kazemzadeh et al., 2014). However, generating scenes is currently out of reach for purely image-based approaches. 3D scene representations serve as an intermediate level of structure between raw image pixels and simpler microcosms (e.g., grid and block worlds). This level of structure is amenable to the generation task but still realistic enough to present a variety of challenges associated with natural scenes. A related line of work focuses on grounding referring expressions to referents in 3D worlds with simple colored geometric shapes (Gorniak and Roy, 2004; Gorniak and Roy, 2005). More r</context>
</contexts>
<marker>FitzGerald, Artzi, Zettlemoyer, 2013</marker>
<rawString>Nicholas FitzGerald, Yoav Artzi, and Luke Zettlemoyer. 2013. Learning distributions over logical forms for referring expression generation. In Proceedings of Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dave Golland</author>
<author>Percy Liang</author>
<author>Dan Klein</author>
</authors>
<title>A game-theoretic approach to generating spatial descriptions.</title>
<date>2010</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context citStr="Golland et al. (2010)" endWordPosition="1919" position="11761" startWordPosition="1916">ions serve as an intermediate level of structure between raw image pixels and simpler microcosms (e.g., grid and block worlds). This level of structure is amenable to the generation task but still realistic enough to present a variety of challenges associated with natural scenes. A related line of work focuses on grounding referring expressions to referents in 3D worlds with simple colored geometric shapes (Gorniak and Roy, 2004; Gorniak and Roy, 2005). More recent work grounds text to object attributes such as color and shape in images (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013). Golland et al. (2010) ground spatial relationship language in 3D scenes (e.g., to the left of, behind) by learning from pairwise object relations provided by crowdworkers. In contrast, we ground general descriptions to a wide variety of possible objects. The objects in our scenes represent a broader space of possible referents than the first two lines of work. Unlike the latter work, our descriptions are provided as unrestricted free-form text, rather than filling in specific templates of object references and fixed spatial relationships. 55 4 Dataset We introduce a new dataset of 1128 scenes and 4284 free-form na</context>
</contexts>
<marker>Golland, Liang, Klein, 2010</marker>
<rawString>Dave Golland, Percy Liang, and Dan Klein. 2010. A game-theoretic approach to generating spatial descriptions. In Proceedings of Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Gorniak</author>
<author>Deb Roy</author>
</authors>
<title>Grounded semantic composition for visual scenes.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<volume>21</volume>
<issue>1</issue>
<contexts>
<context citStr="Gorniak and Roy, 2004" endWordPosition="1887" position="11572" startWordPosition="1884">r specific objects in images (FitzGerald et al., 2013; Kazemzadeh et al., 2014). However, generating scenes is currently out of reach for purely image-based approaches. 3D scene representations serve as an intermediate level of structure between raw image pixels and simpler microcosms (e.g., grid and block worlds). This level of structure is amenable to the generation task but still realistic enough to present a variety of challenges associated with natural scenes. A related line of work focuses on grounding referring expressions to referents in 3D worlds with simple colored geometric shapes (Gorniak and Roy, 2004; Gorniak and Roy, 2005). More recent work grounds text to object attributes such as color and shape in images (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013). Golland et al. (2010) ground spatial relationship language in 3D scenes (e.g., to the left of, behind) by learning from pairwise object relations provided by crowdworkers. In contrast, we ground general descriptions to a wide variety of possible objects. The objects in our scenes represent a broader space of possible referents than the first two lines of work. Unlike the latter work, our descriptions are provided as unrestricted</context>
</contexts>
<marker>Gorniak, Roy, 2004</marker>
<rawString>Peter Gorniak and Deb Roy. 2004. Grounded semantic composition for visual scenes. Journal of Artificial Intelligence Research (JAIR), 21(1):429–470.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Gorniak</author>
<author>Deb Roy</author>
</authors>
<title>Probabilistic grounding of situated speech using plan recognition and reference resolution.</title>
<date>2005</date>
<booktitle>In Proceedings of the 7th International Conference on Multimodal Interfaces.</booktitle>
<contexts>
<context citStr="Gorniak and Roy, 2005" endWordPosition="1891" position="11596" startWordPosition="1888">mages (FitzGerald et al., 2013; Kazemzadeh et al., 2014). However, generating scenes is currently out of reach for purely image-based approaches. 3D scene representations serve as an intermediate level of structure between raw image pixels and simpler microcosms (e.g., grid and block worlds). This level of structure is amenable to the generation task but still realistic enough to present a variety of challenges associated with natural scenes. A related line of work focuses on grounding referring expressions to referents in 3D worlds with simple colored geometric shapes (Gorniak and Roy, 2004; Gorniak and Roy, 2005). More recent work grounds text to object attributes such as color and shape in images (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013). Golland et al. (2010) ground spatial relationship language in 3D scenes (e.g., to the left of, behind) by learning from pairwise object relations provided by crowdworkers. In contrast, we ground general descriptions to a wide variety of possible objects. The objects in our scenes represent a broader space of possible referents than the first two lines of work. Unlike the latter work, our descriptions are provided as unrestricted free-form text, rather </context>
</contexts>
<marker>Gorniak, Roy, 2005</marker>
<rawString>Peter Gorniak and Deb Roy. 2005. Probabilistic grounding of situated speech using plan recognition and reference resolution. In Proceedings of the 7th International Conference on Multimodal Interfaces.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrej Karpathy</author>
<author>Armand Joulin</author>
<author>Li Fei-Fei</author>
</authors>
<title>Deep fragment embeddings for bidirectional image sentence mapping.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context citStr="Karpathy et al., 2014" endWordPosition="1783" position="10922" startWordPosition="1780"> is two feet to the south of the window). We propose reducing reliance on manual lexicons by learning to map descriptions to objects from a corpus of 3D scenes and associated textual descriptions. While we find that lexical knowledge alone is not sufficient to generate high-quality scenes, a learned approach to lexical grounding can be used in combination with a rule-based system for handling compositional knowledge, resulting in better scenes than either component alone. 3.2 Related Tasks Prior work has generated sentences that describe 2D images (Farhadi et al., 2010; Kulkarni et al., 2011; Karpathy et al., 2014) and referring expressions for specific objects in images (FitzGerald et al., 2013; Kazemzadeh et al., 2014). However, generating scenes is currently out of reach for purely image-based approaches. 3D scene representations serve as an intermediate level of structure between raw image pixels and simpler microcosms (e.g., grid and block worlds). This level of structure is amenable to the generation task but still realistic enough to present a variety of challenges associated with natural scenes. A related line of work focuses on grounding referring expressions to referents in 3D worlds with simp</context>
</contexts>
<marker>Karpathy, Joulin, Fei-Fei, 2014</marker>
<rawString>Andrej Karpathy, Armand Joulin, and Li Fei-Fei. 2014. Deep fragment embeddings for bidirectional image sentence mapping. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sahar Kazemzadeh</author>
<author>Vicente Ordonez</author>
<author>Mark Matten</author>
<author>Tamara L Berg</author>
</authors>
<title>ReferItGame: Referring to objects in photographs of natural scenes.</title>
<date>2014</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context citStr="Kazemzadeh et al., 2014" endWordPosition="1800" position="11030" startWordPosition="1797">ap descriptions to objects from a corpus of 3D scenes and associated textual descriptions. While we find that lexical knowledge alone is not sufficient to generate high-quality scenes, a learned approach to lexical grounding can be used in combination with a rule-based system for handling compositional knowledge, resulting in better scenes than either component alone. 3.2 Related Tasks Prior work has generated sentences that describe 2D images (Farhadi et al., 2010; Kulkarni et al., 2011; Karpathy et al., 2014) and referring expressions for specific objects in images (FitzGerald et al., 2013; Kazemzadeh et al., 2014). However, generating scenes is currently out of reach for purely image-based approaches. 3D scene representations serve as an intermediate level of structure between raw image pixels and simpler microcosms (e.g., grid and block worlds). This level of structure is amenable to the generation task but still realistic enough to present a variety of challenges associated with natural scenes. A related line of work focuses on grounding referring expressions to referents in 3D worlds with simple colored geometric shapes (Gorniak and Roy, 2004; Gorniak and Roy, 2005). More recent work grounds text to</context>
</contexts>
<marker>Kazemzadeh, Ordonez, Matten, Berg, 2014</marker>
<rawString>Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara L. Berg. 2014. ReferItGame: Referring to objects in photographs of natural scenes. In Proceedings of Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jayant Krishnamurthy</author>
<author>Thomas Kollar</author>
</authors>
<title>Jointly learning to parse and perceive: Connecting natural language to the physical world.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>1--193</pages>
<contexts>
<context citStr="Krishnamurthy and Kollar, 2013" endWordPosition="1915" position="11738" startWordPosition="1912"> approaches. 3D scene representations serve as an intermediate level of structure between raw image pixels and simpler microcosms (e.g., grid and block worlds). This level of structure is amenable to the generation task but still realistic enough to present a variety of challenges associated with natural scenes. A related line of work focuses on grounding referring expressions to referents in 3D worlds with simple colored geometric shapes (Gorniak and Roy, 2004; Gorniak and Roy, 2005). More recent work grounds text to object attributes such as color and shape in images (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013). Golland et al. (2010) ground spatial relationship language in 3D scenes (e.g., to the left of, behind) by learning from pairwise object relations provided by crowdworkers. In contrast, we ground general descriptions to a wide variety of possible objects. The objects in our scenes represent a broader space of possible referents than the first two lines of work. Unlike the latter work, our descriptions are provided as unrestricted free-form text, rather than filling in specific templates of object references and fixed spatial relationships. 55 4 Dataset We introduce a new dataset of 1128 scene</context>
</contexts>
<marker>Krishnamurthy, Kollar, 2013</marker>
<rawString>Jayant Krishnamurthy and Thomas Kollar. 2013. Jointly learning to parse and perceive: Connecting natural language to the physical world. Transactions of the Association for Computational Linguistics, 1:193–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Girish Kulkarni</author>
<author>Visruth Premraj</author>
<author>Sagnik Dhar</author>
<author>Siming Li</author>
<author>Yejin Choi</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
</authors>
<title>Baby talk: Understanding and generating simple image descriptions.</title>
<date>2011</date>
<booktitle>In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</booktitle>
<contexts>
<context citStr="Kulkarni et al., 2011" endWordPosition="1779" position="10898" startWordPosition="1776">intent (e.g., the table is two feet to the south of the window). We propose reducing reliance on manual lexicons by learning to map descriptions to objects from a corpus of 3D scenes and associated textual descriptions. While we find that lexical knowledge alone is not sufficient to generate high-quality scenes, a learned approach to lexical grounding can be used in combination with a rule-based system for handling compositional knowledge, resulting in better scenes than either component alone. 3.2 Related Tasks Prior work has generated sentences that describe 2D images (Farhadi et al., 2010; Kulkarni et al., 2011; Karpathy et al., 2014) and referring expressions for specific objects in images (FitzGerald et al., 2013; Kazemzadeh et al., 2014). However, generating scenes is currently out of reach for purely image-based approaches. 3D scene representations serve as an intermediate level of structure between raw image pixels and simpler microcosms (e.g., grid and block worlds). This level of structure is amenable to the generation task but still realistic enough to present a variety of challenges associated with natural scenes. A related line of work focuses on grounding referring expressions to referent</context>
</contexts>
<marker>Kulkarni, Premraj, Dhar, Li, Choi, Berg, Berg, 2011</marker>
<rawString>Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C. Berg, and Tamara L. Berg. 2011. Baby talk: Understanding and generating simple image descriptions. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations.</booktitle>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Matuszek</author>
<author>Nicholas FitzGerald</author>
<author>Luke Zettlemoyer</author>
<author>Liefeng Bo</author>
<author>Dieter Fox</author>
</authors>
<title>A joint model of language and perception for grounded attribute learning.</title>
<date>2012</date>
<booktitle>In International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context citStr="Matuszek et al., 2012" endWordPosition="1911" position="11705" startWordPosition="1908"> for purely image-based approaches. 3D scene representations serve as an intermediate level of structure between raw image pixels and simpler microcosms (e.g., grid and block worlds). This level of structure is amenable to the generation task but still realistic enough to present a variety of challenges associated with natural scenes. A related line of work focuses on grounding referring expressions to referents in 3D worlds with simple colored geometric shapes (Gorniak and Roy, 2004; Gorniak and Roy, 2005). More recent work grounds text to object attributes such as color and shape in images (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013). Golland et al. (2010) ground spatial relationship language in 3D scenes (e.g., to the left of, behind) by learning from pairwise object relations provided by crowdworkers. In contrast, we ground general descriptions to a wide variety of possible objects. The objects in our scenes represent a broader space of possible referents than the first two lines of work. Unlike the latter work, our descriptions are provided as unrestricted free-form text, rather than filling in specific templates of object references and fixed spatial relationships. 55 4 Dataset We intr</context>
</contexts>
<marker>Matuszek, FitzGerald, Zettlemoyer, Bo, Fox, 2012</marker>
<rawString>Cynthia Matuszek, Nicholas FitzGerald, Luke Zettlemoyer, Liefeng Bo, and Dieter Fox. 2012. A joint model of language and perception for grounded attribute learning. In International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: A lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context citStr="Miller, 1995" endWordPosition="2772" position="17192" startWordPosition="2771">s model. This rule-based approach is a three-stage process using established NLP systems: 1) The input text is split into multiple sentences and parsed using the Stanford CoreNLP pipeline (Manning et 56 Figure 4: Some examples extracted from the top 20 highest-weight features in our learned model: lexical terms from the descriptions in our scene corpus are grounded to 3D models within the scene corpus. red cup round yellow table green room black top tan love seat black bed open window al., 2014). Head words of noun phrases are identified as candidate object categories, filtered using WordNet (Miller, 1995) to only include physical objects. 2) References to the same object are collapsed using the Stanford coreference system. 3) Properties are attached to each object by extracting other adjectives and nouns in the noun phrase. These properties are later used to query the 3D model database. We use the same model database as Chang et al. (2014) and also extract spatial relations between objects using the same set of dependency patterns. 5.3 Combined Model The rule-based parsing model is limited in its ability to choose appropriate 3D models. We integrate our learned lexical groundings with this mod</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: A lexical database for English. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context citStr="Papineni et al., 2002" endWordPosition="2354" position="14516" startWordPosition="2351">rom their default orientation, despite the fact that this requires an extra manipulation in the scene-building interface. The descriptions collected for these scenes are similarly diverse and usually differ substantially in length and content from the seed sentences.2 5 Model To create a model for generating scene templates from text, we train a classifier to learn lexical 1Available at http://nlp.stanford.edu/data/ text2scene.shtml. 2Mean 26.2 words, SD 17.4; versus mean 16.6, SD 7.2 for the seed sentences. If one considers seed sentences to be the “reference,” the macro-averaged BLEU score (Papineni et al., 2002) of the Turker descriptions is 12.0. groundings. We then combine our learned lexical groundings with a rule-based scene generation model. The learned groundings allow us to select better models, while the rule-based model offers simple compositionality for handling coreference and relationships between objects. 5.1 Learning lexical groundings To learn lexical mappings from examples, we train a classifier on a related grounding task and extract the weights of lexical features for use in scene generation. This classifier learns from a “discrimination” version of our scene dataset, in which the s</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masoud Rouhizadeh</author>
<author>Margit Bowler</author>
<author>Richard Sproat</author>
<author>Bob Coyne</author>
</authors>
<title>Collecting semantic data by Mechanical Turk for the lexical knowledge resource of a text-to-picture generating system.</title>
<date>2011</date>
<booktitle>In Proceedings of the Ninth International Conference on Computational Semantics.</booktitle>
<contexts>
<context citStr="Rouhizadeh et al., 2011" endWordPosition="1054" position="6700" startWordPosition="1051">eneration rely on having a correctly chosen set of objects to arrange. Another challenge is that much common knowledge about the physical properties of objects and the structure of environments is rarely mentioned in natural language (e.g., that most tables are supported on the floor and in an upright orientation). Unfortunately, common 3D representations of objects and scenes used in computer graphics specify only geometry and appearance, and rarely include such information. Prior work in text to 3D scene generation focused on collecting manual annotations of object properties and relations (Rouhizadeh et al., 2011; Coyne et al., 2012), which are used to drive rule-based generation systems. Regrettably, the task of scene generation has not yet benefited from recent related work in NLP. 3 Related Work There is much prior work in image retrieval given textual queries; a recent overview is provided by Siddiquie et al. (2011). The image retrieval task bears some similarity to our task insofar as 3D scene retrieval is an approach that can approximate 3D scene generation. However, there are fundamental differences between 2D images and 3D scenes. Generation in image space has predominantly focused on composit</context>
</contexts>
<marker>Rouhizadeh, Bowler, Sproat, Coyne, 2011</marker>
<rawString>Masoud Rouhizadeh, Margit Bowler, Richard Sproat, and Bob Coyne. 2011. Collecting semantic data by Mechanical Turk for the lexical knowledge resource of a text-to-picture generating system. In Proceedings of the Ninth International Conference on Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manolis Savva</author>
<author>Angel X Chang</author>
<author>Gilbert Bernstein</author>
<author>Christopher D Manning</author>
<author>Pat Hanrahan</author>
</authors>
<title>On being the right scale: Sizing large collections of 3D models.</title>
<date>2014</date>
<booktitle>In SIGGRAPH Asia 2014 Workshop on Indoor Scene Understanding: Where Graphics meets Vision.</booktitle>
<contexts>
<context citStr="Savva et al., 2014" endWordPosition="271" position="1801" startWordPosition="268">task of text to 3D scene generation. The ability to map descriptions of scenes to 3D geometric representations has a wide variety of applications; many creative industries use 3D scenes. Robotics applications need to interpret commands referring to real-world environments, and the ability to visualize scenarios given highlevel descriptions is of great practical use in educational tools. Unfortunately, 3D scene design user interfaces are prohibitively complex for novice users. Prior work has shown the task remains challenging and time intensive for non-experts, even with simplified interfaces (Savva et al., 2014). *The first two authors are listed in alphabetical order. {...a multicolored table in the middle of the room..., ...four red and white chairs and a colorful table, ...} Figure 1: We learn how to ground references such as “L-shaped room” to 3D models in a paired corpus of 3D scenes and natural language descriptions. Sentence fragments in bold were identified as high-weighted references to the shown objects. Language offers a convenient way for designers to express their creative goals. Systems that can interpret natural descriptions to build a visual representation allow non-experts to visuall</context>
</contexts>
<marker>Savva, Chang, Bernstein, Manning, Hanrahan, 2014</marker>
<rawString>Manolis Savva, Angel X. Chang, Gilbert Bernstein, Christopher D. Manning, and Pat Hanrahan. 2014. On being the right scale: Sizing large collections of 3D models. In SIGGRAPH Asia 2014 Workshop on Indoor Scene Understanding: Where Graphics meets Vision.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lee M Seversky</author>
<author>Lijun Yin</author>
</authors>
<title>Real-time automatic 3D scene generation from natural language voice and text descriptions.</title>
<date>2006</date>
<booktitle>In Proceedings of the 14th Annual ACM International Conference on Multimedia.</booktitle>
<contexts>
<context citStr="Seversky and Yin, 2006" endWordPosition="403" position="2615" startWordPosition="400"> to ground references such as “L-shaped room” to 3D models in a paired corpus of 3D scenes and natural language descriptions. Sentence fragments in bold were identified as high-weighted references to the shown objects. Language offers a convenient way for designers to express their creative goals. Systems that can interpret natural descriptions to build a visual representation allow non-experts to visually express their thoughts with language, as was demonstrated by WordsEye, a pioneering work in text to 3D scene generation (Coyne and Sproat, 2001). WordsEye and other prior work in this area (Seversky and Yin, 2006; Chang et al., 2014) used manually chosen mappings between language and objects in scenes. To our knowledge, we present the first 3D scene generation approach that learns from data how to map textual terms to objects. First, we collect a dataset of 3D scenes along with textual descriptions by people, which we contribute to the community. We then train a classifier on a scene discrimination task and extract high-weight features that ground lexical terms to 3D models. We integrate our learned lexical groundings with a rule-based scene generation approach, and we show through a humanjudgment eva</context>
<context citStr="Seversky and Yin (2006)" endWordPosition="1613" position="9854" startWordPosition="1610"> the specific objects chosen and their placement. Each scene is described by 3 or 4 other people, for a total of 4358 descriptions. 3.1 Text to Scene Systems Pioneering work on the SHRDLU system (Winograd, 1972) demonstrated linguistic manipulation of objects in 3D scenes. However, the discourse domain was restricted to a micro-world with simple geometric shapes to simplify parsing and grounding of natural language input. More recently, prototype text to 3D scene generation systems have been built for broader domains, most notably the WordsEye system (Coyne and Sproat, 2001) and later work by Seversky and Yin (2006). Chang et al. (2014) showed it is possible to learn spatial priors for objects and relations directly from 3D scene data. These systems use manually defined mappings between language and their representation of the physical world. This prevents generalization to more complex object descriptions, variations in word choice and spelling, and other languages. It also forces users to use unnatural language to express their intent (e.g., the table is two feet to the south of the window). We propose reducing reliance on manual lexicons by learning to map descriptions to objects from a corpus of 3D s</context>
</contexts>
<marker>Seversky, Yin, 2006</marker>
<rawString>Lee M. Seversky and Lijun Yin. 2006. Real-time automatic 3D scene generation from natural language voice and text descriptions. In Proceedings of the 14th Annual ACM International Conference on Multimedia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Behjat Siddiquie</author>
<author>Rog´erio Schmidt Feris</author>
<author>Larry S Davis</author>
</authors>
<title>Image ranking and retrieval based on multi-attribute queries.</title>
<date>2011</date>
<booktitle>In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</booktitle>
<contexts>
<context citStr="Siddiquie et al. (2011)" endWordPosition="1107" position="7013" startWordPosition="1104">n). Unfortunately, common 3D representations of objects and scenes used in computer graphics specify only geometry and appearance, and rarely include such information. Prior work in text to 3D scene generation focused on collecting manual annotations of object properties and relations (Rouhizadeh et al., 2011; Coyne et al., 2012), which are used to drive rule-based generation systems. Regrettably, the task of scene generation has not yet benefited from recent related work in NLP. 3 Related Work There is much prior work in image retrieval given textual queries; a recent overview is provided by Siddiquie et al. (2011). The image retrieval task bears some similarity to our task insofar as 3D scene retrieval is an approach that can approximate 3D scene generation. However, there are fundamental differences between 2D images and 3D scenes. Generation in image space has predominantly focused on composition of simple 2D clip art elements, as exemplified recently by Zitnick et al. (2013). The task of composing 3D scenes presents a much higherdimensional search space of scene configurations where finding plausible and desirable configurations is difficult. Unlike prior work in clip art generation which uses a sma</context>
</contexts>
<marker>Siddiquie, Feris, Davis, 2011</marker>
<rawString>Behjat Siddiquie, Rog´erio Schmidt Feris, and Larry S. Davis. 2011. Image ranking and retrieval based on multi-attribute queries. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Winograd</author>
</authors>
<title>Understanding natural language.</title>
<date>1972</date>
<journal>Cognitive Psychology,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context citStr="Winograd, 1972" endWordPosition="1548" position="9442" startWordPosition="1546"> wall. • i see a rectangular table in the center of the room. There are 4 chairs around the table and 4 plates on the table Figure 3: Scenes created by participants from seed description sentences (top). Additional descriptions provided by other participants from the created scene (bottom). Our dataset contains around 19 scenes per seed sentence, for a total of 1129 scenes. Scenes exhibit variation in the specific objects chosen and their placement. Each scene is described by 3 or 4 other people, for a total of 4358 descriptions. 3.1 Text to Scene Systems Pioneering work on the SHRDLU system (Winograd, 1972) demonstrated linguistic manipulation of objects in 3D scenes. However, the discourse domain was restricted to a micro-world with simple geometric shapes to simplify parsing and grounding of natural language input. More recently, prototype text to 3D scene generation systems have been built for broader domains, most notably the WordsEye system (Coyne and Sproat, 2001) and later work by Seversky and Yin (2006). Chang et al. (2014) showed it is possible to learn spatial priors for objects and relations directly from 3D scene data. These systems use manually defined mappings between language and </context>
</contexts>
<marker>Winograd, 1972</marker>
<rawString>Terry Winograd. 1972. Understanding natural language. Cognitive Psychology, 3(1):1–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lawrence Zitnick</author>
<author>Devi Parikh</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Learning the visual interpretation of sentences.</title>
<date>2013</date>
<booktitle>In IEEE International Conference on Computer Vision (ICCV).</booktitle>
<contexts>
<context citStr="Zitnick et al. (2013)" endWordPosition="1169" position="7384" startWordPosition="1166">ation systems. Regrettably, the task of scene generation has not yet benefited from recent related work in NLP. 3 Related Work There is much prior work in image retrieval given textual queries; a recent overview is provided by Siddiquie et al. (2011). The image retrieval task bears some similarity to our task insofar as 3D scene retrieval is an approach that can approximate 3D scene generation. However, there are fundamental differences between 2D images and 3D scenes. Generation in image space has predominantly focused on composition of simple 2D clip art elements, as exemplified recently by Zitnick et al. (2013). The task of composing 3D scenes presents a much higherdimensional search space of scene configurations where finding plausible and desirable configurations is difficult. Unlike prior work in clip art generation which uses a small pre-specified set of objects, we ground to a large database of objects that can occur in various indoor environments: 12490 3D models from roughly 270 categories. 54 There is a bed and there is a There is a chair and a table. There is a table and there are four chairs. There nightstand next to the bed. are four plates and there are four sandwiches. • There is a bed </context>
</contexts>
<marker>Zitnick, Parikh, Vanderwende, 2013</marker>
<rawString>C. Lawrence Zitnick, Devi Parikh, and Lucy Vanderwende. 2013. Learning the visual interpretation of sentences. In IEEE International Conference on Computer Vision (ICCV).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>