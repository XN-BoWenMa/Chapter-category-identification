<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant confidence="0.000073" no="0">
<title confidence="0.9961095">
Hierarchical Directed Acyclic Graph Kernel:
Methods for Structured Natural Language Data
</title>
<author confidence="0.991246">
Jun Suzuki, Tsutomu Hirao, Yutaka Sasaki, and Eisaku Maeda
</author>
<affiliation confidence="0.940397">
NTT Communication Science Laboratories, NTT Corp.
</affiliation>
<address confidence="0.967927">
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
</address>
<email confidence="0.971684">
jun, hirao, sasaki, maeda @cslab.kecl.ntt.co.jp
</email>
<sectionHeader confidence="0.995003" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999442733333333">This paper proposes the “Hierarchical Directed Acyclic Graph (HDAG) Kernel” for structured natural language data. The HDAG Kernel directly accepts several levels of both chunks and their relations, and then efficiently computes the weighed sum of the number of common attribute sequences of the HDAGs. We applied the proposed method to question classification and sentence alignment tasks to evaluate its performance as a similarity measure and a kernel function. The results of the experiments demonstrate that the HDAG Kernel is superior to other kernel functions and baseline methods.</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999925172413793">As it has become easy to get structured corpora such as annotated texts, many researchers have applied statistical and machine learning techniques to NLP tasks, thus the accuracies of basic NLP tools, such as POS taggers, NP chunkers, named entities taggers and dependency analyzers, have been improved to the point that they can realize practical applications in NLP. The motivation of this paper is to identify and use richer information within texts that will improve the performance of NLP applications; this is in contrast to using feature vectors constructed by a bagof-words (Salton et al., 1975). We now are focusing on the methods that use numerical feature vectors to represent the features of natural language data. In this case, since the original natural language data is symbolic, researchers convert the symbolic data into numeric data. This process, feature extraction, is ad-hoc in nature and differs with each NLP task; there has been no neat formulation for generating feature vectors from the semantic and grammatical structures inside texts. Kernel methods (Vapnik, 1995; Cristianini and Shawe-Taylor, 2000) suitable for NLP have recently been devised. Convolution Kernels (Haussler, 1999) demonstrate how to build kernels over discrete structures such as strings, trees, and graphs. One of the most remarkable properties of this kernel methodology is that it retains the original representation of objects and algorithms manipulate the objects simply by computing kernel functions from the inner products between pairs of objects. This means that we do not have to map texts to the feature vectors by explicitly representing them, as long as an efficient calculation for the inner products between a pair of texts is defined. The kernel method is widely adopted in Machine Learning methods, such as the Support Vector Machine (SVM) (Vapnik, 1995). In addition, kernel function has been described as a similarity function that satisfies certain properties (Cristianini and ShaweTaylor, 2000). The similarity measure between texts is one of the most important factors for some tasks in the application areas of NLP such as Machine Translation, Text Categorization, Information Retrieval, and Question Answering. This paper proposes the Hierarchical Directed Acyclic Graph (HDAG) Kernel. It can handle several of the structures found within texts and can calculate the similarity with regard to these structures at practical cost and time. The HDAG Kernel can be widely applied to learning, clustering and similarity measures in NLP tasks. The following sections define the HDAG Kernel and introduce an algorithm that implements it. The results of applying the HDAG Kernel to the tasks of question classification and sentence alignment are then discussed.</bodyText>
<sectionHeader confidence="0.975459" genericHeader="method">
2 Convolution Kernels
</sectionHeader>
<bodyText confidence="0.901530294117647">Convolution Kernels were proposed as a concept of kernels for a discrete structure. This framework defines a kernel function between input objects by applying convolution “sub-kernels” that are the kernels for the decompositions (parts) of the objects. Let be a positive integer and be nonempty, separable metric spaces. This paper focuses on the special case that are countable sets. We start with as a composite structure and as its “parts”, where . is defined as a relation on the set such that is true if are the Suppose ,be the parts of with , and be the parts of with . Then, the similarity between and is defined as the following generalized convolution:</bodyText>
<equation confidence="0.817049">
(1)
</equation>
<bodyText confidence="0.99971737037037">We note that Convolution Kernels are abstract concepts, and that instances of them are determined by the definition of sub-kernel . The Tree Kernel (Collins and Duffy, 2001) and String Subsequence Kernel (SSK) (Lodhi et al., 2002), developed in the NLP field, are examples of Convolution Kernels instances. An explicit definition of both the Tree Kernel and SSK Conceptually, we enumerate all sub-structures occurring in and , where represents the total number of possible sub-structures in the objects. , the feature mapping from the sample space to the feature space, is given by In the case of the Tree Kernel, and be trees. The Tree Kernel computes the number of common subtrees in two trees and . is defined as the number of occurrences of the ’th enumerated subtree in tree . In the case of SSK, input objects and are string sequences, and the kernel function computes the sum of the occurrences of ’th common subsequence weighted according to the length of the subsequence. These two kernels make polynomialtime calculations, based on efficient recursive calculation, possible, see equation (1). Our proposed method uses the framework of Convolution Kernels.</bodyText>
<sectionHeader confidence="0.999625" genericHeader="method">
3 HDAG Kernel
</sectionHeader>
<subsectionHeader confidence="0.999974">
3.1 Definition of HDAG
</subsectionHeader>
<bodyText confidence="0.974138777777778">This paper defines HDAG as a Directed Acyclic Graph (DAG) with hierarchical structures. That is, certain nodes contain DAGs within themselves. In basic NLP tasks, chunking and parsing are used to analyze the text semantically or grammatically. There are several levels of chunks, such as phrases, named entities and sentences, and these are bound by relation structures, such as dependency structure, anaphora, and coreference. HDAG is designed to enable the representation of all of these structures inside texts, hierarchical structures for chunks and DAG structures for the relations of chunks. We believe this richer representation is extremely useful to improve the performance of similarity measure between texts, moreover, learning and clustering tasks in the application areas of NLP. Figure 1 shows an example of the text structures that can be handled by HDAG. Figure 2 contains simple examples of HDAG that elucidate the calculation of similarity. As shown in Figures 1 and 2, the nodes are allowed to have more than zero attributes, because nodes in texts usually have several kinds of attributes. For example, attributes include words, partof-speech tags, semantic information such as Word“parts” of . is defined as is written as:</bodyText>
<figure confidence="0.771000714285714">
(2)
Junichi Tsujii is the Gereral Chair of ACL2003.
He is one of the most famous researchers in the NLP field.
Jun-ichi
NNP
NP
PERSON
word named entity NP chunk sentence
coreference
one of
PRP VBZ CD IN
NP
:node
:direct link
</figure>
<figureCaption confidence="0.994944333333333">
Figure 1: Example of the text structures handled by
HDAG
Figure 2: Examples of HDAG structure
</figureCaption>
<bodyText confidence="0.956544">Net, and class of the named entity.</bodyText>
<subsectionHeader confidence="0.999621">
3.2 Definition of HDAG Kernel
</subsectionHeader>
<bodyText confidence="0.998672672727273">First of all, we define the set of nodes in HDAGs and as and , respectively, and represent nodes in the graph that are defined as and , respectively. We use the expression to represent the path from to through . We define “attribute sequence” as a sequence of attributes extracted from nodes included in a subpath. The attribute sequence is expressed as ‘A-B’ or ‘A-(C-B)’ where ( ) represents a chunk. As a basic example of the extraction of attribute sequences from a sub-path, in Figure 2 contains the four attribute sequences ‘e-b’, ‘e-V’, ‘N-b’ and ‘NV’, which are the combinations of all attributes in and . Section 3.3 explains in detail the method of extracting attribute sequences from sub-paths. Next, we define “terminated nodes” as those that do not contain any graph, such as ,; “nonterminated nodes” are those that do, such as , . Since HDAGs treat not only exact matching of sub-structures but also approximate matching, we allow node skips according to decay factor when extracting attribute sequences from the sub-paths. This framework makes similarity evaluation robust; the similar sub-structures can be evaluated in the value of similarity, in contrast to exact matching that never evaluate the similar substructure. Next, we define parameter ( ) as the number of attributes combined in the attribute sequence. When calculating similarity, we consider only combination lengths of up to . Given the above discussion, the feature vector of HDAG is written as , where represents the explicit feature mapping of HDAG and represents the number of all possible attribute combinations. The value of is the number of occurrences of the ’th attribute sequence in HDAG ; each attribute sequence is weighted according to the node skip. The similarity between HDAGs, which is the definition of the HDAG Kernel, follows equation (2) where input objects and are and , respectively. According to this approach, the HDAG Kernel calculates the inner product of the common attribute sequences weighted according to their node skips and the occurrence between the two HDAGs, and . We note that, in general, if the dimension of the feature space becomes very high or approaches infinity, it becomes computationally infeasible to generate feature vector explicitly. To improve the reader’s understanding of what the HDAG Kernel calculates, before we introduce our efficient calculation method, the next section details the attribute sequences that become elements of the feature vector if the calculation is explicit.</bodyText>
<subsectionHeader confidence="0.9792445">
3.3 Attribute Sequences: The Elements of the
Feature Vector
</subsectionHeader>
<bodyText confidence="0.979560875">We describe the details of the attribute sequences that are elements of the feature vector of the HDAG Kernel using and in Figure 2. The framework of node skip We denote the explicit representation of a node skip by ” ”. The attribute sequences in the sub-path under the “node skip” are written as ‘a-c’. It costs to skip a terminated node. The cost of skipping a are based on decay factor .represents the cost of node skip .</bodyText>
<figure confidence="0.997210534653465">
G+
G2
p+
q+
q2 e q3 b q4 q5 q6
NP
N
p2
a
N
V
p3 p4
c
NP
b
a
p5
p6
q7 q8
d
c
p7
a
d
Tsujii
NNP
is
VBZ
the
DT
general chair
JJ
NP
NN
of
IN
ACL2003
NNP
ORG
.
He is
the most famous
...
DT RBS JJ
NP
dependency structure
attribute:
words
Part-of-speech tags
NP chunk
class of NE
sub-path
val.
a. seq.
sub-path
val.
a. seq.
value
1
1
1
1
1
1
1
1
2
1
1
1
att. seq.
NP
N
a
b
c
d
NP
N
a
b
c
d
value
1
1
2
1
1
1
att. seq.
1
1
1
a-b
N-b
c-b
(c-d)-
(c- )-a
( -d)-a
; all
</figure>
<bodyText confidence="0.90585675">For example, represents the cost of node skip and that of skip .represents the sum of the multiplied cost of the node skips of all of the nodes that have a path to , that is the sum cost of both and that have a path to , Attribute sequences for non-terminated nodes We define the attributes of the non-terminated node as the combinations of all attribute sequences including the node skip. Table 1 shows the attribute sequences and values of and . Details of the elements in the feature vector The elements of the feature vector are not considered in any of the node skips. This means that ‘A-B-C’ is the same element as ‘A-B-C’, and ‘AB-C’ and ‘A-B-C’ are also the same element as ‘A-B-C’. Considering the hierarchical structure, it is natural to assume that ‘(N)-(d)-a’ and ‘(N)-(( d)-a)’ are different elements. However, in the framework of the node skip and the attributes of the nonterminated node, ‘(N)-( )-a’ and ‘(N)-(( )-a)’ are treated as the same element.This framework achieves approximate matching of the structure automatically, The HDAG Kernel judges all pairs of attributes in each attribute sequence that are inside or outside the same chunk.</bodyText>
<figure confidence="0.997715833333333">
(N- )-( )-a
N-b
(N- )-(d)
( -b)-( )-a
( -b)-(d)
(c- )-( )-a
(c- )-(d)
(d)-a
1
1
(N-b)-( )-a
(N-b)-(d)
</figure>
<bodyText confidence="0.961970676470588">If all pairs of attributes in the attribute sequences are in the same condition, inside or outside the chunk, then the attribute sequences judge as the same element. Table 2 shows the similarity, the values of , when the feature vectors are explicitly represented. We only show the common elements of each feature vector that appear in both and , since the number of elements that appear in only or becomes very large. Note that, as shown in Table 2, the attribute sequences of the non-terminated node itself are not addressed by the features of the graph. This is due to the use of the hierarchical structure; the attribute sequences of the non-terminated node come from the combination of the attributes in the terminated nodes. In the case of , attribute sequence ‘N’ comes from ‘N’ in . If we treat both ‘N’ in and ‘N’ in , we evaluate the attribute sequence ‘N’ in twice. That is why the similarity value in Table 2 does not contain ‘c’ in and ‘(c)’ in , see Table 1. non-terminated node is the same as skipping all the graphs inside the non-terminated node. We introduce decay functions ,and ; is the cost of just node .represents the sum of the multiplied cost of the node skips of all the nodes that has a path to.represents the cost of node skip where has a path to .</bodyText>
<figure confidence="0.986369833333333">
1
(c-d)-c
1
(N-b)-(( - )-a)
(N-b)-(( -d)- )
1
</figure>
<tableCaption confidence="0.778476">
Table 1: Attribute sequences and the values of nodes
and
Table 2: Similarity values of and in Figure 2
</tableCaption>
<figure confidence="0.996343590909091">
1
NP
a-
N-
c-
-b
1
NP
( - )-a
(c- )-
( -d)-
(N- )-(( - )-a)
N-b
(N- )-(( -d)- )
( -b)-(( - )-a)
( -b)-(( -d)- )
((c- )-a)
c-d
(c- )-a
1
1
3.4 Calculation
</figure>
<bodyText confidence="0.8795885">First, we determine ,which returns the sum of the common attribute sequences of the combination of attributes between nodes and . returns the number of common attributes of nodes and ,not including the attributes of nodes inside and . We define function as returning a set of nodes inside a non-terminated node . means node is a terminated node. For example, and . We define functions ,and have direct links to .and .</bodyText>
<figure confidence="0.546182428571429">
to calculate .
The boundary conditions are
if (8)
if (9)
if (10)
Function returns the set of nodes that have
direct links to node . means no nodes
</figure>
<bodyText confidence="0.950974333333333">Next, we define as representing the sum of the common attribute sequences that are the combinations of attributes extracted from the subpaths whose sinks are and , respectively.</bodyText>
<figure confidence="0.9546673125">
if
otherwise
(11)
Functions ,and ,
needed for the recursive calculation of , are
written in the same form as ,and
respectively, except for the boundary con-
dition of , which is written as:
if (12)
Finally, an efficient similarity calculation formula is
written as
(13)
According to equation (13), given the recursive
definition of , the similarity between two
HDAGs can be calculated in time1.
3.5 Efficient Calculation Method
</figure>
<bodyText confidence="0.961015958333334">We will now elucidate an efficient processing algorithm. First, as a pre-process, the nodes are sorted under the following condition: all nodes that have a path to the focused node and are in the graph inside the focused node should be set before the focused node. We can get at least one set of ordered nodes since we are treating an HDAG. In the case of , we can get , , , , , , . We can rewrite the recursive calculation formula in “for loops”, if we follow the sorted order. Figure 3 shows the algorithm of the HDAG kernel. Dynamic programming technique is used to compute the HDAG Kernel very efficiently because when following the sorted order, the values that are needed to calculate the focused pair of nodes are already calculated in the previous calculation. We can calculate the table by following the order of the nodes from left to right and top to bottom. We normalize the computed kernels before their use within the algorithms. The normalization corresponds to the standard unit norm normalization of 1We can easily rewrite the equation to calculate all combinations of attributes, but the order of calculation time becomes .</bodyText>
<figure confidence="0.957082666666667">
if and
if and
if and
otherwise
if (3)
otherwise
</figure>
<figureCaption confidence="0.999988">
Figure 3: Algorithm of the HDAG Kernel
examples in the feature space corresponding to the kernel space (Lodhi et al., 2002).</figureCaption>
<equation confidence="0.353947">
(14)
</equation>
<sectionHeader confidence="0.997387" genericHeader="evaluation and result">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999665666666667">We evaluated the performance of the proposed method in an actual application of NLP; the data set is written in Japanese. We compared HDAG and DAG (the latter had no hierarchy structure) to the String Subsequence Kernel (SSK) for word sequence, Dependency Structure</bodyText>
<figure confidence="0.947604">
Question: George Bush purchased a small interest in which baseball team ?
(a) Hierarchical and Dependency Structure
</figure>
<figureCaption confidence="0.9977585">
Figure 4: Examples of Input Object Structure: (a)
HDAG, (b) DAG and DSK’, (c) SSK’
</figureCaption>
<bodyText confidence="0.992067">Kernel (DSK) (Collins and Duffy, 2001) (a special case of the Tree Kernel), and Cosine measure for feature vectors consisting of the occurrence of attributes (BOA), and the same as BOA, but only the attributes of noun and unknown word (BOA’)were used. We expanded SSK and DSK to improve the total performance of the experiments. We denote them as SSK’ and DSK’ respectively. The original SSK treats only exact string combinations based on parameter . We consider string combinations of up to for SSK’. The original DSK was specifically constructed for parse tree use. We expanded it to be able to treat the combinations of nodes and the free order of child node matching. Figure 4 shows some input objects for each evaluated kernel, (a) for HDAG, (b) for DAG and DSK’, and (c) for SSK’. Note, though DAG and DSK’ treat the same input objects, their kernel calculation methods differ as do the return values. We used the words and semantic information of “Goi-taikei” (Ikehara et al., 1997), which is similar to WordNet in English, as the attributes of the node. The chunks and their relations in the texts were analyzed by cabocha (Kudo and Matsumoto, 2002), and named entities were analyzed by the method of (Isozaki and Kazawa, 2002). We tested each -combination case with changing parameter from 0.1 through 0.9 in the step of 0.1. Only the best performance achieved under parameter is shown in each case.</bodyText>
<figure confidence="0.999089236363637">
Algorithm HDAG Kernel n combination
for i++
for ++
end
end
end
end
return
if and
foreach
foreach
for ++
+=
end
end
foreach
for ++
+=
+=
end
end
foreach
for ++
+=
+=
for ++
end
end
end
else if
foreach
end
else if
foreach
end
end
for ++
+=
+=
+=
+=
+=
George Bush
p1
p2
NNP
PERSON
NP
NNP
p3
purchased
VBD
p4
p6 p7
DT JJ
a small
p5
NP
interest
NN
p8
PP
IN
p9
in which
WDT
p11
p10
p12 p13 p14
baseball team
NP
NN
NN
?
.
(b) Dependency Structure
p1 p2 p3 p4 p5 p6 p7
p8 p9 p10
George Bush
PERSON
purchased
VBD
DT JJ
a small
interest
NN
IN WDT
in which baseball team ?
NN
NN
.
(c) Word Order
p1 p2 p3 p4 p5 p6 p7
George Bush
PERSON
a small
IN
DT JJ
in which
WDT
purchased
VBD
interest
NN
p8 p9 p10
baseball team
?
NN
NN
.
</figure>
<tableCaption confidence="0.9745825">
Table 3: Results of the performance as a similarity
measure for question classification
Table 4: Results of the performance as a similarity
measure for sentence alignment
</tableCaption>
<table confidence="0.999334428571429">
1 2 3 4 5 6
HDAG - .580 .583 .580 .579 .573
DAG - .577 .578 .573 .573 .563
DSK’ - .547 .469 .441 .436 .436
SSK’ - .568 .572 .570 .562 .548
BOA .556
BOA’ .555
1 2 3 4 5 6
HDAG - .523 .484 .467 .442 .423
DAG - .503 .478 .461 .439 .420
DSK’ - .174 .083 .035 .020 .021
SSK’ - .479 .444 .422 .412 .398
BOA .394
BOA’ .451
</table>
<subsectionHeader confidence="0.983873">
4.1 Performance as a Similarity Measure
Question Classification
</subsectionHeader>
<bodyText confidence="0.999766642857143">We used the 1011 questions of NTCIR-QAC1 2 and the 2000 questions of CRL-QA data 3 We assigned them into 148 question types based on the CRL-QA data. We evaluated classification performance in the following step. First, we extracted one question from the data. Second, we calculated the similarity between the extracted question and all the other questions. Third, we ranked the questions in order of descending similarity. Finally, we evaluated performance as a similarity measure by Mean Reciprocal Rank (MRR) (Voorhees and Tice, 1999) based on the question type of the ranked questions. Table 3 shows the results of this experiment.</bodyText>
<subsectionHeader confidence="0.905495">
Sentence Alignment
</subsectionHeader>
<bodyText confidence="0.9983676">The data set (Hirao et al., 2003) taken from the “Mainichi Shinbun”, was formed into abstract sentences and manually aligned to sentences in the “Yomiuri Shinbun” according to the meaning of sentence (did they say the same thing). This experiment was prosecuted as follows. First, we extracted one abstract sentence from the “Mainichi Shinbun” data-set. Second, we calculated the similarity between the extracted sentence and the sentences in the “Yomiuri Shinbun” data-set. Third, we ranked the sentences in the “Yomiuri Shinbun” in descending order based on the calculated similarity values. Finally, we evaluated performance as a similarity measure using the MRR measure. Table 4 shows the results of this experiment.</bodyText>
<footnote confidence="0.9963685">
2http://www.nlp.cs.ritsumei.ac.jp/qac/
3http://www.cs.nyu.edu/˜sekine/PROJECT/CRLQA/
</footnote>
<tableCaption confidence="0.9556125">
Table 5: Results of question classification by SVM
with comparison kernel functions
</tableCaption>
<table confidence="0.999537857142857">
1 2 3 4 5 6
HDAG - .862 .865 .866 .864 .865
DAG - .862 .862 .847 .818 .751
DSK’ - .731 .595 .473 .412 .390
SSK’ - .850 .847 .825 .777 .725
BOA+poly .810 .823 .800 .753 .692 .625
BOA’+poly .807 .807 .742 .666 .558 .468
</table>
<subsectionHeader confidence="0.9814155">
4.2 Performance as a Kernel Function
Question Classification
</subsectionHeader>
<bodyText confidence="0.999993166666667">The comparison methods were evaluated the performance as a kernel function in the machine learning approach of the Question Classification. We chose SVM as a kernel-based learning algorithm that produces state-of-the-art performance in several NLP tasks. We used the same data set as used in the previous experiments with the following difference: if a question type had fewer than ten questions, we moved the entries into the upper question type as defined in CRL-QA data to provide enough training samples for each question type. We used one-vs-rest as the multi-class classification method and found a highest scoring question type. In the case of BOA and BOA’, we used the polynomial kernel (Vapnik, 1995) to consider the attribute combinations. Table 5 shows the average accuracy of each question as evaluated by 5-fold cross validation.</bodyText>
<sectionHeader confidence="0.999801" genericHeader="result">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999982325">The experiments in this paper were designed to evaluated how the similarity measure reflects the semantic information of texts. In the task of Question Classification, a given question is classified into Question Type, which reflects the intention of the question. The Sentence Alignment task evaluates which sentence is the most semantically similar to a given sentence. The HDAG Kernel showed the best performance in the experiments as a similarity measure and as a kernel of the learning algorithm. This proves the usefulness of the HDAG Kernel in determining the similarity measure of texts and in providing an SVM kernel for resolving classification problems in NLP tasks. These results indicate that our approach, incorporating richer structures within texts, is well suited to the tasks that require evaluation of the semantical similarity between texts. The potential use of the HDAG Kernel is very wider in NLP tasks, and we believe it will be adopted in other practical NLP applications such as Text Categorization and Question Answering. Our experiments indicate that the optimal parameters of combination number and decay factor depend the task at hand. They can be determined by experiments. The original DSK requires exact matching of the tree structure, even when expanded (DSK’) for flexible matching. This is why DSK’ showed the worst performance. Moreover, in Sentence Alignment task, paraphrasing or different expressions with the same meaning is common, and the structures of the parse tree widely differ in general. Unlike DSK’, SSK’ and HDAG Kernel offer approximate matching which produces better performance. The structure of HDAG approaches that of DAG, if we do not consider the hierarchical structure. In addition, the structure of sequences (strings) is entirely included in that of DAG. Thus, the framework of the HDAG Kernel covers DAG Kernel and SSK.</bodyText>
<sectionHeader confidence="0.999681" genericHeader="conclusion">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999975090909091">This paper proposed the HDAG Kernel, which can reflect the richer information present within texts. Our proposed method is a very generalized framework for handling the structure inside a text. We evaluated the performance of the HDAG Kernel both as a similarity measure and as a kernel function. Our experiments showed that HDAG Kernel offers better performance than SSK, DSK, and the baseline method of the Cosine measure for feature vectors, because HDAG Kernel better utilizes the richer structure present within texts.</bodyText>
<sectionHeader confidence="0.996465" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99513565">
M. Collins and N. Duffy. 2001. Parsing with a Single
Neuron: Convolution Kernels for Natural Language
Problems. In Technical Report UCS-CRL-01-10. UC
Santa Cruz.
N. Cristianini and J. Shawe-Taylor. 2000. An In-
troduction to Support Vector Machines and Other
Kernel-based Learning Methods. Cambridge Univer-
sity Press.
D. Haussler. 1999. Convolution Kernels on Discrete
Structures. In Technical Report UCS-CRL-99-10. UC
Santa Cruz.
T. Hirao, H. Kazawa, H. Isozaki, E. Maeda, and Y. Mat-
sumoto. 2003. Machine Learning Approach to Multi-
Document Summarization. Journal of Natural Lan-
guage Processing, 10(1):81–108. (in Japanese).
S. Ikehara, M. Miyazaki, S. Shirai, A. Yokoo,
H. Nakaiwa, K. Ogura, Y. Oyama, and Y. Hayashi,
editors. 1997. The Semantic Attribute System, Goi-
Taikei — A Japanese Lexicon, volume 1. Iwanami
Publishing. (in Japanese).
H. Isozaki and H. Kazawa. 2002. Efficient Support
Vector Classifiers for Named Entity Recognition. In
Proc. of the 19th International Conference on Compu-
tational Linguistics (COLING 2002), pages 390–396.
T. Kudo and Y. Matsumoto. 2002. Japanese Depen-
dency Analysis using Cascaded Chunking. In Proc.
of the 6th Conference on Natural Language Learning
(CoNLL 2002), pages 63–69.
H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini,
and C. Watkins. 2002. Text Classification Using
String Kernel. Journal ofMachine Learning Research,
2:419–444.
G. Salton, A. Wong, and C. Yang. 1975. A Vector Space
Model for Automatic Indexing. Communication of the
ACM, 11(18):613–620.
V. N. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer.
E. M. Voorhees and D. M. Tice. 1999. The TREC-8
Question Answering Track Evaluation. Proc. of the
8th Text Retrieval Conference (TREC-8).
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant confidence="0.876486" no="0">
<title confidence="0.999491">Hierarchical Directed Acyclic Graph Kernel: Methods for Structured Natural Language Data</title>
<author confidence="0.989397">Jun Suzuki</author>
<author confidence="0.989397">Tsutomu Hirao</author>
<author confidence="0.989397">Yutaka Sasaki</author>
<author confidence="0.989397">Eisaku Maeda</author>
<affiliation confidence="0.998001">NTT Communication Science Laboratories, NTT Corp.</affiliation>
<address confidence="0.977119">2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan</address>
<email confidence="0.923949">jun,hirao,sasaki,maeda@cslab.kecl.ntt.co.jp</email>
<abstract confidence="0.99858875">paper proposes the Di- Acyclic Graph (HDAG) for structured natural language data. The HDAG Kernel directly accepts several levels of both chunks and their relations, and then efficiently computes the weighed sum of the number of common attribute sequences of the HDAGs. We applied the proposed method to question classification and sentence alignment tasks to evaluate its performance as a similarity measure and a kernel function. The results of the experiments demonstrate that the HDAG Kernel is superior to other kernel functions and baseline methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>Parsing with a Single Neuron: Convolution Kernels for Natural Language Problems. In</title>
<date>2001</date>
<tech>Technical Report UCS-CRL-01-10.</tech>
<institution>UC Santa Cruz.</institution>
<contexts>
<context citStr="Collins and Duffy, 2001" endWordPosition="724" position="4554" startWordPosition="721">ecompositions (parts) of the objects. Let be a positive integer and be nonempty, separable metric spaces. This paper focuses on the special case that are countable sets. We start with as a composite structure and as its “parts”, where . is defined as a relation on the set such that is true if are the Suppose ,be the parts of with , and be the parts of with . Then, the similarity between and is defined as the following generalized convolution: (1) We note that Convolution Kernels are abstract concepts, and that instances of them are determined by the definition of sub-kernel . The Tree Kernel (Collins and Duffy, 2001) and String Subsequence Kernel (SSK) (Lodhi et al., 2002), developed in the NLP field, are examples of Convolution Kernels instances. An explicit definition of both the Tree Kernel and SSK Conceptually, we enumerate all sub-structures occurring in and , where represents the total number of possible sub-structures in the objects. , the feature mapping from the sample space to the feature space, is given by In the case of the Tree Kernel, and be trees. The Tree Kernel computes the number of common subtrees in two trees and . is defined as the number of occurrences of the ’th enumerated subtree i</context>
<context citStr="Collins and Duffy, 2001" endWordPosition="2893" position="16560" startWordPosition="2890">the HDAG Kernel examples in the feature space corresponding to the kernel space (Lodhi et al., 2002). (14) 4 Experiments We evaluated the performance of the proposed method in an actual application of NLP; the data set is written in Japanese. We compared HDAG and DAG (the latter had no hierarchy structure) to the String Subsequence Kernel (SSK) for word sequence, Dependency Structure Question: George Bush purchased a small interest in which baseball team ? (a) Hierarchical and Dependency Structure Figure 4: Examples of Input Object Structure: (a) HDAG, (b) DAG and DSK’, (c) SSK’ Kernel (DSK) (Collins and Duffy, 2001) (a special case of the Tree Kernel), and Cosine measure for feature vectors consisting of the occurrence of attributes (BOA), and the same as BOA, but only the attributes of noun and unknown word (BOA’)were used. We expanded SSK and DSK to improve the total performance of the experiments. We denote them as SSK’ and DSK’ respectively. The original SSK treats only exact string combinations based on parameter . We consider string combinations of up to for SSK’. The original DSK was specifically constructed for parse tree use. We expanded it to be able to treat the combinations of nodes and the f</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>M. Collins and N. Duffy. 2001. Parsing with a Single Neuron: Convolution Kernels for Natural Language Problems. In Technical Report UCS-CRL-01-10. UC Santa Cruz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Cristianini</author>
<author>J Shawe-Taylor</author>
</authors>
<title>An Introduction to Support Vector Machines and Other Kernel-based Learning Methods.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context citStr="Cristianini and Shawe-Taylor, 2000" endWordPosition="314" position="2046" startWordPosition="311"> applications; this is in contrast to using feature vectors constructed by a bagof-words (Salton et al., 1975). We now are focusing on the methods that use numerical feature vectors to represent the features of natural language data. In this case, since the original natural language data is symbolic, researchers convert the symbolic data into numeric data. This process, feature extraction, is ad-hoc in nature and differs with each NLP task; there has been no neat formulation for generating feature vectors from the semantic and grammatical structures inside texts. Kernel methods (Vapnik, 1995; Cristianini and Shawe-Taylor, 2000) suitable for NLP have recently been devised. Convolution Kernels (Haussler, 1999) demonstrate how to build kernels over discrete structures such as strings, trees, and graphs. One of the most remarkable properties of this kernel methodology is that it retains the original representation of objects and algorithms manipulate the objects simply by computing kernel functions from the inner products between pairs of objects. This means that we do not have to map texts to the feature vectors by explicitly representing them, as long as an efficient calculation for the inner products between a pair o</context>
</contexts>
<marker>Cristianini, Shawe-Taylor, 2000</marker>
<rawString>N. Cristianini and J. Shawe-Taylor. 2000. An Introduction to Support Vector Machines and Other Kernel-based Learning Methods. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Haussler</author>
</authors>
<title>Convolution Kernels on Discrete Structures. In</title>
<date>1999</date>
<tech>Technical Report UCS-CRL-99-10.</tech>
<institution>UC Santa Cruz.</institution>
<contexts>
<context citStr="Haussler, 1999" endWordPosition="325" position="2128" startWordPosition="324">., 1975). We now are focusing on the methods that use numerical feature vectors to represent the features of natural language data. In this case, since the original natural language data is symbolic, researchers convert the symbolic data into numeric data. This process, feature extraction, is ad-hoc in nature and differs with each NLP task; there has been no neat formulation for generating feature vectors from the semantic and grammatical structures inside texts. Kernel methods (Vapnik, 1995; Cristianini and Shawe-Taylor, 2000) suitable for NLP have recently been devised. Convolution Kernels (Haussler, 1999) demonstrate how to build kernels over discrete structures such as strings, trees, and graphs. One of the most remarkable properties of this kernel methodology is that it retains the original representation of objects and algorithms manipulate the objects simply by computing kernel functions from the inner products between pairs of objects. This means that we do not have to map texts to the feature vectors by explicitly representing them, as long as an efficient calculation for the inner products between a pair of texts is defined. The kernel method is widely adopted in Machine Learning method</context>
</contexts>
<marker>Haussler, 1999</marker>
<rawString>D. Haussler. 1999. Convolution Kernels on Discrete Structures. In Technical Report UCS-CRL-99-10. UC Santa Cruz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hirao</author>
<author>H Kazawa</author>
<author>H Isozaki</author>
<author>E Maeda</author>
<author>Y Matsumoto</author>
</authors>
<title>Machine Learning Approach to MultiDocument Summarization.</title>
<date>2003</date>
<journal>Journal of Natural Language Processing,</journal>
<volume>10</volume>
<issue>1</issue>
<note>(in Japanese).</note>
<contexts>
<context citStr="Hirao et al., 2003" endWordPosition="3523" position="19886" startWordPosition="3520">CRL-QA data 3 We assigned them into 148 question types based on the CRL-QA data. We evaluated classification performance in the following step. First, we extracted one question from the data. Second, we calculated the similarity between the extracted question and all the other questions. Third, we ranked the questions in order of descending similarity. Finally, we evaluated performance as a similarity measure by Mean Reciprocal Rank (MRR) (Voorhees and Tice, 1999) based on the question type of the ranked questions. Table 3 shows the results of this experiment. Sentence Alignment The data set (Hirao et al., 2003) taken from the “Mainichi Shinbun”, was formed into abstract sentences and manually aligned to sentences in the “Yomiuri Shinbun” according to the meaning of sentence (did they say the same thing). This experiment was prosecuted as follows. First, we extracted one abstract sentence from the “Mainichi Shinbun” data-set. Second, we calculated the similarity between the extracted sentence and the sentences in the “Yomiuri Shinbun” data-set. Third, we ranked the sentences in the “Yomiuri Shinbun” in descending order based on the calculated similarity values. Finally, we evaluated performance as a </context>
</contexts>
<marker>Hirao, Kazawa, Isozaki, Maeda, Matsumoto, 2003</marker>
<rawString>T. Hirao, H. Kazawa, H. Isozaki, E. Maeda, and Y. Matsumoto. 2003. Machine Learning Approach to MultiDocument Summarization. Journal of Natural Language Processing, 10(1):81–108. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ikehara</author>
<author>M Miyazaki</author>
<author>S Shirai</author>
<author>A Yokoo</author>
<author>H Nakaiwa</author>
<author>K Ogura</author>
<author>Y Oyama</author>
<author>Y Hayashi</author>
<author>editors</author>
</authors>
<date>1997</date>
<booktitle>The Semantic Attribute System, GoiTaikei — A Japanese Lexicon,</booktitle>
<volume>1</volume>
<note>Iwanami Publishing. (in Japanese).</note>
<contexts>
<context citStr="Ikehara et al., 1997" endWordPosition="3061" position="17511" startWordPosition="3058">ively. The original SSK treats only exact string combinations based on parameter . We consider string combinations of up to for SSK’. The original DSK was specifically constructed for parse tree use. We expanded it to be able to treat the combinations of nodes and the free order of child node matching. Figure 4 shows some input objects for each evaluated kernel, (a) for HDAG, (b) for DAG and DSK’, and (c) for SSK’. Note, though DAG and DSK’ treat the same input objects, their kernel calculation methods differ as do the return values. We used the words and semantic information of “Goi-taikei” (Ikehara et al., 1997), which is similar to WordNet in English, as the attributes of the node. The chunks and their relations in the texts were analyzed by cabocha (Kudo and Matsumoto, 2002), and named entities were analyzed by the method of (Isozaki and Kazawa, 2002). We tested each -combination case with changing parameter from 0.1 through 0.9 in the step of 0.1. Only the best performance achieved under parameter is shown in each case. Algorithm HDAG Kernel n combination for i++ for ++ end end end end return if and foreach foreach for ++ += end end foreach for ++ += += end end foreach for ++ += += for ++ end end </context>
</contexts>
<marker>Ikehara, Miyazaki, Shirai, Yokoo, Nakaiwa, Ogura, Oyama, Hayashi, editors, 1997</marker>
<rawString>S. Ikehara, M. Miyazaki, S. Shirai, A. Yokoo, H. Nakaiwa, K. Ogura, Y. Oyama, and Y. Hayashi, editors. 1997. The Semantic Attribute System, GoiTaikei — A Japanese Lexicon, volume 1. Iwanami Publishing. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Isozaki</author>
<author>H Kazawa</author>
</authors>
<title>Efficient Support Vector Classifiers for Named Entity Recognition.</title>
<date>2002</date>
<booktitle>In Proc. of the 19th International Conference on Computational Linguistics (COLING</booktitle>
<pages>390--396</pages>
<contexts>
<context citStr="Isozaki and Kazawa, 2002" endWordPosition="3104" position="17757" startWordPosition="3101">binations of nodes and the free order of child node matching. Figure 4 shows some input objects for each evaluated kernel, (a) for HDAG, (b) for DAG and DSK’, and (c) for SSK’. Note, though DAG and DSK’ treat the same input objects, their kernel calculation methods differ as do the return values. We used the words and semantic information of “Goi-taikei” (Ikehara et al., 1997), which is similar to WordNet in English, as the attributes of the node. The chunks and their relations in the texts were analyzed by cabocha (Kudo and Matsumoto, 2002), and named entities were analyzed by the method of (Isozaki and Kazawa, 2002). We tested each -combination case with changing parameter from 0.1 through 0.9 in the step of 0.1. Only the best performance achieved under parameter is shown in each case. Algorithm HDAG Kernel n combination for i++ for ++ end end end end return if and foreach foreach for ++ += end end foreach for ++ += += end end foreach for ++ += += for ++ end end end else if foreach end else if foreach end end for ++ += += += += += George Bush p1 p2 NNP PERSON NP NNP p3 purchased VBD p4 p6 p7 DT JJ a small p5 NP interest NN p8 PP IN p9 in which WDT p11 p10 p12 p13 p14 baseball team NP NN NN ? . (b) Depend</context>
</contexts>
<marker>Isozaki, Kazawa, 2002</marker>
<rawString>H. Isozaki and H. Kazawa. 2002. Efficient Support Vector Classifiers for Named Entity Recognition. In Proc. of the 19th International Conference on Computational Linguistics (COLING 2002), pages 390–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Japanese Dependency Analysis using Cascaded Chunking.</title>
<date>2002</date>
<booktitle>In Proc. of the 6th Conference on Natural Language Learning (CoNLL</booktitle>
<pages>63--69</pages>
<contexts>
<context citStr="Kudo and Matsumoto, 2002" endWordPosition="3091" position="17679" startWordPosition="3088">lly constructed for parse tree use. We expanded it to be able to treat the combinations of nodes and the free order of child node matching. Figure 4 shows some input objects for each evaluated kernel, (a) for HDAG, (b) for DAG and DSK’, and (c) for SSK’. Note, though DAG and DSK’ treat the same input objects, their kernel calculation methods differ as do the return values. We used the words and semantic information of “Goi-taikei” (Ikehara et al., 1997), which is similar to WordNet in English, as the attributes of the node. The chunks and their relations in the texts were analyzed by cabocha (Kudo and Matsumoto, 2002), and named entities were analyzed by the method of (Isozaki and Kazawa, 2002). We tested each -combination case with changing parameter from 0.1 through 0.9 in the step of 0.1. Only the best performance achieved under parameter is shown in each case. Algorithm HDAG Kernel n combination for i++ for ++ end end end end return if and foreach foreach for ++ += end end foreach for ++ += += end end foreach for ++ += += for ++ end end end else if foreach end else if foreach end end for ++ += += += += += George Bush p1 p2 NNP PERSON NP NNP p3 purchased VBD p4 p6 p7 DT JJ a small p5 NP interest NN p8 P</context>
</contexts>
<marker>Kudo, Matsumoto, 2002</marker>
<rawString>T. Kudo and Y. Matsumoto. 2002. Japanese Dependency Analysis using Cascaded Chunking. In Proc. of the 6th Conference on Natural Language Learning (CoNLL 2002), pages 63–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Lodhi</author>
<author>C Saunders</author>
<author>J Shawe-Taylor</author>
<author>N Cristianini</author>
<author>C Watkins</author>
</authors>
<title>Text Classification Using String Kernel.</title>
<date>2002</date>
<journal>Journal ofMachine Learning Research,</journal>
<pages>2--419</pages>
<contexts>
<context citStr="Lodhi et al., 2002" endWordPosition="734" position="4611" startWordPosition="731">r and be nonempty, separable metric spaces. This paper focuses on the special case that are countable sets. We start with as a composite structure and as its “parts”, where . is defined as a relation on the set such that is true if are the Suppose ,be the parts of with , and be the parts of with . Then, the similarity between and is defined as the following generalized convolution: (1) We note that Convolution Kernels are abstract concepts, and that instances of them are determined by the definition of sub-kernel . The Tree Kernel (Collins and Duffy, 2001) and String Subsequence Kernel (SSK) (Lodhi et al., 2002), developed in the NLP field, are examples of Convolution Kernels instances. An explicit definition of both the Tree Kernel and SSK Conceptually, we enumerate all sub-structures occurring in and , where represents the total number of possible sub-structures in the objects. , the feature mapping from the sample space to the feature space, is given by In the case of the Tree Kernel, and be trees. The Tree Kernel computes the number of common subtrees in two trees and . is defined as the number of occurrences of the ’th enumerated subtree in tree . In the case of SSK, input objects and are string</context>
<context citStr="Lodhi et al., 2002" endWordPosition="2808" position="16036" startWordPosition="2805"> focused pair of nodes are already calculated in the previous calculation. We can calculate the table by following the order of the nodes from left to right and top to bottom. We normalize the computed kernels before their use within the algorithms. The normalization corresponds to the standard unit norm normalization of 1We can easily rewrite the equation to calculate all combinations of attributes, but the order of calculation time becomes . if and if and if and otherwise if (3) otherwise Figure 3: Algorithm of the HDAG Kernel examples in the feature space corresponding to the kernel space (Lodhi et al., 2002). (14) 4 Experiments We evaluated the performance of the proposed method in an actual application of NLP; the data set is written in Japanese. We compared HDAG and DAG (the latter had no hierarchy structure) to the String Subsequence Kernel (SSK) for word sequence, Dependency Structure Question: George Bush purchased a small interest in which baseball team ? (a) Hierarchical and Dependency Structure Figure 4: Examples of Input Object Structure: (a) HDAG, (b) DAG and DSK’, (c) SSK’ Kernel (DSK) (Collins and Duffy, 2001) (a special case of the Tree Kernel), and Cosine measure for feature vectors</context>
</contexts>
<marker>Lodhi, Saunders, Shawe-Taylor, Cristianini, Watkins, 2002</marker>
<rawString>H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini, and C. Watkins. 2002. Text Classification Using String Kernel. Journal ofMachine Learning Research, 2:419–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Wong</author>
<author>C Yang</author>
</authors>
<title>A Vector Space Model for Automatic Indexing.</title>
<date>1975</date>
<journal>Communication of the ACM,</journal>
<volume>11</volume>
<issue>18</issue>
<contexts>
<context citStr="Salton et al., 1975" endWordPosition="233" position="1521" startWordPosition="230">. 1 Introduction As it has become easy to get structured corpora such as annotated texts, many researchers have applied statistical and machine learning techniques to NLP tasks, thus the accuracies of basic NLP tools, such as POS taggers, NP chunkers, named entities taggers and dependency analyzers, have been improved to the point that they can realize practical applications in NLP. The motivation of this paper is to identify and use richer information within texts that will improve the performance of NLP applications; this is in contrast to using feature vectors constructed by a bagof-words (Salton et al., 1975). We now are focusing on the methods that use numerical feature vectors to represent the features of natural language data. In this case, since the original natural language data is symbolic, researchers convert the symbolic data into numeric data. This process, feature extraction, is ad-hoc in nature and differs with each NLP task; there has been no neat formulation for generating feature vectors from the semantic and grammatical structures inside texts. Kernel methods (Vapnik, 1995; Cristianini and Shawe-Taylor, 2000) suitable for NLP have recently been devised. Convolution Kernels (Haussler</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>G. Salton, A. Wong, and C. Yang. 1975. A Vector Space Model for Automatic Indexing. Communication of the ACM, 11(18):613–620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer.</publisher>
<contexts>
<context citStr="Vapnik, 1995" endWordPosition="310" position="2009" startWordPosition="309">ormance of NLP applications; this is in contrast to using feature vectors constructed by a bagof-words (Salton et al., 1975). We now are focusing on the methods that use numerical feature vectors to represent the features of natural language data. In this case, since the original natural language data is symbolic, researchers convert the symbolic data into numeric data. This process, feature extraction, is ad-hoc in nature and differs with each NLP task; there has been no neat formulation for generating feature vectors from the semantic and grammatical structures inside texts. Kernel methods (Vapnik, 1995; Cristianini and Shawe-Taylor, 2000) suitable for NLP have recently been devised. Convolution Kernels (Haussler, 1999) demonstrate how to build kernels over discrete structures such as strings, trees, and graphs. One of the most remarkable properties of this kernel methodology is that it retains the original representation of objects and algorithms manipulate the objects simply by computing kernel functions from the inner products between pairs of objects. This means that we do not have to map texts to the feature vectors by explicitly representing them, as long as an efficient calculation fo</context>
<context citStr="Vapnik, 1995" endWordPosition="3816" position="21731" startWordPosition="3815">ng approach of the Question Classification. We chose SVM as a kernel-based learning algorithm that produces state-of-the-art performance in several NLP tasks. We used the same data set as used in the previous experiments with the following difference: if a question type had fewer than ten questions, we moved the entries into the upper question type as defined in CRL-QA data to provide enough training samples for each question type. We used one-vs-rest as the multi-class classification method and found a highest scoring question type. In the case of BOA and BOA’, we used the polynomial kernel (Vapnik, 1995) to consider the attribute combinations. Table 5 shows the average accuracy of each question as evaluated by 5-fold cross validation. 5 Discussion The experiments in this paper were designed to evaluated how the similarity measure reflects the semantic information of texts. In the task of Question Classification, a given question is classified into Question Type, which reflects the intention of the question. The Sentence Alignment task evaluates which sentence is the most semantically similar to a given sentence. The HDAG Kernel showed the best performance in the experiments as a similarity me</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>V. N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
<author>D M Tice</author>
</authors>
<date>1999</date>
<booktitle>The TREC-8 Question Answering Track Evaluation. Proc. of the 8th Text Retrieval Conference (TREC-8).</booktitle>
<contexts>
<context citStr="Voorhees and Tice, 1999" endWordPosition="3497" position="19735" startWordPosition="3494">398 BOA .394 BOA’ .451 4.1 Performance as a Similarity Measure Question Classification We used the 1011 questions of NTCIR-QAC1 2 and the 2000 questions of CRL-QA data 3 We assigned them into 148 question types based on the CRL-QA data. We evaluated classification performance in the following step. First, we extracted one question from the data. Second, we calculated the similarity between the extracted question and all the other questions. Third, we ranked the questions in order of descending similarity. Finally, we evaluated performance as a similarity measure by Mean Reciprocal Rank (MRR) (Voorhees and Tice, 1999) based on the question type of the ranked questions. Table 3 shows the results of this experiment. Sentence Alignment The data set (Hirao et al., 2003) taken from the “Mainichi Shinbun”, was formed into abstract sentences and manually aligned to sentences in the “Yomiuri Shinbun” according to the meaning of sentence (did they say the same thing). This experiment was prosecuted as follows. First, we extracted one abstract sentence from the “Mainichi Shinbun” data-set. Second, we calculated the similarity between the extracted sentence and the sentences in the “Yomiuri Shinbun” data-set. Third, </context>
</contexts>
<marker>Voorhees, Tice, 1999</marker>
<rawString>E. M. Voorhees and D. M. Tice. 1999. The TREC-8 Question Answering Track Evaluation. Proc. of the 8th Text Retrieval Conference (TREC-8).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>