<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant confidence="0.001285" no="0">
<title confidence="0.9991755">
Phrase Table Training For Precision and Recall:
What Makes a Good Phrase and a Good Phrase Pair?
</title>
<author confidence="0.970175">
Yonggang Deng* , Jia Xu+ and Yuqing Gao*
</author>
<note confidence="0.526357">
*IBM T.J. Watson Research Center, Yorktown Heights, NY 10598, USA
</note>
<email confidence="0.926848">
{ydeng,yuqing}@us.ibm.com
</email>
<affiliation confidence="0.986022">
+Chair of Computer Science VI, RWTH Aachen University, D-52056 Aachen, Germany
</affiliation>
<email confidence="0.992824">
xujia@cs.rwth-aachen.de
</email>
<sectionHeader confidence="0.995562" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9987252">In this work, the problem of extracting phrase translation is formulated as an information retrieval process implemented with a log-linear model aiming for a balanced precision and recall. We present a generic phrase training algorithm which is parameterized with feature functions and can be optimized jointly with the translation engine to directly maximize the end-to-end system performance. Multiple data-driven feature functions are proposed to capture the quality and confidence of phrases and phrase pairs. Experimental results demonstrate consistent and significant improvement over the widely used method that is based on word alignment matrix only.</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961823529412">Phrase has become the standard basic translation unit in Statistical Machine Translation (SMT) since it naturally captures context dependency and models internal word reordering. In a phrase-based SMT system, the phrase translation table is the defining component which specifies alternative translations and their probabilities for a given source phrase. In learning such a table from parallel corpus, two related issues need to be addressed (either separately or jointly): which pairs are considered valid translations and how to assign weights, such as probabilities, to them. The first problem is referred to as phrase pair extraction, which identifies phrase pairs that are supposed to be translations of each other. Methods have been proposed, based on syntax, that take advantage of linguistic constraints and alignment of grammatical structure, such as in Yamada and Knight (2001) and Wu (1995).</bodyText>
<page confidence="0.978712">
81
</page>
<bodyText confidence="0.999829029411765">The most widely used approach derives phrase pairs from word alignment matrix (Och and Ney, 2003; Koehn et al., 2003). Other methods do not depend on word alignments only, such as directly modeling phrase alignment in a joint generative way (Marcu and Wong, 2002), pursuing information extraction perspective (Venugopal et al., 2003), or augmenting with modelbased phrase pair posterior (Deng and Byrne, 2005). Using relative frequency as translation probability is a common practice to measure goodness of a phrase pair. Since most phrases appear only a few times in training data, a phrase pair translation is also evaluated by lexical weights (Koehn et al., 2003) or term weighting (Zhao et al., 2004) as additional features to avoid overestimation. The translation probability can also be discriminatively trained such as in Tillmann and Zhang (2006). The focus of this paper is the phrase pair extraction problem. As in information retrieval, precision and recall issues need to be addressed with a right balance for building a phrase translation table. High precision requires that identified translation candidates are accurate, while high recall wants as much valid phrase pairs as possible to be extracted, which is important and necessary for online translation that requires coverage. In the word-alignment derived phrase extraction approach, precision can be improved by filtering out most of the entries by using a statistical significance test (Johnson et al., 2007). On the other hand, there are valid translation pairs in the training corpus that are not learned due to word alignment errors as shown in Deng and Byrne (2005).</bodyText>
<note confidence="0.818053">
Proceedings of ACL-08: HLT, pages 81–88,
</note>
<page confidence="0.547822">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.997969711111111">We would like to improve phrase translation acAlgorithm 1 A Generic Phrase Training Procedure curacy and at the same time extract as many as possible valid phrase pairs that are missed due to incorrect word alignments. One approach is to leverage underlying word alignment quality such as in Ayan and Dorr (2006). In this work, we present a generic discriminative phrase pair extraction framework that can integrate multiple features aiming to identify correct phrase translation candidates. A significant deviation from most other approaches is that the framework is parameterized and can be optimized jointly with the decoder to maximize translation performance on a development set. Within the general framework, the main work is on investigating useful metrics. We employ features based on word alignment models and alignment matrix. We also propose information metrics that are derived from both bilingual and monolingual perspectives. All these features are data-driven and independent of languages. The proposed phrase extraction framework is general to apply linguistic features such as semantic, POS tags and syntactic dependency.</bodyText>
<sectionHeader confidence="0.998991" genericHeader="method">
2 A Generic Phrase Training Procedure
</sectionHeader>
<bodyText confidence="0.997969711111111">Let e = ei denote an English sentence and let f = f1 denote its translation in a foreign language, say Chinese. Phrase extraction begins with sentence-aligned parallel corpora {(ei, fi)1. We use E = eie and F = fib to denote an English and ib foreign phrases respectively, where ib(jb) is the position in the sentence of the beginning word of the English(foreign) phrase and i,(j,) is the position of the ending word of the phrase. We first train word alignment models and will use them to evaluate the goodness of a phrase and a phrase pair. Let fk(E, F), k = 1, 2, · · · , K be K feature functions to be used to measure the quality of a given phrase pair (E, F). The generic phrase extraction procedure is an evaluation, ranking, filtering, estimation and tuning process, presented in Algorithm 1. Step 1 (line 1) is the preparation stage. Beginning with a flat lexicon, we train IBM Model-1 word alignment model with 10 iterations for each translation direction. We then train HMM word alignment models (Vogel et al., 1996) in two directions simultaneously by merging statistics collected in the 82 1: Train Model-1 and HMM word alignment models 2: for all sentence pair (e, f) do 3: Identify candidate phrases on each side 4: for all candidate phrase pair (E, F) do 5: Calculate its feature function values fk Obtain the score q(E, F) = �K 6: k�1 akfk(E, F) 7: end for 8: Sort candidate phrase pairs by their final scores q 9: Find the maximum score qm = max q(E, F) 10: for all candidate phrase pair (E, F) do 11: If q(E, F) &gt; qm − T, dump the pair into the pool 12: end for 13: end for 14: Built a phrase translation table from the phrase pair pool 15: Discriminatively train feature weights ak and threshold T</bodyText>
<bodyText confidence="0.998027226415094">E-step from two directions motivated by Zens et al. (2004) with 5 iterations. We use these models to define the feature functions of candidate phrase pairs such as phrase pair posterior distribution. More details will be given in Section 3. Step 2 (line 2) consists of phrase pair evaluation, ranking and filtering. Usually all n-grams up to a pre-defined length limit are considered as candidate phrases. This is also the place where linguistic constraints can be applied, say to avoid noncompositional phrases (Lin, 1999). Each normalized feature score derived from word alignment models or language models will be log-linearly combined to generate the final score. Phrase pair filtering is simply thresholding on the final score by comparing to the maximum within the sentence pair. Note that under the log-linear model, applying threshold for filtering is equivalent to comparing the “likelihood” ratio. Step 3 (line 14) pools all candidate phrase pairs that pass the threshold testing and estimates the final phrase translation table by maximum likelihood criterion. For each candidate phrase pair which is above the threshold, we assign HMM-based phrase pair posterior as its soft count when dumping them into the global phrase pair pool. Other possibilities for the weighting include assigning constant one or the exponential of the final score etc. One of the advantages of the proposed phrase training algorithm is that it is a parameterized procedure that can be optimized jointly with the translation engine to minimize the final translation errors measured by automatic metrics such as BLEU (Papineni et al., 2002). In the final step 4 (line 15), parameters {Ak, T} are discriminatively trained on a development set using the downhill simplex method (Nelder and Mead, 1965). This phrase training procedure is general in the sense that it is configurable and trainable with different feature functions and their parameters. The commonly used phrase extraction approach based on word alignment heuristics (referred as ViterbiExtract algorithm for comparison in this paper) as described in (Och, 2002; Koehn et al., 2003) is a special case of the algorithm, where candidate phrase pairs are restricted to those that respect word alignment boundaries. We rely on multiple feature functions that aim to describe the quality of candidate phrase translations and the generic procedure to figure out the best way of combining these features. A good feature function pops up valid translation pairs and pushes down incorrect ones.</bodyText>
<sectionHeader confidence="0.999385" genericHeader="method">
3 Features
</sectionHeader>
<bodyText confidence="0.9999898">Now we present several feature functions that we investigated to help extracting correct phrase translations. All these features are data-driven and defined based on models, such as statistical word alignment model or language model.</bodyText>
<subsectionHeader confidence="0.999733">
3.1 Model-based Phrase Pair Posterior
</subsectionHeader>
<bodyText confidence="0.9998795">In a statistical generative word alignment model (Brown et al., 1993), it is assumed that (i) a random variable a specifies how each target word fj is generated by (therefore aligned to) a source 1 word eaj; and (ii) the likelihood function f(f, a|e) specifies a generative procedure from the source sentence to the target sentence. Given a phrase pair in a sentence pair, there will be many generative paths that align the source phrase to the target phrase. The likelihood of those generative procedures can be accumulated to get the likelihood of the phrase pair (Deng and Byrne, 2005). This is implemented as the summation of the likelihood function over all valid hidden word alignments.</bodyText>
<footnote confidence="0.716653">
1The word source and target are in the sense of word align-
ment direction, not as in the source-channel formulation.
</footnote>
<bodyText confidence="0.775252333333333">More specifically, let A(j1,j2) (i1,i2) be the set of word alignment a that aligns the source phrase ej1 i1 to the target phrase fj2 j1 (links to NULL word are ignored</bodyText>
<equation confidence="0.850241">
for simplicity):
A(j1,z2� j2) _
(i1, {a : aj E [i1, i2] iff j E [j1, j2]}
</equation>
<bodyText confidence="0.99947375">The alignment set given a phrase pair ignores those pairs with word links across the phrase boundary. Consequently, the phrase-pair posterior distribution is defined as</bodyText>
<equation confidence="0.998552">
EacA(j1,j2) (i1,i2)f(a, f|e; 0)
�(1)
</equation>
<bodyText confidence="0.999953535714286">Switching the source and the target, we can obtain the posterior distribution in another translation direction. This distribution is applicable to all word alignment models that follow assumptions (i) and (ii). However, the complexity of the likelihood function could make it impractical to calculate the summations in Equation 1 unless an approximation is applied. Several feature functions will be defined on top of the posterior distribution. One of them is based on HMM word alignment model. We use the geometric mean of posteriors in two translation directions as a symmetric metric for phrase pair quality evaluation function under HMM alignment models. Table 1 shows the phrase pair posterior matrix of the example. Replacing the word alignment model with IBM Model-1 is another feature function that we added. IBM Model-1 is simple yet has been shown to be effective in many applications (Och et al., 2004). There is a close form solution to calculate the phrase pair posterior under Model-1. Moreover, word to word translation table under HMM is more concentrated than that under Model-1. Therefore, the posterior distribution evaluated by Model-1 is smoother and potentially it can alleviate the overestimation problem in HMM especially when training data size is small.</bodyText>
<subsectionHeader confidence="0.999262">
3.2 Bilingual Information Metric
</subsectionHeader>
<bodyText confidence="0.99996975">Trying to find phrase translations for any possible ngram is not a good idea for two reasons. First, due to data sparsity and/or alignment model’s capability, there would exist n-grams that cannot be aligned</bodyText>
<equation confidence="0.942396875">
Pθ(ei2 i1 fj2
j1 |e,f) _
Ea f(a, f|e; 9)
83
f1 f2 f3
X(that) A-L_(is) f 4(what)
what’s that
e1 e2
</equation>
<table confidence="0.97758225">
e1 e2 e2 HBL(fj2
1 1 2 j1 )
f1 0.0006 0.012 0.89 0.08
1 0.0017 0.035 0.343 0.34
f2 0.07 0.999 0.0004 0.24
1 0.03 0.0001 0.029 0.7
f3 0.89 0.006 0.006 0.05
1 0.343 0.002 0.002 0.06
f2
2
f3
2
f3
3
HBL(ei2 0.869 0.26 0.70
i1)
</table>
<tableCaption confidence="0.999971">
Table 1: Phrase pair posterior distribution for the example
well, for instance, n-grams that are part of a paraphrase translation or metaphorical expression.</tableCaption>
<bodyText confidence="0.999815826086957">To give an example, the unigram ‘tomorrow’ in ‘the day after tomorrow’ whose Chinese translation is a single word ‘��’. Extracting candidate translations for such kind of n-grams for the sake of improving coverage (recall) might hurt translation quality (precision). We will define a confidence metric to estimate how reliably the model can align an n-gram in one side to a phrase on the other side given a parallel sentence. Second, some n-grams themselves carry no linguistic meaning; their phrase translations can be misleading, for example non-compositional phrases (Lin, 1999). We will address this in section 3.3. Given a sentence pair, the basic assumption is that if the HMM word alignment model can align an English phrase well to a foreign phrase, the posterior distribution of the English phrase generating all foreign phrases on the other side is significantly biased. For instance, the posterior of one foreign phrase is far larger than that of the others. We use the entropy of the posterior distribution as the confidence metric:</bodyText>
<equation confidence="0.997679333333333">
HBL(ei2
i1|e, f) = H(ˆPBHMM (ei2
i1 → ∗)) (2)
</equation>
<bodyText confidence="0.996980666666667">where H(P) = − �� P(x) log P(x) is the entropy of a distribution P(x), normalized probability (sum up to 1) of the pos-</bodyText>
<equation confidence="0.6792765">
terior PBHMM(ei2→ ∗) as defined in Equation 1.
i1
</equation>
<bodyText confidence="0.999964454545454">Low entropy signals a high confidence that the English phrase can be aligned correctly. On the other hand, high entropy implies ambiguity presented in discriminating the correct foreign phrase from the others from the viewpoint of the model. Similarly we calculate the confidence metric of aligning a foreign phrase correctly with the word alignment model in foreign to English direction. Table 1 shows the entropy of phrases. The unigram of foreign side f22 is unlikely to survive with such high ambiguity. Adding the entropy in two directions defines the bilingual information metric as another feature function, which describes the reliability of aligning each phrase correctly by the model. Note that we used HMM word alignment model to find the posterior distribution. Other models such as Model-1 can be applied in the same way. This feature function quantitatively captures the goodness of phrases. During phrase pair ranking, it can help to move upward phrases that can be aligned well and push downward phrases that are difficult for the model to find correct translations.</bodyText>
<subsectionHeader confidence="0.98611">
3.3 Monolingual Information Metric
</subsectionHeader>
<bodyText confidence="0.9996785625">Now we turn to monolingual resources to evaluate the quality of an n-gram being a good phrase. A phrase in a sentence is specified by its boundaries. We assume that the boundaries of a good phrase should be the “right” place to break. More generally, we want to quantify how effective a word boundary is as a phrase boundary. One would perform say NP-chunking or parsing to avoid splitting a linguistic constituent. We apply a language model (LM) to describe the predictive uncertainty (PU) between words in two directions. Given a history w�−1 1 , a language model specifies a conditional distribution of the future word being predicted to follow the history. We can find the entropy of such pdf: HLM(w�−1</bodyText>
<equation confidence="0.9897305">
1 ) = H(P(·|w�−1
1 )).
</equation>
<bodyText confidence="0.9965926">So given a sentence wi , the PU of the boundary between word wi and wi+1 is established by two-way entropy sum using a forward and backward language model: PU(wi , i) = HLMF (wi1) + HLMB(wi+1 � ) We assume that the higher the predictive uncertainty is, the more likely the left or right part of the word boundary can be “cut-and-pasted” to form another reasonable sentence. So a good phrase is characterized with high PU values on the boundaries. For example, in ‘we want to have a table near the window’, the PU value of the point after ‘table’ is 0.61, higher than that between ‘near’ and ‘the’ 0.3, using trigram LMs. With this, the feature function derived from monolingual clue for a phrase pair can be defined as the product of PUs of the four word boundaries.</bodyText>
<equation confidence="0.983017333333333">
ˆ
POHMM (ei2
i1 → ∗) is the
</equation>
<page confidence="0.979056">
84
</page>
<subsectionHeader confidence="0.984541">
3.4 Word Alignments Induced Metric
</subsectionHeader>
<bodyText confidence="0.999987181818182">The widely used ViterbiExtract algorithm relies on word alignment matrix and no-crossing-link assumption to extract phrase translation candidates. Practically it has been proved to work well. However, discarding correct phrase pairs due to incorrect word links leaves room for improving recall. This is especially true for not significantly large training corpora. Provided with a word alignment matrix, we define within phrase pair consistency ratio (WPPCR) as another feature function. WPPCR was used as one of the scores in (Venugopal et al., 2003) for phrase extraction. It is defined as the number of consistent word links associated with any words within the phrase pair divided by the number of all word links associated with any words within the phrase pair. An inconsistent link connects a word within the phrase pair to a word outside the phrase pair. For example, the WPPCR for (e2, f21) in Table 1 is 2/3. As a special case, the ViterbiExtract algorithm extracts only phrase pairs with WPPCR is 1. To further discriminate the pairs with higher WPPCR from those with lower ratio, we apply a BiLinear Transform (BLT) (Oppenheim and Schafer, 1989) mapping. BLT is commonly used in signal processing to attenuate the low frequency parts. When used to map WPPCR, it exaggerates the difference between phrase pairs with high WPPCR and those with low WPPCR, making the pairs with low ratio more unlikely to be selected as translation candidates. One of the nice properties of BLT is that there is a parameter that can be changed to adjust the degree of attenuation, which provides another dimension for system optimization.</bodyText>
<sectionHeader confidence="0.998241" genericHeader="evaluation and result">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999671125">We evaluate the effect of the proposed phrase extraction algorithm with translation performance. We do experiments on IWSLT (Paul, 2006) 2006 ChineseEnglish corpus. The task is to translate Chinese utterances in travel domain into English. We report only text (speech transcription) translation results. The training corpus consists of 40K ChineseEnglish parallel sentences in travel domain with to-</bodyText>
<table confidence="0.99924">
Eval Set 04dev 04test 05test 06dev 06test
# of sentences 506 500 506 489 500
# of words 2808 2906 3209 5214 5550
# of refs 16 16 16 7 7
</table>
<tableCaption confidence="0.997066">
Table 2: Dev/test set statistics
tal 306K English words and 295K Chinese words.</tableCaption>
<bodyText confidence="0.999399666666667">In the data processing step, Chinese characters are segmented into words. English text are normalized and lowercased.All punctuation is removed. There are five sets of evaluation sentences in tourism domain for development and test. Their statistics are shown in Table 2. We will tune training and decoding parameters on 06dev and report results on other sets.</bodyText>
<subsectionHeader confidence="0.989572">
4.1 Training and Translation Setup
</subsectionHeader>
<bodyText confidence="0.999178285714285">Our decoder is a phrase-based multi-stack implementation of the log-linear model similar to Pharaoh (Koehn et al., 2003). Like other log-linear model based decoders, active features in our translation engine include translation models in two directions, lexicon weights in two directions, language model, lexicalized distortion models, sentence length penalty and other heuristics. These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method. The language model is a statistical trigram model estimated with Modified Kneser-Ney smoothing (Chen and Goodman, 1996) using only English sentences in the parallel training data. Starting from the collection of parallel training sentences, we build word alignment models in two translation directions, from English to Chinese and from Chinese to English, and derive two sets of Viterbi alignments. By combining word alignments in two directions using heuristics (Och and Ney, 2003), a single set of static word alignments is then formed. Based on alignment models and word alignment matrices, we compare different approaches of building a phrase translation table and show the final translation results. We measure translation performance by the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) scores with multiple translation references.</bodyText>
<page confidence="0.999446">
85
</page>
<table confidence="0.9442283">
BLEU Scores
Table 04dev 04test 05test 06dev 06test
HMM 0.367 0.407 0.473 0.200 0.190
Model-4 0.380 0.403 0.485 0.210 0.204
New 0.411 0.427 0.500 0.216 0.208
METEOR Scores
Table 04dev 04test 05test 06dev 06test
HMM 0.532 0.586 0.675 0.482 0.471
Model-4 0.540 0.593 0.682 0.492 0.480
New 0.568 0.614 0.691 0.505 0.487
</table>
<tableCaption confidence="0.999423">
Table 3: Translation Results
</tableCaption>
<subsectionHeader confidence="0.996781">
4.2 Translation Results
</subsectionHeader>
<bodyText confidence="0.999917545454545">Our baseline phrase table training method is the ViterbiExtract algorithm. All phrase pairs with respect to the word alignment boundary constraint are identified and pooled to build phrase translation tables with the Maximum Likelihood criterion. We prune phrase translation entries by their probabilities. The maximum number of words in Chinese and English phrases is set to 8 and 25 respectively for all conditions2. We perform online style phrase training, i.e., phrase extraction is not particular for any evaluation set. Two different word alignment models are trained as the baseline, one is symmetric HMM word alignment model, the other is IBM Model-4 as implemented in the GIZA++ toolkit (Och and Ney, 2003). The translation results as measured by BLEU and METEOR scores are presented in Table 3. We notice that Model-4 based phrase table performs roughly 1% better in terms of both BLEU and METEOR scores than that based on HMM. We follow the generic phrase training procedure as described in section 2. The most time consuming part is calculating posteriors, which is carried out in parallel with 30 jobs in less than 1.5 hours. We use the Viterbi word alignments from HMM to define within phrase pair consistency ratio as discussed in section 3.4. Although Table 3 implies that Model-4 word alignment quality is better than that of HMM, we did not get benefits by switching to Model-4 to compute word alignments based feature values. In estimating phrase translation probability, we use accumulated HMM-based phrase pair posteriors as their ‘soft’ frequencies and then the final translation probability is the relative frequency.</bodyText>
<footnote confidence="0.8626405">
2We chose large numbers for phrase length limit to build a
strong baseline and to avoid impact of longer phase length.
</footnote>
<bodyText confidence="0.999888178571428">HMMbased posterior was shown to be better than treating each occurrence as count one. Once we have computed all feature values for all phrase pairs in the training corpus, we discriminatively train feature weights Aks and the threshold T using the downhill simplex method to maximize the BLEU score on 06dev set. Since the translation engine implements a log-linear model, the discriminative training of feature weights in the decoder should be embedded in the whole end-to-end system jointly with the discriminative phrase table training process. This is globally optimal but computationally demanding. As a compromise, we fix the decoder feature weights and put all efforts on optimizing phrase training parameters to find out the best phrase table. The translation results with the discriminatively trained phrase table are shown as the row of “New” in Table 3. We observe that the new approach is consistently better than the baseline ViterbiExtract algorithm with either Model-4 or HMM word alignments on all sets. Roughly, it has 0.5% higher BLEU score on 2006 sets and 1.5% to 3% higher on other sets than Model-4 based ViterbiExtract method. Similar superior results are observed when measured with METEOR score.</bodyText>
<sectionHeader confidence="0.995324" genericHeader="result">
5 Discussions
</sectionHeader>
<bodyText confidence="0.999913647058823">The generic phrase training algorithm follows an information retrieval perspective as in (Venugopal et al., 2003) but aims to improve both precision and recall with the trainable log-linear model. A clear advantage of the proposed approach over the widely used ViterbiExtract method is trainability. Under the general framework, one can put as many features as possible together under the log-linear model to evaluate the quality of a phrase and a phase pair. The phrase table extracting procedure is trainable and can be optimized jointly with the translation engine. Another advantage is flexibility, which is provided partially by the threshold T. As the figure 1 shows, when we increase the threshold by allowing more candidate phrase pair hypothesized as valid translation, we observe the phrase table size increases monotonically. On the other hand, we notice</bodyText>
<page confidence="0.996941">
86
</page>
<figureCaption confidence="0.9961755">
Figure 1: Thresholding effects on translation perfor-
mance and phrase table size
that the translation performance improves gradually.</figureCaption>
<bodyText confidence="0.999724709677419">After reaching its peak, the BLEU score drops as the threshold T increases. When T is large enough, the translation performance is not changing much but still worse than the peak value. It implies a balancing process between precision and recall. The final optimal threshold T is around 5. The flexibility is also enabled by multiple configurable features used to evaluate the quality of a phrase and a phrase pair. Ideally, a perfect combination of feature functions divides the correct and incorrect candidate phrase pairs within a parallel sentence into two ordered separate sets. We use feature functions to decide the order and the threshold T to locate the boundary guided with a development set. So the main issue to investigate now is which features are important and valuable in ranking candidate phrase pairs. We propose several information metrics derived from posterior distribution, language model and word alignments as feature functions. The ViterbiExtract is a special case where a single binary feature function defined from word alignments is used. Its good performance (as shown in Table 3) suggests that word alignments are very indicative of phrase pair quality. So we design comparative experiments to capture word alignment impact only. We start with basic features that include model-based posterior, bilingual and monolingual information metrics. Its results on different test sets are presented in the “basic” row of Table 4. We add word alignment feature (“+align” row), and</bodyText>
<table confidence="0.9971915">
Features 04dev 04test 05test 06dev 06test
basic 0.393 0.406 0.496 0.205 0.199
+align 0.401 0.429 0.502 0.208 0.196
+align BLT 0.411 0.427 0.500 0.216 0.208
</table>
<tableCaption confidence="0.9146755">
Table 4: Translation Results (BLEU) of discriminative
phrase training approach using different features
</tableCaption>
<table confidence="0.999768666666667">
Features 04dev 04test 05test 06dev 06test
PP2 0.380 0.395 0.480 0.207 0.202
PP1+PP2 0.380 0.403 0.485 0.210 0.204
PP2+PP3 0.411 0.427 0.500 0.216 0.208
PP1+PP2+PP3 0.412 0.432 0.500 0.217 0.214
ce
</table>
<tableCaption confidence="0.9622355">
Table 5: Translation Results (BLEU) of Different Phrase
Pair Combination
then apply bilinear transform to the consistency ratio WPPCR as described in section 3.4 (“+align BLT” row).</tableCaption>
<bodyText confidence="0.999857892857143">The parameter controlling the degree of attenuation in BLT is also optimized together with other feature weights. With the basic features, the new phrase extraction approach performs better than the baseline method with HMM word alignment models but similar to the baseline method with Model-4. With the word alignment based feature WPPCR, we obtain a 2% improvement on 04test set but not much on other sets except slight degradation on 06test. Finally, applying BLT transform to WPPCR leads to additional 0.8 BLEU point on 06dev set and 1.2 point on 06test set. This confirms the effectiveness of word alignment based features. Now we compare the phrase table using the proposed method to that extracted using the baseline ViterbiExtract method with Model-4 word alignments. The Venn diagram in Table 5 shows how the two phrase tables overlap with each other and size of each part. As expected, they have a large number of common phrase pairs (PP2). The new method is able to extract more phrase pairs than the baseline with Model-4. PP1 is the set of phrase pairs found by Model-4 alignments. Removing PP1 from the baseline phrase table (comparing the first group of scores) or adding PP1 to the new phrase table</bodyText>
<figure confidence="0.998605666666667">
0.2
BLEU
Phrasetab
0.19
0.18
0.17
0 1 2 3 4
T
Model
75K
PP1
I
</figure>
<page confidence="0.995696">
87
</page>
<bodyText confidence="0.9997998">(the second group of scores) overall results in no or marginal performance change. On the other hand, adding phrase pairs extracted by the new method only (PP3) can lead to significant BLEU score increases (comparing row 1 vs. 3, and row 2 vs. 4).</bodyText>
<sectionHeader confidence="0.999285" genericHeader="conclusion">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999967705882353">In this paper, the problem of extracting phrase translation is formulated as an information retrieval process implemented with a log-linear model aiming for a balanced precision and recall. We have presented a generic phrase translation extraction procedure which is parameterized with feature functions. It can be optimized jointly with the translation engine to directly maximize the end-to-end translation performance. Multiple feature functions were investigated. Our experimental results on IWSLT ChineseEnglish corpus have demonstrated consistent and significant improvement over the widely used word alignment matrix based extraction method. 3 Acknowledgement We would like to thank Xiaodong Cui, Radu Florian and other IBM colleagues for useful discussions and the anonymous reviewers for their constructive suggestions.</bodyText>
<sectionHeader confidence="0.99926" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998252746666667">
N. Ayan and B. Dorr. 2006. Going beyond AER: An
extensive analysis of word alignments and their impact
on MT. In Proc. of ACL, pages 9–16.
S. Banerjee and A. Lavie. 2005. METEOR: An auto-
matic metric for MT evaluation with improved cor-
relation with human judgments. In Proc. of the ACL
Workshop on Intrinsic and Extrinsic Evaluation Mea-
sures for Machine Translation and/or Summarization,
pages 65–72.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mer-
cer. 1993. The mathematics of machine transla-
tion: Parameter estimation. Computational Linguis-
tics, 19:263–312.
S. F. Chen and J. Goodman. 1996. An empirical study of
smoothing techniques for language modeling. In Proc.
of ACL, pages 310–318.
Y. Deng and W. Byrne. 2005. HMM word and phrase
alignment for statistical machine translation. In Proc.
of HLT-EMNLP, pages 169–176.
3By parallelism, we have shown the feasibility and effec-
tiveness (results not presented here) of the proposed method in
handling millions of sentence pairs.
H. Johnson, J. Martin, G. Foster, and R. Kuhn. 2007. Im-
proving translation quality by discarding most of the
phrasetable. In Proc. of EMNLP-CoNLL, pages 967–
975.
P. Koehn, F. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of HLT-NAACL, pages 48–
54.
D. Lin. 1999. Automatic identification of non-
compositional phrases. In Proc. of ACL, pages 317–
324.
D. Marcu and D. Wong. 2002. A phrase-based, joint
probability model for statistical machine translation.
In Proc. of EMNLP, pages 133–139.
J. A. Nelder and R. Mead. 1965. A simplex method
for function minimization. Computer Journal, 7:308–
313.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19–51.
F. J. Och, D. Gildea, and et al. 2004. A smorgasbord of
features for statistical machine translation. In Proc. of
HLT-NAACL, pages 161–168.
F. Och. 2002. Statistical Machine Translation: From
Single Word Models to Alignment Templates. Ph.D.
thesis, RWTH Aachen, Germany.
A. V. Oppenheim and R. W. Schafer. 1989. Discrete-
Time Signal Processing. Prentice-Hall.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. of ACL, pages 311–318.
M. Paul. 2006. Overview of the IWSLT 2006 evaluation
campaign. In Proc. of IWSLT, pages 1–15.
C. Tillmann and T. Zhang. 2006. A discriminative global
training algorithm for statistical MT. In Proc. of ACL,
pages 721–728.
A. Venugopal, S. Vogel, and A. Waibel. 2003. Effective
phrase translation extraction from alignment models.
In Proc. of ACL, pages 319–326.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM based
word alignment in statistical translation. In Proc. of
the COLING.
D. Wu. 1995. An algorithm for simultaneously bracket-
ing parallel texts by aligning words. In Proc. of ACL,
pages 244–251.
K. Yamada and K. Knight. 2001. A syntax-based statis-
tical translation model. In Proc. of ACL, pages 523–
530.
R. Zens, E. Matusov, and H. Ney. 2004. Improved word
alignment using a symmetric lexicon model. In Proc.
of COLING, pages 36–42.
B. Zhao, S. Vogel, M. Eck, and A. Waibel. 2004. Phrase
pair rescoring with term weighting for statistical ma-
chine translation. In Proc. of EMNLP, pages 206–213.
</reference>
<page confidence="0.999409">
88
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant confidence="0.134414" no="0">
<title confidence="0.702080333333333">Phrase Table Training For Precision and Recall: What Makes a Good Phrase and a Good Phrase Pair? , and</title>
<address confidence="0.68195">T.J. Watson Research Center, Yorktown Heights, NY 10598, USA</address>
<email confidence="0.996646">ydeng@us.ibm.com</email>
<email confidence="0.996646">yuqing@us.ibm.com</email>
<address confidence="0.673898">of Computer Science VI, RWTH Aachen University, D-52056 Aachen, Germany</address>
<email confidence="0.999018">xujia@cs.rwth-aachen.de</email>
<abstract confidence="0.9983928125">In this work, the problem of extracting phrase translation is formulated as an information retrieval process implemented with a log-linear model aiming for a balanced precision and recall. We present a generic phrase training algorithm which is parameterized with feature functions and can be optimized jointly with the translation engine to directly maximize the end-to-end system performance. Multiple data-driven feature functions are proposed to capture the quality and confidence of phrases and phrase pairs. Experimental results demonstrate consistent and significant improvement over the widely used method that is based on word alignment matrix only.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Ayan</author>
<author>B Dorr</author>
</authors>
<title>Going beyond AER: An extensive analysis of word alignments and their impact on MT.</title>
<date>2006</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>9--16</pages>
<contexts>
<context citStr="Ayan and Dorr (2006)" endWordPosition="627" position="4002" startWordPosition="624">Johnson et al., 2007). On the other hand, there are valid translation pairs in the training corpus that are not learned due to word alignment errors as shown in Deng and Byrne (2005). Proceedings of ACL-08: HLT, pages 81–88, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics We would like to improve phrase translation ac- Algorithm 1 A Generic Phrase Training Procedure curacy and at the same time extract as many as possible valid phrase pairs that are missed due to incorrect word alignments. One approach is to leverage underlying word alignment quality such as in Ayan and Dorr (2006). In this work, we present a generic discriminative phrase pair extraction framework that can integrate multiple features aiming to identify correct phrase translation candidates. A significant deviation from most other approaches is that the framework is parameterized and can be optimized jointly with the decoder to maximize translation performance on a development set. Within the general framework, the main work is on investigating useful metrics. We employ features based on word alignment models and alignment matrix. We also propose information metrics that are derived from both bilingual a</context>
</contexts>
<marker>Ayan, Dorr, 2006</marker>
<rawString>N. Ayan and B. Dorr. 2006. Going beyond AER: An extensive analysis of word alignments and their impact on MT. In Proc. of ACL, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>A Lavie</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>65--72</pages>
<contexts>
<context citStr="Banerjee and Lavie, 2005" endWordPosition="3431" position="20579" startWordPosition="3428"> from the collection of parallel training sentences, we build word alignment models in two translation directions, from English to Chinese and from Chinese to English, and derive two sets of Viterbi alignments. By combining word alignments in two directions using heuristics (Och and Ney, 2003), a single set of static word alignments is then formed. Based on alignment models and word alignment matrices, we compare different approaches of building a phrase translation table and show the final translation results. We measure translation performance by the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) scores with multiple translation references. 85 BLEU Scores Table 04dev 04test 05test 06dev 06test HMM 0.367 0.407 0.473 0.200 0.190 Model-4 0.380 0.403 0.485 0.210 0.204 New 0.411 0.427 0.500 0.216 0.208 METEOR Scores Table 04dev 04test 05test 06dev 06test HMM 0.532 0.586 0.675 0.482 0.471 Model-4 0.540 0.593 0.682 0.492 0.480 New 0.568 0.614 0.691 0.505 0.487 Table 3: Translation Results 4.2 Translation Results Our baseline phrase table training method is the ViterbiExtract algorithm. All phrase pairs with respect to the word alignment boundary constraint are identified and pooled to build </context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>S. Banerjee and A. Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proc. of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>R Mercer</author>
</authors>
<title>The mathematics of machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--263</pages>
<contexts>
<context citStr="Brown et al., 1993" endWordPosition="1547" position="9471" startWordPosition="1544"> on multiple feature functions that aim to describe the quality of candidate phrase translations and the generic procedure to figure out the best way of combining these features. A good feature function pops up valid translation pairs and pushes down incorrect ones. 3 Features Now we present several feature functions that we investigated to help extracting correct phrase translations. All these features are data-driven and defined based on models, such as statistical word alignment model or language model. 3.1 Model-based Phrase Pair Posterior In a statistical generative word alignment model (Brown et al., 1993), it is assumed that (i) a random variable a specifies how each target word fj is generated by (therefore aligned to) a source 1 word eaj; and (ii) the likelihood function f(f, a|e) specifies a generative procedure from the source sentence to the target sentence. Given a phrase pair in a sentence pair, there will be many generative paths that align the source phrase to the target phrase. The likelihood of those generative procedures can be accumulated to get the likelihood of the phrase pair (Deng and Byrne, 2005). This is implemented as the summation of the likelihood function over all valid </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer. 1993. The mathematics of machine translation: Parameter estimation. Computational Linguistics, 19:263–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>310--318</pages>
<contexts>
<context citStr="Chen and Goodman, 1996" endWordPosition="3322" position="19885" startWordPosition="3319"> Our decoder is a phrase-based multi-stack implementation of the log-linear model similar to Pharaoh (Koehn et al., 2003). Like other log-linear model based decoders, active features in our translation engine include translation models in two directions, lexicon weights in two directions, language model, lexicalized distortion models, sentence length penalty and other heuristics. These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method. The language model is a statistical trigram model estimated with Modified Kneser-Ney smoothing (Chen and Goodman, 1996) using only English sentences in the parallel training data. Starting from the collection of parallel training sentences, we build word alignment models in two translation directions, from English to Chinese and from Chinese to English, and derive two sets of Viterbi alignments. By combining word alignments in two directions using heuristics (Och and Ney, 2003), a single set of static word alignments is then formed. Based on alignment models and word alignment matrices, we compare different approaches of building a phrase translation table and show the final translation results. We measure tra</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>S. F. Chen and J. Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proc. of ACL, pages 310–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Deng</author>
<author>W Byrne</author>
</authors>
<title>HMM word and phrase alignment for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of HLT-EMNLP,</booktitle>
<pages>169--176</pages>
<contexts>
<context citStr="Deng and Byrne, 2005" endWordPosition="355" position="2332" startWordPosition="352">ranslations of each other. Methods have been proposed, based on syntax, that take advantage of linguistic constraints and alignment of grammatical structure, such as in Yamada 81 and Knight (2001) and Wu (1995). The most widely used approach derives phrase pairs from word alignment matrix (Och and Ney, 2003; Koehn et al., 2003). Other methods do not depend on word alignments only, such as directly modeling phrase alignment in a joint generative way (Marcu and Wong, 2002), pursuing information extraction perspective (Venugopal et al., 2003), or augmenting with modelbased phrase pair posterior (Deng and Byrne, 2005). Using relative frequency as translation probability is a common practice to measure goodness of a phrase pair. Since most phrases appear only a few times in training data, a phrase pair translation is also evaluated by lexical weights (Koehn et al., 2003) or term weighting (Zhao et al., 2004) as additional features to avoid overestimation. The translation probability can also be discriminatively trained such as in Tillmann and Zhang (2006). The focus of this paper is the phrase pair extraction problem. As in information retrieval, precision and recall issues need to be addressed with a right</context>
<context citStr="Deng and Byrne (2005)" endWordPosition="555" position="3564" startWordPosition="552">r building a phrase translation table. High precision requires that identified translation candidates are accurate, while high recall wants as much valid phrase pairs as possible to be extracted, which is important and necessary for online translation that requires coverage. In the word-alignment derived phrase extraction approach, precision can be improved by filtering out most of the entries by using a statistical significance test (Johnson et al., 2007). On the other hand, there are valid translation pairs in the training corpus that are not learned due to word alignment errors as shown in Deng and Byrne (2005). Proceedings of ACL-08: HLT, pages 81–88, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics We would like to improve phrase translation ac- Algorithm 1 A Generic Phrase Training Procedure curacy and at the same time extract as many as possible valid phrase pairs that are missed due to incorrect word alignments. One approach is to leverage underlying word alignment quality such as in Ayan and Dorr (2006). In this work, we present a generic discriminative phrase pair extraction framework that can integrate multiple features aiming to identify correct phrase transl</context>
<context citStr="Deng and Byrne, 2005" endWordPosition="1637" position="9990" startWordPosition="1634">odel-based Phrase Pair Posterior In a statistical generative word alignment model (Brown et al., 1993), it is assumed that (i) a random variable a specifies how each target word fj is generated by (therefore aligned to) a source 1 word eaj; and (ii) the likelihood function f(f, a|e) specifies a generative procedure from the source sentence to the target sentence. Given a phrase pair in a sentence pair, there will be many generative paths that align the source phrase to the target phrase. The likelihood of those generative procedures can be accumulated to get the likelihood of the phrase pair (Deng and Byrne, 2005). This is implemented as the summation of the likelihood function over all valid hidden word alignments. 1The word source and target are in the sense of word alignment direction, not as in the source-channel formulation. More specifically, let A(j1,j2) (i1,i2) be the set of word alignment a that aligns the source phrase ej1 i1 to the target phrase fj2 j1 (links to NULL word are ignored for simplicity): A(j1,z2� j2) _ (i1, {a : aj E [i1, i2] iff j E [j1, j2]} The alignment set given a phrase pair ignores those pairs with word links across the phrase boundary. Consequently, the phrase-pair poste</context>
</contexts>
<marker>Deng, Byrne, 2005</marker>
<rawString>Y. Deng and W. Byrne. 2005. HMM word and phrase alignment for statistical machine translation. In Proc. of HLT-EMNLP, pages 169–176.</rawString>
</citation>
<citation valid="false">
<title>3By parallelism, we have shown the feasibility and effectiveness (results not presented here) of the proposed method in handling millions of sentence pairs.</title>
<marker/>
<rawString>3By parallelism, we have shown the feasibility and effectiveness (results not presented here) of the proposed method in handling millions of sentence pairs.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Johnson</author>
<author>J Martin</author>
<author>G Foster</author>
<author>R Kuhn</author>
</authors>
<title>Improving translation quality by discarding most of the phrasetable.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL,</booktitle>
<pages>967--975</pages>
<contexts>
<context citStr="Johnson et al., 2007" endWordPosition="526" position="3403" startWordPosition="523"> focus of this paper is the phrase pair extraction problem. As in information retrieval, precision and recall issues need to be addressed with a right balance for building a phrase translation table. High precision requires that identified translation candidates are accurate, while high recall wants as much valid phrase pairs as possible to be extracted, which is important and necessary for online translation that requires coverage. In the word-alignment derived phrase extraction approach, precision can be improved by filtering out most of the entries by using a statistical significance test (Johnson et al., 2007). On the other hand, there are valid translation pairs in the training corpus that are not learned due to word alignment errors as shown in Deng and Byrne (2005). Proceedings of ACL-08: HLT, pages 81–88, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics We would like to improve phrase translation ac- Algorithm 1 A Generic Phrase Training Procedure curacy and at the same time extract as many as possible valid phrase pairs that are missed due to incorrect word alignments. One approach is to leverage underlying word alignment quality such as in Ayan and Dorr (2006).</context>
</contexts>
<marker>Johnson, Martin, Foster, Kuhn, 2007</marker>
<rawString>H. Johnson, J. Martin, G. Foster, and R. Kuhn. 2007. Improving translation quality by discarding most of the phrasetable. In Proc. of EMNLP-CoNLL, pages 967– 975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrasebased translation.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>48--54</pages>
<contexts>
<context citStr="Koehn et al., 2003" endWordPosition="309" position="2040" startWordPosition="306">o related issues need to be addressed (either separately or jointly): which pairs are considered valid translations and how to assign weights, such as probabilities, to them. The first problem is referred to as phrase pair extraction, which identifies phrase pairs that are supposed to be translations of each other. Methods have been proposed, based on syntax, that take advantage of linguistic constraints and alignment of grammatical structure, such as in Yamada 81 and Knight (2001) and Wu (1995). The most widely used approach derives phrase pairs from word alignment matrix (Och and Ney, 2003; Koehn et al., 2003). Other methods do not depend on word alignments only, such as directly modeling phrase alignment in a joint generative way (Marcu and Wong, 2002), pursuing information extraction perspective (Venugopal et al., 2003), or augmenting with modelbased phrase pair posterior (Deng and Byrne, 2005). Using relative frequency as translation probability is a common practice to measure goodness of a phrase pair. Since most phrases appear only a few times in training data, a phrase pair translation is also evaluated by lexical weights (Koehn et al., 2003) or term weighting (Zhao et al., 2004) as additiona</context>
<context citStr="Koehn et al., 2003" endWordPosition="1426" position="8715" startWordPosition="1423"> engine to minimize the final translation errors measured by automatic metrics such as BLEU (Papineni et al., 2002). In the final step 4 (line 15), parameters {Ak, T} are discriminatively trained on a development set using the downhill simplex method (Nelder and Mead, 1965). This phrase training procedure is general in the sense that it is configurable and trainable with different feature functions and their parameters. The commonly used phrase extraction approach based on word alignment heuristics (referred as ViterbiExtract algorithm for comparison in this paper) as described in (Och, 2002; Koehn et al., 2003) is a special case of the algorithm, where candidate phrase pairs are restricted to those that respect word alignment boundaries. We rely on multiple feature functions that aim to describe the quality of candidate phrase translations and the generic procedure to figure out the best way of combining these features. A good feature function pops up valid translation pairs and pushes down incorrect ones. 3 Features Now we present several feature functions that we investigated to help extracting correct phrase translations. All these features are data-driven and defined based on models, such as sta</context>
<context citStr="Koehn et al., 2003" endWordPosition="3246" position="19383" startWordPosition="3243"> of refs 16 16 16 7 7 Table 2: Dev/test set statistics tal 306K English words and 295K Chinese words. In the data processing step, Chinese characters are segmented into words. English text are normalized and lowercased. All punctuation is removed. There are five sets of evaluation sentences in tourism domain for development and test. Their statistics are shown in Table 2. We will tune training and decoding parameters on 06dev and report results on other sets. 4.1 Training and Translation Setup Our decoder is a phrase-based multi-stack implementation of the log-linear model similar to Pharaoh (Koehn et al., 2003). Like other log-linear model based decoders, active features in our translation engine include translation models in two directions, lexicon weights in two directions, language model, lexicalized distortion models, sentence length penalty and other heuristics. These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method. The language model is a statistical trigram model estimated with Modified Kneser-Ney smoothing (Chen and Goodman, 1996) using only English sentences in the parallel training data. Starting from the collection of paral</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. Och, and D. Marcu. 2003. Statistical phrasebased translation. In Proc. of HLT-NAACL, pages 48– 54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic identification of noncompositional phrases.</title>
<date>1999</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>317--324</pages>
<contexts>
<context citStr="Lin, 1999" endWordPosition="1170" position="7108" startWordPosition="1169">rom the phrase pair pool 15: Discriminatively train feature weights ak and threshold T E-step from two directions motivated by Zens et al. (2004) with 5 iterations. We use these models to define the feature functions of candidate phrase pairs such as phrase pair posterior distribution. More details will be given in Section 3. Step 2 (line 2) consists of phrase pair evaluation, ranking and filtering. Usually all n-grams up to a pre-defined length limit are considered as candidate phrases. This is also the place where linguistic constraints can be applied, say to avoid noncompositional phrases (Lin, 1999). Each normalized feature score derived from word alignment models or language models will be log-linearly combined to generate the final score. Phrase pair filtering is simply thresholding on the final score by comparing to the maximum within the sentence pair. Note that under the log-linear model, applying threshold for filtering is equivalent to comparing the “likelihood” ratio. Step 3 (line 14) pools all candidate phrase pairs that pass the threshold testing and estimates the final phrase translation table by maximum likelihood criterion. For each candidate phrase pair which is above the t</context>
<context citStr="Lin, 1999" endWordPosition="2189" position="13241" startWordPosition="2188"> or metaphorical expression. To give an example, the unigram ‘tomorrow’ in ‘the day after tomorrow’ whose Chinese translation is a single word ‘��’. Extracting candidate translations for such kind of n-grams for the sake of improving coverage (recall) might hurt translation quality (precision). We will define a confidence metric to estimate how reliably the model can align an n-gram in one side to a phrase on the other side given a parallel sentence. Second, some n-grams themselves carry no linguistic meaning; their phrase translations can be misleading, for example non-compositional phrases (Lin, 1999). We will address this in section 3.3. Given a sentence pair, the basic assumption is that if the HMM word alignment model can align an English phrase well to a foreign phrase, the posterior distribution of the English phrase generating all foreign phrases on the other side is significantly biased. For instance, the posterior of one foreign phrase is far larger than that of the others. We use the entropy of the posterior distribution as the confidence metric: HBL(ei2 i1|e, f) = H(ˆPBHMM (ei2 i1 → ∗)) (2) where H(P) = − �� P(x) log P(x) is the entropy of a distribution P(x), normalized probabil</context>
</contexts>
<marker>Lin, 1999</marker>
<rawString>D. Lin. 1999. Automatic identification of noncompositional phrases. In Proc. of ACL, pages 317– 324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>D Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>133--139</pages>
<contexts>
<context citStr="Marcu and Wong, 2002" endWordPosition="335" position="2186" startWordPosition="332">uch as probabilities, to them. The first problem is referred to as phrase pair extraction, which identifies phrase pairs that are supposed to be translations of each other. Methods have been proposed, based on syntax, that take advantage of linguistic constraints and alignment of grammatical structure, such as in Yamada 81 and Knight (2001) and Wu (1995). The most widely used approach derives phrase pairs from word alignment matrix (Och and Ney, 2003; Koehn et al., 2003). Other methods do not depend on word alignments only, such as directly modeling phrase alignment in a joint generative way (Marcu and Wong, 2002), pursuing information extraction perspective (Venugopal et al., 2003), or augmenting with modelbased phrase pair posterior (Deng and Byrne, 2005). Using relative frequency as translation probability is a common practice to measure goodness of a phrase pair. Since most phrases appear only a few times in training data, a phrase pair translation is also evaluated by lexical weights (Koehn et al., 2003) or term weighting (Zhao et al., 2004) as additional features to avoid overestimation. The translation probability can also be discriminatively trained such as in Tillmann and Zhang (2006). The foc</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>D. Marcu and D. Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In Proc. of EMNLP, pages 133–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Nelder</author>
<author>R Mead</author>
</authors>
<title>A simplex method for function minimization.</title>
<date>1965</date>
<journal>Computer Journal,</journal>
<volume>7</volume>
<pages>313</pages>
<contexts>
<context citStr="Nelder and Mead, 1965" endWordPosition="1372" position="8370" startWordPosition="1369">r posterior as its soft count when dumping them into the global phrase pair pool. Other possibilities for the weighting include assigning constant one or the exponential of the final score etc. One of the advantages of the proposed phrase training algorithm is that it is a parameterized procedure that can be optimized jointly with the translation engine to minimize the final translation errors measured by automatic metrics such as BLEU (Papineni et al., 2002). In the final step 4 (line 15), parameters {Ak, T} are discriminatively trained on a development set using the downhill simplex method (Nelder and Mead, 1965). This phrase training procedure is general in the sense that it is configurable and trainable with different feature functions and their parameters. The commonly used phrase extraction approach based on word alignment heuristics (referred as ViterbiExtract algorithm for comparison in this paper) as described in (Och, 2002; Koehn et al., 2003) is a special case of the algorithm, where candidate phrase pairs are restricted to those that respect word alignment boundaries. We rely on multiple feature functions that aim to describe the quality of candidate phrase translations and the generic proce</context>
</contexts>
<marker>Nelder, Mead, 1965</marker>
<rawString>J. A. Nelder and R. Mead. 1965. A simplex method for function minimization. Computer Journal, 7:308– 313.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context citStr="Och and Ney, 2003" endWordPosition="305" position="2019" startWordPosition="302">parallel corpus, two related issues need to be addressed (either separately or jointly): which pairs are considered valid translations and how to assign weights, such as probabilities, to them. The first problem is referred to as phrase pair extraction, which identifies phrase pairs that are supposed to be translations of each other. Methods have been proposed, based on syntax, that take advantage of linguistic constraints and alignment of grammatical structure, such as in Yamada 81 and Knight (2001) and Wu (1995). The most widely used approach derives phrase pairs from word alignment matrix (Och and Ney, 2003; Koehn et al., 2003). Other methods do not depend on word alignments only, such as directly modeling phrase alignment in a joint generative way (Marcu and Wong, 2002), pursuing information extraction perspective (Venugopal et al., 2003), or augmenting with modelbased phrase pair posterior (Deng and Byrne, 2005). Using relative frequency as translation probability is a common practice to measure goodness of a phrase pair. Since most phrases appear only a few times in training data, a phrase pair translation is also evaluated by lexical weights (Koehn et al., 2003) or term weighting (Zhao et al</context>
<context citStr="Och and Ney, 2003" endWordPosition="3377" position="20248" startWordPosition="3374">er heuristics. These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method. The language model is a statistical trigram model estimated with Modified Kneser-Ney smoothing (Chen and Goodman, 1996) using only English sentences in the parallel training data. Starting from the collection of parallel training sentences, we build word alignment models in two translation directions, from English to Chinese and from Chinese to English, and derive two sets of Viterbi alignments. By combining word alignments in two directions using heuristics (Och and Ney, 2003), a single set of static word alignments is then formed. Based on alignment models and word alignment matrices, we compare different approaches of building a phrase translation table and show the final translation results. We measure translation performance by the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) scores with multiple translation references. 85 BLEU Scores Table 04dev 04test 05test 06dev 06test HMM 0.367 0.407 0.473 0.200 0.190 Model-4 0.380 0.403 0.485 0.210 0.204 New 0.411 0.427 0.500 0.216 0.208 METEOR Scores Table 04dev 04test 05test 06dev 06test HMM 0.532 </context>
<context citStr="Och and Ney, 2003" endWordPosition="3613" position="21712" startWordPosition="3610">respect to the word alignment boundary constraint are identified and pooled to build phrase translation tables with the Maximum Likelihood criterion. We prune phrase translation entries by their probabilities. The maximum number of words in Chinese and English phrases is set to 8 and 25 respectively for all conditions2. We perform online style phrase training, i.e., phrase extraction is not particular for any evaluation set. Two different word alignment models are trained as the baseline, one is symmetric HMM word alignment model, the other is IBM Model-4 as implemented in the GIZA++ toolkit (Och and Ney, 2003). The translation results as measured by BLEU and METEOR scores are presented in Table 3. We notice that Model-4 based phrase table performs roughly 1% better in terms of both BLEU and METEOR scores than that based on HMM. We follow the generic phrase training procedure as described in section 2. The most time consuming part is calculating posteriors, which is carried out in parallel with 30 jobs in less than 1.5 hours. We use the Viterbi word alignments from HMM to define within phrase pair consistency ratio as discussed in section 3.4. Although Table 3 implies that Model-4 word alignment qua</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>D Gildea</author>
</authors>
<title>A smorgasbord of features for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>161--168</pages>
<marker>Och, Gildea, 2004</marker>
<rawString>F. J. Och, D. Gildea, and et al. 2004. A smorgasbord of features for statistical machine translation. In Proc. of HLT-NAACL, pages 161–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Statistical Machine Translation: From Single Word Models to Alignment Templates.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>RWTH Aachen,</institution>
<contexts>
<context citStr="Och, 2002" endWordPosition="1422" position="8694" startWordPosition="1421">translation engine to minimize the final translation errors measured by automatic metrics such as BLEU (Papineni et al., 2002). In the final step 4 (line 15), parameters {Ak, T} are discriminatively trained on a development set using the downhill simplex method (Nelder and Mead, 1965). This phrase training procedure is general in the sense that it is configurable and trainable with different feature functions and their parameters. The commonly used phrase extraction approach based on word alignment heuristics (referred as ViterbiExtract algorithm for comparison in this paper) as described in (Och, 2002; Koehn et al., 2003) is a special case of the algorithm, where candidate phrase pairs are restricted to those that respect word alignment boundaries. We rely on multiple feature functions that aim to describe the quality of candidate phrase translations and the generic procedure to figure out the best way of combining these features. A good feature function pops up valid translation pairs and pushes down incorrect ones. 3 Features Now we present several feature functions that we investigated to help extracting correct phrase translations. All these features are data-driven and defined based o</context>
</contexts>
<marker>Och, 2002</marker>
<rawString>F. Och. 2002. Statistical Machine Translation: From Single Word Models to Alignment Templates. Ph.D. thesis, RWTH Aachen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A V Oppenheim</author>
<author>R W Schafer</author>
</authors>
<date>1989</date>
<booktitle>DiscreteTime Signal Processing.</booktitle>
<publisher>Prentice-Hall.</publisher>
<contexts>
<context citStr="Oppenheim and Schafer, 1989" endWordPosition="2975" position="17756" startWordPosition="2972">Venugopal et al., 2003) for phrase extraction. It is defined as the number of consistent word links associated with any words within the phrase pair divided by the number of all word links associated with any words within the phrase pair. An inconsistent link connects a word within the phrase pair to a word outside the phrase pair. For example, the WPPCR for (e2, f21) in Table 1 is 2/3. As a special case, the ViterbiExtract algorithm extracts only phrase pairs with WPPCR is 1. To further discriminate the pairs with higher WPPCR from those with lower ratio, we apply a BiLinear Transform (BLT) (Oppenheim and Schafer, 1989) mapping. BLT is commonly used in signal processing to attenuate the low frequency parts. When used to map WPPCR, it exaggerates the difference between phrase pairs with high WPPCR and those with low WPPCR, making the pairs with low ratio more unlikely to be selected as translation candidates. One of the nice properties of BLT is that there is a parameter that can be changed to adjust the degree of attenuation, which provides another dimension for system optimization. 4 Experimental Results We evaluate the effect of the proposed phrase extraction algorithm with translation performance. We do e</context>
</contexts>
<marker>Oppenheim, Schafer, 1989</marker>
<rawString>A. V. Oppenheim and R. W. Schafer. 1989. DiscreteTime Signal Processing. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context citStr="Papineni et al., 2002" endWordPosition="1345" position="8211" startWordPosition="1341">the final phrase translation table by maximum likelihood criterion. For each candidate phrase pair which is above the threshold, we assign HMM-based phrase pair posterior as its soft count when dumping them into the global phrase pair pool. Other possibilities for the weighting include assigning constant one or the exponential of the final score etc. One of the advantages of the proposed phrase training algorithm is that it is a parameterized procedure that can be optimized jointly with the translation engine to minimize the final translation errors measured by automatic metrics such as BLEU (Papineni et al., 2002). In the final step 4 (line 15), parameters {Ak, T} are discriminatively trained on a development set using the downhill simplex method (Nelder and Mead, 1965). This phrase training procedure is general in the sense that it is configurable and trainable with different feature functions and their parameters. The commonly used phrase extraction approach based on word alignment heuristics (referred as ViterbiExtract algorithm for comparison in this paper) as described in (Och, 2002; Koehn et al., 2003) is a special case of the algorithm, where candidate phrase pairs are restricted to those that r</context>
<context citStr="Papineni et al., 2002" endWordPosition="3425" position="20541" startWordPosition="3422">he parallel training data. Starting from the collection of parallel training sentences, we build word alignment models in two translation directions, from English to Chinese and from Chinese to English, and derive two sets of Viterbi alignments. By combining word alignments in two directions using heuristics (Och and Ney, 2003), a single set of static word alignments is then formed. Based on alignment models and word alignment matrices, we compare different approaches of building a phrase translation table and show the final translation results. We measure translation performance by the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) scores with multiple translation references. 85 BLEU Scores Table 04dev 04test 05test 06dev 06test HMM 0.367 0.407 0.473 0.200 0.190 Model-4 0.380 0.403 0.485 0.210 0.204 New 0.411 0.427 0.500 0.216 0.208 METEOR Scores Table 04dev 04test 05test 06dev 06test HMM 0.532 0.586 0.675 0.482 0.471 Model-4 0.540 0.593 0.682 0.492 0.480 New 0.568 0.614 0.691 0.505 0.487 Table 3: Translation Results 4.2 Translation Results Our baseline phrase table training method is the ViterbiExtract algorithm. All phrase pairs with respect to the word alignment boundary constrai</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proc. of ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Paul</author>
</authors>
<title>Overview of the IWSLT</title>
<date>2006</date>
<booktitle>In Proc. of IWSLT,</booktitle>
<pages>1--15</pages>
<contexts>
<context citStr="Paul, 2006" endWordPosition="3081" position="18388" startWordPosition="3080">only used in signal processing to attenuate the low frequency parts. When used to map WPPCR, it exaggerates the difference between phrase pairs with high WPPCR and those with low WPPCR, making the pairs with low ratio more unlikely to be selected as translation candidates. One of the nice properties of BLT is that there is a parameter that can be changed to adjust the degree of attenuation, which provides another dimension for system optimization. 4 Experimental Results We evaluate the effect of the proposed phrase extraction algorithm with translation performance. We do experiments on IWSLT (Paul, 2006) 2006 ChineseEnglish corpus. The task is to translate Chinese utterances in travel domain into English. We report only text (speech transcription) translation results. The training corpus consists of 40K ChineseEnglish parallel sentences in travel domain with toEval Set 04dev 04test 05test 06dev 06test # of sentences 506 500 506 489 500 # of words 2808 2906 3209 5214 5550 # of refs 16 16 16 7 7 Table 2: Dev/test set statistics tal 306K English words and 295K Chinese words. In the data processing step, Chinese characters are segmented into words. English text are normalized and lowercased. All </context>
</contexts>
<marker>Paul, 2006</marker>
<rawString>M. Paul. 2006. Overview of the IWSLT 2006 evaluation campaign. In Proc. of IWSLT, pages 1–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillmann</author>
<author>T Zhang</author>
</authors>
<title>A discriminative global training algorithm for statistical MT.</title>
<date>2006</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>721--728</pages>
<contexts>
<context citStr="Tillmann and Zhang (2006)" endWordPosition="428" position="2777" startWordPosition="425">erative way (Marcu and Wong, 2002), pursuing information extraction perspective (Venugopal et al., 2003), or augmenting with modelbased phrase pair posterior (Deng and Byrne, 2005). Using relative frequency as translation probability is a common practice to measure goodness of a phrase pair. Since most phrases appear only a few times in training data, a phrase pair translation is also evaluated by lexical weights (Koehn et al., 2003) or term weighting (Zhao et al., 2004) as additional features to avoid overestimation. The translation probability can also be discriminatively trained such as in Tillmann and Zhang (2006). The focus of this paper is the phrase pair extraction problem. As in information retrieval, precision and recall issues need to be addressed with a right balance for building a phrase translation table. High precision requires that identified translation candidates are accurate, while high recall wants as much valid phrase pairs as possible to be extracted, which is important and necessary for online translation that requires coverage. In the word-alignment derived phrase extraction approach, precision can be improved by filtering out most of the entries by using a statistical significance t</context>
</contexts>
<marker>Tillmann, Zhang, 2006</marker>
<rawString>C. Tillmann and T. Zhang. 2006. A discriminative global training algorithm for statistical MT. In Proc. of ACL, pages 721–728.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Venugopal</author>
<author>S Vogel</author>
<author>A Waibel</author>
</authors>
<title>Effective phrase translation extraction from alignment models.</title>
<date>2003</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>319--326</pages>
<contexts>
<context citStr="Venugopal et al., 2003" endWordPosition="343" position="2256" startWordPosition="340">hrase pair extraction, which identifies phrase pairs that are supposed to be translations of each other. Methods have been proposed, based on syntax, that take advantage of linguistic constraints and alignment of grammatical structure, such as in Yamada 81 and Knight (2001) and Wu (1995). The most widely used approach derives phrase pairs from word alignment matrix (Och and Ney, 2003; Koehn et al., 2003). Other methods do not depend on word alignments only, such as directly modeling phrase alignment in a joint generative way (Marcu and Wong, 2002), pursuing information extraction perspective (Venugopal et al., 2003), or augmenting with modelbased phrase pair posterior (Deng and Byrne, 2005). Using relative frequency as translation probability is a common practice to measure goodness of a phrase pair. Since most phrases appear only a few times in training data, a phrase pair translation is also evaluated by lexical weights (Koehn et al., 2003) or term weighting (Zhao et al., 2004) as additional features to avoid overestimation. The translation probability can also be discriminatively trained such as in Tillmann and Zhang (2006). The focus of this paper is the phrase pair extraction problem. As in informat</context>
<context citStr="Venugopal et al., 2003" endWordPosition="2867" position="17151" startWordPosition="2864"> of PUs of the four word boundaries. 3.4 Word Alignments Induced Metric The widely used ViterbiExtract algorithm relies on word alignment matrix and no-crossing-link assumption to extract phrase translation candidates. Practically it has been proved to work well. However, discarding correct phrase pairs due to incorrect word links leaves room for improving recall. This is especially true for not significantly large training corpora. Provided with a word alignment matrix, we define within phrase pair consistency ratio (WPPCR) as another feature function. WPPCR was used as one of the scores in (Venugopal et al., 2003) for phrase extraction. It is defined as the number of consistent word links associated with any words within the phrase pair divided by the number of all word links associated with any words within the phrase pair. An inconsistent link connects a word within the phrase pair to a word outside the phrase pair. For example, the WPPCR for (e2, f21) in Table 1 is 2/3. As a special case, the ViterbiExtract algorithm extracts only phrase pairs with WPPCR is 1. To further discriminate the pairs with higher WPPCR from those with lower ratio, we apply a BiLinear Transform (BLT) (Oppenheim and Schafer, </context>
<context citStr="Venugopal et al., 2003" endWordPosition="4006" position="24105" startWordPosition="4003">out the best phrase table. The translation results with the discriminatively trained phrase table are shown as the row of “New” in Table 3. We observe that the new approach is consistently better than the baseline ViterbiExtract algorithm with either Model-4 or HMM word alignments on all sets. Roughly, it has 0.5% higher BLEU score on 2006 sets and 1.5% to 3% higher on other sets than Model-4 based ViterbiExtract method. Similar superior results are observed when measured with METEOR score. 5 Discussions The generic phrase training algorithm follows an information retrieval perspective as in (Venugopal et al., 2003) but aims to improve both precision and recall with the trainable log-linear model. A clear advantage of the proposed approach over the widely used ViterbiExtract method is trainability. Under the general framework, one can put as many features as possible together under the log-linear model to evaluate the quality of a phrase and a phase pair. The phrase table extracting procedure is trainable and can be optimized jointly with the translation engine. Another advantage is flexibility, which is provided partially by the threshold T. As the figure 1 shows, when we increase the threshold by allow</context>
</contexts>
<marker>Venugopal, Vogel, Waibel, 2003</marker>
<rawString>A. Venugopal, S. Vogel, and A. Waibel. 2003. Effective phrase translation extraction from alignment models. In Proc. of ACL, pages 319–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>H Ney</author>
<author>C Tillmann</author>
</authors>
<title>HMM based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proc. of the COLING.</booktitle>
<contexts>
<context citStr="Vogel et al., 1996" endWordPosition="947" position="5894" startWordPosition="944">ding word of the phrase. We first train word alignment models and will use them to evaluate the goodness of a phrase and a phrase pair. Let fk(E, F), k = 1, 2, · · · , K be K feature functions to be used to measure the quality of a given phrase pair (E, F). The generic phrase extraction procedure is an evaluation, ranking, filtering, estimation and tuning process, presented in Algorithm 1. Step 1 (line 1) is the preparation stage. Beginning with a flat lexicon, we train IBM Model-1 word alignment model with 10 iterations for each translation direction. We then train HMM word alignment models (Vogel et al., 1996) in two directions simultaneously by merging statistics collected in the 82 1: Train Model-1 and HMM word alignment models 2: for all sentence pair (e, f) do 3: Identify candidate phrases on each side 4: for all candidate phrase pair (E, F) do 5: Calculate its feature function values fk Obtain the score q(E, F) = �K 6: k�1 akfk(E, F) 7: end for 8: Sort candidate phrase pairs by their final scores q 9: Find the maximum score qm = max q(E, F) 10: for all candidate phrase pair (E, F) do 11: If q(E, F) &gt; qm − T, dump the pair into the pool 12: end for 13: end for 14: Built a phrase translation tab</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>S. Vogel, H. Ney, and C. Tillmann. 1996. HMM based word alignment in statistical translation. In Proc. of the COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>An algorithm for simultaneously bracketing parallel texts by aligning words.</title>
<date>1995</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>244--251</pages>
<contexts>
<context citStr="Wu (1995)" endWordPosition="288" position="1921" startWordPosition="287">slations and their probabilities for a given source phrase. In learning such a table from parallel corpus, two related issues need to be addressed (either separately or jointly): which pairs are considered valid translations and how to assign weights, such as probabilities, to them. The first problem is referred to as phrase pair extraction, which identifies phrase pairs that are supposed to be translations of each other. Methods have been proposed, based on syntax, that take advantage of linguistic constraints and alignment of grammatical structure, such as in Yamada 81 and Knight (2001) and Wu (1995). The most widely used approach derives phrase pairs from word alignment matrix (Och and Ney, 2003; Koehn et al., 2003). Other methods do not depend on word alignments only, such as directly modeling phrase alignment in a joint generative way (Marcu and Wong, 2002), pursuing information extraction perspective (Venugopal et al., 2003), or augmenting with modelbased phrase pair posterior (Deng and Byrne, 2005). Using relative frequency as translation probability is a common practice to measure goodness of a phrase pair. Since most phrases appear only a few times in training data, a phrase pair t</context>
</contexts>
<marker>Wu, 1995</marker>
<rawString>D. Wu. 1995. An algorithm for simultaneously bracketing parallel texts by aligning words. In Proc. of ACL, pages 244–251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yamada</author>
<author>K Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>523--530</pages>
<marker>Yamada, Knight, 2001</marker>
<rawString>K. Yamada and K. Knight. 2001. A syntax-based statistical translation model. In Proc. of ACL, pages 523– 530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>E Matusov</author>
<author>H Ney</author>
</authors>
<title>Improved word alignment using a symmetric lexicon model.</title>
<date>2004</date>
<booktitle>In Proc. of COLING,</booktitle>
<pages>36--42</pages>
<contexts>
<context citStr="Zens et al. (2004)" endWordPosition="1090" position="6643" startWordPosition="1087">ll sentence pair (e, f) do 3: Identify candidate phrases on each side 4: for all candidate phrase pair (E, F) do 5: Calculate its feature function values fk Obtain the score q(E, F) = �K 6: k�1 akfk(E, F) 7: end for 8: Sort candidate phrase pairs by their final scores q 9: Find the maximum score qm = max q(E, F) 10: for all candidate phrase pair (E, F) do 11: If q(E, F) &gt; qm − T, dump the pair into the pool 12: end for 13: end for 14: Built a phrase translation table from the phrase pair pool 15: Discriminatively train feature weights ak and threshold T E-step from two directions motivated by Zens et al. (2004) with 5 iterations. We use these models to define the feature functions of candidate phrase pairs such as phrase pair posterior distribution. More details will be given in Section 3. Step 2 (line 2) consists of phrase pair evaluation, ranking and filtering. Usually all n-grams up to a pre-defined length limit are considered as candidate phrases. This is also the place where linguistic constraints can be applied, say to avoid noncompositional phrases (Lin, 1999). Each normalized feature score derived from word alignment models or language models will be log-linearly combined to generate the fin</context>
</contexts>
<marker>Zens, Matusov, Ney, 2004</marker>
<rawString>R. Zens, E. Matusov, and H. Ney. 2004. Improved word alignment using a symmetric lexicon model. In Proc. of COLING, pages 36–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Zhao</author>
<author>S Vogel</author>
<author>M Eck</author>
<author>A Waibel</author>
</authors>
<title>Phrase pair rescoring with term weighting for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>206--213</pages>
<contexts>
<context citStr="Zhao et al., 2004" endWordPosition="405" position="2627" startWordPosition="402"> Ney, 2003; Koehn et al., 2003). Other methods do not depend on word alignments only, such as directly modeling phrase alignment in a joint generative way (Marcu and Wong, 2002), pursuing information extraction perspective (Venugopal et al., 2003), or augmenting with modelbased phrase pair posterior (Deng and Byrne, 2005). Using relative frequency as translation probability is a common practice to measure goodness of a phrase pair. Since most phrases appear only a few times in training data, a phrase pair translation is also evaluated by lexical weights (Koehn et al., 2003) or term weighting (Zhao et al., 2004) as additional features to avoid overestimation. The translation probability can also be discriminatively trained such as in Tillmann and Zhang (2006). The focus of this paper is the phrase pair extraction problem. As in information retrieval, precision and recall issues need to be addressed with a right balance for building a phrase translation table. High precision requires that identified translation candidates are accurate, while high recall wants as much valid phrase pairs as possible to be extracted, which is important and necessary for online translation that requires coverage. In the w</context>
</contexts>
<marker>Zhao, Vogel, Eck, Waibel, 2004</marker>
<rawString>B. Zhao, S. Vogel, M. Eck, and A. Waibel. 2004. Phrase pair rescoring with term weighting for statistical machine translation. In Proc. of EMNLP, pages 206–213.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>