<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant confidence="0.000000" no="0">
<title confidence="0.981199">
Efficient Third-order Dependency Parsers
</title>
<author confidence="0.927149">
Terry Koo and Michael Collins
</author>
<affiliation confidence="0.41802">
MIT CSAIL, Cambridge, MA, 02139, USA
</affiliation>
<email confidence="0.992841">
{maestro,mcollins}@csail.mit.edu
</email>
<sectionHeader confidence="0.997315" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999855583333333">We present algorithms for higher-order dependency parsing that are “third-order” in the sense that they can evaluate substructures containing three dependencies, and “efficient” in the sense that they require only O(n4) time. Importantly, our new parsers can utilize both sibling-style and grandchild-style interactions. We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively.</bodyText>
<sectionHeader confidence="0.999392" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.989342318181818">Dependency grammar has proven to be a very useful syntactic formalism, due in no small part to the development of efficient parsing algorithms (Eisner, 2000; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007), which can be leveraged for a wide variety of learning methods, such as feature-rich discriminative models (Lafferty et al., 2001; Collins, 2002; Taskar et al., 2003). These parsing algorithms share an important characteristic: they factor dependency trees into sets of parts that have limited interactions. By exploiting the additional constraints arising from the factorization, maximizations or summations over the set of possible dependency trees can be performed efficiently and exactly. A crucial limitation of factored parsing algorithms is that the associated parts are typically quite small, losing much of the contextual information within the dependency tree. For the purposes of improving parsing performance, it is desirable to increase the size and variety of the parts used by the factorization.1 At the same time, the need for more expressive factorizations 1For examples of how performance varies with the degree of the parser’s factorization see, e.g., McDonald and Pereira (2006, Tables 1 and 2), Carreras (2007, Table 2), Koo et al. (2008, Tables 2 and 4), or Suzuki et al. (2009, Tables 3–6). must be balanced against any resulting increase in the computational cost of the parsing algorithm. Consequently, recent work in dependency parsing has been restricted to applications of secondorder parsers, the most powerful of which (Carreras, 2007) requires O(n4) time and O(n3) space, while being limited to second-order parts. In this paper, we present new third-order parsing algorithms that increase both the size and variety of the parts participating in the factorization, while simultaneously maintaining computational requirements of O(n4) time and O(n3) space. We evaluate our parsers on the Penn WSJ Treebank (Marcus et al., 1993) and Prague Dependency Treebank (Hajiˇc et al., 2001), achieving unlabeled attachment scores of 93.04% and 87.38%. In summary, we make three main contributions:</bodyText>
<listItem confidence="0.999121">1. Efficient new third-order parsing algorithms. 2. Empirical evaluations of these parsers. 3. A free distribution of our implementation.2</listItem>
<bodyText confidence="0.9998222">The remainder of this paper is divided as follows: Sections 2 and 3 give background, Sections 4 and 5 describe our new parsing algorithms, Section 6 discusses related work, Section 7 presents our experimental results, and Section 8 concludes.</bodyText>
<sectionHeader confidence="0.985171" genericHeader="method">
2 Dependency parsing
</sectionHeader>
<bodyText confidence="0.999839833333333">In dependency grammar, syntactic relationships are represented as head-modifier dependencies: directed arcs between a head, which is the more “essential” word in the relationship, and a modifier, which supplements the meaning of the head. For example, Figure 1 contains a dependency between the verb “report” (the head) and its object “sales” (the modifier). A complete analysis of a sentence is given by a dependency tree: a set of dependencies that forms a rooted, directed tree spanning the words of the sentence. Every dependency tree is rooted at a special “*” token, allowing the selection of the sentential head to be modeled as if it were a dependency.</bodyText>
<footnote confidence="0.972671">
2http://groups.csail.mit.edu/nlp/dpo3/
</footnote>
<page confidence="0.502673">
1
</page>
<note confidence="0.973843333333333">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1–11,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
* Insiders must report purchases and sales immediately
</note>
<figureCaption confidence="0.998294">
Figure 1: An example dependency structure.
</figureCaption>
<bodyText confidence="0.9996205">For a sentence x, we define dependency parsing as a search for the highest-scoring analysis of x:</bodyText>
<equation confidence="0.9944095">
y*(x) = argmax SCORE(x, y) (1)
yEY(-)
</equation>
<bodyText confidence="0.994984444444444">Here, y(x) is the set of all trees compatible with x and SCORE(x, y) evaluates the event that tree y is the analysis of sentence x. Since the cardinality of y(x) grows exponentially with the length of the sentence, directly solving Eq. 1 is impractical. A common strategy, and one which forms the focus of this paper, is to factor each dependency tree into small parts, which can be scored in isolation. Factored parsing can be formalized as follows:</bodyText>
<equation confidence="0.914016">
SCORE(x, y) = � SCOREPART(x,p)
pEy
</equation>
<bodyText confidence="0.999036">That is, we treat the dependency tree y as a set of parts p, each of which makes a separate contribution to the score of y. For certain factorizations, efficient parsing algorithms exist for solving Eq. 1. We define the order of a part according to the number of dependencies it contains, with analogous terminology for factorizations and parsing algorithms. In the remainder of this paper, we focus on factorizations utilizing the following parts:</bodyText>
<equation confidence="0.5234075">
h m h s m g h m
dependency sibling grandchild
g h s m h t s m
grand-sibling tri-sibling
</equation>
<bodyText confidence="0.99952475">Specifically, Sections 4.1, 4.2, and 4.3 describe parsers that, respectively, factor trees into grandchild parts, grand-sibling parts, and a mixture of grand-sibling and tri-sibling parts.</bodyText>
<sectionHeader confidence="0.995101" genericHeader="method">
3 Existing parsing algorithms
</sectionHeader>
<bodyText confidence="0.99966625">Our new third-order dependency parsers build on ideas from existing parsing algorithms. In this section, we provide background on two relevant parsers from previous work.</bodyText>
<figure confidence="0.327312">
h m h r r+1 m
</figure>
<figureCaption confidence="0.9465326">
Figure 2: The dynamic-programming structures
and derivations of the Eisner (2000) algorithm.
Complete spans are depicted as triangles and in-
complete spans as trapezoids. For brevity, we elide
the symmetric right-headed versions.
</figureCaption>
<subsectionHeader confidence="0.99423">
3.1 First-order factorization
</subsectionHeader>
<bodyText confidence="0.999884405405406">The first type of parser we describe uses a “firstorder” factorization, which decomposes a dependency tree into its individual dependencies. Eisner (2000) introduced a widely-used dynamicprogramming algorithm for first-order parsing; as it is the basis for many parsers, including our new algorithms, we summarize its design here. The Eisner (2000) algorithm is based on two interrelated types of dynamic-programming structures: complete spans, which consist of a headword and its descendents on one side, and incomplete spans, which consist of a dependency and the region between the head and modifier. Formally, we denote a complete span as Ch,e where h and a are the indices of the span’s headword and endpoint. An incomplete span is denoted as Ih,,,t where h and m are the index of the head and modifier of a dependency. Intuitively, a complete span represents a “half-constituent” headed by h, whereas an incomplete span is only a partial half-constituent, since the constituent can be extended by adding more modifiers to m. Each type of span is created by recursively combining two smaller, adjacent spans; the constructions are specified graphically in Figure 2. An incomplete span is constructed from a pair of complete spans, indicating the division of the range [h, m] into constituents headed by h and m. A complete span is created by “completing” an incomplete span with the other half of m’s constituent. The point of concatenation in each construction—m in Figure 2(a) or r in Figure 2(b)—is the split point, a free index that must be enumerated to find the optimal construction. In order to parse a sentence x, it suffices to find optimal constructions for all complete and incomplete spans defined on x. This can be accomplished by adapting standard chart-parsing techniques (Cocke and Schwartz, 1970; Younger, 1967; Kasami, 1965) to the recursive derivations defined in Figure 2.</bodyText>
<equation confidence="0.775591666666667">
=
+
h e h m m e
</equation>
<page confidence="0.676159">
2
</page>
<figure confidence="0.481156">
h m h s s m
</figure>
<figureCaption confidence="0.6193565">
Figure 3: The dynamic-programming structures
and derivations of the second-order sibling parser;
sibling spans are depicted as boxes. For brevity,
we elide the right-headed versions.
</figureCaption>
<bodyText confidence="0.989767375">Since each derivation is defined by two fixed indices (the boundaries of the span) and a third free index (the split point), the parsing algorithm requires O(n3) time and O(n2) space (Eisner, 1996; McAllester, 1999).</bodyText>
<subsectionHeader confidence="0.999913">
3.2 Second-order sibling factorization
</subsectionHeader>
<bodyText confidence="0.999910925925926">As remarked by Eisner (1996) and McDonald and Pereira (2006), it is possible to rearrange the dynamic-programming structures to conform to an improved factorization that decomposes each tree into sibling parts—pairs of dependencies with a shared head. Specifically, a sibling part consists of a triple of indices (h, m, s) where (h, m) and (h, s) are dependencies, and where s and m are successive modifiers to the same side of h. In order to parse this factorization, the secondorder parser introduces a third type of dynamicprogramming structure: sibling spans, which represent the region between successive modifiers of some head. Formally, we denote a sibling span as S,,,,, where s and m are a pair of modifiers involved in a sibling relationship. Modified versions of sibling spans will play an important role in the new parsing algorithms described in Section 4. Figure 3 provides a graphical specification of the second-order parsing algorithm. Note that incomplete spans are constructed in a new way: the second-order parser combines a smaller incomplete span, representing the next-innermost dependency, with a sibling span that covers the region between the two modifiers. Sibling parts (h, m, s) can thus be obtained from Figure 3(b). Despite the use of second-order parts, each derivation is still defined by a span and split point, so the parser requires O(n3) time and O(n2) space.</bodyText>
<figure confidence="0.640135">
g h e g h m h m e
h m g h r g h r+1 m
</figure>
<figureCaption confidence="0.804468">
Figure 4: The dynamic-programming structures
and derivations of Model 0. For brevity, we elide
the right-headed versions. Note that (c) and (d)
differ from (a) and (b) only in the position of g.
</figureCaption>
<sectionHeader confidence="0.924405" genericHeader="method">
4 New third-order parsing algorithms
</sectionHeader>
<bodyText confidence="0.999866375">In this section we describe our new third-order dependency parsing algorithms. Our overall method is characterized by the augmentation of each span with a “grandparent” index: an index external to the span whose role will be made clear below. This section presents three parsing algorithms based on this idea: Model 0, a second-order parser, and Models 1 and 2, which are third-order parsers.</bodyText>
<subsectionHeader confidence="0.997879">
4.1 Model 0: all grandchildren
</subsectionHeader>
<bodyText confidence="0.9997254375">The first parser, Model 0, factors each dependency tree into a set of grandchild parts—pairs of dependencies connected head-to-tail. Specifically, a grandchild part is a triple of indices (g, h, m) where (g, h) and (h, m) are dependencies.3 In order to parse this factorization, we augment both complete and incomplete spans with grandparent indices; for brevity, we refer to these augmented structures as g-spans. Formally, we denote a complete g-span as Ch,e, where Ch,e is a normal complete span and g is an index lying outside the range [h, e], with the implication that (g, h) is a dependency. Incomplete g-spans are defined analogously and are denoted as Ih,.. Figure 4 depicts complete and incomplete gspans and provides a graphical specification of the</bodyText>
<footnote confidence="0.8513895">
3The Carreras (2007) parser also uses grandchild parts but
only in restricted cases; see Section 6 for details.
</footnote>
<figure confidence="0.905662190476191">
h e h m m e
=
+
s m s r r+1 m
=
+
=
+
g h m g h r h r+1 m
(b)
h e g h m g h m e
(d)
=
+
(c)
=
+
(a)
3
OPTIMIZEALLSPANS(x)
1. ∀ g, i Cgi,i = 0 / base case
</figure>
<listItem confidence="0.988964181818182">2. for w = 1... (n − 1) / span width 3. for i = 1... (n − w) / span start index 4. j = i + w / span end index 5. for g &lt; i or g &gt; j / grandparent index 6. Igi,j = maxi&lt;r&lt;j {Cg i,r + Ci j,r+1} + SCOREG(x, g, i, j) 7. Igj,i = maxi&lt;r&lt;j {Cgj,r+1 + Cji,r} + SCOREG(x, g, j, i) 8. Cgi,j = maxi&lt;m&lt;j {Igi,m + Cim } ,j 9. Cgj,i = maxi&lt;m&lt;j {Ig j,m + Cj m,i}</listItem>
<figure confidence="0.926279">
10. endfor
11. endfor
12. endfor
</figure>
<figureCaption confidence="0.995044">
Figure 5: A bottom-up chart parser for Model 0.
</figureCaption>
<bodyText confidence="0.9769465">SCOREG is the scoring function for grandchild parts. We use the g-span identities as shorthand for their chart entries (e.g., Igi,j refers to the entry containing the maximum score of that g-span). Model 0 dynamic-programming algorithm. The algorithm resembles the first-order parser, except that every recursive construction must also set the grandparent indices of the smaller g-spans; fortunately, this can be done deterministically in all cases. For example, Figure 4(a) depicts the decomposition of Cgh,e into an incomplete half and a complete half. The grandparent of the incomplete half is copied from Cgh,e while the grandparent of the complete half is set to h, the head of m as defined by the construction. Clearly, grandchild parts (g, h, m) can be read off of the incomplete g-spans in Figure 4(b,d). Moreover, since each derivation copies the grandparent index g into successively smaller g-spans, grandchild parts will be produced for all grandchildren of g. Model 0 can be parsed by adapting standard top-down or bottom-up chart parsing techniques. For concreteness, Figure 5 provides a pseudocode sketch of a bottom-up chart parser for Model 0; although the sketch omits many details, it suffices for the purposes of illustration. The algorithm progresses from small widths to large in the usual manner, but after defining the endpoints (i, j) there is an additional loop that enumerates all possible grandparents. Since each derivation is defined by three fixed indices (the g-span) and one free index (the split point), the complexity of the algorithm is O(n4) time and O(n3) space. Note that the grandparent indices cause each gspan to have non-contiguous structure.</bodyText>
<figureCaption confidence="0.990471666666667">
Figure 6: The dynamic-programming structures
and derivations of Model 1. Right-headed and
right-grandparented versions are omitted.
</figureCaption>
<bodyText confidence="0.99977275">For example, in Figure 4(a) the words between g and h will be controlled by some other g-span. Due to these discontinuities, the correctness of the Model 0 dynamic-programming algorithm may not be immediately obvious. While a full proof of correctness is beyond the scope of this paper, we note that each structure on the right-hand side of Figure 4 lies completely within the structure on the left-hand side. This nesting of structures implies, in turn, that the usual properties required to ensure the correctness of dynamic programming hold.</bodyText>
<subsectionHeader confidence="0.99917">
4.2 Model 1: all grand-siblings
</subsectionHeader>
<bodyText confidence="0.999988173913044">We now describe our first third-order parsing algorithm. Model 1 decomposes each tree into a set of grand-sibling parts—combinations of sibling parts and grandchild parts. Specifically, a grand-sibling is a 4-tuple of indices (g, h, m, s) where (h, m, s) is a sibling part and (g, h, m) and (g, h, s) are grandchild parts. For example, in Figure 1, the words “must,” “report,” “sales,” and “immediately” form a grand-sibling part. In order to parse this factorization, we introduce sibling g-spans Shm,s, which are composed of a normal sibling span Sm,s and an external index h, with the implication that (h, m, s) forms a valid sibling part. Figure 6 provides a graphical specification of the dynamic-programming algorithm for Model 1. The overall structure of the algorithm resembles the second-order sibling parser, with the addition of grandparent indices; as in Model 0, the grandparent indices can be set deterministically in all cases. Note that the sibling g-spans are crucial: they allow grand-sibling parts (g, h, m, s) to be read off of Figure 6(b), while simultaneously propagating grandparent indices to smaller g-spans.</bodyText>
<figure confidence="0.955498166666667">
g h e g h m h m e
g h m g h s h s m
h s m h s r h r+1 m
(a)
4
g h e g h m h m e
</figure>
<figureCaption confidence="0.990711666666667">
Figure 7: The dynamic-programming structures
and derivations of Model 2. Right-headed and
right-grandparented versions are omitted.
</figureCaption>
<bodyText confidence="0.965171">Like Model 0, Model 1 can be parsed via adaptations of standard chart-parsing techniques; we omit the details for brevity. Despite the move to third-order parts, each derivation is still defined by a g-span and a split point, so that parsing requires only O(n4) time and O(n3) space.</bodyText>
<subsectionHeader confidence="0.998653">
4.3 Model 2: grand-siblings and tri-siblings
</subsectionHeader>
<bodyText confidence="0.991043673076923">Higher-order parsing algorithms have been proposed which extend the second-order sibling factorization to parts containing multiple siblings (McDonald and Pereira, 2006, also see Section 6 for discussion). In this section, we show how our g-span-based techniques can be combined with a third-order sibling parser, resulting in a parser that captures both grand-sibling parts and tri-sibling parts—4-tuples of indices (h, m, s, t) such that both (h, m, s) and (h, s, t) are sibling parts. In order to parse this factorization, we introduce a new type of dynamic-programming structure: sibling-augmented spans, or s-spans. Formally, we denote an incomplete s-span as Ih,m,s where Ih,m is a normal incomplete span and s is an index lying in the strict interior of the range [h, m], such that (h, m, s) forms a valid sibling part. Figure 7 provides a graphical specification of the Model 2 parsing algorithm. An incomplete s-span is constructed by combining a smaller incomplete s-span, representing the next-innermost pair of modifiers, with a sibling g-span, covering the region between the outer two modifiers. As in Model 1, sibling g-spans are crucial for propagating grandparent indices, while allowing the recovery of tri-sibling parts (h, m, s, t). Figure 7(b) shows how an incomplete s-span can be converted into an incomplete g-span by exchanging the internal sibling index for an external grandparent index; in the process, grand-sibling parts (g, h, m, s) are enumerated. Since every derivation is defined by an augmented span and a split point, Model 2 can be parsed in O(n4) time and O(n3) space. It should be noted that unlike Model 1, Model 2 produces grand-sibling parts only for the outermost pair of grandchildren,4 similar to the behavior of the Carreras (2007) parser. In fact, the resemblance is more than passing, as Model 2 can emulate the Carreras (2007) algorithm by “demoting” each third-order part into a second-order part: SCOREGS(x, g, h, m, s) = SCOREG(x, g, h, m) SCORETS(x, h, m, s, t) = SCORES(x, h, m, s) where SCOREG, SCORES, SCOREGS and SCORETS are the scoring functions for grandchildren, siblings, grand-siblings and tri-siblings, respectively. The emulated version has the same computational complexity as the original, so there is no practical reason to prefer it over the original. Nevertheless, the relationship illustrated above highlights the efficiency of our approach: we are able to recover third-order parts in place of second-order parts, at no additional cost.</bodyText>
<subsectionHeader confidence="0.902451">
4.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999980105263158">The technique of grandparent-index augmentation has proven fruitful, as it allows us to parse expressive third-order factorizations while retaining an efficient O(n4) runtime. In fact, our thirdorder parsing algorithms are “optimally” efficient in an asymptotic sense. Since each third-order part is composed of four separate indices, there are 0(n4) distinct parts. Any third-order parsing algorithm must at least consider the score of each part, hence third-order parsing is Q(n4) and it follows that the asymptotic complexity of Models 1 and 2 cannot be improved. The key to the efficiency of our approach is a fundamental asymmetry in the structure of a directed tree: a head can have any number of modifiers, while a modifier always has exactly one head. Factorizations like that of Carreras (2007) obtain grandchild parts by augmenting spans with the indices of modifiers, leading to limitations on the grandchildren that can participate in the factorization.</bodyText>
<footnote confidence="0.97412">
4The reason for the restriction is that in Model 2, grand-
siblings can only be derived via Figure 7(b), which does not
recursively copy the grandparent index for reuse in smaller
g-spans as Model 1 does in Figure 6(b).
</footnote>
<figure confidence="0.984737181818182">
=
+
(b)
g h m h s m
h s m h t s h s m
h s m h s r h r+1 m
(d)
+
(c)
=
(a)
</figure>
<page confidence="0.908541">
5
</page>
<bodyText confidence="0.9998668">Our method, by “inverting” the modifier indices into grandparent indices, exploits the structural asymmetry. As a final note, the parsing algorithms described in this section fall into the category of projective dependency parsers, which forbid crossing dependencies. If crossing dependencies are allowed, it is possible to parse a first-order factorization by finding the maximum directed spanning tree (Chu and Liu, 1965; Edmonds, 1967; McDonald et al., 2005b). Unfortunately, designing efficient higherorder non-projective parsers is likely to be challenging, based on recent hardness results (McDonald and Pereira, 2006; McDonald and Satta, 2007).</bodyText>
<sectionHeader confidence="0.999462" genericHeader="method">
5 Extensions
</sectionHeader>
<bodyText confidence="0.999982">We briefly outline a few extensions to our algorithms; we hope to explore these in future work.</bodyText>
<subsectionHeader confidence="0.955397">
5.1 Probabilistic inference
</subsectionHeader>
<bodyText confidence="0.999976166666667">Many statistical modeling techniques are based on partition functions and marginals—summations over the set of possible trees y(x). Straightforward adaptations of the inside-outside algorithm (Baker, 1979) to our dynamic-programming structures would suffice to compute these quantities.</bodyText>
<subsectionHeader confidence="0.999198">
5.2 Labeled parsing
</subsectionHeader>
<bodyText confidence="0.999021">Our parsers are easily extended to labeled dependencies. Direct integration of labels into Models 1 and 2 would result in third-order parts composed of three labeled dependencies, at the cost of increasing the time and space complexities by factors of O(L3) and O(L2), respectively, where L bounds the number of labels per dependency.</bodyText>
<subsectionHeader confidence="0.997455">
5.3 Word senses
</subsectionHeader>
<bodyText confidence="0.99992">If each word in x has a set of possible “senses,” our parsers can be modified to recover the best joint assignment of syntax and senses for x, by adapting methods in Eisner (2000). Complexity would increase by factors of O(54) time and O(53) space, where 5 bounds the number of senses per word.</bodyText>
<subsectionHeader confidence="0.919666">
5.4 Increased context
</subsectionHeader>
<bodyText confidence="0.999995916666667">If more vertical context is desired, the dynamicprogramming structures can be extended with additional ancestor indices, resulting in a “spine” of ancestors above each span. Each additional ancestor lengthens the vertical scope of the factorization (e.g., from grand-siblings to “great-grandsiblings”), while increasing complexity by a factor of O(n). Horizontal context can also be increased by adding internal sibling indices; each additional sibling widens the scope of the factorization (e.g., from grand-siblings to “grand-tri-siblings”), while increasing complexity by a factor of O(n).</bodyText>
<sectionHeader confidence="0.99997" genericHeader="related work">
6 Related work
</sectionHeader>
<bodyText confidence="0.999939666666667">Our method augments each span with the index of the head that governs that span, in a manner superficially similar to parent annotation in CFGs (Johnson, 1998). However, parent annotation is a grammar transformation that is independent of any particular sentence, whereas our method annotates spans with indices into the current sentence. These indices allow the use of arbitrary features predicated on the position of the grandparent (e.g., word identity, POS tag, contextual POS tags) without affecting the asymptotic complexity of the parsing algorithm. Efficiently encoding this kind of information into a sentence-independent grammar transformation would be challenging at best. Eisner (2000) defines dependency parsing models where each word has a set of possible “senses” and the parser recovers the best joint assignment of syntax and senses. Our new parsing algorithms could be implemented by defining the “sense” of each word as the index of its head. However, when parsing with senses, the complexity of the Eisner (2000) parser increases by factors of O(53) time and O(52) space (ibid., Section 4.2). Since each word has n potential heads, a direct application of the word-sense parser leads to time and space complexities of O(n6) and O(n4), respectively, in contrast to our O(n4) and O(n3).5 Eisner (2000) also uses head automata to score or recognize the dependents of each head. An interesting question is whether these automata could be coerced into modeling the grandparent indices used in our parsing algorithms. However, note that the head automata are defined in a sentenceindependent manner, with two automata per word in the vocabulary (ibid., Section 2). The automata are thus analogous to the rules of a CFG and attempts to use them to model grandparent indices would face difficulties similar to those already described for grammar transformations in CFGs.</bodyText>
<footnote confidence="0.9859112">
5In brief, the reason for the inefficiency is that the word-
sense parser is unable to exploit certain constraints, such as
the fact that the endpoints of a sibling g-span must have the
same head. The word-sense parser would needlessly enumer-
ate all possible pairs of heads in this case.
</footnote>
<page confidence="0.999024">
6
</page>
<bodyText confidence="0.999971368421053">It should be noted that third-order parsers have previously been proposed by McDonald and Pereira (2006), who remarked that their secondorder sibling parser (see Figure 3) could easily be extended to capture m &gt; 1 successive modifiers in O(nm+1) time (ibid., Section 2.2). To our knowledge, however, Models 1 and 2 are the first third-order parsing algorithms capable of modeling grandchild parts. In our experiments, we find that grandchild interactions make important contributions to parsing performance (see Table 3). Carreras (2007) presents a second-order parser that can score both sibling and grandchild parts, with complexities of O(n4) time and O(n3) space. An important limitation of the parser’s factorization is that it only defines grandchild parts for outermost grandchildren: (g, h, m) is scored only when m is the outermost modifier of h in some direction. Note that Models 1 and 2 have the same complexity as Carreras (2007), but strictly greater expressiveness: for each sibling or grandchild part used in the Carreras (2007) factorization, Model 1 defines an enclosing grand-sibling, while Model 2 defines an enclosing tri-sibling or grand-sibling. The factored parsing approach we focus on is sometimes referred to as “graph-based” parsing; a popular alternative is “transition-based” parsing, in which trees are constructed by making a series of incremental decisions (Yamada and Matsumoto, 2003; Attardi, 2006; Nivre et al., 2006; McDonald and Nivre, 2007). Transition-based parsers do not impose factorizations, so they can define arbitrary features on the tree as it is being built. As a result, however, they rely on greedy or approximate search algorithms to solve Eq.1.</bodyText>
<sectionHeader confidence="0.940325" genericHeader="evaluation and result">
7 Parsing experiments
</sectionHeader>
<bodyText confidence="0.999746">In order to evaluate the effectiveness of our parsers in practice, we apply them to the Penn WSJ Treebank (Marcus et al., 1993) and the Prague Dependency Treebank (Hajiˇc et al., 2001; Hajiˇc, 1998).6 We use standard training, validation, and test splits7 to facilitate comparisons. Accuracy is measured with unlabeled attachment score (UAS): the percentage of words with the correct head.8</bodyText>
<footnote confidence="0.993481833333333">
6For English, we extracted dependencies using Joakim
Nivre’s Penn2Malt tool with standard head rules (Yamada
and Matsumoto, 2003); for Czech, we “projectivized” the
training data by finding best-match projective trees.
7For Czech, the PDT has a predefined split; for English,
we split the Sections as: 2–21 training, 22 validation, 23 test.
</footnote>
<subsectionHeader confidence="0.479655" genericHeader="method">
7.1 Features for third-order parsing
</subsectionHeader>
<bodyText confidence="0.984004625">Our parsing algorithms can be applied to scores originating from any source, but in our experiments we chose to use the framework of structured linear models, deriving our scores as: SCOREPART(x, p) = w · f(x, p) Here, f is a feature-vector mapping and w is a vector of associated parameters. Following standard practice for higher-order dependency parsing (McDonald and Pereira, 2006; Carreras, 2007), Models 1 and 2 evaluate not only the relevant third-order parts, but also the lower-order parts that are implicit in their third-order factorizations. For example, Model 1 defines feature mappings for dependencies, siblings, grandchildren, and grand-siblings, so that the score of a dependency parse is given by:</bodyText>
<equation confidence="0.925449222222222">
MODEL1SCORE(x, y) =
11 wdep · fdep(x, h, m)
(h,m)Ey
E wsib · fsib(x, h, m, s)
(h,m,s)Ey
E wgch · fgch(x, g, h, m)
(g,h,m)Ey
11 wgsib · fgsib(x, g, h, m, s)
(g,h,m,s)Ey
</equation>
<bodyText confidence="0.994393588235294">Above, y is simultaneously decomposed into several different types of parts; trivial modifications to the Model 1 parser allow it to evaluate all of the necessary parts in an interleaved fashion. A similar treatment of Model 2 yields five feature mappings: the four above plus ftsib(x, h, m, s, t), which represents tri-sibling parts. The lower-order feature mappings fdep, fsib, and fgch are based on feature sets from previous work (McDonald et al., 2005a; McDonald and Pereira, 2006; Carreras, 2007), to which we added lexicalized versions of several features. For example, fdep contains lexicalized “in-between” features that depend on the head and modifier words as well as a word lying in between the two; in contrast, previous work has generally defined in-between features for POS tags only. As another example, our second-order mappings fsib and fgch define lexical trigram features, while previous work has generally used POS trigrams only.</bodyText>
<footnote confidence="0.9860375">
8As in previous work, English evaluation ignores any to-
ken whose gold-standard POS tag is one off- •• . , -I.
</footnote>
<page confidence="0.999462">
7
</page>
<bodyText confidence="0.999918264705882">Our third-order feature mappings fgsib and ftsib consist of four types of features. First, we define 4-gram features that characterize the four relevant indices using words and POS tags; examples include POS 4-grams and mixed 4-grams with one word and three POS tags. Second, we define 4gram context features consisting of POS 4-grams augmented with adjacent POS tags: for example, fgsib(x, g, h, m, s) includes POS 7-grams for the tags at positions (g, h, m, s, g+1, h+1, m+1). Third, we define backed-offfeatures that track bigram and trigram interactions which are absent in the lower-order feature mappings: for example, ftsib(x, h, m, s, t) contains features predicated on the trigram (m, s, t) and the bigram (m, t), neither of which exist in any lower-order part. Fourth, noting that coordinations are typically annotated as grand-siblings (e.g., “report purchases and sales” in Figure 1), we define coordination features for certain grand-sibling parts. For example, fgsib(x, g, h, m, s) contains features examining the implicit head-modifier relationship (g, m) that are only activated when the POS tag of s is a coordinating conjunction. Finally, we make two brief remarks regarding the use of POS tags. First, we assume that input sentences have been automatically tagged in a preprocessing step.9 Second, for any feature that depends on POS tags, we include two copies of the feature: one using normal POS tags and another using coarsened versions10 of the POS tags.</bodyText>
<subsectionHeader confidence="0.997901">
7.2 Averaged perceptron training
</subsectionHeader>
<bodyText confidence="0.999755777777778">There are a wide variety of parameter estimation methods for structured linear models, such as log-linear models (Lafferty et al., 2001) and max-margin models (Taskar et al., 2003). We chose the averaged structured perceptron (Freund and Schapire, 1999; Collins, 2002) as it combines highly competitive performance with fast training times, typically converging in 5–10 iterations. We train each parser for 10 iterations and select pa-</bodyText>
<footnote confidence="0.999571125">
9For Czech, the PDT provides automatic tags; for English,
we used MXPOST (Ratnaparkhi, 1996) to tag validation and
test data, with 10-fold cross-validation on the training set.
Note that the reliance on POS-tagged input can be relaxed
slightly by treating POS tags as word senses; see Section 5.3
and McDonald (2006, Table 6.1).
10For Czech, we used the first character of the tag; for En-
glish, we used the first two characters, except FRF and FRF$.
</footnote>
<table confidence="0.99892">
Beam Pass Orac Acc1 Acc2 Time1 Time2
0.0001 26.5 99.92 93.49 93.49 49.6m 73.5m
0.001 16.7 99.72 93.37 93.29 25.9m 24.2m
0.01 9.1 99.19 93.26 93.16 6.7m 7.9m
</table>
<tableCaption confidence="0.998411">
Table 1: Effect of the marginal-probability beam
on English parsing.</tableCaption>
<bodyText confidence="0.9992596">For each beam value, parsers were trained on the English training set and evaluated on the English validation set; the same beam value was applied to both training and validation data. Pass = %dependencies surviving the beam in training data, Orac = maximum achievable UAS on validation data, Acc1/Acc2 = UAS of Models 1/2 on validation data, and Time1/Time2 = minutes per perceptron training iteration for Models 1/2, averaged over all 10 iterations. For perspective, the English training set has a total of 39,832 sentences and 950,028 words. A beam of 0.0001 was used in all experiments outside this table. rameters from the iteration that achieves the best score on the validation set.</bodyText>
<subsectionHeader confidence="0.993995">
7.3 Coarse-to-fine pruning
</subsectionHeader>
<bodyText confidence="0.999903416666667">In order to decrease training times, we follow Carreras et al. (2008) and eliminate unlikely dependencies using a form of coarse-to-fine pruning (Charniak and Johnson, 2005; Petrov and Klein, 2007). In brief, we train a log-linear first-order parser11 and for every sentence x in training, validation, and test data we compute the marginal probability P(h, m  |x) of each dependency. Our parsers are then modified to ignore any dependency (h, m) whose marginal probability is below 0.0001xmaxh' P(h', m  |x). Table 1 provides information on the behavior of the pruning method.</bodyText>
<subsectionHeader confidence="0.998943">
7.4 Main results
</subsectionHeader>
<bodyText confidence="0.999739">Table 2 lists the accuracy of Models 1 and 2 on the English and Czech test sets, together with some relevant results from related work.12 The models marked “†” are not directly comparable to our work as they depend on additional sources of information that our models are trained without— unlabeled data in the case of Koo et al. (2008) and</bodyText>
<footnote confidence="0.577006">
11For English, we generate marginals using a projective
parser (Baker, 1979; Eisner, 2000); for Czech, we generate
marginals using a non-projective parser (Smith and Smith,
2007; McDonald and Satta, 2007; Koo et al., 2007). Param-
eters for these models are obtained by running exponentiated
gradient training for 10 iterations (Collins et al., 2008).
12Model 0 was not tested as its factorization is a strict sub-
set of the factorization of Model 1.
</footnote>
<page confidence="0.990283">
8
</page>
<note confidence="0.742106888888889">
Parser Eng Cze
McDonald et al. (2005a,2005b) 90.9 84.4
McDonald and Pereira (2006) 91.5 85.2
Koo et al. (2008), standard 92.02 86.13
Model 1 93.04 87.38
Model 2 92.93 87.37
Koo et al. (2008), semi-sup† 93.16 87.13
Suzuki et al. (2009)† 93.79 88.05
Carreras et al. (2008)† 93.5
</note>
<tableCaption confidence="0.9286">
Table 2: UAS of Models 1 and 2 on test data, with
relevant results from related work.</tableCaption>
<bodyText confidence="0.919388928571428">Note that Koo et al. (2008) is listed with standard features and semi-supervised features.†: see main text.Suzuki et al. (2009) and phrase-structure annotations in the case of Carreras et al.(2008). All three of the “†” models are based on versions of the Carreras (2007) parser, so modifying these methods to work with our new third-order parsing algorithms would be an interesting topic for future research. For example, Models 1 and 2 obtain results comparable to the semi-supervised parsers of Koo et al. (2008), and additive gains might be realized by applying their cluster-based feature sets to our enriched factorizations.</bodyText>
<subsectionHeader confidence="0.998208">
7.5 Ablation studies
</subsectionHeader>
<bodyText confidence="0.999996375">In order to better understand the contributions of the various feature types, we ran additional ablation experiments; the results are listed in Table 3, in addition to the scores of Model 0 and the emulated Carreras (2007) parser (see Section 4.3). Interestingly, grandchild interactions appear to provide important information: for example, when Model 2 is used without grandchild-based features (“Model 2, no-G” in Table 3), its accuracy suffers noticeably. In addition, it seems that grandchild interactions are particularly useful in Czech, while sibling interactions are less important: consider that Model 0, a second-order grandchild parser with no sibling-based features, can easily outperform “Model 2, no-G,” a third-order sibling parser with no grandchild-based features.</bodyText>
<sectionHeader confidence="0.999061" genericHeader="conclusion">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.998932166666667">We have presented new parsing algorithms that are capable of efficiently parsing third-order factorizations, including both grandchild and sibling interactions. Due to space restrictions, we have been necessarily brief at some points in this paper; some additional details can be found in Koo (2010).</bodyText>
<table confidence="0.998983625">
Parser Eng Cze
Model 0 93.07 87.39
Carreras (2007) emulation 93.14 87.25
Model 1 93.49 87.64
Model 1, no-3rd 93.17 87.57
Model 2 93.49 87.46
Model 2, no-3rd 93.20 87.43
Model 2, no-G 92.92 86.76
</table>
<tableCaption confidence="0.974392">
Table 3: UAS for modified versions of our parsers
on validation data.</tableCaption>
<bodyText confidence="0.996671517241379">The term no-3rd indicates a parser that was trained and tested with the thirdorder feature mappings fgsib and ftsib deactivated, though lower-order features were retained; note that “Model 2, no-3rd” is not identical to the Carreras (2007) parser as it defines grandchild parts for the pair of grandchildren. The term no-G indicates a parser that was trained and tested with the grandchild-based feature mappings fgch and fgsib deactivated; note that “Model 2, no-G” emulates the third-order sibling parser proposed by McDonald and Pereira (2006). There are several possibilities for further research involving our third-order parsing algorithms. One idea would be to consider extensions and modifications of our parsers, some of which have been suggested in Sections 5 and 7.4. A second area for future work lies in applications of dependency parsing. While we have evaluated our new algorithms on standard parsing benchmarks, there are a wide variety of tasks that may benefit from the extended context offered by our thirdorder factorizations; for example, the 4-gram substructures enabled by our approach may be useful for dependency-based language modeling in machine translation (Shen et al., 2008). Finally, in the hopes that others in the NLP community may find our parsers useful, we provide a free distribution of our implementation.2</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9996165">We would like to thank the anonymous reviewers for their helpful comments and suggestions. We also thank Regina Barzilay and Alexander Rush for their much-appreciated input during the writing process. The authors gratefully acknowledge the following sources of support: Terry Koo and Michael Collins were both funded by a DARPA subcontract under SRI (#27-001343), and Michael Collins was additionally supported by NTT (Agmt. dtd. 06/21/98).</bodyText>
<page confidence="0.997138">
9
</page>
<sectionHeader confidence="0.996345" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999903392156863">
Giuseppe Attardi. 2006. Experiments with a Multilan-
guage Non-Projective Dependency Parser. In Pro-
ceedings of the 10th CoNLL, pages 166–170. Asso-
ciation for Computational Linguistics.
James Baker. 1979. Trainable Grammars for Speech
Recognition. In Proceedings of the 97th meeting of
the Acoustical Society of America.
Xavier Carreras, Michael Collins, and Terry Koo.
2008. TAG, Dynamic Programming, and the Per-
ceptron for Efficient, Feature-rich Parsing. In Pro-
ceedings of the 12th CoNLL, pages 9–16. Associa-
tion for Computational Linguistics.
Xavier Carreras. 2007. Experiments with a Higher-
Order Projective Dependency Parser. In Proceed-
ings of the CoNLL Shared Task Session of EMNLP-
CoNLL, pages 957–961. Association for Computa-
tional Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine N-best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of the 43rd ACL.
Y.J. Chu and T.H. Liu. 1965. On the Shortest Ar-
borescence of a Directed Graph. Science Sinica,
14:1396–1400.
John Cocke and Jacob T. Schwartz. 1970. Program-
ming Languages and Their Compilers: Preliminary
Notes. Technical report, New York University.
Michael Collins, Amir Globerson, Terry Koo, Xavier
Carreras, and Peter L. Bartlett. 2008. Exponenti-
ated Gradient Algorithms for Conditional Random
Fields and Max-Margin Markov Networks. Journal
of Machine Learning Research, 9:1775–1822, Aug.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Exper-
iments with Perceptron Algorithms. In Proceedings
of the 7th EMNLP, pages 1–8. Association for Com-
putational Linguistics.
Jack R. Edmonds. 1967. Optimum Branchings. Jour-
nal of Research of the National Bureau of Standards,
71B:233–240.
Jason Eisner. 1996. Three New Probabilistic Models
for Dependency Parsing: An Exploration. In Pro-
ceedings of the 16th COLING, pages 340–345. As-
sociation for Computational Linguistics.
Jason Eisner. 2000. Bilexical Grammars and Their
Cubic-Time Parsing Algorithms. In Harry Bunt
and Anton Nijholt, editors, Advances in Probabilis-
tic and Other Parsing Technologies, pages 29–62.
Kluwer Academic Publishers.
Yoav Freund and Robert E. Schapire. 1999. Large
Margin Classification Using the Perceptron Algo-
rithm. Machine Learning, 37(3):277–296.
Jan Hajiˇc, Eva Hajiˇcov´a, Petr Pajas, Jarmila Panevova,
and Petr Sgall. 2001. The Prague Dependency Tree-
bank 1.0, LDC No. LDC2001T10. Linguistics Data
Consortium.
Jan Hajiˇc. 1998. Building a Syntactically Annotated
Corpus: The Prague Dependency Treebank. In Eva
Hajiˇcov´a, editor, Issues of Valency and Meaning.
Studies in Honor of Jarmila Panevov´a, pages 12–19.
Mark Johnson. 1998. PCFG Models of Linguistic
Tree Representations. Computational Linguistics,
24(4):613–632.
Tadao Kasami. 1965. An Efficient Recognition and
Syntax-analysis Algorithm for Context-free Lan-
guages. Technical Report AFCRL-65-758, Air
Force Cambridge Research Lab.
Terry Koo, Amir Globerson, Xavier Carreras, and
Michael Collins. 2007. Structured Prediction Mod-
els via the Matrix-Tree Theorem. In Proceedings
of EMNLP-CoNLL, pages 141–150. Association for
Computational Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple Semi-supervised Dependency Pars-
ing. In Proceedings of the 46th ACL, pages 595–603.
Association for Computational Linguistics.
Terry Koo. 2010. Advances in Discriminative Depen-
dency Parsing. Ph.D. thesis, Massachusetts Institute
of Technology, Cambridge, MA, USA, June.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In Proceedings of the 18th ICML,
pages 282–289. Morgan Kaufmann.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.
David A. McAllester. 1999. On the Complexity
Analysis of Static Analyses. In Proceedings of
the 6th Static Analysis Symposium, pages 312–329.
Springer-Verlag.
Ryan McDonald and Joakim Nivre. 2007. Character-
izing the Errors of Data-Driven Dependency Parsers.
In Proceedings of EMNLP-CoNLL, pages 122–131.
Association for Computational Linguistics.
Ryan McDonald and Fernando Pereira. 2006. Online
Learning of Approximate Dependency Parsing Al-
gorithms. In Proceedings of the 11th EACL, pages
81–88. Association for Computational Linguistics.
Ryan McDonald and Giorgio Satta. 2007. On the
Complexity of Non-Projective Data-Driven Depen-
dency Parsing. In Proceedings of IWPT.
</reference>
<page confidence="0.949278">
10
</page>
<reference confidence="0.999878185185185">
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online Large-Margin Training of
Dependency Parsers. In Proceedings of the 43rd
ACL, pages 91–98. Association for Computational
Linguistics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc. 2005b. Non-Projective Dependency
Parsing using Spanning Tree Algorithms. In Pro-
ceedings of HLT-EMNLP, pages 523–530. Associa-
tion for Computational Linguistics.
Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania, Philadel-
phia, PA, USA, July.
Joakim Nivre, Johan Hall, Jens Nilsson, G¨uls¸en
Eryiˇgit, and Svetoslav Marinov. 2006. Labeled
Pseudo-Projective Dependency Parsing with Sup-
port Vector Machines. In Proceedings of the 10th
CoNLL, pages 221–225. Association for Computa-
tional Linguistics.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Proceedings of HLT-
NAACL, pages 404–411. Association for Computa-
tional Linguistics.
Adwait Ratnaparkhi. 1996. A Maximum Entropy
Model for Part-Of-Speech Tagging. In Proceedings
of the 1st EMNLP, pages 133–142. Association for
Computational Linguistics.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A New String-to-Dependency Machine Translation
Algorithm with a Target Dependency Language
Model. In Proceedings of the 46th ACL, pages 577–
585. Association for Computational Linguistics.
David A. Smith and Noah A. Smith. 2007. Proba-
bilistic Models of Nonprojective Dependency Trees.
In Proceedings of EMNLP-CoNLL, pages 132–140.
Association for Computational Linguistics.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and
Michael Collins. 2009. An Empirical Study of
Semi-supervised Structured Conditional Models for
Dependency Parsing. In Proceedings of EMNLP,
pages 551–560. Association for Computational Lin-
guistics.
Ben Taskar, Carlos Guestrin, and Daphne Koller. 2003.
Max margin markov networks. In Sebastian Thrun,
Lawrence K. Saul, and Bernhard Sch¨olkopf, editors,
NIPS. MIT Press.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical Dependency Analysis with Support Vector Ma-
chines. In Proceedings of the 8th IWPT, pages 195–
206. Association for Computational Linguistics.
David H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189–208.
</reference>
<page confidence="0.999486">
11
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant confidence="0.514752" no="0">
<title confidence="0.999897">Efficient Third-order Dependency Parsers</title>
<author confidence="0.999138">Terry Koo</author>
<author confidence="0.999138">Michael Collins</author>
<address confidence="0.557512">MIT CSAIL, Cambridge, MA, 02139, USA</address>
<abstract confidence="0.993427692307692">We present algorithms for higher-order dependency parsing that are “third-order” in the sense that they can evaluate substructures containing three dependencies, and “efficient” in the sense that they reonly Importantly, our new parsers can utilize both sibling-style and grandchild-style interactions. We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Giuseppe Attardi</author>
</authors>
<title>Experiments with a Multilanguage Non-Projective Dependency Parser.</title>
<date>2006</date>
<booktitle>In Proceedings of the 10th CoNLL,</booktitle>
<pages>166--170</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context citStr="Attardi, 2006" endWordPosition="4273" position="25732" startWordPosition="4272">en m is the outermost modifier of h in some direction. Note that Models 1 and 2 have the same complexity as Carreras (2007), but strictly greater expressiveness: for each sibling or grandchild part used in the Carreras (2007) factorization, Model 1 defines an enclosing grand-sibling, while Model 2 defines an enclosing tri-sibling or grand-sibling. The factored parsing approach we focus on is sometimes referred to as “graph-based” parsing; a popular alternative is “transition-based” parsing, in which trees are constructed by making a series of incremental decisions (Yamada and Matsumoto, 2003; Attardi, 2006; Nivre et al., 2006; McDonald and Nivre, 2007). Transition-based parsers do not impose factorizations, so they can define arbitrary features on the tree as it is being built. As a result, however, they rely on greedy or approximate search algorithms to solve Eq. 1. 7 Parsing experiments In order to evaluate the effectiveness of our parsers in practice, we apply them to the Penn WSJ Treebank (Marcus et al., 1993) and the Prague Dependency Treebank (Hajiˇc et al., 2001; Hajiˇc, 1998).6 We use standard training, validation, and test splits7 to facilitate comparisons. Accuracy is 6For English, we</context>
</contexts>
<marker>Attardi, 2006</marker>
<rawString>Giuseppe Attardi. 2006. Experiments with a Multilanguage Non-Projective Dependency Parser. In Proceedings of the 10th CoNLL, pages 166–170. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Baker</author>
</authors>
<title>Trainable Grammars for Speech Recognition.</title>
<date>1979</date>
<booktitle>In Proceedings of the 97th meeting of the Acoustical</booktitle>
<publisher>Society of America.</publisher>
<contexts>
<context citStr="Baker, 1979" endWordPosition="3474" position="20751" startWordPosition="3473">irected spanning tree (Chu and Liu, 1965; Edmonds, 1967; McDonald et al., 2005b). Unfortunately, designing efficient higherorder non-projective parsers is likely to be challenging, based on recent hardness results (McDonald and Pereira, 2006; McDonald and Satta, 2007). 5 Extensions We briefly outline a few extensions to our algorithms; we hope to explore these in future work. 5.1 Probabilistic inference Many statistical modeling techniques are based on partition functions and marginals—summations over the set of possible trees y(x). Straightforward adaptations of the inside-outside algorithm (Baker, 1979) to our dynamic-programming structures would suffice to compute these quantities. 5.2 Labeled parsing Our parsers are easily extended to labeled dependencies. Direct integration of labels into Models 1 and 2 would result in third-order parts composed of three labeled dependencies, at the cost of increasing the time and space complexities by factors of O(L3) and O(L2), respectively, where L bounds the number of labels per dependency. 5.3 Word senses If each word in x has a set of possible “senses,” our parsers can be modified to recover the best joint assignment of syntax and senses for x, by a</context>
<context citStr="Baker, 1979" endWordPosition="5490" position="33086" startWordPosition="5489">e then modified to ignore any dependency (h, m) whose marginal probability is below 0.0001xmaxh' P(h', m |x). Table 1 provides information on the behavior of the pruning method. 7.4 Main results Table 2 lists the accuracy of Models 1 and 2 on the English and Czech test sets, together with some relevant results from related work.12 The models marked “†” are not directly comparable to our work as they depend on additional sources of information that our models are trained without— unlabeled data in the case of Koo et al. (2008) and 11For English, we generate marginals using a projective parser (Baker, 1979; Eisner, 2000); for Czech, we generate marginals using a non-projective parser (Smith and Smith, 2007; McDonald and Satta, 2007; Koo et al., 2007). Parameters for these models are obtained by running exponentiated gradient training for 10 iterations (Collins et al., 2008). 12Model 0 was not tested as its factorization is a strict subset of the factorization of Model 1. 8 Parser Eng Cze McDonald et al. (2005a,2005b) 90.9 84.4 McDonald and Pereira (2006) 91.5 85.2 Koo et al. (2008), standard 92.02 86.13 Model 1 93.04 87.38 Model 2 92.93 87.37 Koo et al. (2008), semi-sup† 93.16 87.13 Suzuki et a</context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>James Baker. 1979. Trainable Grammars for Speech Recognition. In Proceedings of the 97th meeting of the Acoustical Society of America.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>TAG, Dynamic Programming, and the Perceptron for Efficient, Feature-rich Parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 12th CoNLL,</booktitle>
<pages>9--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context citStr="Carreras et al. (2008)" endWordPosition="5330" position="32147" startWordPosition="5327">and validation data. Pass = %dependencies surviving the beam in training data, Orac = maximum achievable UAS on validation data, Acc1/Acc2 = UAS of Models 1/2 on validation data, and Time1/Time2 = minutes per perceptron training iteration for Models 1/2, averaged over all 10 iterations. For perspective, the English training set has a total of 39,832 sentences and 950,028 words. A beam of 0.0001 was used in all experiments outside this table. rameters from the iteration that achieves the best score on the validation set. 7.3 Coarse-to-fine pruning In order to decrease training times, we follow Carreras et al. (2008) and eliminate unlikely dependencies using a form of coarse-to-fine pruning (Charniak and Johnson, 2005; Petrov and Klein, 2007). In brief, we train a log-linear first-order parser11 and for every sentence x in training, validation, and test data we compute the marginal probability P(h, m |x) of each dependency. Our parsers are then modified to ignore any dependency (h, m) whose marginal probability is below 0.0001xmaxh' P(h', m |x). Table 1 provides information on the behavior of the pruning method. 7.4 Main results Table 2 lists the accuracy of Models 1 and 2 on the English and Czech test se</context>
<context citStr="Carreras et al. (2008)" endWordPosition="5599" position="33731" startWordPosition="5596">ech, we generate marginals using a non-projective parser (Smith and Smith, 2007; McDonald and Satta, 2007; Koo et al., 2007). Parameters for these models are obtained by running exponentiated gradient training for 10 iterations (Collins et al., 2008). 12Model 0 was not tested as its factorization is a strict subset of the factorization of Model 1. 8 Parser Eng Cze McDonald et al. (2005a,2005b) 90.9 84.4 McDonald and Pereira (2006) 91.5 85.2 Koo et al. (2008), standard 92.02 86.13 Model 1 93.04 87.38 Model 2 92.93 87.37 Koo et al. (2008), semi-sup† 93.16 87.13 Suzuki et al. (2009)† 93.79 88.05 Carreras et al. (2008)† 93.5 Table 2: UAS of Models 1 and 2 on test data, with relevant results from related work. Note that Koo et al. (2008) is listed with standard features and semi-supervised features. †: see main text. Suzuki et al. (2009) and phrase-structure annotations in the case of Carreras et al. (2008). All three of the “†” models are based on versions of the Carreras (2007) parser, so modifying these methods to work with our new third-order parsing algorithms would be an interesting topic for future research. For example, Models 1 and 2 obtain results comparable to the semi-supervised parsers of Koo et</context>
</contexts>
<marker>Carreras, Collins, Koo, 2008</marker>
<rawString>Xavier Carreras, Michael Collins, and Terry Koo. 2008. TAG, Dynamic Programming, and the Perceptron for Efficient, Feature-rich Parsing. In Proceedings of the 12th CoNLL, pages 9–16. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
</authors>
<title>Experiments with a HigherOrder Projective Dependency Parser.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLPCoNLL,</booktitle>
<pages>957--961</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context citStr="Carreras, 2007" endWordPosition="123" position="862" startWordPosition="122">t they can evaluate substructures containing three dependencies, and “efficient” in the sense that they require only O(n4) time. Importantly, our new parsers can utilize both sibling-style and grandchild-style interactions. We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively. 1 Introduction Dependency grammar has proven to be a very useful syntactic formalism, due in no small part to the development of efficient parsing algorithms (Eisner, 2000; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007), which can be leveraged for a wide variety of learning methods, such as feature-rich discriminative models (Lafferty et al., 2001; Collins, 2002; Taskar et al., 2003). These parsing algorithms share an important characteristic: they factor dependency trees into sets of parts that have limited interactions. By exploiting the additional constraints arising from the factorization, maximizations or summations over the set of possible dependency trees can be performed efficiently and exactly. A crucial limitation of factored parsing algorithms is that the associated parts are typically quite small</context>
<context citStr="Carreras, 2007" endWordPosition="337" position="2228" startWordPosition="335">the size and variety of the parts used by the factorization.1 At the same time, the need for more expressive factorizations 1For examples of how performance varies with the degree of the parser’s factorization see, e.g., McDonald and Pereira (2006, Tables 1 and 2), Carreras (2007, Table 2), Koo et al. (2008, Tables 2 and 4), or Suzuki et al. (2009, Tables 3–6). must be balanced against any resulting increase in the computational cost of the parsing algorithm. Consequently, recent work in dependency parsing has been restricted to applications of secondorder parsers, the most powerful of which (Carreras, 2007) requires O(n4) time and O(n3) space, while being limited to second-order parts. In this paper, we present new third-order parsing algorithms that increase both the size and variety of the parts participating in the factorization, while simultaneously maintaining computational requirements of O(n4) time and O(n3) space. We evaluate our parsers on the Penn WSJ Treebank (Marcus et al., 1993) and Prague Dependency Treebank (Hajiˇc et al., 2001), achieving unlabeled attachment scores of 93.04% and 87.38%. In summary, we make three main contributions: 1. Efficient new third-order parsing algorithms</context>
<context citStr="Carreras (2007)" endWordPosition="1824" position="11229" startWordPosition="1823">s a triple of indices (g, h, m) where (g, h) and (h, m) are dependencies.3 In order to parse this factorization, we augment both complete and incomplete spans with grandparent indices; for brevity, we refer to these augmented structures as g-spans. Formally, we denote a complete g-span as Ch,e, where Ch,e is a normal complete span and g is an index lying outside the range [h, e], with the implication that (g, h) is a dependency. Incomplete g-spans are defined analogously and are denoted as Ih,.. Figure 4 depicts complete and incomplete gspans and provides a graphical specification of the 3The Carreras (2007) parser also uses grandchild parts but only in restricted cases; see Section 6 for details. h e h m m e = + s m s r r+1 m = + = + g h m g h r h r+1 m (b) h e g h m g h m e (d) = + (c) = + (a) 3 OPTIMIZEALLSPANS(x) 1. ∀ g, i Cgi,i = 0 / base case 2. for w = 1... (n − 1) / span width 3. for i = 1... (n − w) / span start index 4. j = i + w / span end index 5. for g &lt; i or g &gt; j / grandparent index 6. Igi,j = maxi&lt;r&lt;j {Cg i,r + Ci j,r+1} + SCOREG(x, g, i, j) 7. Igj,i = maxi&lt;r&lt;j {Cgj,r+1 + Cji,r} + SCOREG(x, g, j, i) 8. Cgi,j = maxi&lt;m&lt;j {Igi,m + Cim } ,j 9. Cgj,i = maxi&lt;m&lt;j {Ig j,m + Cj m,i} 10. en</context>
<context citStr="Carreras (2007)" endWordPosition="2982" position="17751" startWordPosition="2981">gating grandparent indices, while allowing the recovery of tri-sibling parts (h, m, s, t). Figure 7(b) shows how an incomplete s-span can be converted into an incomplete g-span by exchanging the internal sibling index for an external grandparent index; in the process, grand-sibling parts (g, h, m, s) are enumerated. Since every derivation is defined by an augmented span and a split point, Model 2 can be parsed in O(n4) time and O(n3) space. It should be noted that unlike Model 1, Model 2 produces grand-sibling parts only for the outermost pair of grandchildren,4 similar to the behavior of the Carreras (2007) parser. In fact, the resemblance is more than passing, as Model 2 can emulate the Carreras (2007) algorithm by “demoting” each third-order part into a second-order part: SCOREGS(x, g, h, m, s) = SCOREG(x, g, h, m) SCORETS(x, h, m, s, t) = SCORES(x, h, m, s) where SCOREG, SCORES, SCOREGS and SCORETS are the scoring functions for grandchildren, siblings, grand-siblings and tri-siblings, respectively. The emulated version has the same computational complexity as the original, so there is no practical reason to prefer it over the original. Nevertheless, the relationship illustrated above highligh</context>
<context citStr="Carreras (2007)" endWordPosition="3232" position="19300" startWordPosition="3231"> thirdorder parsing algorithms are “optimally” efficient in an asymptotic sense. Since each third-order part is composed of four separate indices, there are 0(n4) distinct parts. Any third-order parsing algorithm must at least consider the score of each part, hence third-order parsing is Q(n4) and it follows that the asymptotic complexity of Models 1 and 2 cannot be improved. The key to the efficiency of our approach is a fundamental asymmetry in the structure of a directed tree: a head can have any number of modifiers, while a modifier always has exactly one head. Factorizations like that of Carreras (2007) obtain grandchild parts by augmenting spans with the indices of modifiers, leading to limitations on 4The reason for the restriction is that in Model 2, grandsiblings can only be derived via Figure 7(b), which does not recursively copy the grandparent index for reuse in smaller g-spans as Model 1 does in Figure 6(b). = + (b) g h m h s m h s m h t s h s m h s m h s r h r+1 m (d) + (c) = (a) 5 the grandchildren that can participate in the factorization. Our method, by “inverting” the modifier indices into grandparent indices, exploits the structural asymmetry. As a final note, the parsing algor</context>
<context citStr="Carreras (2007)" endWordPosition="4133" position="24837" startWordPosition="4132">lties similar to those already described for grammar transformations in CFGs. It should be noted that third-order parsers have previously been proposed by McDonald and Pereira (2006), who remarked that their secondorder sibling parser (see Figure 3) could easily be extended to capture m &gt; 1 successive modifiers in O(nm+1) time (ibid., Section 2.2). To our knowledge, however, Models 1 and 2 are the first third-order parsing algorithms capable of modeling grandchild parts. In our experiments, we find that grandchild interactions make important contributions to parsing performance (see Table 3). Carreras (2007) presents a second-order parser that can score both sibling and grandchild parts, with complexities of O(n4) time and O(n3) space. An important limitation of the parser’s factorization is that it only defines grandchild parts for outermost grandchildren: (g, h, m) is scored only when m is the outermost modifier of h in some direction. Note that Models 1 and 2 have the same complexity as Carreras (2007), but strictly greater expressiveness: for each sibling or grandchild part used in the Carreras (2007) factorization, Model 1 defines an enclosing grand-sibling, while Model 2 defines an enclosin</context>
<context citStr="Carreras, 2007" endWordPosition="4505" position="27191" startWordPosition="4504">t; for English, we split the Sections as: 2–21 training, 22 validation, 23 test. measured with unlabeled attachment score (UAS): the percentage of words with the correct head.8 7.1 Features for third-order parsing Our parsing algorithms can be applied to scores originating from any source, but in our experiments we chose to use the framework of structured linear models, deriving our scores as: SCOREPART(x, p) = w · f(x, p) Here, f is a feature-vector mapping and w is a vector of associated parameters. Following standard practice for higher-order dependency parsing (McDonald and Pereira, 2006; Carreras, 2007), Models 1 and 2 evaluate not only the relevant third-order parts, but also the lower-order parts that are implicit in their third-order factorizations. For example, Model 1 defines feature mappings for dependencies, siblings, grandchildren, and grand-siblings, so that the score of a dependency parse is given by: MODEL1SCORE(x, y) = 11 wdep · fdep(x, h, m) (h,m)Ey E wsib · fsib(x, h, m, s) (h,m,s)Ey E wgch · fgch(x, g, h, m) (g,h,m)Ey 11 wgsib · fgsib(x, g, h, m, s) (g,h,m,s)Ey Above, y is simultaneously decomposed into several different types of parts; trivial modifications to the Model 1 par</context>
<context citStr="Carreras (2007)" endWordPosition="5666" position="34098" startWordPosition="5664">d et al. (2005a,2005b) 90.9 84.4 McDonald and Pereira (2006) 91.5 85.2 Koo et al. (2008), standard 92.02 86.13 Model 1 93.04 87.38 Model 2 92.93 87.37 Koo et al. (2008), semi-sup† 93.16 87.13 Suzuki et al. (2009)† 93.79 88.05 Carreras et al. (2008)† 93.5 Table 2: UAS of Models 1 and 2 on test data, with relevant results from related work. Note that Koo et al. (2008) is listed with standard features and semi-supervised features. †: see main text. Suzuki et al. (2009) and phrase-structure annotations in the case of Carreras et al. (2008). All three of the “†” models are based on versions of the Carreras (2007) parser, so modifying these methods to work with our new third-order parsing algorithms would be an interesting topic for future research. For example, Models 1 and 2 obtain results comparable to the semi-supervised parsers of Koo et al. (2008), and additive gains might be realized by applying their cluster-based feature sets to our enriched factorizations. 7.5 Ablation studies In order to better understand the contributions of the various feature types, we ran additional ablation experiments; the results are listed in Table 3, in addition to the scores of Model 0 and the emulated Carreras (20</context>
<context citStr="Carreras (2007)" endWordPosition="5901" position="35626" startWordPosition="5900">ul in Czech, while sibling interactions are less important: consider that Model 0, a second-order grandchild parser with no sibling-based features, can easily outperform “Model 2, no-G,” a third-order sibling parser with no grandchild-based features. 8 Conclusion We have presented new parsing algorithms that are capable of efficiently parsing third-order factorizations, including both grandchild and sibling interactions. Due to space restrictions, we have been necessarily brief at some points in this paper; some additional details can be found in Koo (2010). Parser Eng Cze Model 0 93.07 87.39 Carreras (2007) emulation 93.14 87.25 Model 1 93.49 87.64 Model 1, no-3rd 93.17 87.57 Model 2 93.49 87.46 Model 2, no-3rd 93.20 87.43 Model 2, no-G 92.92 86.76 Table 3: UAS for modified versions of our parsers on validation data. The term no-3rd indicates a parser that was trained and tested with the thirdorder feature mappings fgsib and ftsib deactivated, though lower-order features were retained; note that “Model 2, no-3rd” is not identical to the Carreras (2007) parser as it defines grandchild parts for the pair of grandchildren. The term no-G indicates a parser that was trained and tested with the grandc</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>Xavier Carreras. 2007. Experiments with a HigherOrder Projective Dependency Parser. In Proceedings of the CoNLL Shared Task Session of EMNLPCoNLL, pages 957–961. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine N-best Parsing and MaxEnt Discriminative Reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd ACL.</booktitle>
<contexts>
<context citStr="Charniak and Johnson, 2005" endWordPosition="5345" position="32250" startWordPosition="5342">vable UAS on validation data, Acc1/Acc2 = UAS of Models 1/2 on validation data, and Time1/Time2 = minutes per perceptron training iteration for Models 1/2, averaged over all 10 iterations. For perspective, the English training set has a total of 39,832 sentences and 950,028 words. A beam of 0.0001 was used in all experiments outside this table. rameters from the iteration that achieves the best score on the validation set. 7.3 Coarse-to-fine pruning In order to decrease training times, we follow Carreras et al. (2008) and eliminate unlikely dependencies using a form of coarse-to-fine pruning (Charniak and Johnson, 2005; Petrov and Klein, 2007). In brief, we train a log-linear first-order parser11 and for every sentence x in training, validation, and test data we compute the marginal probability P(h, m |x) of each dependency. Our parsers are then modified to ignore any dependency (h, m) whose marginal probability is below 0.0001xmaxh' P(h', m |x). Table 1 provides information on the behavior of the pruning method. 7.4 Main results Table 2 lists the accuracy of Models 1 and 2 on the English and Czech test sets, together with some relevant results from related work.12 The models marked “†” are not directly com</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine N-best Parsing and MaxEnt Discriminative Reranking. In Proceedings of the 43rd ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y J Chu</author>
<author>T H Liu</author>
</authors>
<title>On the Shortest Arborescence of a Directed Graph. Science Sinica,</title>
<date>1965</date>
<pages>14--1396</pages>
<contexts>
<context citStr="Chu and Liu, 1965" endWordPosition="3391" position="20179" startWordPosition="3388"> reuse in smaller g-spans as Model 1 does in Figure 6(b). = + (b) g h m h s m h s m h t s h s m h s m h s r h r+1 m (d) + (c) = (a) 5 the grandchildren that can participate in the factorization. Our method, by “inverting” the modifier indices into grandparent indices, exploits the structural asymmetry. As a final note, the parsing algorithms described in this section fall into the category of projective dependency parsers, which forbid crossing dependencies. If crossing dependencies are allowed, it is possible to parse a first-order factorization by finding the maximum directed spanning tree (Chu and Liu, 1965; Edmonds, 1967; McDonald et al., 2005b). Unfortunately, designing efficient higherorder non-projective parsers is likely to be challenging, based on recent hardness results (McDonald and Pereira, 2006; McDonald and Satta, 2007). 5 Extensions We briefly outline a few extensions to our algorithms; we hope to explore these in future work. 5.1 Probabilistic inference Many statistical modeling techniques are based on partition functions and marginals—summations over the set of possible trees y(x). Straightforward adaptations of the inside-outside algorithm (Baker, 1979) to our dynamic-programming </context>
</contexts>
<marker>Chu, Liu, 1965</marker>
<rawString>Y.J. Chu and T.H. Liu. 1965. On the Shortest Arborescence of a Directed Graph. Science Sinica, 14:1396–1400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Cocke</author>
<author>Jacob T Schwartz</author>
</authors>
<title>Programming Languages and Their Compilers: Preliminary Notes.</title>
<date>1970</date>
<tech>Technical report,</tech>
<location>New York University.</location>
<contexts>
<context citStr="Cocke and Schwartz, 1970" endWordPosition="1284" position="8020" startWordPosition="1281">constituent. The point of concatenation in each construction—m in Figure 2(a) or r in Figure 2(b)—is the split point, a free index that must be enumerated to find the optimal construction. In order to parse a sentence x, it suffices to find optimal constructions for all complete and incomplete spans defined on x. This can be = + h e h m m e 2 h m h s s m Figure 3: The dynamic-programming structures and derivations of the second-order sibling parser; sibling spans are depicted as boxes. For brevity, we elide the right-headed versions. accomplished by adapting standard chart-parsing techniques (Cocke and Schwartz, 1970; Younger, 1967; Kasami, 1965) to the recursive derivations defined in Figure 2. Since each derivation is defined by two fixed indices (the boundaries of the span) and a third free index (the split point), the parsing algorithm requires O(n3) time and O(n2) space (Eisner, 1996; McAllester, 1999). 3.2 Second-order sibling factorization As remarked by Eisner (1996) and McDonald and Pereira (2006), it is possible to rearrange the dynamic-programming structures to conform to an improved factorization that decomposes each tree into sibling parts—pairs of dependencies with a shared head. Specificall</context>
</contexts>
<marker>Cocke, Schwartz, 1970</marker>
<rawString>John Cocke and Jacob T. Schwartz. 1970. Programming Languages and Their Compilers: Preliminary Notes. Technical report, New York University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Amir Globerson</author>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Peter L Bartlett</author>
</authors>
<title>Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1775</pages>
<contexts>
<context citStr="Collins et al., 2008" endWordPosition="5532" position="33359" startWordPosition="5529">h test sets, together with some relevant results from related work.12 The models marked “†” are not directly comparable to our work as they depend on additional sources of information that our models are trained without— unlabeled data in the case of Koo et al. (2008) and 11For English, we generate marginals using a projective parser (Baker, 1979; Eisner, 2000); for Czech, we generate marginals using a non-projective parser (Smith and Smith, 2007; McDonald and Satta, 2007; Koo et al., 2007). Parameters for these models are obtained by running exponentiated gradient training for 10 iterations (Collins et al., 2008). 12Model 0 was not tested as its factorization is a strict subset of the factorization of Model 1. 8 Parser Eng Cze McDonald et al. (2005a,2005b) 90.9 84.4 McDonald and Pereira (2006) 91.5 85.2 Koo et al. (2008), standard 92.02 86.13 Model 1 93.04 87.38 Model 2 92.93 87.37 Koo et al. (2008), semi-sup† 93.16 87.13 Suzuki et al. (2009)† 93.79 88.05 Carreras et al. (2008)† 93.5 Table 2: UAS of Models 1 and 2 on test data, with relevant results from related work. Note that Koo et al. (2008) is listed with standard features and semi-supervised features. †: see main text. Suzuki et al. (2009) and p</context>
</contexts>
<marker>Collins, Globerson, Koo, Carreras, Bartlett, 2008</marker>
<rawString>Michael Collins, Amir Globerson, Terry Koo, Xavier Carreras, and Peter L. Bartlett. 2008. Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks. Journal of Machine Learning Research, 9:1775–1822, Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 7th EMNLP,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context citStr="Collins, 2002" endWordPosition="146" position="1007" startWordPosition="145">ew parsers can utilize both sibling-style and grandchild-style interactions. We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively. 1 Introduction Dependency grammar has proven to be a very useful syntactic formalism, due in no small part to the development of efficient parsing algorithms (Eisner, 2000; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007), which can be leveraged for a wide variety of learning methods, such as feature-rich discriminative models (Lafferty et al., 2001; Collins, 2002; Taskar et al., 2003). These parsing algorithms share an important characteristic: they factor dependency trees into sets of parts that have limited interactions. By exploiting the additional constraints arising from the factorization, maximizations or summations over the set of possible dependency trees can be performed efficiently and exactly. A crucial limitation of factored parsing algorithms is that the associated parts are typically quite small, losing much of the contextual information within the dependency tree. For the purposes of improving parsing performance, it is desirable to inc</context>
<context citStr="Collins, 2002" endWordPosition="5060" position="30519" startWordPosition="5059">wo brief remarks regarding the use of POS tags. First, we assume that input sentences have been automatically tagged in a preprocessing step.9 Second, for any feature that depends on POS tags, we include two copies of the feature: one using normal POS tags and another using coarsened versions10 of the POS tags. 7.2 Averaged perceptron training There are a wide variety of parameter estimation methods for structured linear models, such as log-linear models (Lafferty et al., 2001) and max-margin models (Taskar et al., 2003). We chose the averaged structured perceptron (Freund and Schapire, 1999; Collins, 2002) as it combines highly competitive performance with fast training times, typically converging in 5–10 iterations. We train each parser for 10 iterations and select pa9For Czech, the PDT provides automatic tags; for English, we used MXPOST (Ratnaparkhi, 1996) to tag validation and test data, with 10-fold cross-validation on the training set. Note that the reliance on POS-tagged input can be relaxed slightly by treating POS tags as word senses; see Section 5.3 and McDonald (2006, Table 6.1). 10For Czech, we used the first character of the tag; for English, we used the first two characters, excep</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In Proceedings of the 7th EMNLP, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jack R Edmonds</author>
</authors>
<title>Optimum Branchings.</title>
<date>1967</date>
<journal>Journal of Research of the National Bureau of Standards,</journal>
<pages>71--233</pages>
<contexts>
<context citStr="Edmonds, 1967" endWordPosition="3393" position="20194" startWordPosition="3392">-spans as Model 1 does in Figure 6(b). = + (b) g h m h s m h s m h t s h s m h s m h s r h r+1 m (d) + (c) = (a) 5 the grandchildren that can participate in the factorization. Our method, by “inverting” the modifier indices into grandparent indices, exploits the structural asymmetry. As a final note, the parsing algorithms described in this section fall into the category of projective dependency parsers, which forbid crossing dependencies. If crossing dependencies are allowed, it is possible to parse a first-order factorization by finding the maximum directed spanning tree (Chu and Liu, 1965; Edmonds, 1967; McDonald et al., 2005b). Unfortunately, designing efficient higherorder non-projective parsers is likely to be challenging, based on recent hardness results (McDonald and Pereira, 2006; McDonald and Satta, 2007). 5 Extensions We briefly outline a few extensions to our algorithms; we hope to explore these in future work. 5.1 Probabilistic inference Many statistical modeling techniques are based on partition functions and marginals—summations over the set of possible trees y(x). Straightforward adaptations of the inside-outside algorithm (Baker, 1979) to our dynamic-programming structures woul</context>
</contexts>
<marker>Edmonds, 1967</marker>
<rawString>Jack R. Edmonds. 1967. Optimum Branchings. Journal of Research of the National Bureau of Standards, 71B:233–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Three New Probabilistic Models for Dependency Parsing: An Exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th COLING,</booktitle>
<pages>340--345</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context citStr="Eisner, 1996" endWordPosition="1330" position="8297" startWordPosition="1329">plete spans defined on x. This can be = + h e h m m e 2 h m h s s m Figure 3: The dynamic-programming structures and derivations of the second-order sibling parser; sibling spans are depicted as boxes. For brevity, we elide the right-headed versions. accomplished by adapting standard chart-parsing techniques (Cocke and Schwartz, 1970; Younger, 1967; Kasami, 1965) to the recursive derivations defined in Figure 2. Since each derivation is defined by two fixed indices (the boundaries of the span) and a third free index (the split point), the parsing algorithm requires O(n3) time and O(n2) space (Eisner, 1996; McAllester, 1999). 3.2 Second-order sibling factorization As remarked by Eisner (1996) and McDonald and Pereira (2006), it is possible to rearrange the dynamic-programming structures to conform to an improved factorization that decomposes each tree into sibling parts—pairs of dependencies with a shared head. Specifically, a sibling part consists of a triple of indices (h, m, s) where (h, m) and (h, s) are dependencies, and where s and m are successive modifiers to the same side of h. In order to parse this factorization, the secondorder parser introduces a third type of dynamicprogramming st</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason Eisner. 1996. Three New Probabilistic Models for Dependency Parsing: An Exploration. In Proceedings of the 16th COLING, pages 340–345. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Bilexical Grammars and Their Cubic-Time Parsing Algorithms.</title>
<date>2000</date>
<booktitle>Advances in Probabilistic and Other Parsing Technologies,</booktitle>
<pages>29--62</pages>
<editor>In Harry Bunt and Anton Nijholt, editors,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context citStr="Eisner, 2000" endWordPosition="113" position="793" startWordPosition="111">r-order dependency parsing that are “third-order” in the sense that they can evaluate substructures containing three dependencies, and “efficient” in the sense that they require only O(n4) time. Importantly, our new parsers can utilize both sibling-style and grandchild-style interactions. We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively. 1 Introduction Dependency grammar has proven to be a very useful syntactic formalism, due in no small part to the development of efficient parsing algorithms (Eisner, 2000; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007), which can be leveraged for a wide variety of learning methods, such as feature-rich discriminative models (Lafferty et al., 2001; Collins, 2002; Taskar et al., 2003). These parsing algorithms share an important characteristic: they factor dependency trees into sets of parts that have limited interactions. By exploiting the additional constraints arising from the factorization, maximizations or summations over the set of possible dependency trees can be performed efficiently and exactly. A crucial limitation of factored pars</context>
<context citStr="Eisner (2000)" endWordPosition="916" position="5812" startWordPosition="915">izations utilizing the following parts: h m h s m g h m dependency sibling grandchild g h s m h t s m grand-sibling tri-sibling Specifically, Sections 4.1, 4.2, and 4.3 describe parsers that, respectively, factor trees into grandchild parts, grand-sibling parts, and a mixture of grand-sibling and tri-sibling parts. 3 Existing parsing algorithms Our new third-order dependency parsers build on ideas from existing parsing algorithms. In this section, we provide background on two relevant parsers from previous work. h m h r r+1 m Figure 2: The dynamic-programming structures and derivations of the Eisner (2000) algorithm. Complete spans are depicted as triangles and incomplete spans as trapezoids. For brevity, we elide the symmetric right-headed versions. 3.1 First-order factorization The first type of parser we describe uses a “firstorder” factorization, which decomposes a dependency tree into its individual dependencies. Eisner (2000) introduced a widely-used dynamicprogramming algorithm for first-order parsing; as it is the basis for many parsers, including our new algorithms, we summarize its design here. The Eisner (2000) algorithm is based on two interrelated types of dynamic-programming struc</context>
<context citStr="Eisner (2000)" endWordPosition="3581" position="21383" startWordPosition="3580">gramming structures would suffice to compute these quantities. 5.2 Labeled parsing Our parsers are easily extended to labeled dependencies. Direct integration of labels into Models 1 and 2 would result in third-order parts composed of three labeled dependencies, at the cost of increasing the time and space complexities by factors of O(L3) and O(L2), respectively, where L bounds the number of labels per dependency. 5.3 Word senses If each word in x has a set of possible “senses,” our parsers can be modified to recover the best joint assignment of syntax and senses for x, by adapting methods in Eisner (2000). Complexity would increase by factors of O(54) time and O(53) space, where 5 bounds the number of senses per word. 5.4 Increased context If more vertical context is desired, the dynamicprogramming structures can be extended with additional ancestor indices, resulting in a “spine” of ancestors above each span. Each additional ancestor lengthens the vertical scope of the factorization (e.g., from grand-siblings to “great-grandsiblings”), while increasing complexity by a factor of O(n). Horizontal context can also be increased by adding internal sibling indices; each additional sibling widens th</context>
<context citStr="Eisner (2000)" endWordPosition="3800" position="22826" startWordPosition="3799">a manner superficially similar to parent annotation in CFGs (Johnson, 1998). However, parent annotation is a grammar transformation that is independent of any particular sentence, whereas our method annotates spans with indices into the current sentence. These indices allow the use of arbitrary features predicated on the position of the grandparent (e.g., word identity, POS tag, contextual POS tags) without affecting the asymptotic complexity of the parsing algorithm. Efficiently encoding this kind of information into a sentence-independent grammar transformation would be challenging at best. Eisner (2000) defines dependency parsing models where each word has a set of possible “senses” and the parser recovers the best joint assignment of syntax and senses. Our new parsing algorithms could be implemented by defining the “sense” of each word as the index of its head. However, when parsing with senses, the complexity of the Eisner (2000) parser increases by factors of O(53) time and O(52) space (ibid., Section 4.2). Since each word has n potential heads, a direct application of the word-sense parser leads to time and space complexities of O(n6) and O(n4), respectively, in contrast to our O(n4) and</context>
<context citStr="Eisner, 2000" endWordPosition="5492" position="33101" startWordPosition="5491">ed to ignore any dependency (h, m) whose marginal probability is below 0.0001xmaxh' P(h', m |x). Table 1 provides information on the behavior of the pruning method. 7.4 Main results Table 2 lists the accuracy of Models 1 and 2 on the English and Czech test sets, together with some relevant results from related work.12 The models marked “†” are not directly comparable to our work as they depend on additional sources of information that our models are trained without— unlabeled data in the case of Koo et al. (2008) and 11For English, we generate marginals using a projective parser (Baker, 1979; Eisner, 2000); for Czech, we generate marginals using a non-projective parser (Smith and Smith, 2007; McDonald and Satta, 2007; Koo et al., 2007). Parameters for these models are obtained by running exponentiated gradient training for 10 iterations (Collins et al., 2008). 12Model 0 was not tested as its factorization is a strict subset of the factorization of Model 1. 8 Parser Eng Cze McDonald et al. (2005a,2005b) 90.9 84.4 McDonald and Pereira (2006) 91.5 85.2 Koo et al. (2008), standard 92.02 86.13 Model 1 93.04 87.38 Model 2 92.93 87.37 Koo et al. (2008), semi-sup† 93.16 87.13 Suzuki et al. (2009)† 93.7</context>
</contexts>
<marker>Eisner, 2000</marker>
<rawString>Jason Eisner. 2000. Bilexical Grammars and Their Cubic-Time Parsing Algorithms. In Harry Bunt and Anton Nijholt, editors, Advances in Probabilistic and Other Parsing Technologies, pages 29–62. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Large Margin Classification Using the Perceptron Algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context citStr="Freund and Schapire, 1999" endWordPosition="5058" position="30503" startWordPosition="5055">unction. Finally, we make two brief remarks regarding the use of POS tags. First, we assume that input sentences have been automatically tagged in a preprocessing step.9 Second, for any feature that depends on POS tags, we include two copies of the feature: one using normal POS tags and another using coarsened versions10 of the POS tags. 7.2 Averaged perceptron training There are a wide variety of parameter estimation methods for structured linear models, such as log-linear models (Lafferty et al., 2001) and max-margin models (Taskar et al., 2003). We chose the averaged structured perceptron (Freund and Schapire, 1999; Collins, 2002) as it combines highly competitive performance with fast training times, typically converging in 5–10 iterations. We train each parser for 10 iterations and select pa9For Czech, the PDT provides automatic tags; for English, we used MXPOST (Ratnaparkhi, 1996) to tag validation and test data, with 10-fold cross-validation on the training set. Note that the reliance on POS-tagged input can be relaxed slightly by treating POS tags as word senses; see Section 5.3 and McDonald (2006, Table 6.1). 10For Czech, we used the first character of the tag; for English, we used the first two c</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1999. Large Margin Classification Using the Perceptron Algorithm. Machine Learning, 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Eva Hajiˇcov´a</author>
<author>Petr Pajas</author>
<author>Jarmila Panevova</author>
<author>Petr Sgall</author>
</authors>
<date>2001</date>
<booktitle>The Prague Dependency Treebank 1.0, LDC No. LDC2001T10. Linguistics Data Consortium.</booktitle>
<marker>Hajiˇc, Hajiˇcov´a, Pajas, Panevova, Sgall, 2001</marker>
<rawString>Jan Hajiˇc, Eva Hajiˇcov´a, Petr Pajas, Jarmila Panevova, and Petr Sgall. 2001. The Prague Dependency Treebank 1.0, LDC No. LDC2001T10. Linguistics Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
</authors>
<title>Building a Syntactically Annotated Corpus: The Prague Dependency Treebank.</title>
<date>1998</date>
<booktitle>Issues of Valency and Meaning. Studies in Honor of Jarmila Panevov´a,</booktitle>
<pages>12--19</pages>
<editor>In Eva Hajiˇcov´a, editor,</editor>
<marker>Hajiˇc, 1998</marker>
<rawString>Jan Hajiˇc. 1998. Building a Syntactically Annotated Corpus: The Prague Dependency Treebank. In Eva Hajiˇcov´a, editor, Issues of Valency and Meaning. Studies in Honor of Jarmila Panevov´a, pages 12–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFG Models of Linguistic Tree Representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context citStr="Johnson, 1998" endWordPosition="3721" position="22288" startWordPosition="3720">ors above each span. Each additional ancestor lengthens the vertical scope of the factorization (e.g., from grand-siblings to “great-grandsiblings”), while increasing complexity by a factor of O(n). Horizontal context can also be increased by adding internal sibling indices; each additional sibling widens the scope of the factorization (e.g., from grand-siblings to “grand-tri-siblings”), while increasing complexity by a factor of O(n). 6 Related work Our method augments each span with the index of the head that governs that span, in a manner superficially similar to parent annotation in CFGs (Johnson, 1998). However, parent annotation is a grammar transformation that is independent of any particular sentence, whereas our method annotates spans with indices into the current sentence. These indices allow the use of arbitrary features predicated on the position of the grandparent (e.g., word identity, POS tag, contextual POS tags) without affecting the asymptotic complexity of the parsing algorithm. Efficiently encoding this kind of information into a sentence-independent grammar transformation would be challenging at best. Eisner (2000) defines dependency parsing models where each word has a set o</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998. PCFG Models of Linguistic Tree Representations. Computational Linguistics, 24(4):613–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadao Kasami</author>
</authors>
<title>An Efficient Recognition and Syntax-analysis Algorithm for Context-free Languages.</title>
<date>1965</date>
<tech>Technical Report AFCRL-65-758,</tech>
<institution>Air Force Cambridge Research Lab.</institution>
<contexts>
<context citStr="Kasami, 1965" endWordPosition="1288" position="8050" startWordPosition="1287">n each construction—m in Figure 2(a) or r in Figure 2(b)—is the split point, a free index that must be enumerated to find the optimal construction. In order to parse a sentence x, it suffices to find optimal constructions for all complete and incomplete spans defined on x. This can be = + h e h m m e 2 h m h s s m Figure 3: The dynamic-programming structures and derivations of the second-order sibling parser; sibling spans are depicted as boxes. For brevity, we elide the right-headed versions. accomplished by adapting standard chart-parsing techniques (Cocke and Schwartz, 1970; Younger, 1967; Kasami, 1965) to the recursive derivations defined in Figure 2. Since each derivation is defined by two fixed indices (the boundaries of the span) and a third free index (the split point), the parsing algorithm requires O(n3) time and O(n2) space (Eisner, 1996; McAllester, 1999). 3.2 Second-order sibling factorization As remarked by Eisner (1996) and McDonald and Pereira (2006), it is possible to rearrange the dynamic-programming structures to conform to an improved factorization that decomposes each tree into sibling parts—pairs of dependencies with a shared head. Specifically, a sibling part consists of </context>
</contexts>
<marker>Kasami, 1965</marker>
<rawString>Tadao Kasami. 1965. An Efficient Recognition and Syntax-analysis Algorithm for Context-free Languages. Technical Report AFCRL-65-758, Air Force Cambridge Research Lab.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Amir Globerson</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Structured Prediction Models via the Matrix-Tree Theorem.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>141--150</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context citStr="Koo et al., 2007" endWordPosition="5513" position="33233" startWordPosition="5510"> the behavior of the pruning method. 7.4 Main results Table 2 lists the accuracy of Models 1 and 2 on the English and Czech test sets, together with some relevant results from related work.12 The models marked “†” are not directly comparable to our work as they depend on additional sources of information that our models are trained without— unlabeled data in the case of Koo et al. (2008) and 11For English, we generate marginals using a projective parser (Baker, 1979; Eisner, 2000); for Czech, we generate marginals using a non-projective parser (Smith and Smith, 2007; McDonald and Satta, 2007; Koo et al., 2007). Parameters for these models are obtained by running exponentiated gradient training for 10 iterations (Collins et al., 2008). 12Model 0 was not tested as its factorization is a strict subset of the factorization of Model 1. 8 Parser Eng Cze McDonald et al. (2005a,2005b) 90.9 84.4 McDonald and Pereira (2006) 91.5 85.2 Koo et al. (2008), standard 92.02 86.13 Model 1 93.04 87.38 Model 2 92.93 87.37 Koo et al. (2008), semi-sup† 93.16 87.13 Suzuki et al. (2009)† 93.79 88.05 Carreras et al. (2008)† 93.5 Table 2: UAS of Models 1 and 2 on test data, with relevant results from related work. Note that</context>
</contexts>
<marker>Koo, Globerson, Carreras, Collins, 2007</marker>
<rawString>Terry Koo, Amir Globerson, Xavier Carreras, and Michael Collins. 2007. Structured Prediction Models via the Matrix-Tree Theorem. In Proceedings of EMNLP-CoNLL, pages 141–150. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple Semi-supervised Dependency Parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th ACL,</booktitle>
<pages>595--603</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context citStr="Koo et al. (2008" endWordPosition="287" position="1921" startWordPosition="284">cy trees can be performed efficiently and exactly. A crucial limitation of factored parsing algorithms is that the associated parts are typically quite small, losing much of the contextual information within the dependency tree. For the purposes of improving parsing performance, it is desirable to increase the size and variety of the parts used by the factorization.1 At the same time, the need for more expressive factorizations 1For examples of how performance varies with the degree of the parser’s factorization see, e.g., McDonald and Pereira (2006, Tables 1 and 2), Carreras (2007, Table 2), Koo et al. (2008, Tables 2 and 4), or Suzuki et al. (2009, Tables 3–6). must be balanced against any resulting increase in the computational cost of the parsing algorithm. Consequently, recent work in dependency parsing has been restricted to applications of secondorder parsers, the most powerful of which (Carreras, 2007) requires O(n4) time and O(n3) space, while being limited to second-order parts. In this paper, we present new third-order parsing algorithms that increase both the size and variety of the parts participating in the factorization, while simultaneously maintaining computational requirements of</context>
<context citStr="Koo et al. (2008)" endWordPosition="5478" position="33006" startWordPosition="5475">data we compute the marginal probability P(h, m |x) of each dependency. Our parsers are then modified to ignore any dependency (h, m) whose marginal probability is below 0.0001xmaxh' P(h', m |x). Table 1 provides information on the behavior of the pruning method. 7.4 Main results Table 2 lists the accuracy of Models 1 and 2 on the English and Czech test sets, together with some relevant results from related work.12 The models marked “†” are not directly comparable to our work as they depend on additional sources of information that our models are trained without— unlabeled data in the case of Koo et al. (2008) and 11For English, we generate marginals using a projective parser (Baker, 1979; Eisner, 2000); for Czech, we generate marginals using a non-projective parser (Smith and Smith, 2007; McDonald and Satta, 2007; Koo et al., 2007). Parameters for these models are obtained by running exponentiated gradient training for 10 iterations (Collins et al., 2008). 12Model 0 was not tested as its factorization is a strict subset of the factorization of Model 1. 8 Parser Eng Cze McDonald et al. (2005a,2005b) 90.9 84.4 McDonald and Pereira (2006) 91.5 85.2 Koo et al. (2008), standard 92.02 86.13 Model 1 93.0</context>
<context citStr="Koo et al. (2008)" endWordPosition="5706" position="34342" startWordPosition="5703">(2008)† 93.5 Table 2: UAS of Models 1 and 2 on test data, with relevant results from related work. Note that Koo et al. (2008) is listed with standard features and semi-supervised features. †: see main text. Suzuki et al. (2009) and phrase-structure annotations in the case of Carreras et al. (2008). All three of the “†” models are based on versions of the Carreras (2007) parser, so modifying these methods to work with our new third-order parsing algorithms would be an interesting topic for future research. For example, Models 1 and 2 obtain results comparable to the semi-supervised parsers of Koo et al. (2008), and additive gains might be realized by applying their cluster-based feature sets to our enriched factorizations. 7.5 Ablation studies In order to better understand the contributions of the various feature types, we ran additional ablation experiments; the results are listed in Table 3, in addition to the scores of Model 0 and the emulated Carreras (2007) parser (see Section 4.3). Interestingly, grandchild interactions appear to provide important information: for example, when Model 2 is used without grandchild-based features (“Model 2, no-G” in Table 3), its accuracy suffers noticeably. In </context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple Semi-supervised Dependency Parsing. In Proceedings of the 46th ACL, pages 595–603. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
</authors>
<title>Advances in Discriminative Dependency Parsing.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>Massachusetts Institute of Technology,</institution>
<location>Cambridge, MA, USA,</location>
<contexts>
<context citStr="Koo (2010)" endWordPosition="5892" position="35574" startWordPosition="5891">t grandchild interactions are particularly useful in Czech, while sibling interactions are less important: consider that Model 0, a second-order grandchild parser with no sibling-based features, can easily outperform “Model 2, no-G,” a third-order sibling parser with no grandchild-based features. 8 Conclusion We have presented new parsing algorithms that are capable of efficiently parsing third-order factorizations, including both grandchild and sibling interactions. Due to space restrictions, we have been necessarily brief at some points in this paper; some additional details can be found in Koo (2010). Parser Eng Cze Model 0 93.07 87.39 Carreras (2007) emulation 93.14 87.25 Model 1 93.49 87.64 Model 1, no-3rd 93.17 87.57 Model 2 93.49 87.46 Model 2, no-3rd 93.20 87.43 Model 2, no-G 92.92 86.76 Table 3: UAS for modified versions of our parsers on validation data. The term no-3rd indicates a parser that was trained and tested with the thirdorder feature mappings fgsib and ftsib deactivated, though lower-order features were retained; note that “Model 2, no-3rd” is not identical to the Carreras (2007) parser as it defines grandchild parts for the pair of grandchildren. The term no-G indicates </context>
</contexts>
<marker>Koo, 2010</marker>
<rawString>Terry Koo. 2010. Advances in Discriminative Dependency Parsing. Ph.D. thesis, Massachusetts Institute of Technology, Cambridge, MA, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proceedings of the 18th ICML,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context citStr="Lafferty et al., 2001" endWordPosition="144" position="992" startWordPosition="140">ime. Importantly, our new parsers can utilize both sibling-style and grandchild-style interactions. We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively. 1 Introduction Dependency grammar has proven to be a very useful syntactic formalism, due in no small part to the development of efficient parsing algorithms (Eisner, 2000; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007), which can be leveraged for a wide variety of learning methods, such as feature-rich discriminative models (Lafferty et al., 2001; Collins, 2002; Taskar et al., 2003). These parsing algorithms share an important characteristic: they factor dependency trees into sets of parts that have limited interactions. By exploiting the additional constraints arising from the factorization, maximizations or summations over the set of possible dependency trees can be performed efficiently and exactly. A crucial limitation of factored parsing algorithms is that the associated parts are typically quite small, losing much of the contextual information within the dependency tree. For the purposes of improving parsing performance, it is d</context>
<context citStr="Lafferty et al., 2001" endWordPosition="5041" position="30387" startWordPosition="5038">e implicit head-modifier relationship (g, m) that are only activated when the POS tag of s is a coordinating conjunction. Finally, we make two brief remarks regarding the use of POS tags. First, we assume that input sentences have been automatically tagged in a preprocessing step.9 Second, for any feature that depends on POS tags, we include two copies of the feature: one using normal POS tags and another using coarsened versions10 of the POS tags. 7.2 Averaged perceptron training There are a wide variety of parameter estimation methods for structured linear models, such as log-linear models (Lafferty et al., 2001) and max-margin models (Taskar et al., 2003). We chose the averaged structured perceptron (Freund and Schapire, 1999; Collins, 2002) as it combines highly competitive performance with fast training times, typically converging in 5–10 iterations. We train each parser for 10 iterations and select pa9For Czech, the PDT provides automatic tags; for English, we used MXPOST (Ratnaparkhi, 1996) to tag validation and test data, with 10-fold cross-validation on the training set. Note that the reliance on POS-tagged input can be relaxed slightly by treating POS tags as word senses; see Section 5.3 and M</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedings of the 18th ICML, pages 282–289. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context citStr="Marcus et al., 1993" endWordPosition="398" position="2620" startWordPosition="395">t any resulting increase in the computational cost of the parsing algorithm. Consequently, recent work in dependency parsing has been restricted to applications of secondorder parsers, the most powerful of which (Carreras, 2007) requires O(n4) time and O(n3) space, while being limited to second-order parts. In this paper, we present new third-order parsing algorithms that increase both the size and variety of the parts participating in the factorization, while simultaneously maintaining computational requirements of O(n4) time and O(n3) space. We evaluate our parsers on the Penn WSJ Treebank (Marcus et al., 1993) and Prague Dependency Treebank (Hajiˇc et al., 2001), achieving unlabeled attachment scores of 93.04% and 87.38%. In summary, we make three main contributions: 1. Efficient new third-order parsing algorithms. 2. Empirical evaluations of these parsers. 3. A free distribution of our implementation.2 The remainder of this paper is divided as follows: Sections 2 and 3 give background, Sections 4 and 5 describe our new parsing algorithms, Section 6 discusses related work, Section 7 presents our experimental results, and Section 8 concludes. 2 Dependency parsing In dependency grammar, syntactic rel</context>
<context citStr="Marcus et al., 1993" endWordPosition="4344" position="26148" startWordPosition="4341">rred to as “graph-based” parsing; a popular alternative is “transition-based” parsing, in which trees are constructed by making a series of incremental decisions (Yamada and Matsumoto, 2003; Attardi, 2006; Nivre et al., 2006; McDonald and Nivre, 2007). Transition-based parsers do not impose factorizations, so they can define arbitrary features on the tree as it is being built. As a result, however, they rely on greedy or approximate search algorithms to solve Eq. 1. 7 Parsing experiments In order to evaluate the effectiveness of our parsers in practice, we apply them to the Penn WSJ Treebank (Marcus et al., 1993) and the Prague Dependency Treebank (Hajiˇc et al., 2001; Hajiˇc, 1998).6 We use standard training, validation, and test splits7 to facilitate comparisons. Accuracy is 6For English, we extracted dependencies using Joakim Nivre’s Penn2Malt tool with standard head rules (Yamada and Matsumoto, 2003); for Czech, we “projectivized” the training data by finding best-match projective trees. 7For Czech, the PDT has a predefined split; for English, we split the Sections as: 2–21 training, 22 validation, 23 test. measured with unlabeled attachment score (UAS): the percentage of words with the correct he</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A McAllester</author>
</authors>
<title>On the Complexity Analysis of Static Analyses.</title>
<date>1999</date>
<booktitle>In Proceedings of the 6th Static Analysis Symposium,</booktitle>
<pages>312--329</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context citStr="McAllester, 1999" endWordPosition="1332" position="8316" startWordPosition="1331">fined on x. This can be = + h e h m m e 2 h m h s s m Figure 3: The dynamic-programming structures and derivations of the second-order sibling parser; sibling spans are depicted as boxes. For brevity, we elide the right-headed versions. accomplished by adapting standard chart-parsing techniques (Cocke and Schwartz, 1970; Younger, 1967; Kasami, 1965) to the recursive derivations defined in Figure 2. Since each derivation is defined by two fixed indices (the boundaries of the span) and a third free index (the split point), the parsing algorithm requires O(n3) time and O(n2) space (Eisner, 1996; McAllester, 1999). 3.2 Second-order sibling factorization As remarked by Eisner (1996) and McDonald and Pereira (2006), it is possible to rearrange the dynamic-programming structures to conform to an improved factorization that decomposes each tree into sibling parts—pairs of dependencies with a shared head. Specifically, a sibling part consists of a triple of indices (h, m, s) where (h, m) and (h, s) are dependencies, and where s and m are successive modifiers to the same side of h. In order to parse this factorization, the secondorder parser introduces a third type of dynamicprogramming structure: sibling sp</context>
</contexts>
<marker>McAllester, 1999</marker>
<rawString>David A. McAllester. 1999. On the Complexity Analysis of Static Analyses. In Proceedings of the 6th Static Analysis Symposium, pages 312–329. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Characterizing the Errors of Data-Driven Dependency Parsers.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>122--131</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context citStr="McDonald and Nivre, 2007" endWordPosition="4281" position="25779" startWordPosition="4278"> in some direction. Note that Models 1 and 2 have the same complexity as Carreras (2007), but strictly greater expressiveness: for each sibling or grandchild part used in the Carreras (2007) factorization, Model 1 defines an enclosing grand-sibling, while Model 2 defines an enclosing tri-sibling or grand-sibling. The factored parsing approach we focus on is sometimes referred to as “graph-based” parsing; a popular alternative is “transition-based” parsing, in which trees are constructed by making a series of incremental decisions (Yamada and Matsumoto, 2003; Attardi, 2006; Nivre et al., 2006; McDonald and Nivre, 2007). Transition-based parsers do not impose factorizations, so they can define arbitrary features on the tree as it is being built. As a result, however, they rely on greedy or approximate search algorithms to solve Eq. 1. 7 Parsing experiments In order to evaluate the effectiveness of our parsers in practice, we apply them to the Penn WSJ Treebank (Marcus et al., 1993) and the Prague Dependency Treebank (Hajiˇc et al., 2001; Hajiˇc, 1998).6 We use standard training, validation, and test splits7 to facilitate comparisons. Accuracy is 6For English, we extracted dependencies using Joakim Nivre’s Pe</context>
</contexts>
<marker>McDonald, Nivre, 2007</marker>
<rawString>Ryan McDonald and Joakim Nivre. 2007. Characterizing the Errors of Data-Driven Dependency Parsers. In Proceedings of EMNLP-CoNLL, pages 122–131. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online Learning of Approximate Dependency Parsing Algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th EACL,</booktitle>
<pages>81--88</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context citStr="McDonald and Pereira, 2006" endWordPosition="121" position="845" startWordPosition="118">hird-order” in the sense that they can evaluate substructures containing three dependencies, and “efficient” in the sense that they require only O(n4) time. Importantly, our new parsers can utilize both sibling-style and grandchild-style interactions. We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively. 1 Introduction Dependency grammar has proven to be a very useful syntactic formalism, due in no small part to the development of efficient parsing algorithms (Eisner, 2000; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007), which can be leveraged for a wide variety of learning methods, such as feature-rich discriminative models (Lafferty et al., 2001; Collins, 2002; Taskar et al., 2003). These parsing algorithms share an important characteristic: they factor dependency trees into sets of parts that have limited interactions. By exploiting the additional constraints arising from the factorization, maximizations or summations over the set of possible dependency trees can be performed efficiently and exactly. A crucial limitation of factored parsing algorithms is that the associated parts are typi</context>
<context citStr="McDonald and Pereira (2006)" endWordPosition="1346" position="8417" startWordPosition="1343">uctures and derivations of the second-order sibling parser; sibling spans are depicted as boxes. For brevity, we elide the right-headed versions. accomplished by adapting standard chart-parsing techniques (Cocke and Schwartz, 1970; Younger, 1967; Kasami, 1965) to the recursive derivations defined in Figure 2. Since each derivation is defined by two fixed indices (the boundaries of the span) and a third free index (the split point), the parsing algorithm requires O(n3) time and O(n2) space (Eisner, 1996; McAllester, 1999). 3.2 Second-order sibling factorization As remarked by Eisner (1996) and McDonald and Pereira (2006), it is possible to rearrange the dynamic-programming structures to conform to an improved factorization that decomposes each tree into sibling parts—pairs of dependencies with a shared head. Specifically, a sibling part consists of a triple of indices (h, m, s) where (h, m) and (h, s) are dependencies, and where s and m are successive modifiers to the same side of h. In order to parse this factorization, the secondorder parser introduces a third type of dynamicprogramming structure: sibling spans, which represent the region between successive modifiers of some head. Formally, we denote a sibl</context>
<context citStr="McDonald and Pereira, 2006" endWordPosition="2708" position="16142" startWordPosition="2705">ure 7: The dynamic-programming structures and derivations of Model 2. Right-headed and right-grandparented versions are omitted. Like Model 0, Model 1 can be parsed via adaptations of standard chart-parsing techniques; we omit the details for brevity. Despite the move to third-order parts, each derivation is still defined by a g-span and a split point, so that parsing requires only O(n4) time and O(n3) space. 4.3 Model 2: grand-siblings and tri-siblings Higher-order parsing algorithms have been proposed which extend the second-order sibling factorization to parts containing multiple siblings (McDonald and Pereira, 2006, also see Section 6 for discussion). In this section, we show how our g-span-based techniques can be combined with a third-order sibling parser, resulting in a parser that captures both grand-sibling parts and tri-sibling parts—4-tuples of indices (h, m, s, t) such that both (h, m, s) and (h, s, t) are sibling parts. In order to parse this factorization, we introduce a new type of dynamic-programming structure: sibling-augmented spans, or s-spans. Formally, we denote an incomplete s-span as Ih,m,s where Ih,m is a normal incomplete span and s is an index lying in the strict interior of the ran</context>
<context citStr="McDonald and Pereira, 2006" endWordPosition="3420" position="20380" startWordPosition="3416">n. Our method, by “inverting” the modifier indices into grandparent indices, exploits the structural asymmetry. As a final note, the parsing algorithms described in this section fall into the category of projective dependency parsers, which forbid crossing dependencies. If crossing dependencies are allowed, it is possible to parse a first-order factorization by finding the maximum directed spanning tree (Chu and Liu, 1965; Edmonds, 1967; McDonald et al., 2005b). Unfortunately, designing efficient higherorder non-projective parsers is likely to be challenging, based on recent hardness results (McDonald and Pereira, 2006; McDonald and Satta, 2007). 5 Extensions We briefly outline a few extensions to our algorithms; we hope to explore these in future work. 5.1 Probabilistic inference Many statistical modeling techniques are based on partition functions and marginals—summations over the set of possible trees y(x). Straightforward adaptations of the inside-outside algorithm (Baker, 1979) to our dynamic-programming structures would suffice to compute these quantities. 5.2 Labeled parsing Our parsers are easily extended to labeled dependencies. Direct integration of labels into Models 1 and 2 would result in third</context>
<context citStr="McDonald and Pereira (2006)" endWordPosition="4064" position="24404" startWordPosition="4061">vocabulary (ibid., Section 2). The automata are thus analogous to the rules of a CFG and at5In brief, the reason for the inefficiency is that the wordsense parser is unable to exploit certain constraints, such as the fact that the endpoints of a sibling g-span must have the same head. The word-sense parser would needlessly enumerate all possible pairs of heads in this case. 6 tempts to use them to model grandparent indices would face difficulties similar to those already described for grammar transformations in CFGs. It should be noted that third-order parsers have previously been proposed by McDonald and Pereira (2006), who remarked that their secondorder sibling parser (see Figure 3) could easily be extended to capture m &gt; 1 successive modifiers in O(nm+1) time (ibid., Section 2.2). To our knowledge, however, Models 1 and 2 are the first third-order parsing algorithms capable of modeling grandchild parts. In our experiments, we find that grandchild interactions make important contributions to parsing performance (see Table 3). Carreras (2007) presents a second-order parser that can score both sibling and grandchild parts, with complexities of O(n4) time and O(n3) space. An important limitation of the parse</context>
<context citStr="McDonald and Pereira, 2006" endWordPosition="4503" position="27174" startWordPosition="4500">he PDT has a predefined split; for English, we split the Sections as: 2–21 training, 22 validation, 23 test. measured with unlabeled attachment score (UAS): the percentage of words with the correct head.8 7.1 Features for third-order parsing Our parsing algorithms can be applied to scores originating from any source, but in our experiments we chose to use the framework of structured linear models, deriving our scores as: SCOREPART(x, p) = w · f(x, p) Here, f is a feature-vector mapping and w is a vector of associated parameters. Following standard practice for higher-order dependency parsing (McDonald and Pereira, 2006; Carreras, 2007), Models 1 and 2 evaluate not only the relevant third-order parts, but also the lower-order parts that are implicit in their third-order factorizations. For example, Model 1 defines feature mappings for dependencies, siblings, grandchildren, and grand-siblings, so that the score of a dependency parse is given by: MODEL1SCORE(x, y) = 11 wdep · fdep(x, h, m) (h,m)Ey E wsib · fsib(x, h, m, s) (h,m,s)Ey E wgch · fgch(x, g, h, m) (g,h,m)Ey 11 wgsib · fgsib(x, g, h, m, s) (g,h,m,s)Ey Above, y is simultaneously decomposed into several different types of parts; trivial modifications t</context>
<context citStr="McDonald and Pereira (2006)" endWordPosition="5565" position="33543" startWordPosition="5562">ation that our models are trained without— unlabeled data in the case of Koo et al. (2008) and 11For English, we generate marginals using a projective parser (Baker, 1979; Eisner, 2000); for Czech, we generate marginals using a non-projective parser (Smith and Smith, 2007; McDonald and Satta, 2007; Koo et al., 2007). Parameters for these models are obtained by running exponentiated gradient training for 10 iterations (Collins et al., 2008). 12Model 0 was not tested as its factorization is a strict subset of the factorization of Model 1. 8 Parser Eng Cze McDonald et al. (2005a,2005b) 90.9 84.4 McDonald and Pereira (2006) 91.5 85.2 Koo et al. (2008), standard 92.02 86.13 Model 1 93.04 87.38 Model 2 92.93 87.37 Koo et al. (2008), semi-sup† 93.16 87.13 Suzuki et al. (2009)† 93.79 88.05 Carreras et al. (2008)† 93.5 Table 2: UAS of Models 1 and 2 on test data, with relevant results from related work. Note that Koo et al. (2008) is listed with standard features and semi-supervised features. †: see main text. Suzuki et al. (2009) and phrase-structure annotations in the case of Carreras et al. (2008). All three of the “†” models are based on versions of the Carreras (2007) parser, so modifying these methods to work w</context>
<context citStr="McDonald and Pereira (2006)" endWordPosition="6027" position="36387" startWordPosition="6023">2.92 86.76 Table 3: UAS for modified versions of our parsers on validation data. The term no-3rd indicates a parser that was trained and tested with the thirdorder feature mappings fgsib and ftsib deactivated, though lower-order features were retained; note that “Model 2, no-3rd” is not identical to the Carreras (2007) parser as it defines grandchild parts for the pair of grandchildren. The term no-G indicates a parser that was trained and tested with the grandchild-based feature mappings fgch and fgsib deactivated; note that “Model 2, no-G” emulates the third-order sibling parser proposed by McDonald and Pereira (2006). There are several possibilities for further research involving our third-order parsing algorithms. One idea would be to consider extensions and modifications of our parsers, some of which have been suggested in Sections 5 and 7.4. A second area for future work lies in applications of dependency parsing. While we have evaluated our new algorithms on standard parsing benchmarks, there are a wide variety of tasks that may benefit from the extended context offered by our thirdorder factorizations; for example, the 4-gram substructures enabled by our approach may be useful for dependency-based la</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online Learning of Approximate Dependency Parsing Algorithms. In Proceedings of the 11th EACL, pages 81–88. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Giorgio Satta</author>
</authors>
<title>On the Complexity of Non-Projective Data-Driven Dependency Parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of IWPT.</booktitle>
<contexts>
<context citStr="McDonald and Satta, 2007" endWordPosition="3424" position="20407" startWordPosition="3421">” the modifier indices into grandparent indices, exploits the structural asymmetry. As a final note, the parsing algorithms described in this section fall into the category of projective dependency parsers, which forbid crossing dependencies. If crossing dependencies are allowed, it is possible to parse a first-order factorization by finding the maximum directed spanning tree (Chu and Liu, 1965; Edmonds, 1967; McDonald et al., 2005b). Unfortunately, designing efficient higherorder non-projective parsers is likely to be challenging, based on recent hardness results (McDonald and Pereira, 2006; McDonald and Satta, 2007). 5 Extensions We briefly outline a few extensions to our algorithms; we hope to explore these in future work. 5.1 Probabilistic inference Many statistical modeling techniques are based on partition functions and marginals—summations over the set of possible trees y(x). Straightforward adaptations of the inside-outside algorithm (Baker, 1979) to our dynamic-programming structures would suffice to compute these quantities. 5.2 Labeled parsing Our parsers are easily extended to labeled dependencies. Direct integration of labels into Models 1 and 2 would result in third-order parts composed of th</context>
<context citStr="McDonald and Satta, 2007" endWordPosition="5509" position="33214" startWordPosition="5506"> 1 provides information on the behavior of the pruning method. 7.4 Main results Table 2 lists the accuracy of Models 1 and 2 on the English and Czech test sets, together with some relevant results from related work.12 The models marked “†” are not directly comparable to our work as they depend on additional sources of information that our models are trained without— unlabeled data in the case of Koo et al. (2008) and 11For English, we generate marginals using a projective parser (Baker, 1979; Eisner, 2000); for Czech, we generate marginals using a non-projective parser (Smith and Smith, 2007; McDonald and Satta, 2007; Koo et al., 2007). Parameters for these models are obtained by running exponentiated gradient training for 10 iterations (Collins et al., 2008). 12Model 0 was not tested as its factorization is a strict subset of the factorization of Model 1. 8 Parser Eng Cze McDonald et al. (2005a,2005b) 90.9 84.4 McDonald and Pereira (2006) 91.5 85.2 Koo et al. (2008), standard 92.02 86.13 Model 1 93.04 87.38 Model 2 92.93 87.37 Koo et al. (2008), semi-sup† 93.16 87.13 Suzuki et al. (2009)† 93.79 88.05 Carreras et al. (2008)† 93.5 Table 2: UAS of Models 1 and 2 on test data, with relevant results from rela</context>
</contexts>
<marker>McDonald, Satta, 2007</marker>
<rawString>Ryan McDonald and Giorgio Satta. 2007. On the Complexity of Non-Projective Data-Driven Dependency Parsing. In Proceedings of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online Large-Margin Training of Dependency Parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd ACL,</booktitle>
<pages>91--98</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context citStr="McDonald et al., 2005" endWordPosition="117" position="816" startWordPosition="114">ency parsing that are “third-order” in the sense that they can evaluate substructures containing three dependencies, and “efficient” in the sense that they require only O(n4) time. Importantly, our new parsers can utilize both sibling-style and grandchild-style interactions. We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively. 1 Introduction Dependency grammar has proven to be a very useful syntactic formalism, due in no small part to the development of efficient parsing algorithms (Eisner, 2000; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007), which can be leveraged for a wide variety of learning methods, such as feature-rich discriminative models (Lafferty et al., 2001; Collins, 2002; Taskar et al., 2003). These parsing algorithms share an important characteristic: they factor dependency trees into sets of parts that have limited interactions. By exploiting the additional constraints arising from the factorization, maximizations or summations over the set of possible dependency trees can be performed efficiently and exactly. A crucial limitation of factored parsing algorithms is that </context>
<context citStr="McDonald et al., 2005" endWordPosition="3397" position="20217" startWordPosition="3394"> 1 does in Figure 6(b). = + (b) g h m h s m h s m h t s h s m h s m h s r h r+1 m (d) + (c) = (a) 5 the grandchildren that can participate in the factorization. Our method, by “inverting” the modifier indices into grandparent indices, exploits the structural asymmetry. As a final note, the parsing algorithms described in this section fall into the category of projective dependency parsers, which forbid crossing dependencies. If crossing dependencies are allowed, it is possible to parse a first-order factorization by finding the maximum directed spanning tree (Chu and Liu, 1965; Edmonds, 1967; McDonald et al., 2005b). Unfortunately, designing efficient higherorder non-projective parsers is likely to be challenging, based on recent hardness results (McDonald and Pereira, 2006; McDonald and Satta, 2007). 5 Extensions We briefly outline a few extensions to our algorithms; we hope to explore these in future work. 5.1 Probabilistic inference Many statistical modeling techniques are based on partition functions and marginals—summations over the set of possible trees y(x). Straightforward adaptations of the inside-outside algorithm (Baker, 1979) to our dynamic-programming structures would suffice to compute th</context>
<context citStr="McDonald et al., 2005" endWordPosition="4665" position="28130" startWordPosition="4662">EL1SCORE(x, y) = 11 wdep · fdep(x, h, m) (h,m)Ey E wsib · fsib(x, h, m, s) (h,m,s)Ey E wgch · fgch(x, g, h, m) (g,h,m)Ey 11 wgsib · fgsib(x, g, h, m, s) (g,h,m,s)Ey Above, y is simultaneously decomposed into several different types of parts; trivial modifications to the Model 1 parser allow it to evaluate all of the necessary parts in an interleaved fashion. A similar treatment of Model 2 yields five feature mappings: the four above plus ftsib(x, h, m, s, t), which represents tri-sibling parts. The lower-order feature mappings fdep, fsib, and fgch are based on feature sets from previous work (McDonald et al., 2005a; McDonald and Pereira, 2006; Carreras, 2007), to which we added lexicalized versions of several features. For example, fdep contains lexicalized “in-between” features that depend on the head and modifier words as well as a word lying in between the two; in contrast, previous work has generally defined in-between features for POS tags only. As another example, our 8As in previous work, English evaluation ignores any token whose gold-standard POS tag is one off- •• . , -I. 7 second-order mappings fsib and fgch define lexical trigram features, while previous work has generally used POS trigrams</context>
<context citStr="McDonald et al. (2005" endWordPosition="5559" position="33497" startWordPosition="5556">y depend on additional sources of information that our models are trained without— unlabeled data in the case of Koo et al. (2008) and 11For English, we generate marginals using a projective parser (Baker, 1979; Eisner, 2000); for Czech, we generate marginals using a non-projective parser (Smith and Smith, 2007; McDonald and Satta, 2007; Koo et al., 2007). Parameters for these models are obtained by running exponentiated gradient training for 10 iterations (Collins et al., 2008). 12Model 0 was not tested as its factorization is a strict subset of the factorization of Model 1. 8 Parser Eng Cze McDonald et al. (2005a,2005b) 90.9 84.4 McDonald and Pereira (2006) 91.5 85.2 Koo et al. (2008), standard 92.02 86.13 Model 1 93.04 87.38 Model 2 92.93 87.37 Koo et al. (2008), semi-sup† 93.16 87.13 Suzuki et al. (2009)† 93.79 88.05 Carreras et al. (2008)† 93.5 Table 2: UAS of Models 1 and 2 on test data, with relevant results from related work. Note that Koo et al. (2008) is listed with standard features and semi-supervised features. †: see main text. Suzuki et al. (2009) and phrase-structure annotations in the case of Carreras et al. (2008). All three of the “†” models are based on versions of the Carreras (2007</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005a. Online Large-Margin Training of Dependency Parsers. In Proceedings of the 43rd ACL, pages 91–98. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-Projective Dependency Parsing using Spanning Tree Algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP,</booktitle>
<pages>523--530</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005b. Non-Projective Dependency Parsing using Spanning Tree Algorithms. In Proceedings of HLT-EMNLP, pages 523–530. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>Discriminative Training and Spanning Tree Algorithms for Dependency Parsing.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA, USA,</location>
<contexts>
<context citStr="McDonald (2006" endWordPosition="5136" position="31000" startWordPosition="5135">) and max-margin models (Taskar et al., 2003). We chose the averaged structured perceptron (Freund and Schapire, 1999; Collins, 2002) as it combines highly competitive performance with fast training times, typically converging in 5–10 iterations. We train each parser for 10 iterations and select pa9For Czech, the PDT provides automatic tags; for English, we used MXPOST (Ratnaparkhi, 1996) to tag validation and test data, with 10-fold cross-validation on the training set. Note that the reliance on POS-tagged input can be relaxed slightly by treating POS tags as word senses; see Section 5.3 and McDonald (2006, Table 6.1). 10For Czech, we used the first character of the tag; for English, we used the first two characters, except FRF and FRF$. Beam Pass Orac Acc1 Acc2 Time1 Time2 0.0001 26.5 99.92 93.49 93.49 49.6m 73.5m 0.001 16.7 99.72 93.37 93.29 25.9m 24.2m 0.01 9.1 99.19 93.26 93.16 6.7m 7.9m Table 1: Effect of the marginal-probability beam on English parsing. For each beam value, parsers were trained on the English training set and evaluated on the English validation set; the same beam value was applied to both training and validation data. Pass = %dependencies surviving the beam in training da</context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>Ryan McDonald. 2006. Discriminative Training and Spanning Tree Algorithms for Dependency Parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
<author>G¨uls¸en Eryiˇgit</author>
<author>Svetoslav Marinov</author>
</authors>
<title>Labeled Pseudo-Projective Dependency Parsing with Support Vector Machines.</title>
<date>2006</date>
<booktitle>In Proceedings of the 10th CoNLL,</booktitle>
<pages>221--225</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Nivre, Hall, Nilsson, Eryiˇgit, Marinov, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, G¨uls¸en Eryiˇgit, and Svetoslav Marinov. 2006. Labeled Pseudo-Projective Dependency Parsing with Support Vector Machines. In Proceedings of the 10th CoNLL, pages 221–225. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved Inference for Unlexicalized Parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of HLTNAACL,</booktitle>
<pages>404--411</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context citStr="Petrov and Klein, 2007" endWordPosition="5349" position="32275" startWordPosition="5346">, Acc1/Acc2 = UAS of Models 1/2 on validation data, and Time1/Time2 = minutes per perceptron training iteration for Models 1/2, averaged over all 10 iterations. For perspective, the English training set has a total of 39,832 sentences and 950,028 words. A beam of 0.0001 was used in all experiments outside this table. rameters from the iteration that achieves the best score on the validation set. 7.3 Coarse-to-fine pruning In order to decrease training times, we follow Carreras et al. (2008) and eliminate unlikely dependencies using a form of coarse-to-fine pruning (Charniak and Johnson, 2005; Petrov and Klein, 2007). In brief, we train a log-linear first-order parser11 and for every sentence x in training, validation, and test data we compute the marginal probability P(h, m |x) of each dependency. Our parsers are then modified to ignore any dependency (h, m) whose marginal probability is below 0.0001xmaxh' P(h', m |x). Table 1 provides information on the behavior of the pruning method. 7.4 Main results Table 2 lists the accuracy of Models 1 and 2 on the English and Czech test sets, together with some relevant results from related work.12 The models marked “†” are not directly comparable to our work as th</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved Inference for Unlexicalized Parsing. In Proceedings of HLTNAACL, pages 404–411. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Model for Part-Of-Speech Tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of the 1st EMNLP,</booktitle>
<pages>133--142</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context citStr="Ratnaparkhi, 1996" endWordPosition="5099" position="30777" startWordPosition="5098">gs and another using coarsened versions10 of the POS tags. 7.2 Averaged perceptron training There are a wide variety of parameter estimation methods for structured linear models, such as log-linear models (Lafferty et al., 2001) and max-margin models (Taskar et al., 2003). We chose the averaged structured perceptron (Freund and Schapire, 1999; Collins, 2002) as it combines highly competitive performance with fast training times, typically converging in 5–10 iterations. We train each parser for 10 iterations and select pa9For Czech, the PDT provides automatic tags; for English, we used MXPOST (Ratnaparkhi, 1996) to tag validation and test data, with 10-fold cross-validation on the training set. Note that the reliance on POS-tagged input can be relaxed slightly by treating POS tags as word senses; see Section 5.3 and McDonald (2006, Table 6.1). 10For Czech, we used the first character of the tag; for English, we used the first two characters, except FRF and FRF$. Beam Pass Orac Acc1 Acc2 Time1 Time2 0.0001 26.5 99.92 93.49 93.49 49.6m 73.5m 0.001 16.7 99.72 93.37 93.29 25.9m 24.2m 0.01 9.1 99.19 93.26 93.16 6.7m 7.9m Table 1: Effect of the marginal-probability beam on English parsing. For each beam va</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A Maximum Entropy Model for Part-Of-Speech Tagging. In Proceedings of the 1st EMNLP, pages 133–142. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th ACL,</booktitle>
<pages>577--585</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context citStr="Shen et al., 2008" endWordPosition="6135" position="37045" startWordPosition="6132">rther research involving our third-order parsing algorithms. One idea would be to consider extensions and modifications of our parsers, some of which have been suggested in Sections 5 and 7.4. A second area for future work lies in applications of dependency parsing. While we have evaluated our new algorithms on standard parsing benchmarks, there are a wide variety of tasks that may benefit from the extended context offered by our thirdorder factorizations; for example, the 4-gram substructures enabled by our approach may be useful for dependency-based language modeling in machine translation (Shen et al., 2008). Finally, in the hopes that others in the NLP community may find our parsers useful, we provide a free distribution of our implementation.2 Acknowledgments We would like to thank the anonymous reviewers for their helpful comments and suggestions. We also thank Regina Barzilay and Alexander Rush for their much-appreciated input during the writing process. The authors gratefully acknowledge the following sources of support: Terry Koo and Michael Collins were both funded by a DARPA subcontract under SRI (#27-001343), and Michael Collins was additionally supported by NTT (Agmt. dtd. 06/21/98). 9 </context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model. In Proceedings of the 46th ACL, pages 577– 585. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Noah A Smith</author>
</authors>
<title>Probabilistic Models of Nonprojective Dependency Trees.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>132--140</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context citStr="Smith and Smith, 2007" endWordPosition="5505" position="33188" startWordPosition="5502">axh' P(h', m |x). Table 1 provides information on the behavior of the pruning method. 7.4 Main results Table 2 lists the accuracy of Models 1 and 2 on the English and Czech test sets, together with some relevant results from related work.12 The models marked “†” are not directly comparable to our work as they depend on additional sources of information that our models are trained without— unlabeled data in the case of Koo et al. (2008) and 11For English, we generate marginals using a projective parser (Baker, 1979; Eisner, 2000); for Czech, we generate marginals using a non-projective parser (Smith and Smith, 2007; McDonald and Satta, 2007; Koo et al., 2007). Parameters for these models are obtained by running exponentiated gradient training for 10 iterations (Collins et al., 2008). 12Model 0 was not tested as its factorization is a strict subset of the factorization of Model 1. 8 Parser Eng Cze McDonald et al. (2005a,2005b) 90.9 84.4 McDonald and Pereira (2006) 91.5 85.2 Koo et al. (2008), standard 92.02 86.13 Model 1 93.04 87.38 Model 2 92.93 87.37 Koo et al. (2008), semi-sup† 93.16 87.13 Suzuki et al. (2009)† 93.79 88.05 Carreras et al. (2008)† 93.5 Table 2: UAS of Models 1 and 2 on test data, with </context>
</contexts>
<marker>Smith, Smith, 2007</marker>
<rawString>David A. Smith and Noah A. Smith. 2007. Probabilistic Models of Nonprojective Dependency Trees. In Proceedings of EMNLP-CoNLL, pages 132–140. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>An Empirical Study of Semi-supervised Structured Conditional Models for Dependency Parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>551--560</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context citStr="Suzuki et al. (2009" endWordPosition="296" position="1962" startWordPosition="293">and exactly. A crucial limitation of factored parsing algorithms is that the associated parts are typically quite small, losing much of the contextual information within the dependency tree. For the purposes of improving parsing performance, it is desirable to increase the size and variety of the parts used by the factorization.1 At the same time, the need for more expressive factorizations 1For examples of how performance varies with the degree of the parser’s factorization see, e.g., McDonald and Pereira (2006, Tables 1 and 2), Carreras (2007, Table 2), Koo et al. (2008, Tables 2 and 4), or Suzuki et al. (2009, Tables 3–6). must be balanced against any resulting increase in the computational cost of the parsing algorithm. Consequently, recent work in dependency parsing has been restricted to applications of secondorder parsers, the most powerful of which (Carreras, 2007) requires O(n4) time and O(n3) space, while being limited to second-order parts. In this paper, we present new third-order parsing algorithms that increase both the size and variety of the parts participating in the factorization, while simultaneously maintaining computational requirements of O(n4) time and O(n3) space. We evaluate </context>
<context citStr="Suzuki et al. (2009)" endWordPosition="5593" position="33695" startWordPosition="5590">Baker, 1979; Eisner, 2000); for Czech, we generate marginals using a non-projective parser (Smith and Smith, 2007; McDonald and Satta, 2007; Koo et al., 2007). Parameters for these models are obtained by running exponentiated gradient training for 10 iterations (Collins et al., 2008). 12Model 0 was not tested as its factorization is a strict subset of the factorization of Model 1. 8 Parser Eng Cze McDonald et al. (2005a,2005b) 90.9 84.4 McDonald and Pereira (2006) 91.5 85.2 Koo et al. (2008), standard 92.02 86.13 Model 1 93.04 87.38 Model 2 92.93 87.37 Koo et al. (2008), semi-sup† 93.16 87.13 Suzuki et al. (2009)† 93.79 88.05 Carreras et al. (2008)† 93.5 Table 2: UAS of Models 1 and 2 on test data, with relevant results from related work. Note that Koo et al. (2008) is listed with standard features and semi-supervised features. †: see main text. Suzuki et al. (2009) and phrase-structure annotations in the case of Carreras et al. (2008). All three of the “†” models are based on versions of the Carreras (2007) parser, so modifying these methods to work with our new third-order parsing algorithms would be an interesting topic for future research. For example, Models 1 and 2 obtain results comparable to t</context>
</contexts>
<marker>Suzuki, Isozaki, Carreras, Collins, 2009</marker>
<rawString>Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael Collins. 2009. An Empirical Study of Semi-supervised Structured Conditional Models for Dependency Parsing. In Proceedings of EMNLP, pages 551–560. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Carlos Guestrin</author>
<author>Daphne Koller</author>
</authors>
<title>Max margin markov networks.</title>
<date>2003</date>
<editor>In Sebastian Thrun, Lawrence K. Saul, and Bernhard Sch¨olkopf, editors, NIPS.</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context citStr="Taskar et al., 2003" endWordPosition="150" position="1029" startWordPosition="147">utilize both sibling-style and grandchild-style interactions. We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively. 1 Introduction Dependency grammar has proven to be a very useful syntactic formalism, due in no small part to the development of efficient parsing algorithms (Eisner, 2000; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007), which can be leveraged for a wide variety of learning methods, such as feature-rich discriminative models (Lafferty et al., 2001; Collins, 2002; Taskar et al., 2003). These parsing algorithms share an important characteristic: they factor dependency trees into sets of parts that have limited interactions. By exploiting the additional constraints arising from the factorization, maximizations or summations over the set of possible dependency trees can be performed efficiently and exactly. A crucial limitation of factored parsing algorithms is that the associated parts are typically quite small, losing much of the contextual information within the dependency tree. For the purposes of improving parsing performance, it is desirable to increase the size and var</context>
<context citStr="Taskar et al., 2003" endWordPosition="5048" position="30431" startWordPosition="5045">hat are only activated when the POS tag of s is a coordinating conjunction. Finally, we make two brief remarks regarding the use of POS tags. First, we assume that input sentences have been automatically tagged in a preprocessing step.9 Second, for any feature that depends on POS tags, we include two copies of the feature: one using normal POS tags and another using coarsened versions10 of the POS tags. 7.2 Averaged perceptron training There are a wide variety of parameter estimation methods for structured linear models, such as log-linear models (Lafferty et al., 2001) and max-margin models (Taskar et al., 2003). We chose the averaged structured perceptron (Freund and Schapire, 1999; Collins, 2002) as it combines highly competitive performance with fast training times, typically converging in 5–10 iterations. We train each parser for 10 iterations and select pa9For Czech, the PDT provides automatic tags; for English, we used MXPOST (Ratnaparkhi, 1996) to tag validation and test data, with 10-fold cross-validation on the training set. Note that the reliance on POS-tagged input can be relaxed slightly by treating POS tags as word senses; see Section 5.3 and McDonald (2006, Table 6.1). 10For Czech, we u</context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2003</marker>
<rawString>Ben Taskar, Carlos Guestrin, and Daphne Koller. 2003. Max margin markov networks. In Sebastian Thrun, Lawrence K. Saul, and Bernhard Sch¨olkopf, editors, NIPS. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical Dependency Analysis with Support Vector Machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th IWPT,</booktitle>
<pages>195--206</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context citStr="Yamada and Matsumoto, 2003" endWordPosition="4271" position="25717" startWordPosition="4267"> (g, h, m) is scored only when m is the outermost modifier of h in some direction. Note that Models 1 and 2 have the same complexity as Carreras (2007), but strictly greater expressiveness: for each sibling or grandchild part used in the Carreras (2007) factorization, Model 1 defines an enclosing grand-sibling, while Model 2 defines an enclosing tri-sibling or grand-sibling. The factored parsing approach we focus on is sometimes referred to as “graph-based” parsing; a popular alternative is “transition-based” parsing, in which trees are constructed by making a series of incremental decisions (Yamada and Matsumoto, 2003; Attardi, 2006; Nivre et al., 2006; McDonald and Nivre, 2007). Transition-based parsers do not impose factorizations, so they can define arbitrary features on the tree as it is being built. As a result, however, they rely on greedy or approximate search algorithms to solve Eq. 1. 7 Parsing experiments In order to evaluate the effectiveness of our parsers in practice, we apply them to the Penn WSJ Treebank (Marcus et al., 1993) and the Prague Dependency Treebank (Hajiˇc et al., 2001; Hajiˇc, 1998).6 We use standard training, validation, and test splits7 to facilitate comparisons. Accuracy is 6</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical Dependency Analysis with Support Vector Machines. In Proceedings of the 8th IWPT, pages 195– 206. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David H Younger</author>
</authors>
<title>Recognition and parsing of context-free languages in time n3.</title>
<date>1967</date>
<journal>Information and Control,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context citStr="Younger, 1967" endWordPosition="1286" position="8035" startWordPosition="1285">concatenation in each construction—m in Figure 2(a) or r in Figure 2(b)—is the split point, a free index that must be enumerated to find the optimal construction. In order to parse a sentence x, it suffices to find optimal constructions for all complete and incomplete spans defined on x. This can be = + h e h m m e 2 h m h s s m Figure 3: The dynamic-programming structures and derivations of the second-order sibling parser; sibling spans are depicted as boxes. For brevity, we elide the right-headed versions. accomplished by adapting standard chart-parsing techniques (Cocke and Schwartz, 1970; Younger, 1967; Kasami, 1965) to the recursive derivations defined in Figure 2. Since each derivation is defined by two fixed indices (the boundaries of the span) and a third free index (the split point), the parsing algorithm requires O(n3) time and O(n2) space (Eisner, 1996; McAllester, 1999). 3.2 Second-order sibling factorization As remarked by Eisner (1996) and McDonald and Pereira (2006), it is possible to rearrange the dynamic-programming structures to conform to an improved factorization that decomposes each tree into sibling parts—pairs of dependencies with a shared head. Specifically, a sibling pa</context>
</contexts>
<marker>Younger, 1967</marker>
<rawString>David H. Younger. 1967. Recognition and parsing of context-free languages in time n3. Information and Control, 10(2):189–208.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>